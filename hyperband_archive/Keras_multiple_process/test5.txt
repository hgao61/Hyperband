loading data...
/media/adamsmith/Storage2/DataSet/catsdogs/orig_test1
[0, 1, 2]
s=4
T is of size 81
T=[{'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.29307117696094115}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25913521176558485}, 'layer_3_size': 50, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44530675431740097}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36704401178535906}, 'layer_3_size': 24, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10937773525521029}, 'layer_4_size': 52, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.49007862288269577}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4692606759961784}, 'layer_1_size': 75, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3136693852626846}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 6, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 25, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24578273782844684}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10533293360726957}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44160739263256776}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 68, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15207529849940485}, 'layer_5_size': 63, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19558490676931226}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21186877882763422}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30112794810509574}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3309429567685175}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15937497910649576}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23999844446109977}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15804738600001286}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4323794958666226}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2926196489955891}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.47365031233221966}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 42, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 45, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3383217620683511}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3390640024687481}, 'layer_3_size': 24, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4707528622365418}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 27, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3071854271732375}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37342786095112623}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 76, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22603963012380812}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18748774590944872}, 'layer_2_size': 33, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4715085985201116}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.20698808085843723}, 'layer_4_size': 66, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19345445328349295}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4651933959290042}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42285526178483823}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12960415116227686}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1825073441161847}, 'layer_5_size': 80, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 47, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 26, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41000708099071104}, 'layer_4_size': 79, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18870668435802915}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.37307324267253505}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1748877640403025}, 'layer_5_size': 79, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13463193799911438}, 'layer_2_size': 22, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2175209473469314}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3797722684704009}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1540494386976221}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.316657676909546}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21090723097235556}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 89, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 76, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.39218689546565366}, 'layer_3_size': 22, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 6, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 59, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4409945660614265}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 78, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40849932833172253}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 53, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 50, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21209045336877344}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41928288588536755}, 'layer_3_size': 26, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21163869816357306}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22287552026058943}, 'layer_4_size': 51, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 18, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34888354859047094}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3161346222267394}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 9, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18742034528455595}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.151091328910066}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 7, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2591666001901206}, 'layer_3_size': 31, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1166779665150942}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2678508395581415}, 'layer_2_size': 96, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3251666720295153}, 'layer_3_size': 36, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 49, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45853271917242544}, 'layer_1_size': 2, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19574399120495656}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 88, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42338333595907873}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 9, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.33288877906250336}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45449295077327834}, 'layer_1_size': 51, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 24, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21802797553620104}, 'layer_4_size': 91, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 39, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.33740022211157467}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.29133968717940084}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 71, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14699657394160967}, 'layer_5_size': 88, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.33192373106196005}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21290049306740655}, 'layer_4_size': 3, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28099993686752606}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34974466255496983}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 99, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31272553346498344}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14776750231048486}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.43106598344991776}, 'layer_5_size': 26, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1824974374612169}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3321998893743574}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 6, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 9, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1843222787075455}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 98, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2110462536193236}, 'layer_2_size': 26, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22013542728313276}, 'layer_3_size': 91, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46455360283287184}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.35005484230469486}, 'layer_2_size': 56, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1103615171101521}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 89, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 51, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.43608435128038303}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.27094407825541}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3970048171685735}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 50, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34629468526215}, 'layer_2_size': 18, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30245352346209964}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 35, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27116531123903675}, 'layer_2_size': 63, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.37972466777583236}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18421503775253176}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4770391816485111}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.28431452898181575}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 93, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 58, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15744312117713355}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38983627727809533}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45842520572362544}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3874475255086739}, 'layer_1_size': 80, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 18, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 71, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2679645245210738}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3661054667655005}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 3, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 66, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11788957273039712}, 'layer_3_size': 25, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2211884333351667}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13295701680239463}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4868076109733872}, 'layer_2_size': 76, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.49502287953085233}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22225089583039181}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42559233622805637}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.34859741654439647}, 'layer_3_size': 68, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1345103654255286}, 'layer_4_size': 53, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 31, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 77, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34901075954835103}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14204003070746826}, 'layer_3_size': 97, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4272710545995406}, 'layer_5_size': 30, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 58, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21499127237600707}, 'layer_3_size': 22, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 99, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 42, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24545880961748656}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4200202530984959}, 'layer_1_size': 83, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 72, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3588346811715748}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4239733126315466}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41363825928937503}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13409385254743744}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34968000437951974}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15299083490668175}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 11, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42757086235637687}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3655328568242938}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]
newly formed T structure is:[[0, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.29307117696094115}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25913521176558485}, 'layer_3_size': 50, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44530675431740097}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [1, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [2, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36704401178535906}, 'layer_3_size': 24, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10937773525521029}, 'layer_4_size': 52, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.49007862288269577}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [3, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [4, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4692606759961784}, 'layer_1_size': 75, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3136693852626846}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 6, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 25, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}], [5, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24578273782844684}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10533293360726957}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44160739263256776}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [6, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 68, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15207529849940485}, 'layer_5_size': 63, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [7, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19558490676931226}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21186877882763422}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30112794810509574}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3309429567685175}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15937497910649576}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [8, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23999844446109977}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15804738600001286}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4323794958666226}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [9, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2926196489955891}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.47365031233221966}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 42, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 45, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [10, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3383217620683511}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [11, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3390640024687481}, 'layer_3_size': 24, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [12, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4707528622365418}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [13, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 27, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3071854271732375}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [14, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37342786095112623}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 76, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22603963012380812}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [15, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18748774590944872}, 'layer_2_size': 33, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4715085985201116}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [16, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.20698808085843723}, 'layer_4_size': 66, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [17, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19345445328349295}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4651933959290042}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42285526178483823}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [18, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12960415116227686}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1825073441161847}, 'layer_5_size': 80, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [19, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 47, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 26, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41000708099071104}, 'layer_4_size': 79, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [20, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18870668435802915}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.37307324267253505}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1748877640403025}, 'layer_5_size': 79, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [21, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13463193799911438}, 'layer_2_size': 22, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2175209473469314}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [22, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3797722684704009}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1540494386976221}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.316657676909546}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [23, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21090723097235556}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 89, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [24, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 76, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.39218689546565366}, 'layer_3_size': 22, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 6, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [25, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 59, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [26, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4409945660614265}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 78, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [27, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40849932833172253}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [28, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 53, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 50, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21209045336877344}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [29, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [30, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41928288588536755}, 'layer_3_size': 26, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [31, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21163869816357306}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22287552026058943}, 'layer_4_size': 51, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 18, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [32, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34888354859047094}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3161346222267394}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [33, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [34, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 9, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18742034528455595}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.151091328910066}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}], [35, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 7, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2591666001901206}, 'layer_3_size': 31, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1166779665150942}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [36, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2678508395581415}, 'layer_2_size': 96, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3251666720295153}, 'layer_3_size': 36, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 49, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [37, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45853271917242544}, 'layer_1_size': 2, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19574399120495656}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}], [38, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 88, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42338333595907873}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [39, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 9, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.33288877906250336}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [40, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [41, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45449295077327834}, 'layer_1_size': 51, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 24, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21802797553620104}, 'layer_4_size': 91, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [42, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 39, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}], [43, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.33740022211157467}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [44, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.29133968717940084}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 71, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14699657394160967}, 'layer_5_size': 88, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [45, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.33192373106196005}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21290049306740655}, 'layer_4_size': 3, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [46, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28099993686752606}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34974466255496983}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 99, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31272553346498344}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [47, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14776750231048486}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.43106598344991776}, 'layer_5_size': 26, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [48, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1824974374612169}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [49, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3321998893743574}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 6, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [50, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 9, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1843222787075455}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [51, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 98, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2110462536193236}, 'layer_2_size': 26, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [52, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22013542728313276}, 'layer_3_size': 91, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [53, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46455360283287184}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.35005484230469486}, 'layer_2_size': 56, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1103615171101521}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [54, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 89, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 51, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.43608435128038303}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [55, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.27094407825541}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [56, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3970048171685735}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 50, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [57, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34629468526215}, 'layer_2_size': 18, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30245352346209964}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 35, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [58, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27116531123903675}, 'layer_2_size': 63, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.37972466777583236}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [59, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18421503775253176}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [60, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4770391816485111}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.28431452898181575}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 93, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [61, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 58, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [62, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15744312117713355}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38983627727809533}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45842520572362544}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [63, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3874475255086739}, 'layer_1_size': 80, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 18, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 71, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [64, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2679645245210738}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [65, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3661054667655005}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 3, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [66, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 66, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11788957273039712}, 'layer_3_size': 25, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [67, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2211884333351667}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [68, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13295701680239463}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [69, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4868076109733872}, 'layer_2_size': 76, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.49502287953085233}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22225089583039181}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [70, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42559233622805637}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.34859741654439647}, 'layer_3_size': 68, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1345103654255286}, 'layer_4_size': 53, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [71, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 31, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 77, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [72, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34901075954835103}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14204003070746826}, 'layer_3_size': 97, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4272710545995406}, 'layer_5_size': 30, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [73, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 58, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21499127237600707}, 'layer_3_size': 22, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 99, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [74, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 42, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [75, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24545880961748656}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [76, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4200202530984959}, 'layer_1_size': 83, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 72, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3588346811715748}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [77, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4239733126315466}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41363825928937503}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13409385254743744}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [78, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34968000437951974}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15299083490668175}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 11, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [79, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42757086235637687}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [80, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3655328568242938}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]] 

*** 81 configurations x 1.0 iterations each

1 | Thu Sep 27 18:37:51 2018 | lowest loss so far: inf (run -1)

{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  27 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e9135a8d0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 16s - loss: 0.7055
6530/6530 [==============================] - 1s 112us/step - loss: 0.6041 - val_loss: 0.5322
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  53 | activation: tanh    | extras: dropout - rate: 29.3% 
layer 2 | size:  20 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:55 - loss: 0.4826
 496/6530 [=>............................] - ETA: 9s - loss: 0.3950  
1024/6530 [===>..........................] - ETA: 4s - loss: 0.3192
1568/6530 [======>.......................] - ETA: 2s - loss: 0.2864
2144/6530 [========>.....................] - ETA: 1s - loss: 0.2672
2704/6530 [===========>..................] - ETA: 1s - loss: 0.2543
# training | RMSE: 0.5866, MAE: 0.5257
worker 2  xfile  [2, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36704401178535906}, 'layer_3_size': 24, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10937773525521029}, 'layer_4_size': 52, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.49007862288269577}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.5866068083216661, 'rmse': 0.5866068083216661, 'mae': 0.5256744738346993, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  88 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 50s - loss: 0.5244
3232/6530 [=============>................] - ETA: 1s - loss: 0.2453
 608/6530 [=>............................] - ETA: 1s - loss: 0.3363 
3792/6530 [================>.............] - ETA: 0s - loss: 0.2368
1232/6530 [====>.........................] - ETA: 0s - loss: 0.1890
4384/6530 [===================>..........] - ETA: 0s - loss: 0.2289
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1407
4960/6530 [=====================>........] - ETA: 0s - loss: 0.2242
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1155
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2192
3056/6530 [=============>................] - ETA: 0s - loss: 0.1002
6128/6530 [===========================>..] - ETA: 0s - loss: 0.2158
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0906
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0847
6530/6530 [==============================] - 1s 210us/step - loss: 0.2131 - val_loss: 0.1620

4800/6530 [=====================>........] - ETA: 0s - loss: 0.0798
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0759
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0724
6530/6530 [==============================] - 1s 108us/step - loss: 0.0697 - val_loss: 0.0451
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  55 | activation: tanh    | extras: batchnorm 
layer 2 | size:  58 | activation: relu    | extras: None 
layer 3 | size:  85 | activation: tanh    | extras: batchnorm 
layer 4 | size:  59 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 52s - loss: 1.2345
1920/6530 [=======>......................] - ETA: 2s - loss: 0.7480 
3840/6530 [================>.............] - ETA: 0s - loss: 0.5744
5504/6530 [========================>.....] - ETA: 0s - loss: 0.4886
6530/6530 [==============================] - 1s 198us/step - loss: 0.4535 - val_loss: 0.2575

# training | RMSE: 0.2023, MAE: 0.1612
worker 0  xfile  [0, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.29307117696094115}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25913521176558485}, 'layer_3_size': 50, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44530675431740097}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.2023062674672241, 'rmse': 0.2023062674672241, 'mae': 0.16118185428838472, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  75 | activation: sigmoid | extras: dropout - rate: 46.9% 
layer 2 | size:  32 | activation: tanh    | extras: dropout - rate: 31.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e9135aac8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 20s - loss: 0.5983
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0927 
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0810
6530/6530 [==============================] - 0s 56us/step - loss: 0.0781 - val_loss: 0.0681

# training | RMSE: 0.2617, MAE: 0.2135
worker 0  xfile  [4, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4692606759961784}, 'layer_1_size': 75, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3136693852626846}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 6, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 25, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2616774253071672, 'rmse': 0.2616774253071672, 'mae': 0.21347698627409573, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  31 | activation: relu    | extras: dropout - rate: 19.6% 
layer 2 | size:  36 | activation: sigmoid | extras: dropout - rate: 21.2% 
layer 3 | size:  86 | activation: tanh    | extras: dropout - rate: 30.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e9019cba8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 4s - loss: 5.0658
5376/6530 [=======================>......] - ETA: 0s - loss: 0.4816
6530/6530 [==============================] - 0s 42us/step - loss: 0.4279 - val_loss: 0.0623

# training | RMSE: 0.2093, MAE: 0.1693
worker 2  xfile  [3, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20933312541754298, 'rmse': 0.20933312541754298, 'mae': 0.1693393257536813, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  45 | activation: relu    | extras: dropout - rate: 24.6% 
layer 2 | size:  71 | activation: tanh    | extras: None 
layer 3 | size:  46 | activation: relu    | extras: batchnorm 
layer 4 | size:  55 | activation: tanh    | extras: dropout - rate: 10.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e900fa400>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 20s - loss: 1.5439
1792/6530 [=======>......................] - ETA: 1s - loss: 0.9050 
3328/6530 [==============>...............] - ETA: 0s - loss: 0.6732
5120/6530 [======================>.......] - ETA: 0s - loss: 0.5286
6530/6530 [==============================] - 1s 101us/step - loss: 0.4591 - val_loss: 0.1289

# training | RMSE: 0.2448, MAE: 0.1984
worker 0  xfile  [7, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19558490676931226}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21186877882763422}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30112794810509574}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3309429567685175}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15937497910649576}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.24476600889565409, 'rmse': 0.24476600889565409, 'mae': 0.19844300687170194, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  56 | activation: relu    | extras: dropout - rate: 24.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e907ee128>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 14s - loss: 0.4040
2624/6530 [===========>..................] - ETA: 0s - loss: 0.3439 
5184/6530 [======================>.......] - ETA: 0s - loss: 0.3088
6530/6530 [==============================] - 0s 45us/step - loss: 0.2957 - val_loss: 0.2350

# training | RMSE: 0.3006, MAE: 0.2371
worker 1  xfile  [1, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.3006153438636222, 'rmse': 0.3006153438636222, 'mae': 0.23708659537995352, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  48 | activation: tanh    | extras: batchnorm 
layer 2 | size:  68 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  45 | activation: tanh    | extras: None 
layer 4 | size:  10 | activation: tanh    | extras: batchnorm 
layer 5 | size:  63 | activation: tanh    | extras: dropout - rate: 15.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e9135aa90>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 19s - loss: 1.0061
2560/6530 [==========>...................] - ETA: 1s - loss: 0.5614 
4864/6530 [=====================>........] - ETA: 0s - loss: 0.4311
6530/6530 [==============================] - 1s 151us/step - loss: 0.3647 - val_loss: 0.1135

# training | RMSE: 0.3648, MAE: 0.2892
worker 2  xfile  [5, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24578273782844684}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10533293360726957}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44160739263256776}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.3647543209762869, 'rmse': 0.3647543209762869, 'mae': 0.2892138873300356, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  63 | activation: tanh    | extras: batchnorm 
layer 2 | size:  24 | activation: relu    | extras: dropout - rate: 29.3% 
layer 3 | size:  73 | activation: tanh    | extras: dropout - rate: 47.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e64783550>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:08 - loss: 0.6695
 240/6530 [>.............................] - ETA: 13s - loss: 0.4683 
 480/6530 [=>............................] - ETA: 7s - loss: 0.3860 
# training | RMSE: 0.2949, MAE: 0.2368
worker 0  xfile  [8, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23999844446109977}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15804738600001286}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4323794958666226}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2949374563580409, 'rmse': 0.2949374563580409, 'mae': 0.2367534368548339, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  46 | activation: relu    | extras: dropout - rate: 33.8% 
layer 2 | size:  79 | activation: sigmoid | extras: None 
layer 3 | size:  69 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  52 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e6b6aed30>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:45 - loss: 0.5750
 752/6530 [==>...........................] - ETA: 4s - loss: 0.3397
 336/6530 [>.............................] - ETA: 8s - loss: 0.3928  
1040/6530 [===>..........................] - ETA: 3s - loss: 0.3063
 656/6530 [==>...........................] - ETA: 4s - loss: 0.2416
1312/6530 [=====>........................] - ETA: 2s - loss: 0.2845
 960/6530 [===>..........................] - ETA: 3s - loss: 0.1842
1552/6530 [======>.......................] - ETA: 2s - loss: 0.2705
1296/6530 [====>.........................] - ETA: 2s - loss: 0.1503
1840/6530 [=======>......................] - ETA: 2s - loss: 0.2603
1632/6530 [======>.......................] - ETA: 1s - loss: 0.1280
2144/6530 [========>.....................] - ETA: 1s - loss: 0.2513
1952/6530 [=======>......................] - ETA: 1s - loss: 0.1154
2448/6530 [==========>...................] - ETA: 1s - loss: 0.2444
2288/6530 [=========>....................] - ETA: 1s - loss: 0.1053
2752/6530 [===========>..................] - ETA: 1s - loss: 0.2359
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0999
3056/6530 [=============>................] - ETA: 1s - loss: 0.2299
2944/6530 [============>.................] - ETA: 1s - loss: 0.0945
3328/6530 [==============>...............] - ETA: 1s - loss: 0.2260
3264/6530 [=============>................] - ETA: 0s - loss: 0.0902
3600/6530 [===============>..............] - ETA: 0s - loss: 0.2224
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0869
3888/6530 [================>.............] - ETA: 0s - loss: 0.2193
3936/6530 [=================>............] - ETA: 0s - loss: 0.0837
4144/6530 [==================>...........] - ETA: 0s - loss: 0.2166
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0809
4448/6530 [===================>..........] - ETA: 0s - loss: 0.2143
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0788
4752/6530 [====================>.........] - ETA: 0s - loss: 0.2125
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0764
5040/6530 [======================>.......] - ETA: 0s - loss: 0.2098
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0747
5344/6530 [=======================>......] - ETA: 0s - loss: 0.2076
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0732
5648/6530 [========================>.....] - ETA: 0s - loss: 0.2048
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0719
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2030
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0708
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2010
6432/6530 [============================>.] - ETA: 0s - loss: 0.0695
# training | RMSE: 0.3361, MAE: 0.2893
worker 1  xfile  [6, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 68, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15207529849940485}, 'layer_5_size': 63, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.33608962659485325, 'rmse': 0.33608962659485325, 'mae': 0.2893435374974713, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  99 | activation: tanh    | extras: batchnorm 
layer 2 | size:  65 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  24 | activation: tanh    | extras: dropout - rate: 33.9% 
layer 4 | size:  67 | activation: tanh    | extras: None 
layer 5 | size:   7 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e8074f550>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 15s - loss: 0.7203
4096/6530 [=================>............] - ETA: 0s - loss: 0.6640 
6530/6530 [==============================] - 2s 262us/step - loss: 0.1999 - val_loss: 0.1828

6530/6530 [==============================] - 2s 234us/step - loss: 0.0691 - val_loss: 0.1031

6530/6530 [==============================] - 1s 123us/step - loss: 0.5974 - val_loss: 0.3026

# training | RMSE: 0.2187, MAE: 0.1785
worker 2  xfile  [9, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2926196489955891}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.47365031233221966}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 42, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 45, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2186938894246973, 'rmse': 0.2186938894246973, 'mae': 0.17853284871638117, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  39 | activation: sigmoid | extras: None 
layer 2 | size:  11 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e66ebd8d0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 22s - loss: 0.3408
1728/6530 [======>.......................] - ETA: 0s - loss: 0.2377 
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2192
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2091
6530/6530 [==============================] - 0s 66us/step - loss: 0.2049 - val_loss: 0.1805

# training | RMSE: 0.3661, MAE: 0.2984
worker 1  xfile  [11, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3390640024687481}, 'layer_3_size': 24, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.3660863046703895, 'rmse': 0.3660863046703895, 'mae': 0.29842138603932367, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  30 | activation: relu    | extras: batchnorm 
layer 2 | size:  10 | activation: relu    | extras: None 
layer 3 | size:  82 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b501ab710>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 11s - loss: 0.7120
3072/6530 [=============>................] - ETA: 0s - loss: 0.6535 
6400/6530 [============================>.] - ETA: 0s - loss: 0.4452
6530/6530 [==============================] - 1s 98us/step - loss: 0.4398 - val_loss: 0.1838

# training | RMSE: 0.2235, MAE: 0.1819
worker 2  xfile  [13, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 27, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3071854271732375}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.22352870224434201, 'rmse': 0.22352870224434201, 'mae': 0.18189225393296407, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  52 | activation: relu    | extras: None 
layer 2 | size:  33 | activation: tanh    | extras: dropout - rate: 18.7% 
layer 3 | size:   4 | activation: sigmoid | extras: None 
layer 4 | size:   2 | activation: tanh    | extras: dropout - rate: 47.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e5c19c630>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 57s - loss: 1.7851
# training | RMSE: 0.3149, MAE: 0.2721
worker 0  xfile  [10, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3383217620683511}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.31488254685646394, 'rmse': 0.31488254685646394, 'mae': 0.27209660148252895, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  78 | activation: relu    | extras: dropout - rate: 37.3% 
layer 2 | size:  76 | activation: relu    | extras: batchnorm 
layer 3 | size:   5 | activation: tanh    | extras: dropout - rate: 22.6% 
layer 4 | size:  96 | activation: sigmoid | extras: None 
layer 5 | size:  53 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e6b6aeb38>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 27s - loss: 4.3374
 832/6530 [==>...........................] - ETA: 2s - loss: 1.3659 
1920/6530 [=======>......................] - ETA: 1s - loss: 1.1082 
1696/6530 [======>.......................] - ETA: 1s - loss: 1.0482
3712/6530 [================>.............] - ETA: 0s - loss: 0.6857
2592/6530 [==========>...................] - ETA: 0s - loss: 0.8236
5632/6530 [========================>.....] - ETA: 0s - loss: 0.4789
3424/6530 [==============>...............] - ETA: 0s - loss: 0.6858
4352/6530 [==================>...........] - ETA: 0s - loss: 0.5851
6530/6530 [==============================] - 1s 122us/step - loss: 0.4204 - val_loss: 0.0477

5248/6530 [=======================>......] - ETA: 0s - loss: 0.5204
6048/6530 [==========================>...] - ETA: 0s - loss: 0.4766
6530/6530 [==============================] - 1s 113us/step - loss: 0.4546 - val_loss: 0.1751

# training | RMSE: 0.2191, MAE: 0.1779
worker 0  xfile  [14, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37342786095112623}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 76, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22603963012380812}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.21913086134892013, 'rmse': 0.21913086134892013, 'mae': 0.17791341755375764, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  19 | activation: tanh    | extras: dropout - rate: 19.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e70067f60>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 5s - loss: 0.5842
# training | RMSE: 0.2269, MAE: 0.1779
worker 1  xfile  [12, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4707528622365418}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.22689745609141695, 'rmse': 0.22689745609141695, 'mae': 0.17787573457486097, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  81 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  10 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b501ab5f8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 21s - loss: 0.7684
2816/6530 [===========>..................] - ETA: 0s - loss: 0.5093 
6530/6530 [==============================] - 0s 47us/step - loss: 0.4919 - val_loss: 0.2604

5504/6530 [========================>.....] - ETA: 0s - loss: 0.3596
# training | RMSE: 0.2202, MAE: 0.1767
worker 2  xfile  [15, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18748774590944872}, 'layer_2_size': 33, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4715085985201116}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2202285607838009, 'rmse': 0.2202285607838009, 'mae': 0.17674723577898563, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  79 | activation: tanh    | extras: dropout - rate: 13.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e5c19c4a8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 5s - loss: 1.7382
6530/6530 [==============================] - 1s 98us/step - loss: 0.3294 - val_loss: 0.1820

6530/6530 [==============================] - 0s 42us/step - loss: 0.3971 - val_loss: 0.1001

# training | RMSE: 0.5095, MAE: 0.4630
worker 0  xfile  [17, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19345445328349295}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4651933959290042}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42285526178483823}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.5094744590336984, 'rmse': 0.5094744590336984, 'mae': 0.463013062137373, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  47 | activation: tanh    | extras: batchnorm 
layer 2 | size:  26 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b542b3208>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 38s - loss: 0.7457
1024/6530 [===>..........................] - ETA: 2s - loss: 0.6915 
2176/6530 [========>.....................] - ETA: 0s - loss: 0.6009
3584/6530 [===============>..............] - ETA: 0s - loss: 0.5183
4928/6530 [=====================>........] - ETA: 0s - loss: 0.4633
6528/6530 [============================>.] - ETA: 0s - loss: 0.4152
# training | RMSE: 0.2305, MAE: 0.1811
worker 1  xfile  [16, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.20698808085843723}, 'layer_4_size': 66, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.23048528303997787, 'rmse': 0.23048528303997787, 'mae': 0.1810618707052838, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  74 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  22 | activation: tanh    | extras: dropout - rate: 13.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e4c6c37b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 23s - loss: 1.7232
6530/6530 [==============================] - 1s 107us/step - loss: 0.4154 - val_loss: 0.2628

3072/6530 [=============>................] - ETA: 0s - loss: 0.5949 
6144/6530 [===========================>..] - ETA: 0s - loss: 0.3905
6530/6530 [==============================] - 1s 99us/step - loss: 0.3747 - val_loss: 0.1079

# training | RMSE: 0.3123, MAE: 0.2556
worker 2  xfile  [18, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12960415116227686}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1825073441161847}, 'layer_5_size': 80, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.31231850704626396, 'rmse': 0.31231850704626396, 'mae': 0.2555969703539291, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  62 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  74 | activation: tanh    | extras: batchnorm 
layer 3 | size:  38 | activation: sigmoid | extras: dropout - rate: 18.9% 
layer 4 | size:  61 | activation: sigmoid | extras: dropout - rate: 37.3% 
layer 5 | size:  79 | activation: tanh    | extras: dropout - rate: 17.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e48606fd0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 18s - loss: 0.4939
3328/6530 [==============>...............] - ETA: 0s - loss: 0.3747 
6144/6530 [===========================>..] - ETA: 0s - loss: 0.3328
# training | RMSE: 0.3238, MAE: 0.2596
worker 1  xfile  [21, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13463193799911438}, 'layer_2_size': 22, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2175209473469314}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.3238206051547236, 'rmse': 0.3238206051547236, 'mae': 0.2596314545674705, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  67 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b31adb7f0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 8s - loss: 0.7142
6530/6530 [==============================] - 1s 144us/step - loss: 0.3276 - val_loss: 0.1674

6530/6530 [==============================] - 0s 70us/step - loss: 0.5307 - val_loss: 0.3990

# training | RMSE: 0.3297, MAE: 0.2628
worker 0  xfile  [19, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 47, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 26, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41000708099071104}, 'layer_4_size': 79, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.32972860473203736, 'rmse': 0.32972860473203736, 'mae': 0.2628039751292015, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  93 | activation: relu    | extras: dropout - rate: 38.0% 
layer 2 | size:  20 | activation: sigmoid | extras: dropout - rate: 15.4% 
layer 3 | size:  16 | activation: relu    | extras: batchnorm 
layer 4 | size:  95 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b542b2f98>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:02 - loss: 2.3062
 544/6530 [=>............................] - ETA: 7s - loss: 1.3804  
# training | RMSE: 0.2093, MAE: 0.1668
worker 2  xfile  [20, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18870668435802915}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.37307324267253505}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1748877640403025}, 'layer_5_size': 79, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.20926089216928737, 'rmse': 0.20926089216928737, 'mae': 0.16684361633852657, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  40 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e484e14a8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 6s - loss: 0.6483
1088/6530 [===>..........................] - ETA: 3s - loss: 0.9827
6530/6530 [==============================] - 0s 54us/step - loss: 0.3252 - val_loss: 0.2027

1632/6530 [======>.......................] - ETA: 2s - loss: 0.7737
2144/6530 [========>.....................] - ETA: 1s - loss: 0.6477
2656/6530 [===========>..................] - ETA: 1s - loss: 0.5609
# training | RMSE: 0.4694, MAE: 0.3909
worker 1  xfile  [23, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21090723097235556}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 89, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.4693932624364615, 'rmse': 0.4693932624364615, 'mae': 0.39087631238220466, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  68 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e9010ac50>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 39s - loss: 0.6120
3168/6530 [=============>................] - ETA: 0s - loss: 0.4967
2240/6530 [=========>....................] - ETA: 0s - loss: 0.2736 
3712/6530 [================>.............] - ETA: 0s - loss: 0.4459
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1736
4288/6530 [==================>...........] - ETA: 0s - loss: 0.4004
4832/6530 [=====================>........] - ETA: 0s - loss: 0.3676
6530/6530 [==============================] - 1s 90us/step - loss: 0.1394 - val_loss: 0.0520

5440/6530 [=======================>......] - ETA: 0s - loss: 0.3394
5984/6530 [==========================>...] - ETA: 0s - loss: 0.3185
6496/6530 [============================>.] - ETA: 0s - loss: 0.3021
6530/6530 [==============================] - 1s 201us/step - loss: 0.3010 - val_loss: 0.0704

# training | RMSE: 0.2488, MAE: 0.2025
worker 2  xfile  [24, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 76, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.39218689546565366}, 'layer_3_size': 22, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 6, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.24877130829711364, 'rmse': 0.24877130829711364, 'mae': 0.2024966527390149, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  82 | activation: relu    | extras: dropout - rate: 44.1% 
layer 2 | size:  31 | activation: sigmoid | extras: None 
layer 3 | size:  78 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4db53748>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 9s - loss: 0.1999
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0822
6530/6530 [==============================] - 1s 79us/step - loss: 0.0800 - val_loss: 0.0671

# training | RMSE: 0.2605, MAE: 0.2131
worker 2  xfile  [26, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4409945660614265}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 78, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.2605095322245654, 'rmse': 0.2605095322245654, 'mae': 0.2131374067953399, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  54 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4d9e9a20>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 16s - loss: 0.7308
3328/6530 [==============>...............] - ETA: 0s - loss: 0.5692 
6530/6530 [==============================] - 1s 79us/step - loss: 0.4107 - val_loss: 0.1702

# training | RMSE: 0.2278, MAE: 0.1840
worker 1  xfile  [25, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 59, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.22777626867677023, 'rmse': 0.22777626867677023, 'mae': 0.18401154989852567, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  39 | activation: relu    | extras: None 
layer 2 | size:  43 | activation: tanh    | extras: None 
layer 3 | size:  22 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  27 | activation: tanh    | extras: batchnorm 
layer 5 | size:  13 | activation: sigmoid | extras: dropout - rate: 40.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b316bacf8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 22s - loss: 1.4616
3072/6530 [=============>................] - ETA: 1s - loss: 1.1907 
5888/6530 [==========================>...] - ETA: 0s - loss: 1.0446
6530/6530 [==============================] - 1s 173us/step - loss: 1.0149 - val_loss: 0.6351

# training | RMSE: 0.2706, MAE: 0.2195
worker 0  xfile  [22, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3797722684704009}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1540494386976221}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.316657676909546}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.27060762152807005, 'rmse': 0.27060762152807005, 'mae': 0.21948667884939554, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  53 | activation: relu    | extras: batchnorm 
layer 2 | size:  73 | activation: tanh    | extras: None 
layer 3 | size:  50 | activation: relu    | extras: batchnorm 
layer 4 | size:  67 | activation: tanh    | extras: batchnorm 
layer 5 | size:  76 | activation: tanh    | extras: dropout - rate: 21.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e68724eb8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 46s - loss: 1.0867
1280/6530 [====>.........................] - ETA: 4s - loss: 0.7521 
2560/6530 [==========>...................] - ETA: 1s - loss: 0.5974
3968/6530 [=================>............] - ETA: 0s - loss: 0.4867
5248/6530 [=======================>......] - ETA: 0s - loss: 0.4223
# training | RMSE: 0.2155, MAE: 0.1697
worker 2  xfile  [29, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.2154688253830196, 'rmse': 0.2154688253830196, 'mae': 0.16969697254880073, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  28 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  62 | activation: relu    | extras: None 
layer 3 | size:  26 | activation: sigmoid | extras: dropout - rate: 41.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4dc410f0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:18 - loss: 0.3377
6530/6530 [==============================] - 1s 195us/step - loss: 0.3779 - val_loss: 0.1821

 672/6530 [==>...........................] - ETA: 6s - loss: 0.1490  
1280/6530 [====>.........................] - ETA: 3s - loss: 0.1041
1888/6530 [=======>......................] - ETA: 2s - loss: 0.0840
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0742
3104/6530 [=============>................] - ETA: 1s - loss: 0.0671
# training | RMSE: 0.7973, MAE: 0.7560
worker 1  xfile  [27, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40849932833172253}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.7972823926694038, 'rmse': 0.7972823926694038, 'mae': 0.7560056051836649, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  57 | activation: tanh    | extras: dropout - rate: 21.2% 
layer 2 | size:  63 | activation: tanh    | extras: None 
layer 3 | size:   7 | activation: sigmoid | extras: None 
layer 4 | size:  51 | activation: sigmoid | extras: dropout - rate: 22.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b314ac4a8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:30 - loss: 4.4154
3744/6530 [================>.............] - ETA: 0s - loss: 0.0621
 432/6530 [>.............................] - ETA: 7s - loss: 1.8761  
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0588
 864/6530 [==>...........................] - ETA: 4s - loss: 1.0753
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0561
1280/6530 [====>.........................] - ETA: 2s - loss: 0.7500
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0537
1680/6530 [======>.......................] - ETA: 2s - loss: 0.5882
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0521
2096/6530 [========>.....................] - ETA: 1s - loss: 0.4851
2480/6530 [==========>...................] - ETA: 1s - loss: 0.4208
6530/6530 [==============================] - 1s 202us/step - loss: 0.0510 - val_loss: 0.0304

2864/6530 [============>.................] - ETA: 1s - loss: 0.3727
3280/6530 [==============>...............] - ETA: 0s - loss: 0.3328
3648/6530 [===============>..............] - ETA: 0s - loss: 0.3058
4048/6530 [=================>............] - ETA: 0s - loss: 0.2808
4464/6530 [===================>..........] - ETA: 0s - loss: 0.2599
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2423
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2282
5680/6530 [=========================>....] - ETA: 0s - loss: 0.2149
# training | RMSE: 0.2196, MAE: 0.1734
worker 0  xfile  [28, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 53, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 50, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21209045336877344}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.21957880019843726, 'rmse': 0.21957880019843726, 'mae': 0.1733934187608411, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  86 | activation: sigmoid | extras: dropout - rate: 34.9% 
layer 2 | size:  21 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4cefa828>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 23s - loss: 0.5567
6128/6530 [===========================>..] - ETA: 0s - loss: 0.2026
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2025 
6530/6530 [==============================] - 1s 218us/step - loss: 0.1932 - val_loss: 0.0508

6530/6530 [==============================] - 1s 97us/step - loss: 0.1435 - val_loss: 0.0686

# training | RMSE: 0.1682, MAE: 0.1332
worker 2  xfile  [30, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41928288588536755}, 'layer_3_size': 26, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.16823179208915304, 'rmse': 0.16823179208915304, 'mae': 0.1331612750124683, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  56 | activation: relu    | extras: batchnorm 
layer 2 | size:  66 | activation: relu    | extras: None 
layer 3 | size:  44 | activation: tanh    | extras: batchnorm 
layer 4 | size:  61 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4da43f98>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:24 - loss: 0.6872
 384/6530 [>.............................] - ETA: 12s - loss: 0.4586 
# training | RMSE: 0.2652, MAE: 0.2175
worker 0  xfile  [32, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34888354859047094}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3161346222267394}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2651548589598659, 'rmse': 0.2651548589598659, 'mae': 0.2174988111615423, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:   9 | activation: relu    | extras: batchnorm 
layer 2 | size:  62 | activation: relu    | extras: dropout - rate: 18.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4d037dd8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 27s - loss: 0.9458
 768/6530 [==>...........................] - ETA: 6s - loss: 0.3686 
2432/6530 [==========>...................] - ETA: 1s - loss: 0.3134 
1216/6530 [====>.........................] - ETA: 3s - loss: 0.3012
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2450
1728/6530 [======>.......................] - ETA: 2s - loss: 0.2580
2240/6530 [=========>....................] - ETA: 1s - loss: 0.2331
6530/6530 [==============================] - 1s 116us/step - loss: 0.2305 - val_loss: 0.2194

2752/6530 [===========>..................] - ETA: 1s - loss: 0.2158
3264/6530 [=============>................] - ETA: 1s - loss: 0.2028
3776/6530 [================>.............] - ETA: 0s - loss: 0.1945
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1868
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1817
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1761
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1718
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1683
# training | RMSE: 0.2246, MAE: 0.1861
worker 1  xfile  [31, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21163869816357306}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22287552026058943}, 'layer_4_size': 51, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 18, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.22457271710291757, 'rmse': 0.22457271710291757, 'mae': 0.1860811095854423, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  21 | activation: relu    | extras: None 
layer 2 | size:   7 | activation: tanh    | extras: batchnorm 
layer 3 | size:  31 | activation: tanh    | extras: dropout - rate: 25.9% 
layer 4 | size:  22 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b30c2f0b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:44 - loss: 0.7970
 544/6530 [=>............................] - ETA: 9s - loss: 0.6010  
6530/6530 [==============================] - 2s 231us/step - loss: 0.1663 - val_loss: 0.1167

1056/6530 [===>..........................] - ETA: 4s - loss: 0.4777
1568/6530 [======>.......................] - ETA: 3s - loss: 0.4003
2048/6530 [========>.....................] - ETA: 2s - loss: 0.3590
2496/6530 [==========>...................] - ETA: 1s - loss: 0.3297
2976/6530 [============>.................] - ETA: 1s - loss: 0.3040
3488/6530 [===============>..............] - ETA: 1s - loss: 0.2839
4032/6530 [=================>............] - ETA: 0s - loss: 0.2668
4576/6530 [====================>.........] - ETA: 0s - loss: 0.2543
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2442
5600/6530 [========================>.....] - ETA: 0s - loss: 0.2366
6080/6530 [==========================>...] - ETA: 0s - loss: 0.2300
6530/6530 [==============================] - 2s 243us/step - loss: 0.2251 - val_loss: 0.1543

# training | RMSE: 0.2728, MAE: 0.2201
worker 0  xfile  [34, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 9, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18742034528455595}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.151091328910066}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.27284905750281796, 'rmse': 0.27284905750281796, 'mae': 0.22008132133158417, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  86 | activation: tanh    | extras: None 
layer 2 | size:  96 | activation: sigmoid | extras: dropout - rate: 26.8% 
layer 3 | size:  36 | activation: tanh    | extras: dropout - rate: 32.5% 
layer 4 | size:  57 | activation: sigmoid | extras: None 
layer 5 | size:  49 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4c5ed588>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 16s - loss: 0.6247
4096/6530 [=================>............] - ETA: 0s - loss: 0.2886 
6530/6530 [==============================] - 1s 126us/step - loss: 0.2066 - val_loss: 0.0692

# training | RMSE: 0.1454, MAE: 0.1109
worker 2  xfile  [33, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.14540570531495545, 'rmse': 0.14540570531495545, 'mae': 0.11091950039518869, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:   2 | activation: sigmoid | extras: dropout - rate: 45.9% 
layer 2 | size:  12 | activation: relu    | extras: None 
layer 3 | size:  30 | activation: relu    | extras: None 
layer 4 | size:  86 | activation: relu    | extras: batchnorm 
layer 5 | size:  89 | activation: tanh    | extras: dropout - rate: 19.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4cfdeb70>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 37s - loss: 0.6088
1920/6530 [=======>......................] - ETA: 1s - loss: 0.1819 
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1305
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1087
6530/6530 [==============================] - 1s 161us/step - loss: 0.1023 - val_loss: 0.0746

# training | RMSE: 0.1965, MAE: 0.1512
worker 1  xfile  [35, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 7, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2591666001901206}, 'layer_3_size': 31, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1166779665150942}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.1964600042044821, 'rmse': 0.1964600042044821, 'mae': 0.1511929662359733, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  88 | activation: relu    | extras: batchnorm 
layer 2 | size:  55 | activation: tanh    | extras: None 
layer 3 | size:  13 | activation: tanh    | extras: None 
layer 4 | size:  13 | activation: tanh    | extras: dropout - rate: 42.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b308e2f98>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:04 - loss: 0.5658
 448/6530 [=>............................] - ETA: 13s - loss: 0.5800 
 864/6530 [==>...........................] - ETA: 6s - loss: 0.5475 
1280/6530 [====>.........................] - ETA: 4s - loss: 0.4926
1728/6530 [======>.......................] - ETA: 3s - loss: 0.4442
2144/6530 [========>.....................] - ETA: 2s - loss: 0.3989
2560/6530 [==========>...................] - ETA: 1s - loss: 0.3554
2944/6530 [============>.................] - ETA: 1s - loss: 0.3171
3360/6530 [==============>...............] - ETA: 1s - loss: 0.2837
3712/6530 [================>.............] - ETA: 1s - loss: 0.2614
4128/6530 [=================>............] - ETA: 0s - loss: 0.2390
4544/6530 [===================>..........] - ETA: 0s - loss: 0.2202
# training | RMSE: 0.2646, MAE: 0.2167
worker 0  xfile  [36, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2678508395581415}, 'layer_2_size': 96, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3251666720295153}, 'layer_3_size': 36, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 49, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2646105616702299, 'rmse': 0.2646105616702299, 'mae': 0.21665060525073934, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  61 | activation: tanh    | extras: batchnorm 
layer 2 | size:  47 | activation: tanh    | extras: batchnorm 
layer 3 | size:   9 | activation: relu    | extras: batchnorm 
layer 4 | size:  95 | activation: tanh    | extras: dropout - rate: 33.3% 
layer 5 | size:  29 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4c750da0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 49s - loss: 1.4193
5024/6530 [======================>.......] - ETA: 0s - loss: 0.2024
1664/6530 [======>.......................] - ETA: 3s - loss: 0.5099 
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1874
3072/6530 [=============>................] - ETA: 1s - loss: 0.3871
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1749
4608/6530 [====================>.........] - ETA: 0s - loss: 0.3216
6528/6530 [============================>.] - ETA: 0s - loss: 0.1627
6144/6530 [===========================>..] - ETA: 0s - loss: 0.2780
6530/6530 [==============================] - 2s 280us/step - loss: 0.1627 - val_loss: 0.0265

6530/6530 [==============================] - 1s 214us/step - loss: 0.2705 - val_loss: 0.1417

# training | RMSE: 0.2732, MAE: 0.2222
worker 2  xfile  [37, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45853271917242544}, 'layer_1_size': 2, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19574399120495656}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2732373697634362, 'rmse': 0.2732373697634362, 'mae': 0.2222363367281035, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  68 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  91 | activation: tanh    | extras: batchnorm 
layer 3 | size:  29 | activation: tanh    | extras: batchnorm 
layer 4 | size:  90 | activation: tanh    | extras: None 
layer 5 | size:  57 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4cc08358>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 32s - loss: 0.5680
2560/6530 [==========>...................] - ETA: 2s - loss: 0.1597 
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1064
# training | RMSE: 0.3770, MAE: 0.3006
worker 0  xfile  [39, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 9, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.33288877906250336}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.377045445515654, 'rmse': 0.377045445515654, 'mae': 0.30064798047876745, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  12 | activation: relu    | extras: None 
layer 2 | size:  39 | activation: relu    | extras: None 
layer 3 | size:  58 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b47b77cf8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 28s - loss: 4.6677
4352/6530 [==================>...........] - ETA: 0s - loss: 0.4971 
6530/6530 [==============================] - 2s 243us/step - loss: 0.0878 - val_loss: 0.0420

6530/6530 [==============================] - 1s 114us/step - loss: 0.3605 - val_loss: 0.0890

# training | RMSE: 0.1574, MAE: 0.1248
worker 1  xfile  [38, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 88, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42338333595907873}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.15737763943390126, 'rmse': 0.15737763943390126, 'mae': 0.12475876505257386, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  51 | activation: tanh    | extras: dropout - rate: 45.4% 
layer 2 | size:  24 | activation: tanh    | extras: batchnorm 
layer 3 | size:  38 | activation: relu    | extras: None 
layer 4 | size:  91 | activation: relu    | extras: dropout - rate: 21.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b302f3198>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 51s - loss: 0.7126
1536/6530 [======>.......................] - ETA: 3s - loss: 0.6895 
3200/6530 [=============>................] - ETA: 1s - loss: 0.6342
4736/6530 [====================>.........] - ETA: 0s - loss: 0.5354
6144/6530 [===========================>..] - ETA: 0s - loss: 0.4742
6530/6530 [==============================] - 1s 209us/step - loss: 0.4610 - val_loss: 0.2717

# training | RMSE: 0.2002, MAE: 0.1566
worker 2  xfile  [40, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20024231708635035, 'rmse': 0.20024231708635035, 'mae': 0.1565697286585894, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  83 | activation: tanh    | extras: None 
layer 2 | size:  37 | activation: relu    | extras: batchnorm 
layer 3 | size:  49 | activation: relu    | extras: None 
layer 4 | size:  50 | activation: tanh    | extras: dropout - rate: 33.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4c677ac8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 18s - loss: 0.5265
3840/6530 [================>.............] - ETA: 0s - loss: 0.2704 
6530/6530 [==============================] - 1s 149us/step - loss: 0.2406 - val_loss: 0.1865

# training | RMSE: 0.2952, MAE: 0.2395
worker 0  xfile  [42, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 39, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.29516163489680464, 'rmse': 0.29516163489680464, 'mae': 0.23945027063762256, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  40 | activation: relu    | extras: batchnorm 
layer 2 | size:   6 | activation: tanh    | extras: dropout - rate: 29.1% 
layer 3 | size:  35 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4c2dae80>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:28 - loss: 0.5090
1216/6530 [====>.........................] - ETA: 4s - loss: 0.1592  
2496/6530 [==========>...................] - ETA: 1s - loss: 0.1152
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0990
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0899
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0829
6530/6530 [==============================] - 1s 191us/step - loss: 0.0811 - val_loss: 0.0580

# training | RMSE: 0.3191, MAE: 0.2627
worker 1  xfile  [41, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45449295077327834}, 'layer_1_size': 51, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 24, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21802797553620104}, 'layer_4_size': 91, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.31910275129449417, 'rmse': 0.31910275129449417, 'mae': 0.26265769248140386, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  84 | activation: sigmoid | extras: dropout - rate: 33.2% 
layer 2 | size:  32 | activation: relu    | extras: batchnorm 
layer 3 | size:  57 | activation: relu    | extras: batchnorm 
layer 4 | size:   3 | activation: tanh    | extras: dropout - rate: 21.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b17800a58>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 48s - loss: 0.8184
1536/6530 [======>.......................] - ETA: 3s - loss: 0.5650 
3072/6530 [=============>................] - ETA: 1s - loss: 0.4673
# training | RMSE: 0.2284, MAE: 0.1793
worker 2  xfile  [43, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.33740022211157467}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.22837789286982524, 'rmse': 0.22837789286982524, 'mae': 0.1792708364031782, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  96 | activation: sigmoid | extras: dropout - rate: 28.1% 
layer 2 | size:  42 | activation: sigmoid | extras: dropout - rate: 35.0% 
layer 3 | size:  25 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  99 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4d14fa90>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:54 - loss: 0.6856
4480/6530 [===================>..........] - ETA: 0s - loss: 0.4144
 480/6530 [=>............................] - ETA: 11s - loss: 0.5196 
6144/6530 [===========================>..] - ETA: 0s - loss: 0.3740
 960/6530 [===>..........................] - ETA: 5s - loss: 0.3871 
1440/6530 [=====>........................] - ETA: 3s - loss: 0.3060
1952/6530 [=======>......................] - ETA: 2s - loss: 0.2528
6530/6530 [==============================] - 1s 202us/step - loss: 0.3678 - val_loss: 0.2138

2496/6530 [==========>...................] - ETA: 1s - loss: 0.2173
3072/6530 [=============>................] - ETA: 1s - loss: 0.1914
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1738
4128/6530 [=================>............] - ETA: 0s - loss: 0.1634
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1544
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1473
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1415
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1359
# training | RMSE: 0.2362, MAE: 0.1925
worker 0  xfile  [44, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.29133968717940084}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 71, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14699657394160967}, 'layer_5_size': 88, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2362093873860955, 'rmse': 0.2362093873860955, 'mae': 0.19245239548898094, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  18 | activation: tanh    | extras: dropout - rate: 14.8% 
layer 2 | size:  67 | activation: tanh    | extras: None 
layer 3 | size: 100 | activation: tanh    | extras: batchnorm 
layer 4 | size:  26 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b46d1be10>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:14 - loss: 1.0465
6530/6530 [==============================] - 2s 250us/step - loss: 0.1337 - val_loss: 0.0853

 576/6530 [=>............................] - ETA: 10s - loss: 0.6433 
1152/6530 [====>.........................] - ETA: 4s - loss: 0.4998 
1696/6530 [======>.......................] - ETA: 3s - loss: 0.4128
2144/6530 [========>.....................] - ETA: 2s - loss: 0.3581
2624/6530 [===========>..................] - ETA: 1s - loss: 0.3162
3104/6530 [=============>................] - ETA: 1s - loss: 0.2841
3648/6530 [===============>..............] - ETA: 1s - loss: 0.2565
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2368
4672/6530 [====================>.........] - ETA: 0s - loss: 0.2224
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2106
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1977
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1865
# training | RMSE: 0.2650, MAE: 0.2177
worker 1  xfile  [45, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.33192373106196005}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21290049306740655}, 'layer_4_size': 3, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.26495188246116635, 'rmse': 0.26495188246116635, 'mae': 0.21774199868299124, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  21 | activation: tanh    | extras: dropout - rate: 18.2% 
layer 2 | size:  49 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b171a3a20>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:24 - loss: 0.5930
6530/6530 [==============================] - 2s 264us/step - loss: 0.1813 - val_loss: 0.0644

 960/6530 [===>..........................] - ETA: 4s - loss: 0.5118  
1920/6530 [=======>......................] - ETA: 1s - loss: 0.4314
2752/6530 [===========>..................] - ETA: 1s - loss: 0.3980
3584/6530 [===============>..............] - ETA: 0s - loss: 0.3721
4448/6530 [===================>..........] - ETA: 0s - loss: 0.3511
5248/6530 [=======================>......] - ETA: 0s - loss: 0.3367
6112/6530 [===========================>..] - ETA: 0s - loss: 0.3247
6530/6530 [==============================] - 1s 185us/step - loss: 0.3184 - val_loss: 0.2048

# training | RMSE: 0.2967, MAE: 0.2437
worker 2  xfile  [46, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28099993686752606}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34974466255496983}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 99, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31272553346498344}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2967463232992383, 'rmse': 0.2967463232992383, 'mae': 0.24369980749965967, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  68 | activation: relu    | extras: dropout - rate: 33.2% 
layer 2 | size:  33 | activation: tanh    | extras: batchnorm 
layer 3 | size:  70 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4d0fddd8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 7:26 - loss: 0.5163
 208/6530 [..............................] - ETA: 34s - loss: 0.3999 
 400/6530 [>.............................] - ETA: 18s - loss: 0.2855
 592/6530 [=>............................] - ETA: 12s - loss: 0.2166
 816/6530 [==>...........................] - ETA: 9s - loss: 0.1738 
1056/6530 [===>..........................] - ETA: 7s - loss: 0.1457
1328/6530 [=====>........................] - ETA: 5s - loss: 0.1237
1584/6530 [======>.......................] - ETA: 4s - loss: 0.1100
# training | RMSE: 0.2433, MAE: 0.1942
worker 0  xfile  [47, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14776750231048486}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.43106598344991776}, 'layer_5_size': 26, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.24334785221441319, 'rmse': 0.24334785221441319, 'mae': 0.19415770970283275, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  89 | activation: sigmoid | extras: None 
layer 2 | size:   9 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4661b710>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 20s - loss: 0.8213
1904/6530 [=======>......................] - ETA: 3s - loss: 0.0989
5376/6530 [=======================>......] - ETA: 0s - loss: 0.6640 
2208/6530 [=========>....................] - ETA: 3s - loss: 0.0914
2512/6530 [==========>...................] - ETA: 2s - loss: 0.0854
6530/6530 [==============================] - 1s 152us/step - loss: 0.6339 - val_loss: 0.4829

2784/6530 [===========>..................] - ETA: 2s - loss: 0.0805
3136/6530 [=============>................] - ETA: 1s - loss: 0.0749
3456/6530 [==============>...............] - ETA: 1s - loss: 0.0719
3776/6530 [================>.............] - ETA: 1s - loss: 0.0688
4096/6530 [=================>............] - ETA: 1s - loss: 0.0665
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0640
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0622
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0604
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0590
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0576
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0564
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0554
6496/6530 [============================>.] - ETA: 0s - loss: 0.0542
6530/6530 [==============================] - 2s 376us/step - loss: 0.0541 - val_loss: 0.0274

# training | RMSE: 0.2437, MAE: 0.1951
worker 1  xfile  [48, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1824974374612169}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.24366937960500815, 'rmse': 0.24366937960500815, 'mae': 0.19512851367110826, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  98 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  26 | activation: relu    | extras: dropout - rate: 21.1% 
layer 3 | size:  14 | activation: relu    | extras: batchnorm 
layer 4 | size:   7 | activation: sigmoid | extras: None 
layer 5 | size:  92 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b16e86668>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 7:38 - loss: 0.7546
 208/6530 [..............................] - ETA: 35s - loss: 0.2382 
 416/6530 [>.............................] - ETA: 18s - loss: 0.2239
 640/6530 [=>............................] - ETA: 11s - loss: 0.2128
 848/6530 [==>...........................] - ETA: 8s - loss: 0.2041 
1088/6530 [===>..........................] - ETA: 6s - loss: 0.1978
1296/6530 [====>.........................] - ETA: 5s - loss: 0.1970
1488/6530 [=====>........................] - ETA: 5s - loss: 0.1914
1712/6530 [======>.......................] - ETA: 4s - loss: 0.1865
1920/6530 [=======>......................] - ETA: 3s - loss: 0.1832
2112/6530 [========>.....................] - ETA: 3s - loss: 0.1808
2352/6530 [=========>....................] - ETA: 3s - loss: 0.1790
2576/6530 [==========>...................] - ETA: 2s - loss: 0.1782
2784/6530 [===========>..................] - ETA: 2s - loss: 0.1764
2976/6530 [============>.................] - ETA: 2s - loss: 0.1753
3152/6530 [=============>................] - ETA: 2s - loss: 0.1748
3344/6530 [==============>...............] - ETA: 1s - loss: 0.1740
# training | RMSE: 0.6892, MAE: 0.6366
worker 0  xfile  [50, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 9, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1843222787075455}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.689209623155366, 'rmse': 0.689209623155366, 'mae': 0.6366411275634648, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:   9 | activation: tanh    | extras: None 
layer 2 | size:  22 | activation: tanh    | extras: None 
layer 3 | size:  91 | activation: tanh    | extras: dropout - rate: 22.0% 
layer 4 | size:  40 | activation: sigmoid | extras: batchnorm 
layer 5 | size:  65 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4c5349b0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:43 - loss: 0.5176
3552/6530 [===============>..............] - ETA: 1s - loss: 0.1722
 416/6530 [>.............................] - ETA: 16s - loss: 0.5637 
3744/6530 [================>.............] - ETA: 1s - loss: 0.1710
 864/6530 [==>...........................] - ETA: 7s - loss: 0.5242 
3952/6530 [=================>............] - ETA: 1s - loss: 0.1699
1280/6530 [====>.........................] - ETA: 5s - loss: 0.4478
4160/6530 [==================>...........] - ETA: 1s - loss: 0.1688
1696/6530 [======>.......................] - ETA: 3s - loss: 0.3896
4384/6530 [===================>..........] - ETA: 1s - loss: 0.1687
2144/6530 [========>.....................] - ETA: 2s - loss: 0.3431
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1682
2592/6530 [==========>...................] - ETA: 2s - loss: 0.3043
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1672
3040/6530 [============>.................] - ETA: 1s - loss: 0.2687
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1663
3488/6530 [===============>..............] - ETA: 1s - loss: 0.2397
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1650
3968/6530 [=================>............] - ETA: 1s - loss: 0.2159
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1648
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1974
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1643
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1816
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1639
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1696
# training | RMSE: 0.1632, MAE: 0.1306
worker 2  xfile  [49, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3321998893743574}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 6, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1631804920654569, 'rmse': 0.1631804920654569, 'mae': 0.1305882031201093, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  55 | activation: relu    | extras: dropout - rate: 46.5% 
layer 2 | size:  56 | activation: sigmoid | extras: dropout - rate: 35.0% 
layer 3 | size:  73 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b173fe7f0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:06 - loss: 2.5730
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1635
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1592
 800/6530 [==>...........................] - ETA: 6s - loss: 0.5846  
6448/6530 [============================>.] - ETA: 0s - loss: 0.1633
6368/6530 [============================>.] - ETA: 0s - loss: 0.1511
1568/6530 [======>.......................] - ETA: 3s - loss: 0.3584
2304/6530 [=========>....................] - ETA: 1s - loss: 0.2782
3008/6530 [============>.................] - ETA: 1s - loss: 0.2342
6530/6530 [==============================] - 2s 306us/step - loss: 0.1484 - val_loss: 0.1607

3744/6530 [================>.............] - ETA: 0s - loss: 0.2056
6530/6530 [==============================] - 3s 445us/step - loss: 0.1630 - val_loss: 0.1277

4576/6530 [====================>.........] - ETA: 0s - loss: 0.1834
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1674
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1565
6530/6530 [==============================] - 2s 231us/step - loss: 0.1526 - val_loss: 0.0532

# training | RMSE: 0.4007, MAE: 0.3483
worker 0  xfile  [52, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22013542728313276}, 'layer_3_size': 91, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.40069370882883837, 'rmse': 0.40069370882883837, 'mae': 0.3482893623322823, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  33 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4c534518>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 35s - loss: 0.6087
3072/6530 [=============>................] - ETA: 0s - loss: 0.5131 
6144/6530 [===========================>..] - ETA: 0s - loss: 0.4317
6530/6530 [==============================] - 1s 142us/step - loss: 0.4210 - val_loss: 0.2581

# training | RMSE: 0.1570, MAE: 0.1220
worker 1  xfile  [51, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 98, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2110462536193236}, 'layer_2_size': 26, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.15701549094212638, 'rmse': 0.15701549094212638, 'mae': 0.12198505318722638, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  77 | activation: relu    | extras: batchnorm 
layer 2 | size:  36 | activation: tanh    | extras: dropout - rate: 39.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b16e86470>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 23s - loss: 0.6036
3840/6530 [================>.............] - ETA: 0s - loss: 0.5068 
# training | RMSE: 0.2313, MAE: 0.1887
worker 2  xfile  [53, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46455360283287184}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.35005484230469486}, 'layer_2_size': 56, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1103615171101521}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2313023028882588, 'rmse': 0.2313023028882588, 'mae': 0.18871510282802548, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  96 | activation: sigmoid | extras: None 
layer 2 | size:  37 | activation: relu    | extras: batchnorm 
layer 3 | size:  46 | activation: tanh    | extras: None 
layer 4 | size:  57 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b16f29da0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:30 - loss: 0.7763
 480/6530 [=>............................] - ETA: 13s - loss: 0.6567 
 928/6530 [===>..........................] - ETA: 6s - loss: 0.5602 
6530/6530 [==============================] - 1s 176us/step - loss: 0.3950 - val_loss: 0.1638

1472/6530 [=====>........................] - ETA: 4s - loss: 0.4298
2048/6530 [========>.....................] - ETA: 2s - loss: 0.3554
2656/6530 [===========>..................] - ETA: 1s - loss: 0.3119
3200/6530 [=============>................] - ETA: 1s - loss: 0.2872
3744/6530 [================>.............] - ETA: 1s - loss: 0.2692
4288/6530 [==================>...........] - ETA: 0s - loss: 0.2560
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2458
5440/6530 [=======================>......] - ETA: 0s - loss: 0.2365
6048/6530 [==========================>...] - ETA: 0s - loss: 0.2287
6530/6530 [==============================] - 2s 271us/step - loss: 0.2245 - val_loss: 0.3477

# training | RMSE: 0.5054, MAE: 0.4655
worker 0  xfile  [54, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 89, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 51, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.43608435128038303}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.5053726070153925, 'rmse': 0.5053726070153925, 'mae': 0.46554315854150286, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  24 | activation: sigmoid | extras: None 
layer 2 | size:  18 | activation: sigmoid | extras: dropout - rate: 34.6% 
layer 3 | size:  37 | activation: relu    | extras: dropout - rate: 30.2% 
layer 4 | size:  35 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b45e002e8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:50 - loss: 0.5855
 608/6530 [=>............................] - ETA: 8s - loss: 0.4433  
1216/6530 [====>.........................] - ETA: 4s - loss: 0.3736
1824/6530 [=======>......................] - ETA: 2s - loss: 0.3409
2496/6530 [==========>...................] - ETA: 1s - loss: 0.3188
3232/6530 [=============>................] - ETA: 1s - loss: 0.3023
4032/6530 [=================>............] - ETA: 0s - loss: 0.2918
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2823
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2774
6464/6530 [============================>.] - ETA: 0s - loss: 0.2709
6530/6530 [==============================] - 1s 224us/step - loss: 0.2706 - val_loss: 0.2576

# training | RMSE: 0.3845, MAE: 0.3100
worker 1  xfile  [56, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3970048171685735}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 50, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.38445340770082587, 'rmse': 0.38445340770082587, 'mae': 0.30996737812848385, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  96 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  63 | activation: sigmoid | extras: dropout - rate: 27.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b30ce4198>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:39 - loss: 0.9988
1152/6530 [====>.........................] - ETA: 4s - loss: 0.3788  
2368/6530 [=========>....................] - ETA: 1s - loss: 0.2103
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1521
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1207
6464/6530 [============================>.] - ETA: 0s - loss: 0.1032
6530/6530 [==============================] - 1s 211us/step - loss: 0.1025 - val_loss: 0.0394

# training | RMSE: 0.3910, MAE: 0.3485
worker 2  xfile  [55, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.27094407825541}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.3910090637171421, 'rmse': 0.3910090637171421, 'mae': 0.3485107141985299, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  14 | activation: relu    | extras: None 
layer 2 | size:  11 | activation: relu    | extras: batchnorm 
layer 3 | size:  67 | activation: tanh    | extras: None 
layer 4 | size:  40 | activation: sigmoid | extras: None 
layer 5 | size:  77 | activation: relu    | extras: dropout - rate: 18.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b17427f98>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 1:01 - loss: 0.6562
1792/6530 [=======>......................] - ETA: 3s - loss: 0.3388  
3456/6530 [==============>...............] - ETA: 1s - loss: 0.2685
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2302
# training | RMSE: 0.3127, MAE: 0.2552
worker 0  xfile  [57, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34629468526215}, 'layer_2_size': 18, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30245352346209964}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 35, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.3127112771611007, 'rmse': 0.3127112771611007, 'mae': 0.2551712624355869, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  70 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b459e65c0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 43s - loss: 0.5907
3200/6530 [=============>................] - ETA: 0s - loss: 0.4299 
6400/6530 [============================>.] - ETA: 0s - loss: 0.3219
6530/6530 [==============================] - 2s 246us/step - loss: 0.2164 - val_loss: 0.1677

6530/6530 [==============================] - 1s 166us/step - loss: 0.3179 - val_loss: 0.1131

# training | RMSE: 0.1979, MAE: 0.1598
worker 1  xfile  [58, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27116531123903675}, 'layer_2_size': 63, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.37972466777583236}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.197949714299848, 'rmse': 0.197949714299848, 'mae': 0.15983414202057367, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  54 | activation: relu    | extras: None 
layer 2 | size:  58 | activation: tanh    | extras: batchnorm 
layer 3 | size:  91 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b16620390>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:41 - loss: 1.3261
 960/6530 [===>..........................] - ETA: 6s - loss: 0.4257  
1920/6530 [=======>......................] - ETA: 2s - loss: 0.2817
3008/6530 [============>.................] - ETA: 1s - loss: 0.2106
4096/6530 [=================>............] - ETA: 0s - loss: 0.1750
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1492
6530/6530 [==============================] - 1s 222us/step - loss: 0.1319 - val_loss: 0.1494

# training | RMSE: 0.2009, MAE: 0.1633
worker 2  xfile  [59, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18421503775253176}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2009206544199728, 'rmse': 0.2009206544199728, 'mae': 0.1633398645709318, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:   8 | activation: sigmoid | extras: dropout - rate: 15.7% 
layer 2 | size:  77 | activation: tanh    | extras: dropout - rate: 39.0% 
layer 3 | size:  69 | activation: sigmoid | extras: dropout - rate: 45.8% 
layer 4 | size:   4 | activation: tanh    | extras: batchnorm 
layer 5 | size:  74 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b17427128>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:52 - loss: 0.7553
 896/6530 [===>..........................] - ETA: 7s - loss: 0.4618  
1728/6530 [======>.......................] - ETA: 3s - loss: 0.3719
2560/6530 [==========>...................] - ETA: 1s - loss: 0.3032
3392/6530 [==============>...............] - ETA: 1s - loss: 0.2547
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2223
# training | RMSE: 0.3338, MAE: 0.2809
worker 0  xfile  [60, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4770391816485111}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.28431452898181575}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 93, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.3337766802105597, 'rmse': 0.3337766802105597, 'mae': 0.28093101076567784, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  80 | activation: relu    | extras: dropout - rate: 38.7% 
layer 2 | size:  40 | activation: tanh    | extras: None 
layer 3 | size:  18 | activation: relu    | extras: batchnorm 
layer 4 | size:  47 | activation: relu    | extras: batchnorm 
layer 5 | size:  71 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b459e6400>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 59s - loss: 0.2203
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1974
1536/6530 [======>.......................] - ETA: 4s - loss: 0.0909 
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1774
3072/6530 [=============>................] - ETA: 1s - loss: 0.0760
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0702
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0672
6530/6530 [==============================] - 2s 256us/step - loss: 0.1685 - val_loss: 0.0692

6530/6530 [==============================] - 2s 240us/step - loss: 0.0656 - val_loss: 0.0470

# training | RMSE: 0.3959, MAE: 0.3168
worker 1  xfile  [61, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 58, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.3959075298789873, 'rmse': 0.3959075298789873, 'mae': 0.3168155000541314, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  35 | activation: relu    | extras: None 
layer 2 | size:   4 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  52 | activation: relu    | extras: None 
layer 4 | size:  27 | activation: tanh    | extras: dropout - rate: 26.8% 
layer 5 | size:  94 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b15dd40f0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 34s - loss: 0.4900
2560/6530 [==========>...................] - ETA: 2s - loss: 0.3503 
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2784
# training | RMSE: 0.2655, MAE: 0.2181
worker 2  xfile  [62, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15744312117713355}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38983627727809533}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45842520572362544}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.26551893023946793, 'rmse': 0.26551893023946793, 'mae': 0.21811227327742422, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  66 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b16521320>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 20s - loss: 1.1217
5632/6530 [========================>.....] - ETA: 0s - loss: 0.5138 
6530/6530 [==============================] - 2s 260us/step - loss: 0.2531 - val_loss: 0.0950

6530/6530 [==============================] - 1s 159us/step - loss: 0.4619 - val_loss: 0.1220

# training | RMSE: 0.2197, MAE: 0.1745
worker 0  xfile  [63, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3874475255086739}, 'layer_1_size': 80, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 18, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 71, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.21969819419551706, 'rmse': 0.21969819419551706, 'mae': 0.1744724646016155, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  78 | activation: relu    | extras: dropout - rate: 36.6% 
layer 2 | size:   3 | activation: tanh    | extras: None 
layer 3 | size:  99 | activation: relu    | extras: None 
layer 4 | size:  93 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b45898f98>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 48s - loss: 2.1067
2944/6530 [============>.................] - ETA: 1s - loss: 0.2754 
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1904
6530/6530 [==============================] - 1s 192us/step - loss: 0.1823 - val_loss: 0.1057

# training | RMSE: 0.3465, MAE: 0.2834
worker 2  xfile  [66, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 66, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11788957273039712}, 'layer_3_size': 25, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.3465149683636997, 'rmse': 0.3465149683636997, 'mae': 0.28342242247849414, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  42 | activation: sigmoid | extras: None 
layer 2 | size:  36 | activation: sigmoid | extras: None 
layer 3 | size:  91 | activation: tanh    | extras: batchnorm 
layer 4 | size:  11 | activation: sigmoid | extras: dropout - rate: 22.1% 
layer 5 | size:  43 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b15f51358>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 29s - loss: 0.5200
2560/6530 [==========>...................] - ETA: 1s - loss: 0.3453 
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2848
# training | RMSE: 0.3031, MAE: 0.2436
worker 1  xfile  [64, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2679645245210738}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.3031435628153073, 'rmse': 0.3031435628153073, 'mae': 0.24362812402413794, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  13 | activation: relu    | extras: batchnorm 
layer 2 | size:  60 | activation: tanh    | extras: batchnorm 
layer 3 | size:  47 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e90069320>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 2:07 - loss: 1.3709
6530/6530 [==============================] - 1s 227us/step - loss: 0.2651 - val_loss: 0.2054

 768/6530 [==>...........................] - ETA: 9s - loss: 0.6637  
1536/6530 [======>.......................] - ETA: 4s - loss: 0.4862
2304/6530 [=========>....................] - ETA: 2s - loss: 0.4035
# training | RMSE: 0.3290, MAE: 0.2622
worker 0  xfile  [65, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3661054667655005}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 3, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.32898001772661134, 'rmse': 0.32898001772661134, 'mae': 0.26223165760050926, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  13 | activation: relu    | extras: batchnorm 
layer 2 | size:  76 | activation: relu    | extras: dropout - rate: 48.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b451644a8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 7:49 - loss: 0.3478
3200/6530 [=============>................] - ETA: 1s - loss: 0.3428
 320/6530 [>.............................] - ETA: 23s - loss: 0.1712 
4096/6530 [=================>............] - ETA: 0s - loss: 0.2991
 624/6530 [=>............................] - ETA: 11s - loss: 0.1366
5056/6530 [======================>.......] - ETA: 0s - loss: 0.2676
 944/6530 [===>..........................] - ETA: 7s - loss: 0.1165 
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2465
1248/6530 [====>.........................] - ETA: 5s - loss: 0.1034
1552/6530 [======>.......................] - ETA: 4s - loss: 0.0937
1904/6530 [=======>......................] - ETA: 3s - loss: 0.0868
2240/6530 [=========>....................] - ETA: 2s - loss: 0.0814
2592/6530 [==========>...................] - ETA: 2s - loss: 0.0786
6530/6530 [==============================] - 2s 283us/step - loss: 0.2352 - val_loss: 0.1192

2960/6530 [============>.................] - ETA: 1s - loss: 0.0743
3344/6530 [==============>...............] - ETA: 1s - loss: 0.0706
3696/6530 [===============>..............] - ETA: 1s - loss: 0.0686
4048/6530 [=================>............] - ETA: 1s - loss: 0.0669
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0650
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0640
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0625
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0615
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0604
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0592
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0583
6530/6530 [==============================] - 2s 362us/step - loss: 0.0576 - val_loss: 0.0408

# training | RMSE: 0.2540, MAE: 0.2087
worker 2  xfile  [67, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2211884333351667}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.25397388014007477, 'rmse': 0.25397388014007477, 'mae': 0.20868826727397707, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  83 | activation: sigmoid | extras: None 
layer 2 | size:  88 | activation: tanh    | extras: dropout - rate: 42.6% 
layer 3 | size:  68 | activation: tanh    | extras: dropout - rate: 34.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b15fbc1d0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:16 - loss: 0.5505
 576/6530 [=>............................] - ETA: 10s - loss: 0.1545 
1120/6530 [====>.........................] - ETA: 5s - loss: 0.1279 
1760/6530 [=======>......................] - ETA: 3s - loss: 0.1083
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0980
3264/6530 [=============>................] - ETA: 1s - loss: 0.0907
4064/6530 [=================>............] - ETA: 0s - loss: 0.0851
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0808
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0784
6464/6530 [============================>.] - ETA: 0s - loss: 0.0759
6530/6530 [==============================] - 2s 246us/step - loss: 0.0755 - val_loss: 0.0515

# training | RMSE: 0.3273, MAE: 0.2578
worker 1  xfile  [68, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13295701680239463}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.3272591483983671, 'rmse': 0.3272591483983671, 'mae': 0.257839637577863, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  62 | activation: sigmoid | extras: None 
layer 2 | size:  31 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  77 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e90069240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:52 - loss: 0.8209
 480/6530 [=>............................] - ETA: 15s - loss: 0.5906 
 864/6530 [==>...........................] - ETA: 8s - loss: 0.4986 
1376/6530 [=====>........................] - ETA: 4s - loss: 0.4212
1984/6530 [========>.....................] - ETA: 3s - loss: 0.3640
2592/6530 [==========>...................] - ETA: 2s - loss: 0.3319
3168/6530 [=============>................] - ETA: 1s - loss: 0.3092
3776/6530 [================>.............] - ETA: 1s - loss: 0.2921
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2784
4928/6530 [=====================>........] - ETA: 0s - loss: 0.2679
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2574
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2484
# training | RMSE: 0.1950, MAE: 0.1527
worker 0  xfile  [69, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4868076109733872}, 'layer_2_size': 76, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.49502287953085233}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22225089583039181}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.19502455532360305, 'rmse': 0.19502455532360305, 'mae': 0.152728370166565, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size: 100 | activation: tanh    | extras: None 
layer 2 | size:  34 | activation: relu    | extras: dropout - rate: 34.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b45231fd0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:13 - loss: 0.8143
 992/6530 [===>..........................] - ETA: 5s - loss: 0.1566  
1888/6530 [=======>......................] - ETA: 2s - loss: 0.1124
2720/6530 [===========>..................] - ETA: 1s - loss: 0.0937
6530/6530 [==============================] - 2s 295us/step - loss: 0.2447 - val_loss: 0.2657

3520/6530 [===============>..............] - ETA: 0s - loss: 0.0817
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0738
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0695
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0646
6530/6530 [==============================] - 2s 232us/step - loss: 0.0623 - val_loss: 0.0559

# training | RMSE: 0.2267, MAE: 0.1846
worker 2  xfile  [70, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42559233622805637}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.34859741654439647}, 'layer_3_size': 68, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1345103654255286}, 'layer_4_size': 53, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.22669530235214697, 'rmse': 0.22669530235214697, 'mae': 0.18462197131829783, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  56 | activation: tanh    | extras: batchnorm 
layer 2 | size:  58 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4db41518>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 27s - loss: 0.8839
4608/6530 [====================>.........] - ETA: 0s - loss: 0.3770 
6530/6530 [==============================] - 1s 208us/step - loss: 0.3259 - val_loss: 0.2256

# training | RMSE: 0.3149, MAE: 0.2571
worker 1  xfile  [71, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 31, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 77, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.3148640946673206, 'rmse': 0.3148640946673206, 'mae': 0.25705675364942054, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  54 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b1522e2b0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:51 - loss: 0.7215
1600/6530 [======>.......................] - ETA: 3s - loss: 0.6175  
3328/6530 [==============>...............] - ETA: 1s - loss: 0.5136
# training | RMSE: 0.2206, MAE: 0.1755
worker 0  xfile  [72, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34901075954835103}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14204003070746826}, 'layer_3_size': 97, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4272710545995406}, 'layer_5_size': 30, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2205598186997392, 'rmse': 0.2205598186997392, 'mae': 0.1755420747257551, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  17 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4519e6a0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 47s - loss: 0.7993
5184/6530 [======================>.......] - ETA: 0s - loss: 0.4172
3584/6530 [===============>..............] - ETA: 0s - loss: 0.5913 
6530/6530 [==============================] - 1s 220us/step - loss: 0.3700 - val_loss: 0.1705

6530/6530 [==============================] - 1s 181us/step - loss: 0.4816 - val_loss: 0.2753

# training | RMSE: 0.2799, MAE: 0.2187
worker 2  xfile  [73, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 58, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21499127237600707}, 'layer_3_size': 22, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 99, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.27990821647715075, 'rmse': 0.27990821647715075, 'mae': 0.21869492318097397, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  83 | activation: relu    | extras: dropout - rate: 42.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b157392b0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:10 - loss: 1.0607
 960/6530 [===>..........................] - ETA: 5s - loss: 0.2773  
1920/6530 [=======>......................] - ETA: 2s - loss: 0.1990
2880/6530 [============>.................] - ETA: 1s - loss: 0.1656
3968/6530 [=================>............] - ETA: 0s - loss: 0.1408
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1261
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1175
6530/6530 [==============================] - 1s 229us/step - loss: 0.1124 - val_loss: 0.0664

# training | RMSE: 0.3322, MAE: 0.2695
worker 0  xfile  [75, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24545880961748656}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.33218191213256076, 'rmse': 0.33218191213256076, 'mae': 0.26953103302419434, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  24 | activation: relu    | extras: None 
layer 2 | size:  28 | activation: tanh    | extras: dropout - rate: 42.4% 
layer 3 | size:  44 | activation: sigmoid | extras: None 
layer 4 | size:  32 | activation: relu    | extras: dropout - rate: 41.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b45f5a7f0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 1:00 - loss: 1.7245
1664/6530 [======>.......................] - ETA: 3s - loss: 0.9489  
3584/6530 [===============>..............] - ETA: 1s - loss: 0.6249
5888/6530 [==========================>...] - ETA: 0s - loss: 0.4766
6530/6530 [==============================] - 2s 236us/step - loss: 0.4513 - val_loss: 0.1981

# training | RMSE: 0.2152, MAE: 0.1717
worker 1  xfile  [74, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 42, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.21524223247380078, 'rmse': 0.21524223247380078, 'mae': 0.17171999592703968, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  77 | activation: relu    | extras: dropout - rate: 35.0% 
layer 2 | size:  77 | activation: relu    | extras: dropout - rate: 15.3% 
layer 3 | size:  11 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  85 | activation: tanh    | extras: batchnorm 
layer 5 | size:  68 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b14e20208>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 9:48 - loss: 0.6577
 144/6530 [..............................] - ETA: 1:06 - loss: 0.7068
 304/6530 [>.............................] - ETA: 31s - loss: 0.6484 
 496/6530 [=>............................] - ETA: 19s - loss: 0.5304
 704/6530 [==>...........................] - ETA: 13s - loss: 0.4463
 912/6530 [===>..........................] - ETA: 10s - loss: 0.3953
1152/6530 [====>.........................] - ETA: 8s - loss: 0.3535 
1360/6530 [=====>........................] - ETA: 6s - loss: 0.3259
1552/6530 [======>.......................] - ETA: 5s - loss: 0.3084
1744/6530 [=======>......................] - ETA: 5s - loss: 0.2961
1936/6530 [=======>......................] - ETA: 4s - loss: 0.2848
2144/6530 [========>.....................] - ETA: 4s - loss: 0.2744
2336/6530 [=========>....................] - ETA: 3s - loss: 0.2662
2512/6530 [==========>...................] - ETA: 3s - loss: 0.2615
2688/6530 [===========>..................] - ETA: 3s - loss: 0.2573
2848/6530 [============>.................] - ETA: 2s - loss: 0.2529
3008/6530 [============>.................] - ETA: 2s - loss: 0.2481
3168/6530 [=============>................] - ETA: 2s - loss: 0.2444
3296/6530 [==============>...............] - ETA: 2s - loss: 0.2425
3424/6530 [==============>...............] - ETA: 2s - loss: 0.2401
3568/6530 [===============>..............] - ETA: 2s - loss: 0.2386
3712/6530 [================>.............] - ETA: 1s - loss: 0.2364
3856/6530 [================>.............] - ETA: 1s - loss: 0.2340
# training | RMSE: 0.2421, MAE: 0.1915
worker 2  xfile  [76, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4200202530984959}, 'layer_1_size': 83, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 72, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3588346811715748}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.24206976425903018, 'rmse': 0.24206976425903018, 'mae': 0.1915369375594151, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  18 | activation: tanh    | extras: batchnorm 
layer 2 | size:  95 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  64 | activation: tanh    | extras: None 
layer 4 | size:  59 | activation: relu    | extras: batchnorm 
layer 5 | size:   3 | activation: tanh    | extras: dropout - rate: 42.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b16510a58>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 35s - loss: 0.9767
4032/6530 [=================>............] - ETA: 1s - loss: 0.2316
3072/6530 [=============>................] - ETA: 1s - loss: 0.4712 
4224/6530 [==================>...........] - ETA: 1s - loss: 0.2293
6144/6530 [===========================>..] - ETA: 0s - loss: 0.3543
4416/6530 [===================>..........] - ETA: 1s - loss: 0.2271
4592/6530 [====================>.........] - ETA: 1s - loss: 0.2252
4752/6530 [====================>.........] - ETA: 1s - loss: 0.2233
6530/6530 [==============================] - 2s 264us/step - loss: 0.3462 - val_loss: 0.2218

4912/6530 [=====================>........] - ETA: 0s - loss: 0.2214
5088/6530 [======================>.......] - ETA: 0s - loss: 0.2199
5264/6530 [=======================>......] - ETA: 0s - loss: 0.2186
5440/6530 [=======================>......] - ETA: 0s - loss: 0.2169
5648/6530 [========================>.....] - ETA: 0s - loss: 0.2150
5840/6530 [=========================>....] - ETA: 0s - loss: 0.2139
6032/6530 [==========================>...] - ETA: 0s - loss: 0.2120
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2108
6368/6530 [============================>.] - ETA: 0s - loss: 0.2093
6512/6530 [============================>.] - ETA: 0s - loss: 0.2082
# training | RMSE: 0.2785, MAE: 0.2211
worker 2  xfile  [79, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42757086235637687}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.2784622011547405, 'rmse': 0.2784622011547405, 'mae': 0.2210592778458591, 'early_stop': False}
vggnet done  2

6530/6530 [==============================] - 4s 557us/step - loss: 0.2080 - val_loss: 0.3305

# training | RMSE: 0.2451, MAE: 0.1995
worker 0  xfile  [77, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4239733126315466}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41363825928937503}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13409385254743744}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.2450875948185709, 'rmse': 0.2450875948185709, 'mae': 0.1995201175588194, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  93 | activation: sigmoid | extras: dropout - rate: 36.6% 
layer 2 | size:   6 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  76 | activation: relu    | extras: batchnorm 
layer 4 | size:  26 | activation: relu    | extras: batchnorm 
layer 5 | size:  32 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4514e5c0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 38s - loss: 0.6953
3328/6530 [==============>...............] - ETA: 1s - loss: 0.4973 
6400/6530 [============================>.] - ETA: 0s - loss: 0.3762
6530/6530 [==============================] - 2s 285us/step - loss: 0.3739 - val_loss: 0.2039

# training | RMSE: 0.3743, MAE: 0.3326
worker 1  xfile  [78, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34968000437951974}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15299083490668175}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 11, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.37428619751566083, 'rmse': 0.37428619751566083, 'mae': 0.33260279631676337, 'early_stop': False}
vggnet done  1

# training | RMSE: 0.2646, MAE: 0.2137
worker 0  xfile  [80, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3655328568242938}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.26457595488315394, 'rmse': 0.26457595488315394, 'mae': 0.21371756868550587, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#2 epoch=1.0 loss={'loss': 0.5866068083216661, 'rmse': 0.5866068083216661, 'mae': 0.5256744738346993, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36704401178535906}, 'layer_3_size': 24, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10937773525521029}, 'layer_4_size': 52, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.49007862288269577}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#0 epoch=1.0 loss={'loss': 0.2023062674672241, 'rmse': 0.2023062674672241, 'mae': 0.16118185428838472, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.29307117696094115}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25913521176558485}, 'layer_3_size': 50, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44530675431740097}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#3 epoch=1.0 loss={'loss': 0.20933312541754298, 'rmse': 0.20933312541754298, 'mae': 0.1693393257536813, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#1 epoch=1.0 loss={'loss': 0.3006153438636222, 'rmse': 0.3006153438636222, 'mae': 0.23708659537995352, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#4 epoch=1.0 loss={'loss': 0.2616774253071672, 'rmse': 0.2616774253071672, 'mae': 0.21347698627409573, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4692606759961784}, 'layer_1_size': 75, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3136693852626846}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 6, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 25, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}
#7 epoch=1.0 loss={'loss': 0.24476600889565409, 'rmse': 0.24476600889565409, 'mae': 0.19844300687170194, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19558490676931226}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21186877882763422}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30112794810509574}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3309429567685175}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15937497910649576}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#5 epoch=1.0 loss={'loss': 0.3647543209762869, 'rmse': 0.3647543209762869, 'mae': 0.2892138873300356, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24578273782844684}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10533293360726957}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44160739263256776}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#8 epoch=1.0 loss={'loss': 0.2949374563580409, 'rmse': 0.2949374563580409, 'mae': 0.2367534368548339, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23999844446109977}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15804738600001286}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4323794958666226}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#6 epoch=1.0 loss={'loss': 0.33608962659485325, 'rmse': 0.33608962659485325, 'mae': 0.2893435374974713, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 68, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15207529849940485}, 'layer_5_size': 63, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#11 epoch=1.0 loss={'loss': 0.3660863046703895, 'rmse': 0.3660863046703895, 'mae': 0.29842138603932367, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3390640024687481}, 'layer_3_size': 24, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#9 epoch=1.0 loss={'loss': 0.2186938894246973, 'rmse': 0.2186938894246973, 'mae': 0.17853284871638117, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2926196489955891}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.47365031233221966}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 42, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 45, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#10 epoch=1.0 loss={'loss': 0.31488254685646394, 'rmse': 0.31488254685646394, 'mae': 0.27209660148252895, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3383217620683511}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#13 epoch=1.0 loss={'loss': 0.22352870224434201, 'rmse': 0.22352870224434201, 'mae': 0.18189225393296407, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 27, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3071854271732375}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#12 epoch=1.0 loss={'loss': 0.22689745609141695, 'rmse': 0.22689745609141695, 'mae': 0.17787573457486097, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4707528622365418}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#14 epoch=1.0 loss={'loss': 0.21913086134892013, 'rmse': 0.21913086134892013, 'mae': 0.17791341755375764, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37342786095112623}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 76, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22603963012380812}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#15 epoch=1.0 loss={'loss': 0.2202285607838009, 'rmse': 0.2202285607838009, 'mae': 0.17674723577898563, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18748774590944872}, 'layer_2_size': 33, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4715085985201116}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#17 epoch=1.0 loss={'loss': 0.5094744590336984, 'rmse': 0.5094744590336984, 'mae': 0.463013062137373, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19345445328349295}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4651933959290042}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42285526178483823}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#18 epoch=1.0 loss={'loss': 0.31231850704626396, 'rmse': 0.31231850704626396, 'mae': 0.2555969703539291, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12960415116227686}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1825073441161847}, 'layer_5_size': 80, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#16 epoch=1.0 loss={'loss': 0.23048528303997787, 'rmse': 0.23048528303997787, 'mae': 0.1810618707052838, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.20698808085843723}, 'layer_4_size': 66, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#19 epoch=1.0 loss={'loss': 0.32972860473203736, 'rmse': 0.32972860473203736, 'mae': 0.2628039751292015, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 47, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 26, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41000708099071104}, 'layer_4_size': 79, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#21 epoch=1.0 loss={'loss': 0.3238206051547236, 'rmse': 0.3238206051547236, 'mae': 0.2596314545674705, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13463193799911438}, 'layer_2_size': 22, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2175209473469314}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#20 epoch=1.0 loss={'loss': 0.20926089216928737, 'rmse': 0.20926089216928737, 'mae': 0.16684361633852657, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18870668435802915}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.37307324267253505}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1748877640403025}, 'layer_5_size': 79, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
#23 epoch=1.0 loss={'loss': 0.4693932624364615, 'rmse': 0.4693932624364615, 'mae': 0.39087631238220466, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21090723097235556}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 89, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#24 epoch=1.0 loss={'loss': 0.24877130829711364, 'rmse': 0.24877130829711364, 'mae': 0.2024966527390149, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 76, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.39218689546565366}, 'layer_3_size': 22, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 6, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#25 epoch=1.0 loss={'loss': 0.22777626867677023, 'rmse': 0.22777626867677023, 'mae': 0.18401154989852567, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 59, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#22 epoch=1.0 loss={'loss': 0.27060762152807005, 'rmse': 0.27060762152807005, 'mae': 0.21948667884939554, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3797722684704009}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1540494386976221}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.316657676909546}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#26 epoch=1.0 loss={'loss': 0.2605095322245654, 'rmse': 0.2605095322245654, 'mae': 0.2131374067953399, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4409945660614265}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 78, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
#29 epoch=1.0 loss={'loss': 0.2154688253830196, 'rmse': 0.2154688253830196, 'mae': 0.16969697254880073, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#27 epoch=1.0 loss={'loss': 0.7972823926694038, 'rmse': 0.7972823926694038, 'mae': 0.7560056051836649, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40849932833172253}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#28 epoch=1.0 loss={'loss': 0.21957880019843726, 'rmse': 0.21957880019843726, 'mae': 0.1733934187608411, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 53, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 50, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21209045336877344}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#30 epoch=1.0 loss={'loss': 0.16823179208915304, 'rmse': 0.16823179208915304, 'mae': 0.1331612750124683, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41928288588536755}, 'layer_3_size': 26, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#32 epoch=1.0 loss={'loss': 0.2651548589598659, 'rmse': 0.2651548589598659, 'mae': 0.2174988111615423, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34888354859047094}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3161346222267394}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#31 epoch=1.0 loss={'loss': 0.22457271710291757, 'rmse': 0.22457271710291757, 'mae': 0.1860811095854423, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21163869816357306}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22287552026058943}, 'layer_4_size': 51, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 18, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#34 epoch=1.0 loss={'loss': 0.27284905750281796, 'rmse': 0.27284905750281796, 'mae': 0.22008132133158417, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 9, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18742034528455595}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.151091328910066}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}
#33 epoch=1.0 loss={'loss': 0.14540570531495545, 'rmse': 0.14540570531495545, 'mae': 0.11091950039518869, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#35 epoch=1.0 loss={'loss': 0.1964600042044821, 'rmse': 0.1964600042044821, 'mae': 0.1511929662359733, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 7, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2591666001901206}, 'layer_3_size': 31, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1166779665150942}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#36 epoch=1.0 loss={'loss': 0.2646105616702299, 'rmse': 0.2646105616702299, 'mae': 0.21665060525073934, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2678508395581415}, 'layer_2_size': 96, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3251666720295153}, 'layer_3_size': 36, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 49, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#37 epoch=1.0 loss={'loss': 0.2732373697634362, 'rmse': 0.2732373697634362, 'mae': 0.2222363367281035, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45853271917242544}, 'layer_1_size': 2, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19574399120495656}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}
#38 epoch=1.0 loss={'loss': 0.15737763943390126, 'rmse': 0.15737763943390126, 'mae': 0.12475876505257386, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 88, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42338333595907873}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#39 epoch=1.0 loss={'loss': 0.377045445515654, 'rmse': 0.377045445515654, 'mae': 0.30064798047876745, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 9, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.33288877906250336}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#40 epoch=1.0 loss={'loss': 0.20024231708635035, 'rmse': 0.20024231708635035, 'mae': 0.1565697286585894, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#42 epoch=1.0 loss={'loss': 0.29516163489680464, 'rmse': 0.29516163489680464, 'mae': 0.23945027063762256, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 39, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}
#41 epoch=1.0 loss={'loss': 0.31910275129449417, 'rmse': 0.31910275129449417, 'mae': 0.26265769248140386, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45449295077327834}, 'layer_1_size': 51, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 24, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21802797553620104}, 'layer_4_size': 91, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#43 epoch=1.0 loss={'loss': 0.22837789286982524, 'rmse': 0.22837789286982524, 'mae': 0.1792708364031782, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.33740022211157467}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#44 epoch=1.0 loss={'loss': 0.2362093873860955, 'rmse': 0.2362093873860955, 'mae': 0.19245239548898094, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.29133968717940084}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 71, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14699657394160967}, 'layer_5_size': 88, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#45 epoch=1.0 loss={'loss': 0.26495188246116635, 'rmse': 0.26495188246116635, 'mae': 0.21774199868299124, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.33192373106196005}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21290049306740655}, 'layer_4_size': 3, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#46 epoch=1.0 loss={'loss': 0.2967463232992383, 'rmse': 0.2967463232992383, 'mae': 0.24369980749965967, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28099993686752606}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34974466255496983}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 99, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31272553346498344}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#47 epoch=1.0 loss={'loss': 0.24334785221441319, 'rmse': 0.24334785221441319, 'mae': 0.19415770970283275, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14776750231048486}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.43106598344991776}, 'layer_5_size': 26, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#48 epoch=1.0 loss={'loss': 0.24366937960500815, 'rmse': 0.24366937960500815, 'mae': 0.19512851367110826, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1824974374612169}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#50 epoch=1.0 loss={'loss': 0.689209623155366, 'rmse': 0.689209623155366, 'mae': 0.6366411275634648, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 9, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1843222787075455}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#49 epoch=1.0 loss={'loss': 0.1631804920654569, 'rmse': 0.1631804920654569, 'mae': 0.1305882031201093, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3321998893743574}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 6, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#52 epoch=1.0 loss={'loss': 0.40069370882883837, 'rmse': 0.40069370882883837, 'mae': 0.3482893623322823, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22013542728313276}, 'layer_3_size': 91, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
#53 epoch=1.0 loss={'loss': 0.2313023028882588, 'rmse': 0.2313023028882588, 'mae': 0.18871510282802548, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46455360283287184}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.35005484230469486}, 'layer_2_size': 56, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1103615171101521}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#51 epoch=1.0 loss={'loss': 0.15701549094212638, 'rmse': 0.15701549094212638, 'mae': 0.12198505318722638, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 98, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2110462536193236}, 'layer_2_size': 26, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#54 epoch=1.0 loss={'loss': 0.5053726070153925, 'rmse': 0.5053726070153925, 'mae': 0.46554315854150286, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 89, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 51, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.43608435128038303}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#56 epoch=1.0 loss={'loss': 0.38445340770082587, 'rmse': 0.38445340770082587, 'mae': 0.30996737812848385, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3970048171685735}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 50, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#55 epoch=1.0 loss={'loss': 0.3910090637171421, 'rmse': 0.3910090637171421, 'mae': 0.3485107141985299, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.27094407825541}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#57 epoch=1.0 loss={'loss': 0.3127112771611007, 'rmse': 0.3127112771611007, 'mae': 0.2551712624355869, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34629468526215}, 'layer_2_size': 18, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30245352346209964}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 35, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#58 epoch=1.0 loss={'loss': 0.197949714299848, 'rmse': 0.197949714299848, 'mae': 0.15983414202057367, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27116531123903675}, 'layer_2_size': 63, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.37972466777583236}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#59 epoch=1.0 loss={'loss': 0.2009206544199728, 'rmse': 0.2009206544199728, 'mae': 0.1633398645709318, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18421503775253176}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#60 epoch=1.0 loss={'loss': 0.3337766802105597, 'rmse': 0.3337766802105597, 'mae': 0.28093101076567784, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4770391816485111}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.28431452898181575}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 93, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#61 epoch=1.0 loss={'loss': 0.3959075298789873, 'rmse': 0.3959075298789873, 'mae': 0.3168155000541314, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 58, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#63 epoch=1.0 loss={'loss': 0.21969819419551706, 'rmse': 0.21969819419551706, 'mae': 0.1744724646016155, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3874475255086739}, 'layer_1_size': 80, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 18, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 71, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#62 epoch=1.0 loss={'loss': 0.26551893023946793, 'rmse': 0.26551893023946793, 'mae': 0.21811227327742422, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15744312117713355}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38983627727809533}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45842520572362544}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#66 epoch=1.0 loss={'loss': 0.3465149683636997, 'rmse': 0.3465149683636997, 'mae': 0.28342242247849414, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 66, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11788957273039712}, 'layer_3_size': 25, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#64 epoch=1.0 loss={'loss': 0.3031435628153073, 'rmse': 0.3031435628153073, 'mae': 0.24362812402413794, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2679645245210738}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#65 epoch=1.0 loss={'loss': 0.32898001772661134, 'rmse': 0.32898001772661134, 'mae': 0.26223165760050926, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3661054667655005}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 3, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#67 epoch=1.0 loss={'loss': 0.25397388014007477, 'rmse': 0.25397388014007477, 'mae': 0.20868826727397707, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2211884333351667}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#68 epoch=1.0 loss={'loss': 0.3272591483983671, 'rmse': 0.3272591483983671, 'mae': 0.257839637577863, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13295701680239463}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#69 epoch=1.0 loss={'loss': 0.19502455532360305, 'rmse': 0.19502455532360305, 'mae': 0.152728370166565, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4868076109733872}, 'layer_2_size': 76, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.49502287953085233}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22225089583039181}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#70 epoch=1.0 loss={'loss': 0.22669530235214697, 'rmse': 0.22669530235214697, 'mae': 0.18462197131829783, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42559233622805637}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.34859741654439647}, 'layer_3_size': 68, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1345103654255286}, 'layer_4_size': 53, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#71 epoch=1.0 loss={'loss': 0.3148640946673206, 'rmse': 0.3148640946673206, 'mae': 0.25705675364942054, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 31, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 77, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#72 epoch=1.0 loss={'loss': 0.2205598186997392, 'rmse': 0.2205598186997392, 'mae': 0.1755420747257551, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34901075954835103}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14204003070746826}, 'layer_3_size': 97, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4272710545995406}, 'layer_5_size': 30, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#73 epoch=1.0 loss={'loss': 0.27990821647715075, 'rmse': 0.27990821647715075, 'mae': 0.21869492318097397, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 58, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21499127237600707}, 'layer_3_size': 22, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 99, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#75 epoch=1.0 loss={'loss': 0.33218191213256076, 'rmse': 0.33218191213256076, 'mae': 0.26953103302419434, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24545880961748656}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#74 epoch=1.0 loss={'loss': 0.21524223247380078, 'rmse': 0.21524223247380078, 'mae': 0.17171999592703968, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 42, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#76 epoch=1.0 loss={'loss': 0.24206976425903018, 'rmse': 0.24206976425903018, 'mae': 0.1915369375594151, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4200202530984959}, 'layer_1_size': 83, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 72, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3588346811715748}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#77 epoch=1.0 loss={'loss': 0.2450875948185709, 'rmse': 0.2450875948185709, 'mae': 0.1995201175588194, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4239733126315466}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41363825928937503}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13409385254743744}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#79 epoch=1.0 loss={'loss': 0.2784622011547405, 'rmse': 0.2784622011547405, 'mae': 0.2210592778458591, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42757086235637687}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#78 epoch=1.0 loss={'loss': 0.37428619751566083, 'rmse': 0.37428619751566083, 'mae': 0.33260279631676337, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34968000437951974}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15299083490668175}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 11, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#80 epoch=1.0 loss={'loss': 0.26457595488315394, 'rmse': 0.26457595488315394, 'mae': 0.21371756868550587, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3655328568242938}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
get a list [results] of length 81
get a list [loss] of length 81
get a list [val_loss] of length 81
length of indices is [34 53 38 50 30 69 35 58 40 59  1 21  2 75 27 10 14 29 62 15 72 12 32 70
 13 24 43 18 52 44 76 47 48  5 77 23 67 26  4 80 36 45 31 63 25 33 37 78
 73  7 41 46  3 65 17 57 71 11 42 20 68 66 19 74 60  8 64  6  9 79 39 55
 56 61 51 22 54 16  0 49 28]
length of indices is 81
length of T is 81
newly formed T structure is:[[0, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 9, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18742034528455595}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.151091328910066}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}], [1, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46455360283287184}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.35005484230469486}, 'layer_2_size': 56, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1103615171101521}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [2, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 88, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42338333595907873}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [3, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 9, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1843222787075455}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [4, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41928288588536755}, 'layer_3_size': 26, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [5, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4868076109733872}, 'layer_2_size': 76, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.49502287953085233}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22225089583039181}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [6, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 7, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2591666001901206}, 'layer_3_size': 31, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1166779665150942}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [7, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27116531123903675}, 'layer_2_size': 63, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.37972466777583236}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [8, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [9, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18421503775253176}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [10, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [11, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13463193799911438}, 'layer_2_size': 22, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2175209473469314}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [12, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36704401178535906}, 'layer_3_size': 24, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10937773525521029}, 'layer_4_size': 52, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.49007862288269577}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [13, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24545880961748656}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [14, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40849932833172253}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [15, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3383217620683511}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [16, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37342786095112623}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 76, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22603963012380812}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [17, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [18, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15744312117713355}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38983627727809533}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45842520572362544}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [19, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18748774590944872}, 'layer_2_size': 33, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4715085985201116}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [20, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34901075954835103}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14204003070746826}, 'layer_3_size': 97, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4272710545995406}, 'layer_5_size': 30, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [21, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4707528622365418}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [22, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34888354859047094}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3161346222267394}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [23, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42559233622805637}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.34859741654439647}, 'layer_3_size': 68, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1345103654255286}, 'layer_4_size': 53, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [24, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 27, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3071854271732375}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [25, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 76, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.39218689546565366}, 'layer_3_size': 22, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 6, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [26, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.33740022211157467}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]] 

*** 27.0 configurations x 3.0 iterations each

81 | Thu Sep 27 18:38:47 2018 | lowest loss so far: 0.1454 (run 33)

{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  55 | activation: relu    | extras: dropout - rate: 46.5% 
layer 2 | size:  56 | activation: sigmoid | extras: dropout - rate: 35.0% 
layer 3 | size:  73 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 2:37 - loss: 1.8310
 672/6530 [==>...........................] - ETA: 7s - loss: 0.4451  {'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:   9 | activation: relu    | extras: batchnorm 
layer 2 | size:  62 | activation: relu    | extras: dropout - rate: 18.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 41s - loss: 0.8868
1472/6530 [=====>........................] - ETA: 3s - loss: 0.2837
3072/6530 [=============>................] - ETA: 0s - loss: 0.2957 
2336/6530 [=========>....................] - ETA: 1s - loss: 0.2203
6400/6530 [============================>.] - ETA: 0s - loss: 0.2346
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1861
6530/6530 [==============================] - 1s 147us/step - loss: 0.2333 - val_loss: 0.1791
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1682
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1659
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1607
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1518
6530/6530 [==============================] - 0s 17us/step - loss: 0.1589 - val_loss: 0.1703
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1780
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1418
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1521
6530/6530 [==============================] - 1s 183us/step - loss: 0.1376 - val_loss: 0.0512
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0741
6530/6530 [==============================] - 0s 16us/step - loss: 0.1509 - val_loss: 0.1671

1088/6530 [===>..........................] - ETA: 0s - loss: 0.0761
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0749{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  88 | activation: relu    | extras: batchnorm 
layer 2 | size:  55 | activation: tanh    | extras: None 
layer 3 | size:  13 | activation: tanh    | extras: None 
layer 4 | size:  13 | activation: tanh    | extras: dropout - rate: 42.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 3:27 - loss: 0.5660
2848/6530 [============>.................] - ETA: 0s - loss: 0.0756
 640/6530 [=>............................] - ETA: 9s - loss: 0.5679  
3776/6530 [================>.............] - ETA: 0s - loss: 0.0758
1280/6530 [====>.........................] - ETA: 4s - loss: 0.4937
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0746
1792/6530 [=======>......................] - ETA: 3s - loss: 0.4396
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0734
2336/6530 [=========>....................] - ETA: 2s - loss: 0.3804
6400/6530 [============================>.] - ETA: 0s - loss: 0.0725
6530/6530 [==============================] - 0s 58us/step - loss: 0.0724 - val_loss: 0.0432
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0633
2944/6530 [============>.................] - ETA: 1s - loss: 0.3194
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0580
3584/6530 [===============>..............] - ETA: 1s - loss: 0.2719
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0604
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2368
2848/6530 [============>.................] - ETA: 0s - loss: 0.0610
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2101
# training | RMSE: 0.1985, MAE: 0.1593
worker 0  xfile  [0, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 9, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18742034528455595}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.151091328910066}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.1985243965995024, 'rmse': 0.1985243965995024, 'mae': 0.15929453449944772, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  89 | activation: sigmoid | extras: None 
layer 2 | size:   9 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e906c2fd0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 4s - loss: 0.6171
3744/6530 [================>.............] - ETA: 0s - loss: 0.0611
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1903
6530/6530 [==============================] - 0s 35us/step - loss: 0.4320 - val_loss: 0.2791
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2865
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0606
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1759
6530/6530 [==============================] - 0s 7us/step - loss: 0.1911 - val_loss: 0.1253
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1297
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0598
6530/6530 [==============================] - 0s 7us/step - loss: 0.0959 - val_loss: 0.0766

6530/6530 [==============================] - 0s 57us/step - loss: 0.0593 - val_loss: 0.0387

6530/6530 [==============================] - 2s 251us/step - loss: 0.1643 - val_loss: 0.0271
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0352
 640/6530 [=>............................] - ETA: 0s - loss: 0.0278
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0262
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0244
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0238
2912/6530 [============>.................] - ETA: 0s - loss: 0.0231
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0224
3968/6530 [=================>............] - ETA: 0s - loss: 0.0219
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0213
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0208
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0205
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0203
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0202
6530/6530 [==============================] - 1s 104us/step - loss: 0.0198 - val_loss: 0.0148
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0189
 480/6530 [=>............................] - ETA: 0s - loss: 0.0158
# training | RMSE: 0.1954, MAE: 0.1565
worker 1  xfile  [1, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46455360283287184}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.35005484230469486}, 'layer_2_size': 56, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1103615171101521}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.19538084759011642, 'rmse': 0.19538084759011642, 'mae': 0.15654468077120473, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  13 | activation: relu    | extras: batchnorm 
layer 2 | size:  76 | activation: relu    | extras: dropout - rate: 48.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e913654e0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:47 - loss: 0.6528
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0150
 352/6530 [>.............................] - ETA: 5s - loss: 0.2518  
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0145
# training | RMSE: 0.2762, MAE: 0.2240
worker 0  xfile  [3, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 9, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1843222787075455}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.27620603293845836, 'rmse': 0.27620603293845836, 'mae': 0.22401436892441598, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  28 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  62 | activation: relu    | extras: None 
layer 3 | size:  26 | activation: sigmoid | extras: dropout - rate: 41.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e906c2dd8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:24 - loss: 1.6130
 688/6530 [==>...........................] - ETA: 3s - loss: 0.1831
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0144
 608/6530 [=>............................] - ETA: 4s - loss: 0.8355  
1072/6530 [===>..........................] - ETA: 2s - loss: 0.1419
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0150
1216/6530 [====>.........................] - ETA: 2s - loss: 0.4839
1456/6530 [=====>........................] - ETA: 1s - loss: 0.1248
3200/6530 [=============>................] - ETA: 0s - loss: 0.0146
1824/6530 [=======>......................] - ETA: 1s - loss: 0.3496
1840/6530 [=======>......................] - ETA: 1s - loss: 0.1106
3808/6530 [================>.............] - ETA: 0s - loss: 0.0147
2464/6530 [==========>...................] - ETA: 1s - loss: 0.2767
2224/6530 [=========>....................] - ETA: 1s - loss: 0.1007
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0145
3104/6530 [=============>................] - ETA: 0s - loss: 0.2319
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0941
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0144
3744/6530 [================>.............] - ETA: 0s - loss: 0.2018
2992/6530 [============>.................] - ETA: 0s - loss: 0.0873
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0143
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1826
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0824
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0143
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1650
3792/6530 [================>.............] - ETA: 0s - loss: 0.0782
6530/6530 [==============================] - 1s 95us/step - loss: 0.0143 - val_loss: 0.0121

4176/6530 [==================>...........] - ETA: 0s - loss: 0.0751
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1516
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0723
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1410
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0695
6530/6530 [==============================] - 1s 154us/step - loss: 0.1372 - val_loss: 0.0446
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0435
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0672
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0452
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0649
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0416
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0631
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0409
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0415
6530/6530 [==============================] - 1s 181us/step - loss: 0.0621 - val_loss: 0.0327
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0416
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0410
 432/6530 [>.............................] - ETA: 0s - loss: 0.0389
3936/6530 [=================>............] - ETA: 0s - loss: 0.0412
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0370
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0411
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0387
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0406
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0381
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0403
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0369
6530/6530 [==============================] - 1s 81us/step - loss: 0.0400 - val_loss: 0.0353
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0314
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0372
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0347
2912/6530 [============>.................] - ETA: 0s - loss: 0.0363
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0325
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0357
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0323
3744/6530 [================>.............] - ETA: 0s - loss: 0.0351
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0331
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0349
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0328
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0348
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0329
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0343
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0328
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0339
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0324
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0335
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0323
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0333
# training | RMSE: 0.1009, MAE: 0.0789
worker 2  xfile  [2, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 88, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42338333595907873}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.10085561998038037, 'rmse': 0.10085561998038037, 'mae': 0.07893742653284587, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  21 | activation: relu    | extras: None 
layer 2 | size:   7 | activation: tanh    | extras: batchnorm 
layer 3 | size:  31 | activation: tanh    | extras: dropout - rate: 25.9% 
layer 4 | size:  22 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e91365588>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:30 - loss: 0.6355
6530/6530 [==============================] - 1s 77us/step - loss: 0.0321 - val_loss: 0.0280

6530/6530 [==============================] - 1s 126us/step - loss: 0.0335 - val_loss: 0.0252
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0340
 576/6530 [=>............................] - ETA: 5s - loss: 0.5046  
 432/6530 [>.............................] - ETA: 0s - loss: 0.0307
1120/6530 [====>.........................] - ETA: 2s - loss: 0.3954
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0297
1664/6530 [======>.......................] - ETA: 1s - loss: 0.3412
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0319
2176/6530 [========>.....................] - ETA: 1s - loss: 0.3084
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0315
2720/6530 [===========>..................] - ETA: 0s - loss: 0.2810
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0304
3232/6530 [=============>................] - ETA: 0s - loss: 0.2629
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0309
3776/6530 [================>.............] - ETA: 0s - loss: 0.2482
2928/6530 [============>.................] - ETA: 0s - loss: 0.0300
4320/6530 [==================>...........] - ETA: 0s - loss: 0.2363
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0299
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2280
3760/6530 [================>.............] - ETA: 0s - loss: 0.0295
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2208
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0294
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2155
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0293
6432/6530 [============================>.] - ETA: 0s - loss: 0.2101
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0291
6530/6530 [==============================] - 1s 174us/step - loss: 0.2093 - val_loss: 0.1895
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1493
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0289
 576/6530 [=>............................] - ETA: 0s - loss: 0.1552
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0285
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1587
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0284
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1578
6530/6530 [==============================] - 1s 129us/step - loss: 0.0286 - val_loss: 0.0223

2208/6530 [=========>....................] - ETA: 0s - loss: 0.1555
# training | RMSE: 0.1604, MAE: 0.1262
worker 0  xfile  [4, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41928288588536755}, 'layer_3_size': 26, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.16035272748052612, 'rmse': 0.16035272748052612, 'mae': 0.1262275192768203, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  96 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  63 | activation: sigmoid | extras: dropout - rate: 27.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e90a049b0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 34s - loss: 0.0895
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1547
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0484 
3232/6530 [=============>................] - ETA: 0s - loss: 0.1534
3072/6530 [=============>................] - ETA: 0s - loss: 0.0462
3744/6530 [================>.............] - ETA: 0s - loss: 0.1513
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0441
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1502
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0430
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1492
6530/6530 [==============================] - 1s 93us/step - loss: 0.0430 - val_loss: 0.0407
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0337
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1485
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0390
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1475
3136/6530 [=============>................] - ETA: 0s - loss: 0.0383
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1470
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0384
6530/6530 [==============================] - 1s 103us/step - loss: 0.1472 - val_loss: 0.1549
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1492
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0375
 480/6530 [=>............................] - ETA: 0s - loss: 0.1533
6530/6530 [==============================] - 0s 36us/step - loss: 0.0375 - val_loss: 0.0416
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0424
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1466
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0373
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1427
3136/6530 [=============>................] - ETA: 0s - loss: 0.0351
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1378
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0343
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1366
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0332
6530/6530 [==============================] - 0s 35us/step - loss: 0.0329 - val_loss: 0.0408

2944/6530 [============>.................] - ETA: 0s - loss: 0.1370
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1387
4032/6530 [=================>............] - ETA: 0s - loss: 0.1376
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1369
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1375
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1368
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1358
6530/6530 [==============================] - 1s 102us/step - loss: 0.1354 - val_loss: 0.1533

# training | RMSE: 0.1427, MAE: 0.1097
worker 1  xfile  [5, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4868076109733872}, 'layer_2_size': 76, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.49502287953085233}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22225089583039181}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.14268407762980503, 'rmse': 0.14268407762980503, 'mae': 0.10968092201992242, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  68 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  91 | activation: tanh    | extras: batchnorm 
layer 3 | size:  29 | activation: tanh    | extras: batchnorm 
layer 4 | size:  90 | activation: tanh    | extras: None 
layer 5 | size:  57 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e904b9940>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 17s - loss: 0.5648
# training | RMSE: 0.1999, MAE: 0.1555
worker 0  xfile  [7, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27116531123903675}, 'layer_2_size': 63, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.37972466777583236}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.19992408764795153, 'rmse': 0.19992408764795153, 'mae': 0.15554816729526266, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  14 | activation: relu    | extras: None 
layer 2 | size:  11 | activation: relu    | extras: batchnorm 
layer 3 | size:  67 | activation: tanh    | extras: None 
layer 4 | size:  40 | activation: sigmoid | extras: None 
layer 5 | size:  77 | activation: relu    | extras: dropout - rate: 18.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e9073f198>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 20s - loss: 0.7782
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1633 
2304/6530 [=========>....................] - ETA: 0s - loss: 0.3362 
6400/6530 [============================>.] - ETA: 0s - loss: 0.0991
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2599
6528/6530 [============================>.] - ETA: 0s - loss: 0.2235
6530/6530 [==============================] - 1s 133us/step - loss: 0.0980 - val_loss: 0.0475
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0370
6530/6530 [==============================] - 1s 94us/step - loss: 0.2235 - val_loss: 0.1643
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1644
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0386
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1375
6530/6530 [==============================] - 0s 16us/step - loss: 0.0358 - val_loss: 0.0453
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0263
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1321
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0281
6530/6530 [==============================] - 0s 23us/step - loss: 0.1290 - val_loss: 0.1309
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1184
6530/6530 [==============================] - 0s 16us/step - loss: 0.0284 - val_loss: 0.0282

2560/6530 [==========>...................] - ETA: 0s - loss: 0.1199
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1167
6530/6530 [==============================] - 0s 25us/step - loss: 0.1167 - val_loss: 0.1306

# training | RMSE: 0.1588, MAE: 0.1209
worker 0  xfile  [9, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18421503775253176}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.15878221465631842, 'rmse': 0.15878221465631842, 'mae': 0.12091582632183993, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  27 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e8816a908>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 4s - loss: 0.7331
# training | RMSE: 0.1922, MAE: 0.1498
worker 2  xfile  [6, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 7, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2591666001901206}, 'layer_3_size': 31, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1166779665150942}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.19220691600278755, 'rmse': 0.19220691600278755, 'mae': 0.1498364947171333, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  55 | activation: tanh    | extras: batchnorm 
layer 2 | size:  58 | activation: relu    | extras: None 
layer 3 | size:  85 | activation: tanh    | extras: batchnorm 
layer 4 | size:  59 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e73726518>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 30s - loss: 1.0247
6530/6530 [==============================] - 0s 45us/step - loss: 0.6446 - val_loss: 0.5859
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.5950
1792/6530 [=======>......................] - ETA: 1s - loss: 0.7536 
6530/6530 [==============================] - 0s 5us/step - loss: 0.5205 - val_loss: 0.4599
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.4681
6530/6530 [==============================] - 0s 6us/step - loss: 0.3973 - val_loss: 0.3313

3456/6530 [==============>...............] - ETA: 0s - loss: 0.5948
5120/6530 [======================>.......] - ETA: 0s - loss: 0.5006
# training | RMSE: 0.1629, MAE: 0.1280
worker 1  xfile  [8, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.1628601088045768, 'rmse': 0.1628601088045768, 'mae': 0.1280336674942376, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  74 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  22 | activation: tanh    | extras: dropout - rate: 13.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e887f4940>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 18s - loss: 1.8110
3200/6530 [=============>................] - ETA: 0s - loss: 0.7188 
6144/6530 [===========================>..] - ETA: 0s - loss: 0.4969
6530/6530 [==============================] - 1s 132us/step - loss: 0.4502 - val_loss: 0.2517
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2031
6530/6530 [==============================] - 1s 81us/step - loss: 0.4784 - val_loss: 0.1708

2048/6530 [========>.....................] - ETA: 0s - loss: 0.2251Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1496
3840/6530 [================>.............] - ETA: 0s - loss: 0.2151
3072/6530 [=============>................] - ETA: 0s - loss: 0.1595
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1489
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2088
6530/6530 [==============================] - 0s 18us/step - loss: 0.1465 - val_loss: 0.1216
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1147
6530/6530 [==============================] - 0s 30us/step - loss: 0.2076 - val_loss: 0.1961
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1876
3072/6530 [=============>................] - ETA: 0s - loss: 0.1157
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1802
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1115
6530/6530 [==============================] - 0s 18us/step - loss: 0.1105 - val_loss: 0.0960

3328/6530 [==============>...............] - ETA: 0s - loss: 0.1759
# training | RMSE: 0.4040, MAE: 0.3344
worker 0  xfile  [12, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36704401178535906}, 'layer_3_size': 24, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10937773525521029}, 'layer_4_size': 52, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.49007862288269577}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.4040108182583054, 'rmse': 0.4040108182583054, 'mae': 0.33437753948073407, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  17 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e502bd160>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 11s - loss: 0.7852
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1751
5632/6530 [========================>.....] - ETA: 0s - loss: 0.5022 
6530/6530 [==============================] - 0s 51us/step - loss: 0.4728 - val_loss: 0.2723
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2744
6530/6530 [==============================] - 0s 33us/step - loss: 0.1735 - val_loss: 0.1787

5632/6530 [========================>.....] - ETA: 0s - loss: 0.2186
6530/6530 [==============================] - 0s 10us/step - loss: 0.2151 - val_loss: 0.1944
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2052
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1919
6530/6530 [==============================] - 0s 10us/step - loss: 0.1900 - val_loss: 0.1834

# training | RMSE: 0.1993, MAE: 0.1563
worker 2  xfile  [10, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.19929943705187209, 'rmse': 0.19929943705187209, 'mae': 0.15626503982931486, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  46 | activation: relu    | extras: dropout - rate: 33.8% 
layer 2 | size:  79 | activation: sigmoid | extras: None 
layer 3 | size:  69 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  52 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e4c19b438>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 3:43 - loss: 0.7292
 224/6530 [>.............................] - ETA: 16s - loss: 0.4629 
 448/6530 [=>............................] - ETA: 8s - loss: 0.3064 
 704/6530 [==>...........................] - ETA: 5s - loss: 0.2187
 992/6530 [===>..........................] - ETA: 4s - loss: 0.1740
1296/6530 [====>.........................] - ETA: 3s - loss: 0.1460
1600/6530 [======>.......................] - ETA: 2s - loss: 0.1276
# training | RMSE: 0.3092, MAE: 0.2463
worker 1  xfile  [11, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13463193799911438}, 'layer_2_size': 22, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2175209473469314}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.3092455147570381, 'rmse': 0.3092455147570381, 'mae': 0.24626761500334035, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  39 | activation: relu    | extras: None 
layer 2 | size:  43 | activation: tanh    | extras: None 
layer 3 | size:  22 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  27 | activation: tanh    | extras: batchnorm 
layer 5 | size:  13 | activation: sigmoid | extras: dropout - rate: 40.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e6c00f668>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 16s - loss: 0.8465
# training | RMSE: 0.2265, MAE: 0.1841
worker 0  xfile  [13, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24545880961748656}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.22648029468489184, 'rmse': 0.22648029468489184, 'mae': 0.18406781405565828, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  78 | activation: relu    | extras: dropout - rate: 37.3% 
layer 2 | size:  76 | activation: relu    | extras: batchnorm 
layer 3 | size:   5 | activation: tanh    | extras: dropout - rate: 22.6% 
layer 4 | size:  96 | activation: sigmoid | extras: None 
layer 5 | size:  53 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b51a28a20>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 32s - loss: 0.1503
1904/6530 [=======>......................] - ETA: 2s - loss: 0.1161
3328/6530 [==============>...............] - ETA: 0s - loss: 0.7083 
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0795 
2192/6530 [=========>....................] - ETA: 1s - loss: 0.1073
2496/6530 [==========>...................] - ETA: 1s - loss: 0.1009
4096/6530 [=================>............] - ETA: 0s - loss: 0.0661
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0963
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0593
6530/6530 [==============================] - 1s 131us/step - loss: 0.5814 - val_loss: 0.3350
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.3454
3088/6530 [=============>................] - ETA: 1s - loss: 0.0918
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2845
3376/6530 [==============>...............] - ETA: 1s - loss: 0.0883
6530/6530 [==============================] - 1s 138us/step - loss: 0.0581 - val_loss: 0.0344
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0484
6530/6530 [==============================] - 0s 16us/step - loss: 0.2408 - val_loss: 0.1742
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1481
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0855
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0425
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1147
3952/6530 [=================>............] - ETA: 0s - loss: 0.0829
3840/6530 [================>.............] - ETA: 0s - loss: 0.0414
6530/6530 [==============================] - 0s 16us/step - loss: 0.0962 - val_loss: 0.0580

4256/6530 [==================>...........] - ETA: 0s - loss: 0.0802
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0404
6530/6530 [==============================] - 0s 29us/step - loss: 0.0398 - val_loss: 0.0277
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0384
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0784
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0362
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0765
4096/6530 [=================>............] - ETA: 0s - loss: 0.0353
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0749
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0354
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0734
6530/6530 [==============================] - 0s 27us/step - loss: 0.0351 - val_loss: 0.0231

5760/6530 [=========================>....] - ETA: 0s - loss: 0.0721
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0711
6320/6530 [============================>.] - ETA: 0s - loss: 0.0699
6530/6530 [==============================] - 2s 277us/step - loss: 0.0689 - val_loss: 0.0588
Epoch 2/3

  16/6530 [..............................] - ETA: 2s - loss: 0.0498
# training | RMSE: 0.2418, MAE: 0.1975
worker 1  xfile  [14, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40849932833172253}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.24176014503754995, 'rmse': 0.24176014503754995, 'mae': 0.19745337471806157, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  54 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b5065de80>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 13s - loss: 0.7444
 256/6530 [>.............................] - ETA: 1s - loss: 0.0459
4480/6530 [===================>..........] - ETA: 0s - loss: 0.4941 
 528/6530 [=>............................] - ETA: 1s - loss: 0.0485
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0495
6530/6530 [==============================] - 0s 63us/step - loss: 0.3973 - val_loss: 0.1623
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1662
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0495
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1638
6530/6530 [==============================] - 0s 12us/step - loss: 0.1635 - val_loss: 0.1593
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1645
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0472
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1597
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0467
6530/6530 [==============================] - 0s 11us/step - loss: 0.1611 - val_loss: 0.1568

1920/6530 [=======>......................] - ETA: 0s - loss: 0.0464
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0458
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0461
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0457
3104/6530 [=============>................] - ETA: 0s - loss: 0.0451
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0453
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0452
3968/6530 [=================>............] - ETA: 0s - loss: 0.0451
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0447
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0444
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0441
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0439
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0439
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0438
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0434
# training | RMSE: 0.2027, MAE: 0.1593
worker 1  xfile  [17, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.2026811314665027, 'rmse': 0.2026811314665027, 'mae': 0.1593200347355356, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  52 | activation: relu    | extras: None 
layer 2 | size:  33 | activation: tanh    | extras: dropout - rate: 18.7% 
layer 3 | size:   4 | activation: sigmoid | extras: None 
layer 4 | size:   2 | activation: tanh    | extras: dropout - rate: 47.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4253c4a8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:17 - loss: 0.8621
6384/6530 [============================>.] - ETA: 0s - loss: 0.0432
# training | RMSE: 0.1500, MAE: 0.1155
worker 0  xfile  [16, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37342786095112623}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 76, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22603963012380812}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1500333819545819, 'rmse': 0.1500333819545819, 'mae': 0.11550333414780778, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:   8 | activation: sigmoid | extras: dropout - rate: 15.7% 
layer 2 | size:  77 | activation: tanh    | extras: dropout - rate: 39.0% 
layer 3 | size:  69 | activation: sigmoid | extras: dropout - rate: 45.8% 
layer 4 | size:   4 | activation: tanh    | extras: batchnorm 
layer 5 | size:  74 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b51b5eb70>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 1:00 - loss: 0.8019
 864/6530 [==>...........................] - ETA: 2s - loss: 0.6221  
1152/6530 [====>.........................] - ETA: 3s - loss: 0.4423  
6530/6530 [==============================] - 1s 182us/step - loss: 0.0430 - val_loss: 0.2454
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0495
1664/6530 [======>.......................] - ETA: 1s - loss: 0.4893
2176/6530 [========>.....................] - ETA: 1s - loss: 0.3388
 320/6530 [>.............................] - ETA: 1s - loss: 0.0413
2496/6530 [==========>...................] - ETA: 0s - loss: 0.4126
3136/6530 [=============>................] - ETA: 0s - loss: 0.2732
 624/6530 [=>............................] - ETA: 1s - loss: 0.0420
3328/6530 [==============>...............] - ETA: 0s - loss: 0.3627
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2218
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0422
4160/6530 [==================>...........] - ETA: 0s - loss: 0.3315
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1906
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0410
4992/6530 [=====================>........] - ETA: 0s - loss: 0.3084
6400/6530 [============================>.] - ETA: 0s - loss: 0.1695
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0397
5824/6530 [=========================>....] - ETA: 0s - loss: 0.2901
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0399
6530/6530 [==============================] - 1s 154us/step - loss: 0.1675 - val_loss: 0.0628
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0701
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0396
6530/6530 [==============================] - 1s 129us/step - loss: 0.2767 - val_loss: 0.1651
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2082
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0652
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0393
 864/6530 [==>...........................] - ETA: 0s - loss: 0.1738
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0663
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0388
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1678
3264/6530 [=============>................] - ETA: 0s - loss: 0.0654
3040/6530 [============>.................] - ETA: 0s - loss: 0.0378
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1670
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0642
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0380
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1642
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0638
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0383
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1620
3952/6530 [=================>............] - ETA: 0s - loss: 0.0383
6530/6530 [==============================] - 0s 50us/step - loss: 0.0630 - val_loss: 0.0459
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0633
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1609
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0381
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0593
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1599
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0383
6530/6530 [==============================] - 0s 61us/step - loss: 0.1591 - val_loss: 0.1553
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1750
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0574
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0382
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1553
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0571
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0381
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1469
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0563
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0381
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1466
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0559
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0380
3168/6530 [=============>................] - ETA: 0s - loss: 0.1452
6528/6530 [============================>.] - ETA: 0s - loss: 0.0554
6530/6530 [==============================] - 0s 50us/step - loss: 0.0554 - val_loss: 0.0466

6032/6530 [==========================>...] - ETA: 0s - loss: 0.0378
4064/6530 [=================>............] - ETA: 0s - loss: 0.1455
6320/6530 [============================>.] - ETA: 0s - loss: 0.0378
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1446
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1438
6530/6530 [==============================] - 1s 177us/step - loss: 0.0377 - val_loss: 0.0372

6530/6530 [==============================] - 0s 63us/step - loss: 0.1429 - val_loss: 0.1376

# training | RMSE: 0.2175, MAE: 0.1761
worker 0  xfile  [18, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15744312117713355}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38983627727809533}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45842520572362544}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2174909258067743, 'rmse': 0.2174909258067743, 'mae': 0.17606964892630997, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size: 100 | activation: tanh    | extras: None 
layer 2 | size:  34 | activation: relu    | extras: dropout - rate: 34.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b50d52ac8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:19 - loss: 1.9994
 864/6530 [==>...........................] - ETA: 2s - loss: 0.5416  
1632/6530 [======>.......................] - ETA: 1s - loss: 0.3240
2592/6530 [==========>...................] - ETA: 0s - loss: 0.2278
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1771
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1474
# training | RMSE: 0.1916, MAE: 0.1562
worker 2  xfile  [15, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3383217620683511}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.19157164924974993, 'rmse': 0.19157164924974993, 'mae': 0.15624950407801758, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  86 | activation: sigmoid | extras: dropout - rate: 34.9% 
layer 2 | size:  21 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e90228c50>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 16s - loss: 0.5499
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1279
3712/6530 [================>.............] - ETA: 0s - loss: 0.1812 
6530/6530 [==============================] - 1s 122us/step - loss: 0.1184 - val_loss: 0.0467
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0333
6530/6530 [==============================] - 0s 74us/step - loss: 0.1330 - val_loss: 0.0650
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0739
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0366
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0662
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0359
6528/6530 [============================>.] - ETA: 0s - loss: 0.0654
6530/6530 [==============================] - 0s 17us/step - loss: 0.0654 - val_loss: 0.0606
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0667
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0346
3200/6530 [=============>................] - ETA: 0s - loss: 0.0624
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0332
6530/6530 [==============================] - 0s 16us/step - loss: 0.0615 - val_loss: 0.0557

5536/6530 [========================>.....] - ETA: 0s - loss: 0.0324
# training | RMSE: 0.1720, MAE: 0.1307
worker 1  xfile  [19, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18748774590944872}, 'layer_2_size': 33, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4715085985201116}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.172007863096466, 'rmse': 0.172007863096466, 'mae': 0.13074627758220325, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  30 | activation: relu    | extras: batchnorm 
layer 2 | size:  10 | activation: relu    | extras: None 
layer 3 | size:  82 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b421c8be0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 14s - loss: 0.7161
6530/6530 [==============================] - 0s 48us/step - loss: 0.0319 - val_loss: 0.0381
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0304
4864/6530 [=====================>........] - ETA: 0s - loss: 0.4948 
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0250
6530/6530 [==============================] - 1s 109us/step - loss: 0.4163 - val_loss: 0.1842
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1614
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0251
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1484
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0243
6530/6530 [==============================] - 0s 12us/step - loss: 0.1416 - val_loss: 0.2097
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1310
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0234
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1253
6530/6530 [==============================] - 0s 13us/step - loss: 0.1241 - val_loss: 0.1109

5568/6530 [========================>.....] - ETA: 0s - loss: 0.0230
6530/6530 [==============================] - 0s 48us/step - loss: 0.0228 - val_loss: 0.0292

# training | RMSE: 0.2382, MAE: 0.1945
worker 2  xfile  [22, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34888354859047094}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3161346222267394}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.23818820870326918, 'rmse': 0.23818820870326918, 'mae': 0.1944502496117888, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  83 | activation: sigmoid | extras: None 
layer 2 | size:  88 | activation: tanh    | extras: dropout - rate: 42.6% 
layer 3 | size:  68 | activation: tanh    | extras: dropout - rate: 34.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e4c73ff98>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:08 - loss: 0.4868
 832/6530 [==>...........................] - ETA: 2s - loss: 0.1350  
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1066
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0944
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0870
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0817
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0783
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0759
# training | RMSE: 0.1355, MAE: 0.1069
worker 1  xfile  [21, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4707528622365418}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.1355359015746394, 'rmse': 0.1355359015746394, 'mae': 0.10694037149551805, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  39 | activation: sigmoid | extras: None 
layer 2 | size:  11 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b5065d160>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 40s - loss: 0.4934
6530/6530 [==============================] - 1s 124us/step - loss: 0.0734 - val_loss: 0.0482
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0701
1984/6530 [========>.....................] - ETA: 1s - loss: 0.2811 
# training | RMSE: 0.1620, MAE: 0.1262
worker 0  xfile  [20, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34901075954835103}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14204003070746826}, 'layer_3_size': 97, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4272710545995406}, 'layer_5_size': 30, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.1620272494368415, 'rmse': 0.1620272494368415, 'mae': 0.12620944941884674, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  40 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b50d52978>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 9s - loss: 0.9808
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0554
3776/6530 [================>.............] - ETA: 0s - loss: 0.2441
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0501
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2280
6530/6530 [==============================] - 0s 72us/step - loss: 0.5687 - val_loss: 0.2980
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2974
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0497
6530/6530 [==============================] - 0s 6us/step - loss: 0.2225 - val_loss: 0.1937
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1899
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0483
6530/6530 [==============================] - 1s 99us/step - loss: 0.2223 - val_loss: 0.1876
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2062
6530/6530 [==============================] - 0s 6us/step - loss: 0.1885 - val_loss: 0.1832

4448/6530 [===================>..........] - ETA: 0s - loss: 0.0480
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1862
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0481
3840/6530 [================>.............] - ETA: 0s - loss: 0.1805
6336/6530 [============================>.] - ETA: 0s - loss: 0.0480
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1774
6530/6530 [==============================] - 0s 59us/step - loss: 0.0479 - val_loss: 0.0415
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0530
6530/6530 [==============================] - 0s 29us/step - loss: 0.1758 - val_loss: 0.1671
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1824
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0480
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1646
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0458
3968/6530 [=================>............] - ETA: 0s - loss: 0.1632
2880/6530 [============>.................] - ETA: 0s - loss: 0.0459
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1633
6530/6530 [==============================] - 0s 27us/step - loss: 0.1627 - val_loss: 0.1986

3840/6530 [================>.............] - ETA: 0s - loss: 0.0455
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0459
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0459
6496/6530 [============================>.] - ETA: 0s - loss: 0.0459
# training | RMSE: 0.2368, MAE: 0.1974
worker 1  xfile  [24, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 27, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3071854271732375}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.23684830479593494, 'rmse': 0.23684830479593494, 'mae': 0.19739683607277023, 'early_stop': False}
vggnet done  1

6530/6530 [==============================] - 0s 57us/step - loss: 0.0458 - val_loss: 0.0417

# training | RMSE: 0.2035, MAE: 0.1652
worker 2  xfile  [23, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42559233622805637}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.34859741654439647}, 'layer_3_size': 68, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1345103654255286}, 'layer_4_size': 53, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.20354279396364947, 'rmse': 0.20354279396364947, 'mae': 0.16519404025916434, 'early_stop': False}
vggnet done  2

# training | RMSE: 0.2251, MAE: 0.1828
worker 0  xfile  [25, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 76, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.39218689546565366}, 'layer_3_size': 22, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 6, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.22514713635019856, 'rmse': 0.22514713635019856, 'mae': 0.18276126819233188, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  83 | activation: tanh    | extras: None 
layer 2 | size:  37 | activation: relu    | extras: batchnorm 
layer 3 | size:  49 | activation: relu    | extras: None 
layer 4 | size:  50 | activation: tanh    | extras: dropout - rate: 33.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b50af9320>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 13s - loss: 0.9638
3584/6530 [===============>..............] - ETA: 0s - loss: 0.3760 
6530/6530 [==============================] - 1s 116us/step - loss: 0.3117 - val_loss: 0.2517
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2197
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2044
6530/6530 [==============================] - 0s 16us/step - loss: 0.1970 - val_loss: 0.2281
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2342
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1801
6530/6530 [==============================] - 0s 18us/step - loss: 0.1754 - val_loss: 0.1807

# training | RMSE: 0.2181, MAE: 0.1724
worker 0  xfile  [26, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.33740022211157467}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2181057522323223, 'rmse': 0.2181057522323223, 'mae': 0.17242729776358842, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=3.0 loss={'loss': 0.1985243965995024, 'rmse': 0.1985243965995024, 'mae': 0.15929453449944772, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 9, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18742034528455595}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.151091328910066}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}
#3 epoch=3.0 loss={'loss': 0.27620603293845836, 'rmse': 0.27620603293845836, 'mae': 0.22401436892441598, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 9, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1843222787075455}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#1 epoch=3.0 loss={'loss': 0.19538084759011642, 'rmse': 0.19538084759011642, 'mae': 0.15654468077120473, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46455360283287184}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.35005484230469486}, 'layer_2_size': 56, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1103615171101521}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#2 epoch=3.0 loss={'loss': 0.10085561998038037, 'rmse': 0.10085561998038037, 'mae': 0.07893742653284587, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 88, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42338333595907873}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#4 epoch=3.0 loss={'loss': 0.16035272748052612, 'rmse': 0.16035272748052612, 'mae': 0.1262275192768203, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41928288588536755}, 'layer_3_size': 26, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#5 epoch=3.0 loss={'loss': 0.14268407762980503, 'rmse': 0.14268407762980503, 'mae': 0.10968092201992242, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4868076109733872}, 'layer_2_size': 76, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.49502287953085233}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22225089583039181}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#7 epoch=3.0 loss={'loss': 0.19992408764795153, 'rmse': 0.19992408764795153, 'mae': 0.15554816729526266, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27116531123903675}, 'layer_2_size': 63, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.37972466777583236}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#6 epoch=3.0 loss={'loss': 0.19220691600278755, 'rmse': 0.19220691600278755, 'mae': 0.1498364947171333, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 7, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2591666001901206}, 'layer_3_size': 31, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1166779665150942}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#8 epoch=3.0 loss={'loss': 0.1628601088045768, 'rmse': 0.1628601088045768, 'mae': 0.1280336674942376, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#9 epoch=3.0 loss={'loss': 0.15878221465631842, 'rmse': 0.15878221465631842, 'mae': 0.12091582632183993, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18421503775253176}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#12 epoch=3.0 loss={'loss': 0.4040108182583054, 'rmse': 0.4040108182583054, 'mae': 0.33437753948073407, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36704401178535906}, 'layer_3_size': 24, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10937773525521029}, 'layer_4_size': 52, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.49007862288269577}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#11 epoch=3.0 loss={'loss': 0.3092455147570381, 'rmse': 0.3092455147570381, 'mae': 0.24626761500334035, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13463193799911438}, 'layer_2_size': 22, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2175209473469314}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#10 epoch=3.0 loss={'loss': 0.19929943705187209, 'rmse': 0.19929943705187209, 'mae': 0.15626503982931486, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#13 epoch=3.0 loss={'loss': 0.22648029468489184, 'rmse': 0.22648029468489184, 'mae': 0.18406781405565828, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24545880961748656}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#14 epoch=3.0 loss={'loss': 0.24176014503754995, 'rmse': 0.24176014503754995, 'mae': 0.19745337471806157, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40849932833172253}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#16 epoch=3.0 loss={'loss': 0.1500333819545819, 'rmse': 0.1500333819545819, 'mae': 0.11550333414780778, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37342786095112623}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 76, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22603963012380812}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#17 epoch=3.0 loss={'loss': 0.2026811314665027, 'rmse': 0.2026811314665027, 'mae': 0.1593200347355356, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#18 epoch=3.0 loss={'loss': 0.2174909258067743, 'rmse': 0.2174909258067743, 'mae': 0.17606964892630997, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15744312117713355}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38983627727809533}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45842520572362544}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#19 epoch=3.0 loss={'loss': 0.172007863096466, 'rmse': 0.172007863096466, 'mae': 0.13074627758220325, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18748774590944872}, 'layer_2_size': 33, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4715085985201116}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#15 epoch=3.0 loss={'loss': 0.19157164924974993, 'rmse': 0.19157164924974993, 'mae': 0.15624950407801758, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3383217620683511}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#22 epoch=3.0 loss={'loss': 0.23818820870326918, 'rmse': 0.23818820870326918, 'mae': 0.1944502496117888, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34888354859047094}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3161346222267394}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#21 epoch=3.0 loss={'loss': 0.1355359015746394, 'rmse': 0.1355359015746394, 'mae': 0.10694037149551805, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4707528622365418}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#20 epoch=3.0 loss={'loss': 0.1620272494368415, 'rmse': 0.1620272494368415, 'mae': 0.12620944941884674, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34901075954835103}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14204003070746826}, 'layer_3_size': 97, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4272710545995406}, 'layer_5_size': 30, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#25 epoch=3.0 loss={'loss': 0.22514713635019856, 'rmse': 0.22514713635019856, 'mae': 0.18276126819233188, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 76, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.39218689546565366}, 'layer_3_size': 22, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 6, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#24 epoch=3.0 loss={'loss': 0.23684830479593494, 'rmse': 0.23684830479593494, 'mae': 0.19739683607277023, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 27, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3071854271732375}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#23 epoch=3.0 loss={'loss': 0.20354279396364947, 'rmse': 0.20354279396364947, 'mae': 0.16519404025916434, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42559233622805637}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.34859741654439647}, 'layer_3_size': 68, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1345103654255286}, 'layer_4_size': 53, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#26 epoch=3.0 loss={'loss': 0.2181057522323223, 'rmse': 0.2181057522323223, 'mae': 0.17242729776358842, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.33740022211157467}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
get a list [results] of length 108
get a list [loss] of length 27
get a list [val_loss] of length 27
length of indices is [ 3 21  5 15  9  4 22  8 18 19  7  2  0 12  6 16 25 17 26 23 13 24 20 14
  1 11 10]
length of indices is 27
length of T is 27
newly formed T structure is:[[0, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 9, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1843222787075455}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [1, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4707528622365418}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [2, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4868076109733872}, 'layer_2_size': 76, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.49502287953085233}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22225089583039181}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [3, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3383217620683511}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [4, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18421503775253176}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [5, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41928288588536755}, 'layer_3_size': 26, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [6, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34888354859047094}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3161346222267394}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [7, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [8, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15744312117713355}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38983627727809533}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45842520572362544}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]] 

*** 9.0 configurations x 9.0 iterations each

27 | Thu Sep 27 18:39:05 2018 | lowest loss so far: 0.1009 (run 2)

{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  89 | activation: sigmoid | extras: None 
layer 2 | size:   9 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 21s - loss: 0.5373{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  13 | activation: relu    | extras: batchnorm 
layer 2 | size:  76 | activation: relu    | extras: dropout - rate: 48.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 5:51 - loss: 1.2318
6144/6530 [===========================>..] - ETA: 0s - loss: 0.4091 
6530/6530 [==============================] - 1s 142us/step - loss: 0.4005 - val_loss: 0.2767
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2840
 352/6530 [>.............................] - ETA: 16s - loss: 0.4569 
6530/6530 [==============================] - 0s 8us/step - loss: 0.1878 - val_loss: 0.1189
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1230
 736/6530 [==>...........................] - ETA: 7s - loss: 0.2796 
6530/6530 [==============================] - 0s 7us/step - loss: 0.0901 - val_loss: 0.0727
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0747
1104/6530 [====>.........................] - ETA: 4s - loss: 0.2126
6530/6530 [==============================] - 0s 7us/step - loss: 0.0697 - val_loss: 0.0673
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0685
1472/6530 [=====>........................] - ETA: 3s - loss: 0.1739
6530/6530 [==============================] - 0s 7us/step - loss: 0.0678 - val_loss: 0.0666
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0676
1872/6530 [=======>......................] - ETA: 2s - loss: 0.1498
6530/6530 [==============================] - 0s 6us/step - loss: 0.0671 - val_loss: 0.0660
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0670
2240/6530 [=========>....................] - ETA: 2s - loss: 0.1340
6530/6530 [==============================] - 0s 7us/step - loss: 0.0663 - val_loss: 0.0653
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0663
2640/6530 [===========>..................] - ETA: 1s - loss: 0.1234
6530/6530 [==============================] - 0s 7us/step - loss: 0.0655 - val_loss: 0.0645
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0655{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  30 | activation: relu    | extras: batchnorm 
layer 2 | size:  10 | activation: relu    | extras: None 
layer 3 | size:  82 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 23s - loss: 0.6922
6530/6530 [==============================] - 0s 7us/step - loss: 0.0646 - val_loss: 0.0637

3008/6530 [============>.................] - ETA: 1s - loss: 0.1140
4608/6530 [====================>.........] - ETA: 0s - loss: 0.5112 
3408/6530 [==============>...............] - ETA: 1s - loss: 0.1062
6530/6530 [==============================] - 1s 164us/step - loss: 0.4148 - val_loss: 0.1918
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1531
3744/6530 [================>.............] - ETA: 1s - loss: 0.1010
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1366
6530/6530 [==============================] - 0s 12us/step - loss: 0.1319 - val_loss: 0.1295
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1358
4128/6530 [=================>............] - ETA: 0s - loss: 0.0969
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1068
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0933
6530/6530 [==============================] - 0s 11us/step - loss: 0.1059 - val_loss: 0.1233
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1430
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0894
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1056
6530/6530 [==============================] - 0s 12us/step - loss: 0.1065 - val_loss: 0.0998
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1060
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0863
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0992
6530/6530 [==============================] - 0s 11us/step - loss: 0.1014 - val_loss: 0.0955

5648/6530 [========================>.....] - ETA: 0s - loss: 0.0834Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0899
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0811
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0919
6530/6530 [==============================] - 0s 12us/step - loss: 0.0953 - val_loss: 0.0961
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0901
6464/6530 [============================>.] - ETA: 0s - loss: 0.0788
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0919
6530/6530 [==============================] - 0s 12us/step - loss: 0.0918 - val_loss: 0.1121
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0835
6530/6530 [==============================] - 2s 275us/step - loss: 0.0786 - val_loss: 0.0418
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0347
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0859
 448/6530 [=>............................] - ETA: 0s - loss: 0.0436
6530/6530 [==============================] - 0s 12us/step - loss: 0.0844 - val_loss: 0.1123
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0865
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0457
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0927
6530/6530 [==============================] - 0s 12us/step - loss: 0.0935 - val_loss: 0.1301

1248/6530 [====>.........................] - ETA: 0s - loss: 0.0445
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0430
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0428
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0431
2880/6530 [============>.................] - ETA: 0s - loss: 0.0428
3264/6530 [=============>................] - ETA: 0s - loss: 0.0421
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0419
# training | RMSE: 0.2530, MAE: 0.2072
worker 0  xfile  [0, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 9, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1843222787075455}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.25299882796219, 'rmse': 0.25299882796219, 'mae': 0.2072108252135767, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  46 | activation: relu    | extras: dropout - rate: 33.8% 
layer 2 | size:  79 | activation: sigmoid | extras: None 
layer 3 | size:  69 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  52 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e90161eb8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 2:38 - loss: 1.0853
4048/6530 [=================>............] - ETA: 0s - loss: 0.0421
 320/6530 [>.............................] - ETA: 8s - loss: 0.4349  
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0418
 640/6530 [=>............................] - ETA: 4s - loss: 0.2627
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0415
 960/6530 [===>..........................] - ETA: 3s - loss: 0.1951
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0410
1280/6530 [====>.........................] - ETA: 2s - loss: 0.1606
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0407
1600/6530 [======>.......................] - ETA: 1s - loss: 0.1387
1904/6530 [=======>......................] - ETA: 1s - loss: 0.1250
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0410
6384/6530 [============================>.] - ETA: 0s - loss: 0.0406
2224/6530 [=========>....................] - ETA: 1s - loss: 0.1139
6530/6530 [==============================] - 1s 133us/step - loss: 0.0406 - val_loss: 0.0346

2544/6530 [==========>...................] - ETA: 1s - loss: 0.1067Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0317
2864/6530 [============>.................] - ETA: 1s - loss: 0.1004
 432/6530 [>.............................] - ETA: 0s - loss: 0.0349
3152/6530 [=============>................] - ETA: 0s - loss: 0.0953
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0381
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0917
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0370
# training | RMSE: 0.1434, MAE: 0.1203
worker 1  xfile  [1, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4707528622365418}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.14339793922537744, 'rmse': 0.14339793922537744, 'mae': 0.12028895550884779, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  14 | activation: relu    | extras: None 
layer 2 | size:  11 | activation: relu    | extras: batchnorm 
layer 3 | size:  67 | activation: tanh    | extras: None 
layer 4 | size:  40 | activation: sigmoid | extras: None 
layer 5 | size:  77 | activation: relu    | extras: dropout - rate: 18.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e9131d320>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 18s - loss: 0.7258
3792/6530 [================>.............] - ETA: 0s - loss: 0.0883
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0361
2560/6530 [==========>...................] - ETA: 0s - loss: 0.3306 
4096/6530 [=================>............] - ETA: 0s - loss: 0.0859
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0360
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2484
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0830
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0365
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0809
2848/6530 [============>.................] - ETA: 0s - loss: 0.0361
6530/6530 [==============================] - 1s 86us/step - loss: 0.2257 - val_loss: 0.1515
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1388
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0790
3216/6530 [=============>................] - ETA: 0s - loss: 0.0356
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1413
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0774
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0356
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1376
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0757
3968/6530 [=================>............] - ETA: 0s - loss: 0.0356
6530/6530 [==============================] - 0s 23us/step - loss: 0.1344 - val_loss: 0.1428
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1272
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0741
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0353
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1274
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0728
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0352
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1248
6480/6530 [============================>.] - ETA: 0s - loss: 0.0717
6530/6530 [==============================] - 0s 23us/step - loss: 0.1224 - val_loss: 0.1230

5136/6530 [======================>.......] - ETA: 0s - loss: 0.0347Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1561
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0345
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1152
6530/6530 [==============================] - 2s 236us/step - loss: 0.0714 - val_loss: 0.0554
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0383
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0345
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1119
 336/6530 [>.............................] - ETA: 1s - loss: 0.0410
6530/6530 [==============================] - 0s 22us/step - loss: 0.1100 - val_loss: 0.1933
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1773
6336/6530 [============================>.] - ETA: 0s - loss: 0.0343
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0438
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1008
6530/6530 [==============================] - 1s 134us/step - loss: 0.0344 - val_loss: 0.0288
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0234
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0446
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0999
 400/6530 [>.............................] - ETA: 0s - loss: 0.0308
6530/6530 [==============================] - 0s 22us/step - loss: 0.1004 - val_loss: 0.1493
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1370
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0445
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0329
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0436
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0948
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0320
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0427
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0940
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0313
6530/6530 [==============================] - 0s 22us/step - loss: 0.0936 - val_loss: 0.1756
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1533
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0426
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0312
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0946
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0432
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0312
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0932
2832/6530 [============>.................] - ETA: 0s - loss: 0.0433
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0312
6530/6530 [==============================] - 0s 22us/step - loss: 0.0928 - val_loss: 0.1559
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1250
3104/6530 [=============>................] - ETA: 0s - loss: 0.0426
3200/6530 [=============>................] - ETA: 0s - loss: 0.0306
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0885
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0426
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0306
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0878
3744/6530 [================>.............] - ETA: 0s - loss: 0.0425
6530/6530 [==============================] - 0s 22us/step - loss: 0.0894 - val_loss: 0.1625

4000/6530 [=================>............] - ETA: 0s - loss: 0.0306
4048/6530 [=================>............] - ETA: 0s - loss: 0.0425
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0304
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0421
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0304
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0420
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0301
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0417
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0299
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0419
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0299
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0418
6448/6530 [============================>.] - ETA: 0s - loss: 0.0297
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0417
6530/6530 [==============================] - 1s 132us/step - loss: 0.0298 - val_loss: 0.0240
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0198
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0415
 416/6530 [>.............................] - ETA: 0s - loss: 0.0262
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0290
6530/6530 [==============================] - 1s 168us/step - loss: 0.0412 - val_loss: 0.1832
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0391
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0286
 304/6530 [>.............................] - ETA: 1s - loss: 0.0390
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0282
 592/6530 [=>............................] - ETA: 1s - loss: 0.0420
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0281
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0411
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0282
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0403
2880/6530 [============>.................] - ETA: 0s - loss: 0.0275
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0388
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0274
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0384
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0273
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0378
4080/6530 [=================>............] - ETA: 0s - loss: 0.0274
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0376
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0275
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0372
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0274
3136/6530 [=============>................] - ETA: 0s - loss: 0.0364
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0272
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0366
# training | RMSE: 0.1844, MAE: 0.1579
worker 1  xfile  [4, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18421503775253176}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.18440718677994616, 'rmse': 0.18440718677994616, 'mae': 0.157871921737146, 'early_stop': True}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  28 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  62 | activation: relu    | extras: None 
layer 3 | size:  26 | activation: sigmoid | extras: dropout - rate: 41.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e902b4438>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 1:41 - loss: 1.0036
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0271
3760/6530 [================>.............] - ETA: 0s - loss: 0.0366
 608/6530 [=>............................] - ETA: 5s - loss: 0.5025  
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0270
4032/6530 [=================>............] - ETA: 0s - loss: 0.0371
1280/6530 [====>.........................] - ETA: 2s - loss: 0.2794
6416/6530 [============================>.] - ETA: 0s - loss: 0.0268
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0365
6530/6530 [==============================] - 1s 132us/step - loss: 0.0270 - val_loss: 0.0218

1920/6530 [=======>......................] - ETA: 1s - loss: 0.2031Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0198
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0366
 384/6530 [>.............................] - ETA: 0s - loss: 0.0235
2560/6530 [==========>...................] - ETA: 1s - loss: 0.1649
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0366
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0270
3200/6530 [=============>................] - ETA: 0s - loss: 0.1407
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0366
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0275
3808/6530 [================>.............] - ETA: 0s - loss: 0.1253
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0365
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0265
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1134
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0367
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0268
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1040
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0365
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0966
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0264
6528/6530 [============================>.] - ETA: 0s - loss: 0.0363
6368/6530 [============================>.] - ETA: 0s - loss: 0.0910
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0263
6530/6530 [==============================] - 1s 171us/step - loss: 0.0363 - val_loss: 0.0410
Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0200
3152/6530 [=============>................] - ETA: 0s - loss: 0.0257
 336/6530 [>.............................] - ETA: 1s - loss: 0.0302
6530/6530 [==============================] - 1s 166us/step - loss: 0.0896 - val_loss: 0.0345
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0359
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0259
 640/6530 [=>............................] - ETA: 0s - loss: 0.0325
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0347
3888/6530 [================>.............] - ETA: 0s - loss: 0.0257
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0337
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0327
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0257
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0343
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0322
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0259
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0331
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0327
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0257
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0333
3136/6530 [=============>................] - ETA: 0s - loss: 0.0323
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0256
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0333
3776/6530 [================>.............] - ETA: 0s - loss: 0.0321
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0255
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0340
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0318
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0254
2832/6530 [============>.................] - ETA: 0s - loss: 0.0341
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0316
3152/6530 [=============>................] - ETA: 0s - loss: 0.0337
6530/6530 [==============================] - 1s 135us/step - loss: 0.0254 - val_loss: 0.0207
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0190
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0312
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0342
 416/6530 [>.............................] - ETA: 0s - loss: 0.0233
6336/6530 [============================>.] - ETA: 0s - loss: 0.0311
6530/6530 [==============================] - 1s 84us/step - loss: 0.0309 - val_loss: 0.0248
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0276
3792/6530 [================>.............] - ETA: 0s - loss: 0.0338
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0260
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0243
4096/6530 [=================>............] - ETA: 0s - loss: 0.0340
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0258
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0235
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0338
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0256
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0232
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0337
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0255
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0240
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0334
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0253
3264/6530 [=============>................] - ETA: 0s - loss: 0.0233
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0333
2848/6530 [============>.................] - ETA: 0s - loss: 0.0248
3840/6530 [================>.............] - ETA: 0s - loss: 0.0233
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0332
3248/6530 [=============>................] - ETA: 0s - loss: 0.0245
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0231
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0333
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0247
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0229
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0332
4016/6530 [=================>............] - ETA: 0s - loss: 0.0247
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0227
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0247
6530/6530 [==============================] - 1s 170us/step - loss: 0.0330 - val_loss: 0.0542
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0254
6400/6530 [============================>.] - ETA: 0s - loss: 0.0227
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0248
6530/6530 [==============================] - 1s 83us/step - loss: 0.0226 - val_loss: 0.0180
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0254
 336/6530 [>.............................] - ETA: 1s - loss: 0.0281
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0246
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0196
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0308
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0245
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0188
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0313
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0244
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0183
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0308
6400/6530 [============================>.] - ETA: 0s - loss: 0.0242
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0189
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0299
6530/6530 [==============================] - 1s 133us/step - loss: 0.0244 - val_loss: 0.0199
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0183
3232/6530 [=============>................] - ETA: 0s - loss: 0.0186
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0296
 416/6530 [>.............................] - ETA: 0s - loss: 0.0226
3872/6530 [================>.............] - ETA: 0s - loss: 0.0188
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0299
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0251
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0187
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0303
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0251
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0187
2864/6530 [============>.................] - ETA: 0s - loss: 0.0303
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0244
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0187
3184/6530 [=============>................] - ETA: 0s - loss: 0.0300
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0248
6432/6530 [============================>.] - ETA: 0s - loss: 0.0188
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0303
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0243
6530/6530 [==============================] - 1s 83us/step - loss: 0.0187 - val_loss: 0.0158
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0247
3808/6530 [================>.............] - ETA: 0s - loss: 0.0301
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0242
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0179
4128/6530 [=================>............] - ETA: 0s - loss: 0.0304
3168/6530 [=============>................] - ETA: 0s - loss: 0.0237
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0171
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0304
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0241
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0166
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0304
3872/6530 [================>.............] - ETA: 0s - loss: 0.0238
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0169
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0302
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0239
3232/6530 [=============>................] - ETA: 0s - loss: 0.0167
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0302
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0241
3872/6530 [================>.............] - ETA: 0s - loss: 0.0169
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0301
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0239
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0169
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0302
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0238
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0170
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0302
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0237
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0169
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0237
6530/6530 [==============================] - 1s 169us/step - loss: 0.0301 - val_loss: 0.0314

6368/6530 [============================>.] - ETA: 0s - loss: 0.0170Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0332
6530/6530 [==============================] - 1s 84us/step - loss: 0.0170 - val_loss: 0.0150
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0245
 320/6530 [>.............................] - ETA: 1s - loss: 0.0256
6530/6530 [==============================] - 1s 136us/step - loss: 0.0237 - val_loss: 0.0196
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0170
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0167
 640/6530 [=>............................] - ETA: 0s - loss: 0.0267
 432/6530 [>.............................] - ETA: 0s - loss: 0.0225
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0161
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0274
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0246
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0154
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0285
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0244
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0157
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0274
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0240
3264/6530 [=============>................] - ETA: 0s - loss: 0.0157
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0239
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0275
3904/6530 [================>.............] - ETA: 0s - loss: 0.0158
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0236
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0282
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0158
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0234
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0283
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0158
3184/6530 [=============>................] - ETA: 0s - loss: 0.0230
2848/6530 [============>.................] - ETA: 0s - loss: 0.0284
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0158
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0233
3168/6530 [=============>................] - ETA: 0s - loss: 0.0279
6464/6530 [============================>.] - ETA: 0s - loss: 0.0159
3952/6530 [=================>............] - ETA: 0s - loss: 0.0233
6530/6530 [==============================] - 1s 82us/step - loss: 0.0159 - val_loss: 0.0145

3488/6530 [===============>..............] - ETA: 0s - loss: 0.0282Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0244
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0232
3808/6530 [================>.............] - ETA: 0s - loss: 0.0281
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0159
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0234
4128/6530 [=================>............] - ETA: 0s - loss: 0.0287
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0152
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0232
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0286
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0146
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0231
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0149
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0286
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0231
3264/6530 [=============>................] - ETA: 0s - loss: 0.0149
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0285
6352/6530 [============================>.] - ETA: 0s - loss: 0.0230
3904/6530 [================>.............] - ETA: 0s - loss: 0.0150
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0285
6530/6530 [==============================] - 1s 133us/step - loss: 0.0231 - val_loss: 0.0191

4544/6530 [===================>..........] - ETA: 0s - loss: 0.0150
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0283
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0149
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0283
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0149
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0284
6464/6530 [============================>.] - ETA: 0s - loss: 0.0151
6530/6530 [==============================] - 1s 170us/step - loss: 0.0284 - val_loss: 0.0326
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0271
6530/6530 [==============================] - 1s 83us/step - loss: 0.0151 - val_loss: 0.0140
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0242
 336/6530 [>.............................] - ETA: 1s - loss: 0.0248
 640/6530 [=>............................] - ETA: 0s - loss: 0.0154
 640/6530 [=>............................] - ETA: 0s - loss: 0.0261
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0146
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0258
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0140
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0282
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0142
3168/6530 [=============>................] - ETA: 0s - loss: 0.0141
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0279
3840/6530 [================>.............] - ETA: 0s - loss: 0.0143
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0270
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0143
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0267
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0269
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0143
2912/6530 [============>.................] - ETA: 0s - loss: 0.0267
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0143
3248/6530 [=============>................] - ETA: 0s - loss: 0.0268
6400/6530 [============================>.] - ETA: 0s - loss: 0.0144
6530/6530 [==============================] - 1s 83us/step - loss: 0.0144 - val_loss: 0.0135
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0240
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0271
# training | RMSE: 0.1324, MAE: 0.1037
worker 2  xfile  [2, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4868076109733872}, 'layer_2_size': 76, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.49502287953085233}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22225089583039181}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.1324079386157688, 'rmse': 0.1324079386157688, 'mae': 0.10365948998212653, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  86 | activation: sigmoid | extras: dropout - rate: 34.9% 
layer 2 | size:  21 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e9131d3c8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 9s - loss: 0.5573
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0146
3904/6530 [================>.............] - ETA: 0s - loss: 0.0270
3840/6530 [================>.............] - ETA: 0s - loss: 0.1897
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0142
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0270
6530/6530 [==============================] - 0s 47us/step - loss: 0.1394 - val_loss: 0.0782
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0897
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0135
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0268
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0692
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0137
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0267
6530/6530 [==============================] - 0s 16us/step - loss: 0.0668 - val_loss: 0.0627
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0777
3104/6530 [=============>................] - ETA: 0s - loss: 0.0136
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0266
3840/6530 [================>.............] - ETA: 0s - loss: 0.0636
3744/6530 [================>.............] - ETA: 0s - loss: 0.0138
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0265
6530/6530 [==============================] - 0s 15us/step - loss: 0.0611 - val_loss: 0.0615
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0713
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0137
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0264
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0606
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0138
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0263
6530/6530 [==============================] - 0s 15us/step - loss: 0.0583 - val_loss: 0.0522
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0620
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0137
6368/6530 [============================>.] - ETA: 0s - loss: 0.0261
3840/6530 [================>.............] - ETA: 0s - loss: 0.0576
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0138
6530/6530 [==============================] - 0s 15us/step - loss: 0.0562 - val_loss: 0.0695
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0655
6530/6530 [==============================] - 1s 84us/step - loss: 0.0139 - val_loss: 0.0131

6530/6530 [==============================] - 1s 168us/step - loss: 0.0262 - val_loss: 0.0164
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0204
3840/6530 [================>.............] - ETA: 0s - loss: 0.0549
 320/6530 [>.............................] - ETA: 1s - loss: 0.0228
6530/6530 [==============================] - 0s 15us/step - loss: 0.0536 - val_loss: 0.0521
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0525
 624/6530 [=>............................] - ETA: 1s - loss: 0.0270
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0531
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0276
6530/6530 [==============================] - 0s 15us/step - loss: 0.0519 - val_loss: 0.0540
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0588
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0282
3840/6530 [================>.............] - ETA: 0s - loss: 0.0505
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0268
6530/6530 [==============================] - 0s 15us/step - loss: 0.0502 - val_loss: 0.0511
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0585
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0263
3712/6530 [================>.............] - ETA: 0s - loss: 0.0488
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0263
6530/6530 [==============================] - 0s 15us/step - loss: 0.0489 - val_loss: 0.0573

2544/6530 [==========>...................] - ETA: 0s - loss: 0.0261
2896/6530 [============>.................] - ETA: 0s - loss: 0.0258
3248/6530 [=============>................] - ETA: 0s - loss: 0.0260
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0259
3872/6530 [================>.............] - ETA: 0s - loss: 0.0259
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0257
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0258
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0257
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0255
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0255
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0255
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0254
6480/6530 [============================>.] - ETA: 0s - loss: 0.0254
6530/6530 [==============================] - 1s 164us/step - loss: 0.0254 - val_loss: 0.0162
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0335
 320/6530 [>.............................] - ETA: 1s - loss: 0.0237
 592/6530 [=>............................] - ETA: 1s - loss: 0.0235
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0249
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0261
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0245
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0237
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0239
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0242
# training | RMSE: 0.2389, MAE: 0.1966
worker 2  xfile  [6, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34888354859047094}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3161346222267394}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.23889025112779308, 'rmse': 0.23889025112779308, 'mae': 0.1966488145120768, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:   8 | activation: sigmoid | extras: dropout - rate: 15.7% 
layer 2 | size:  77 | activation: tanh    | extras: dropout - rate: 39.0% 
layer 3 | size:  69 | activation: sigmoid | extras: dropout - rate: 45.8% 
layer 4 | size:   4 | activation: tanh    | extras: batchnorm 
layer 5 | size:  74 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e90696128>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 47s - loss: 0.7424
2880/6530 [============>.................] - ETA: 0s - loss: 0.0240
1280/6530 [====>.........................] - ETA: 2s - loss: 0.4053 
# training | RMSE: 0.1072, MAE: 0.0837
worker 1  xfile  [5, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41928288588536755}, 'layer_3_size': 26, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.10718218570984477, 'rmse': 0.10718218570984477, 'mae': 0.08368475982680836, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  68 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  91 | activation: tanh    | extras: batchnorm 
layer 3 | size:  29 | activation: tanh    | extras: batchnorm 
layer 4 | size:  90 | activation: tanh    | extras: None 
layer 5 | size:  57 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e881667b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 15s - loss: 0.6264
3200/6530 [=============>................] - ETA: 0s - loss: 0.0239
2432/6530 [==========>...................] - ETA: 0s - loss: 0.3045
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1553 
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0245
3648/6530 [===============>..............] - ETA: 0s - loss: 0.2351
3824/6530 [================>.............] - ETA: 0s - loss: 0.0247
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1969
6530/6530 [==============================] - 1s 121us/step - loss: 0.1063 - val_loss: 0.0460
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0387
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0245
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1734
3840/6530 [================>.............] - ETA: 0s - loss: 0.0402
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0244
6530/6530 [==============================] - 0s 16us/step - loss: 0.0368 - val_loss: 0.0479

6530/6530 [==============================] - 1s 126us/step - loss: 0.1631 - val_loss: 0.0710
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0878Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0283
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0243
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0767
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0276
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0243
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0748
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0242
6530/6530 [==============================] - 0s 17us/step - loss: 0.0258 - val_loss: 0.0491
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0269
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0241
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0724
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0221
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0241
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0721
6530/6530 [==============================] - 0s 17us/step - loss: 0.0201 - val_loss: 0.0531
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0210
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0242
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0700
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0174
6530/6530 [==============================] - 0s 48us/step - loss: 0.0687 - val_loss: 0.0535
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0620
6530/6530 [==============================] - 0s 16us/step - loss: 0.0181 - val_loss: 0.0439
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0198
6530/6530 [==============================] - 1s 171us/step - loss: 0.0242 - val_loss: 0.0223

1216/6530 [====>.........................] - ETA: 0s - loss: 0.0632
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0155
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0623
6530/6530 [==============================] - 0s 16us/step - loss: 0.0154 - val_loss: 0.0308
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0178
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0615
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0155
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0607
6530/6530 [==============================] - 0s 17us/step - loss: 0.0151 - val_loss: 0.0242
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0153
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0601
3072/6530 [=============>................] - ETA: 0s - loss: 0.0133
# training | RMSE: 0.1447, MAE: 0.1178
worker 0  xfile  [3, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3383217620683511}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.14473760856904502, 'rmse': 0.14473760856904502, 'mae': 0.11782546951597624, 'early_stop': False}
vggnet done  0

6530/6530 [==============================] - 0s 48us/step - loss: 0.0599 - val_loss: 0.0471
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0750
6400/6530 [============================>.] - ETA: 0s - loss: 0.0131
6530/6530 [==============================] - 0s 18us/step - loss: 0.0131 - val_loss: 0.0261
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0127
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0540
3840/6530 [================>.............] - ETA: 0s - loss: 0.0127
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0550
6530/6530 [==============================] - 0s 15us/step - loss: 0.0127 - val_loss: 0.0243

3648/6530 [===============>..............] - ETA: 0s - loss: 0.0541
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0538
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0543
# training | RMSE: 0.1423, MAE: 0.1117
worker 1  xfile  [7, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.14227285996882114, 'rmse': 0.14227285996882114, 'mae': 0.11165900342381585, 'early_stop': False}
vggnet done  1

6530/6530 [==============================] - 0s 45us/step - loss: 0.0544 - val_loss: 0.0488
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0472
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0521
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0515
3968/6530 [=================>............] - ETA: 0s - loss: 0.0531
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0517
6530/6530 [==============================] - 0s 42us/step - loss: 0.0511 - val_loss: 0.0449
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0478
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0502
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0483
3840/6530 [================>.............] - ETA: 0s - loss: 0.0489
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0500
6400/6530 [============================>.] - ETA: 0s - loss: 0.0496
6530/6530 [==============================] - 0s 43us/step - loss: 0.0499 - val_loss: 0.0474
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0594
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0529
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0503
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0511
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0507
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0492
6464/6530 [============================>.] - ETA: 0s - loss: 0.0491
6530/6530 [==============================] - 0s 51us/step - loss: 0.0492 - val_loss: 0.0443
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0603
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0518
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0491
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0482
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0485
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0483
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0488
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0488
6530/6530 [==============================] - 0s 66us/step - loss: 0.0489 - val_loss: 0.0416
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0559
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0479
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0484
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0476
3200/6530 [=============>................] - ETA: 0s - loss: 0.0480
3968/6530 [=================>............] - ETA: 0s - loss: 0.0484
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0488
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0483
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0480
6530/6530 [==============================] - 0s 75us/step - loss: 0.0482 - val_loss: 0.0507

# training | RMSE: 0.2256, MAE: 0.1839
worker 2  xfile  [8, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15744312117713355}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38983627727809533}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45842520572362544}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.22555119768090345, 'rmse': 0.22555119768090345, 'mae': 0.18388252205567618, 'early_stop': False}
vggnet done  2
all of workers have been done
calculation finished
#0 epoch=9.0 loss={'loss': 0.25299882796219, 'rmse': 0.25299882796219, 'mae': 0.2072108252135767, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 9, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1843222787075455}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#1 epoch=9.0 loss={'loss': 0.14339793922537744, 'rmse': 0.14339793922537744, 'mae': 0.12028895550884779, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4707528622365418}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#4 epoch=9.0 loss={'loss': 0.18440718677994616, 'rmse': 0.18440718677994616, 'mae': 0.157871921737146, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18421503775253176}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#2 epoch=9.0 loss={'loss': 0.1324079386157688, 'rmse': 0.1324079386157688, 'mae': 0.10365948998212653, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4868076109733872}, 'layer_2_size': 76, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.49502287953085233}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22225089583039181}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#5 epoch=9.0 loss={'loss': 0.10718218570984477, 'rmse': 0.10718218570984477, 'mae': 0.08368475982680836, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41928288588536755}, 'layer_3_size': 26, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#6 epoch=9.0 loss={'loss': 0.23889025112779308, 'rmse': 0.23889025112779308, 'mae': 0.1966488145120768, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34888354859047094}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3161346222267394}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#3 epoch=9.0 loss={'loss': 0.14473760856904502, 'rmse': 0.14473760856904502, 'mae': 0.11782546951597624, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3383217620683511}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#7 epoch=9.0 loss={'loss': 0.14227285996882114, 'rmse': 0.14227285996882114, 'mae': 0.11165900342381585, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#8 epoch=9.0 loss={'loss': 0.22555119768090345, 'rmse': 0.22555119768090345, 'mae': 0.18388252205567618, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15744312117713355}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38983627727809533}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45842520572362544}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
get a list [results] of length 117
get a list [loss] of length 9
get a list [val_loss] of length 9
length of indices is [4 3 7 1 6 2 8 5 0]
length of indices is 9
length of T is 9
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18421503775253176}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [1, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3383217620683511}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [2, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]] 

*** 3.0 configurations x 27.0 iterations each

9 | Thu Sep 27 18:39:20 2018 | lowest loss so far: 0.1009 (run 2)

{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  14 | activation: relu    | extras: None 
layer 2 | size:  11 | activation: relu    | extras: batchnorm 
layer 3 | size:  67 | activation: tanh    | extras: None 
layer 4 | size:  40 | activation: sigmoid | extras: None 
layer 5 | size:  77 | activation: relu    | extras: dropout - rate: 18.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 128/6530 [..............................] - ETA: 48s - loss: 0.7404
2176/6530 [========>.....................] - ETA: 2s - loss: 0.3531 
4096/6530 [=================>............] - ETA: 0s - loss: 0.2643{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  46 | activation: relu    | extras: dropout - rate: 33.8% 
layer 2 | size:  79 | activation: sigmoid | extras: None 
layer 3 | size:  69 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  52 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 6:44 - loss: 0.6694
6400/6530 [============================>.] - ETA: 0s - loss: 0.2243
 336/6530 [>.............................] - ETA: 19s - loss: 0.3726 
6530/6530 [==============================] - 1s 180us/step - loss: 0.2233 - val_loss: 0.2217
Epoch 2/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1738
 624/6530 [=>............................] - ETA: 10s - loss: 0.2390
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1426
 944/6530 [===>..........................] - ETA: 6s - loss: 0.1773 
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1388
6530/6530 [==============================] - 0s 23us/step - loss: 0.1370 - val_loss: 0.1725
Epoch 3/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1247
1248/6530 [====>.........................] - ETA: 5s - loss: 0.1475
1568/6530 [======>.......................] - ETA: 3s - loss: 0.1269
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1226
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1237
1888/6530 [=======>......................] - ETA: 3s - loss: 0.1150
6530/6530 [==============================] - 0s 22us/step - loss: 0.1226 - val_loss: 0.1279
Epoch 4/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1070
2192/6530 [=========>....................] - ETA: 2s - loss: 0.1062
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1135
2480/6530 [==========>...................] - ETA: 2s - loss: 0.1003
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1162
2784/6530 [===========>..................] - ETA: 1s - loss: 0.0951
6530/6530 [==============================] - 0s 22us/step - loss: 0.1151 - val_loss: 0.1792
Epoch 5/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1704{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  68 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  91 | activation: tanh    | extras: batchnorm 
layer 3 | size:  29 | activation: tanh    | extras: batchnorm 
layer 4 | size:  90 | activation: tanh    | extras: None 
layer 5 | size:  57 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e91320a58>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 256/6530 [>.............................] - ETA: 27s - loss: 0.5761
3072/6530 [=============>................] - ETA: 1s - loss: 0.0903
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1183
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1255 
3392/6530 [==============>...............] - ETA: 1s - loss: 0.0866
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1143
3712/6530 [================>.............] - ETA: 1s - loss: 0.0833
6530/6530 [==============================] - 0s 24us/step - loss: 0.1120 - val_loss: 0.1094
Epoch 6/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1222
6530/6530 [==============================] - 1s 197us/step - loss: 0.0877 - val_loss: 0.0359
Epoch 2/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0355
4016/6530 [=================>............] - ETA: 1s - loss: 0.0808
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1086
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0318
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0783
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1072
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0766
6530/6530 [==============================] - 0s 17us/step - loss: 0.0296 - val_loss: 0.0398
Epoch 3/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0346
6530/6530 [==============================] - 0s 22us/step - loss: 0.1064 - val_loss: 0.1657
Epoch 7/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1141
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0748
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0233
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1042
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0734
6530/6530 [==============================] - 0s 17us/step - loss: 0.0211 - val_loss: 0.0325
Epoch 4/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0181
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1000
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0720
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0182
6530/6530 [==============================] - 0s 23us/step - loss: 0.0994 - val_loss: 0.1116
Epoch 8/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1573
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0708
6530/6530 [==============================] - 0s 17us/step - loss: 0.0173 - val_loss: 0.0311

2432/6530 [==========>...................] - ETA: 0s - loss: 0.0992Epoch 5/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0165
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0697
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0945
3840/6530 [================>.............] - ETA: 0s - loss: 0.0146
6530/6530 [==============================] - 0s 23us/step - loss: 0.0944 - val_loss: 0.1199
Epoch 9/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0997
6530/6530 [==============================] - 0s 16us/step - loss: 0.0142 - val_loss: 0.0362
Epoch 6/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0129
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0928
6530/6530 [==============================] - 2s 327us/step - loss: 0.0687 - val_loss: 0.0428
Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0367
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0130
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0916
 304/6530 [>.............................] - ETA: 1s - loss: 0.0427
6530/6530 [==============================] - 0s 16us/step - loss: 0.0137 - val_loss: 0.0254
Epoch 7/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0128
6530/6530 [==============================] - 0s 23us/step - loss: 0.0913 - val_loss: 0.0928
Epoch 10/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0822
 624/6530 [=>............................] - ETA: 0s - loss: 0.0459
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0131
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0868
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0468
6530/6530 [==============================] - 0s 17us/step - loss: 0.0129 - val_loss: 0.0275
Epoch 8/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0135
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0875
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0465
3840/6530 [================>.............] - ETA: 0s - loss: 0.0122
6530/6530 [==============================] - 0s 22us/step - loss: 0.0871 - val_loss: 0.1107
Epoch 11/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0889
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0450
6530/6530 [==============================] - 0s 16us/step - loss: 0.0123 - val_loss: 0.0162
Epoch 9/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0111
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0885
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0452
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0106
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0871
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0453
6530/6530 [==============================] - 0s 17us/step - loss: 0.0116 - val_loss: 0.0181
Epoch 10/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 0s 22us/step - loss: 0.0861 - val_loss: 0.1591
Epoch 12/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1418
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0456
3840/6530 [================>.............] - ETA: 0s - loss: 0.0106
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0850
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0456
6530/6530 [==============================] - 0s 16us/step - loss: 0.0104 - val_loss: 0.0153

5120/6530 [======================>.......] - ETA: 0s - loss: 0.0839Epoch 11/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0108
3104/6530 [=============>................] - ETA: 0s - loss: 0.0449
6530/6530 [==============================] - 0s 22us/step - loss: 0.0839 - val_loss: 0.1459
Epoch 13/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1472
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0102
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0448
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0861
6530/6530 [==============================] - 0s 17us/step - loss: 0.0103 - val_loss: 0.0154
Epoch 12/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0113
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0448
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0851
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0100
3968/6530 [=================>............] - ETA: 0s - loss: 0.0445
6530/6530 [==============================] - 0s 22us/step - loss: 0.0852 - val_loss: 0.1476
Epoch 14/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1811
6530/6530 [==============================] - 0s 18us/step - loss: 0.0100 - val_loss: 0.0136
Epoch 13/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0129
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0445
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0944
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0104
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0444
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0858
6530/6530 [==============================] - 0s 17us/step - loss: 0.0105 - val_loss: 0.0157
Epoch 14/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0194
6530/6530 [==============================] - 0s 23us/step - loss: 0.0863 - val_loss: 0.1091

4848/6530 [=====================>........] - ETA: 0s - loss: 0.0441
3840/6530 [================>.............] - ETA: 0s - loss: 0.0108
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0438
# training | RMSE: 0.1237, MAE: 0.0993
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18421503775253176}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.12366451734901707, 'rmse': 0.12366451734901707, 'mae': 0.09930953835734008, 'early_stop': True}
vggnet done  0

6530/6530 [==============================] - 0s 16us/step - loss: 0.0104 - val_loss: 0.0145
Epoch 15/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0107
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0439
3840/6530 [================>.............] - ETA: 0s - loss: 0.0095
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0439
6530/6530 [==============================] - 0s 15us/step - loss: 0.0097 - val_loss: 0.0120
Epoch 16/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0093
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0435
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0098
6512/6530 [============================>.] - ETA: 0s - loss: 0.0432
6530/6530 [==============================] - 1s 172us/step - loss: 0.0432 - val_loss: 0.0345

6530/6530 [==============================] - 0s 16us/step - loss: 0.0095 - val_loss: 0.0125
Epoch 3/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0361Epoch 17/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0120
 352/6530 [>.............................] - ETA: 0s - loss: 0.0396
3840/6530 [================>.............] - ETA: 0s - loss: 0.0103
6530/6530 [==============================] - 0s 15us/step - loss: 0.0098 - val_loss: 0.0103
Epoch 18/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0071
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0397
3840/6530 [================>.............] - ETA: 0s - loss: 0.0088
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0390
6530/6530 [==============================] - 0s 15us/step - loss: 0.0085 - val_loss: 0.0105
Epoch 19/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0086
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0380
3840/6530 [================>.............] - ETA: 0s - loss: 0.0086
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0373
6530/6530 [==============================] - 0s 16us/step - loss: 0.0087 - val_loss: 0.0108
Epoch 20/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0074
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0367
3840/6530 [================>.............] - ETA: 0s - loss: 0.0094
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0371
6530/6530 [==============================] - 0s 15us/step - loss: 0.0091 - val_loss: 0.0096
Epoch 21/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0102
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0380
3840/6530 [================>.............] - ETA: 0s - loss: 0.0086
2992/6530 [============>.................] - ETA: 0s - loss: 0.0374
6530/6530 [==============================] - 0s 16us/step - loss: 0.0092 - val_loss: 0.0096
Epoch 22/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0076
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0373
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0080
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0376
6530/6530 [==============================] - 0s 16us/step - loss: 0.0088 - val_loss: 0.0098
Epoch 23/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0082
4000/6530 [=================>............] - ETA: 0s - loss: 0.0374
3840/6530 [================>.............] - ETA: 0s - loss: 0.0086
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0370
6530/6530 [==============================] - 0s 15us/step - loss: 0.0086 - val_loss: 0.0092
Epoch 24/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0090
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0371
3840/6530 [================>.............] - ETA: 0s - loss: 0.0081
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0370
6530/6530 [==============================] - 0s 15us/step - loss: 0.0080 - val_loss: 0.0110
Epoch 25/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0078
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0371
3840/6530 [================>.............] - ETA: 0s - loss: 0.0083
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0368
6530/6530 [==============================] - 0s 16us/step - loss: 0.0080 - val_loss: 0.0105
Epoch 26/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0094
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0367
4096/6530 [=================>............] - ETA: 0s - loss: 0.0080
6400/6530 [============================>.] - ETA: 0s - loss: 0.0364
6530/6530 [==============================] - 0s 15us/step - loss: 0.0081 - val_loss: 0.0096
Epoch 27/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0075
6530/6530 [==============================] - 1s 157us/step - loss: 0.0362 - val_loss: 0.0262
Epoch 4/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0349
3840/6530 [================>.............] - ETA: 0s - loss: 0.0086
 336/6530 [>.............................] - ETA: 0s - loss: 0.0329
6530/6530 [==============================] - 0s 15us/step - loss: 0.0083 - val_loss: 0.0108

 672/6530 [==>...........................] - ETA: 0s - loss: 0.0334
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0341
# training | RMSE: 0.0926, MAE: 0.0710
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.09263230687982994, 'rmse': 0.09263230687982994, 'mae': 0.07096493459402949, 'early_stop': False}
vggnet done  2

1376/6530 [=====>........................] - ETA: 0s - loss: 0.0329
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0328
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0319
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0319
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0319
3088/6530 [=============>................] - ETA: 0s - loss: 0.0315
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0316
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0321
3984/6530 [=================>............] - ETA: 0s - loss: 0.0319
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0318
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0321
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0321
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0321
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0321
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0320
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0319
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0319
6530/6530 [==============================] - 1s 171us/step - loss: 0.0317 - val_loss: 0.0260
Epoch 5/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0226
 336/6530 [>.............................] - ETA: 1s - loss: 0.0266
 640/6530 [=>............................] - ETA: 0s - loss: 0.0285
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0284
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0294
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0285
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0296
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0298
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0300
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0301
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0301
2960/6530 [============>.................] - ETA: 0s - loss: 0.0296
3216/6530 [=============>................] - ETA: 0s - loss: 0.0292
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0294
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0294
3936/6530 [=================>............] - ETA: 0s - loss: 0.0293
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0292
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0291
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0293
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0292
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0294
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0293
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0293
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0294
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0292
6448/6530 [============================>.] - ETA: 0s - loss: 0.0292
6530/6530 [==============================] - 1s 210us/step - loss: 0.0291 - val_loss: 0.0215
Epoch 6/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0207
 256/6530 [>.............................] - ETA: 1s - loss: 0.0261
 496/6530 [=>............................] - ETA: 1s - loss: 0.0255
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0255
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0264
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0282
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0290
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0283
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0275
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0276
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0276
2224/6530 [=========>....................] - ETA: 1s - loss: 0.0278
2400/6530 [==========>...................] - ETA: 1s - loss: 0.0277
2608/6530 [==========>...................] - ETA: 1s - loss: 0.0278
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0274
3040/6530 [============>.................] - ETA: 0s - loss: 0.0270
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0271
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0273
3808/6530 [================>.............] - ETA: 0s - loss: 0.0272
4080/6530 [=================>............] - ETA: 0s - loss: 0.0273
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0270
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0272
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0271
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0269
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0269
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0267
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0268
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0266
6320/6530 [============================>.] - ETA: 0s - loss: 0.0266
6530/6530 [==============================] - 2s 237us/step - loss: 0.0266 - val_loss: 0.0331
Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0196
 272/6530 [>.............................] - ETA: 1s - loss: 0.0253
 544/6530 [=>............................] - ETA: 1s - loss: 0.0268
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0269
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0281
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0275
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0267
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0262
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0261
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0260
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0261
2848/6530 [============>.................] - ETA: 0s - loss: 0.0259
3056/6530 [=============>................] - ETA: 0s - loss: 0.0256
3264/6530 [=============>................] - ETA: 0s - loss: 0.0255
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0257
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0256
3888/6530 [================>.............] - ETA: 0s - loss: 0.0256
4096/6530 [=================>............] - ETA: 0s - loss: 0.0256
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0256
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0257
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0257
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0256
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0255
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0256
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0258
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0256
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0256
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0257
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0255
6368/6530 [============================>.] - ETA: 0s - loss: 0.0255
6530/6530 [==============================] - 2s 243us/step - loss: 0.0254 - val_loss: 0.0175
Epoch 8/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0140
 224/6530 [>.............................] - ETA: 1s - loss: 0.0243
 432/6530 [>.............................] - ETA: 1s - loss: 0.0234
 624/6530 [=>............................] - ETA: 1s - loss: 0.0243
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0241
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0245
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0246
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0240
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0240
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0233
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0238
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0234
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0234
2912/6530 [============>.................] - ETA: 0s - loss: 0.0236
3088/6530 [=============>................] - ETA: 0s - loss: 0.0235
3248/6530 [=============>................] - ETA: 0s - loss: 0.0235
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0235
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0240
3776/6530 [================>.............] - ETA: 0s - loss: 0.0238
4000/6530 [=================>............] - ETA: 0s - loss: 0.0240
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0241
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0241
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0241
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0240
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0240
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0241
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0241
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0241
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0240
6320/6530 [============================>.] - ETA: 0s - loss: 0.0239
6530/6530 [==============================] - 2s 244us/step - loss: 0.0239 - val_loss: 0.0152
Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0192
 272/6530 [>.............................] - ETA: 1s - loss: 0.0250
 512/6530 [=>............................] - ETA: 1s - loss: 0.0245
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0252
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0258
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0261
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0251
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0250
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0246
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0241
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0238
3024/6530 [============>.................] - ETA: 0s - loss: 0.0236
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0234
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0234
3760/6530 [================>.............] - ETA: 0s - loss: 0.0235
3952/6530 [=================>............] - ETA: 0s - loss: 0.0235
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0234
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0232
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0233
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0234
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0233
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0232
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0232
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0230
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0232
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0233
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0233
6448/6530 [============================>.] - ETA: 0s - loss: 0.0232
6530/6530 [==============================] - 1s 227us/step - loss: 0.0232 - val_loss: 0.0141
Epoch 10/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0236
 224/6530 [>.............................] - ETA: 1s - loss: 0.0218
 416/6530 [>.............................] - ETA: 1s - loss: 0.0221
 608/6530 [=>............................] - ETA: 1s - loss: 0.0224
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0230
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0230
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0237
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0233
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0228
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0229
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0225
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0224
2256/6530 [=========>....................] - ETA: 1s - loss: 0.0227
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0228
2688/6530 [===========>..................] - ETA: 1s - loss: 0.0229
2880/6530 [============>.................] - ETA: 0s - loss: 0.0229
3072/6530 [=============>................] - ETA: 0s - loss: 0.0227
3264/6530 [=============>................] - ETA: 0s - loss: 0.0226
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0228
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0228
3792/6530 [================>.............] - ETA: 0s - loss: 0.0228
3952/6530 [=================>............] - ETA: 0s - loss: 0.0229
4112/6530 [=================>............] - ETA: 0s - loss: 0.0229
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0227
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0228
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0230
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0230
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0230
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0229
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0227
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0228
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0228
6336/6530 [============================>.] - ETA: 0s - loss: 0.0226
6530/6530 [==============================] - 2s 272us/step - loss: 0.0227 - val_loss: 0.0137
Epoch 11/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0172
 240/6530 [>.............................] - ETA: 1s - loss: 0.0192
 480/6530 [=>............................] - ETA: 1s - loss: 0.0193
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0206
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0211
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0218
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0212
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0210
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0210
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0213
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0214
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0216
2928/6530 [============>.................] - ETA: 0s - loss: 0.0215
3152/6530 [=============>................] - ETA: 0s - loss: 0.0214
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0213
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0216
3792/6530 [================>.............] - ETA: 0s - loss: 0.0216
4000/6530 [=================>............] - ETA: 0s - loss: 0.0217
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0216
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0217
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0218
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0217
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0218
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0216
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0216
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0216
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0215
6384/6530 [============================>.] - ETA: 0s - loss: 0.0214
6530/6530 [==============================] - 1s 226us/step - loss: 0.0214 - val_loss: 0.0138
Epoch 12/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0163
 272/6530 [>.............................] - ETA: 1s - loss: 0.0217
 480/6530 [=>............................] - ETA: 1s - loss: 0.0215
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0212
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0210
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0212
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0206
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0205
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0207
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0206
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0206
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0207
2928/6530 [============>.................] - ETA: 0s - loss: 0.0205
3200/6530 [=============>................] - ETA: 0s - loss: 0.0203
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0207
3712/6530 [================>.............] - ETA: 0s - loss: 0.0209
3984/6530 [=================>............] - ETA: 0s - loss: 0.0211
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0210
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0212
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0211
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0211
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0211
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0211
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0210
6368/6530 [============================>.] - ETA: 0s - loss: 0.0208
6530/6530 [==============================] - 1s 204us/step - loss: 0.0209 - val_loss: 0.0122
Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0116
 240/6530 [>.............................] - ETA: 1s - loss: 0.0193
 480/6530 [=>............................] - ETA: 1s - loss: 0.0211
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0204
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0207
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0209
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0203
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0199
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0199
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0198
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0198
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0201
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0202
2896/6530 [============>.................] - ETA: 0s - loss: 0.0200
3072/6530 [=============>................] - ETA: 0s - loss: 0.0199
3264/6530 [=============>................] - ETA: 0s - loss: 0.0198
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0200
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0200
3856/6530 [================>.............] - ETA: 0s - loss: 0.0201
4064/6530 [=================>............] - ETA: 0s - loss: 0.0203
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0202
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0202
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0202
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0204
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0205
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0204
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0204
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0203
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0204
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0203
6416/6530 [============================>.] - ETA: 0s - loss: 0.0202
6530/6530 [==============================] - 2s 254us/step - loss: 0.0203 - val_loss: 0.0190
Epoch 14/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0134
 224/6530 [>.............................] - ETA: 1s - loss: 0.0191
 416/6530 [>.............................] - ETA: 1s - loss: 0.0187
 608/6530 [=>............................] - ETA: 1s - loss: 0.0200
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0200
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0205
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0207
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0201
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0200
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0198
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0200
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0204
3120/6530 [=============>................] - ETA: 0s - loss: 0.0199
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0200
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0203
3968/6530 [=================>............] - ETA: 0s - loss: 0.0204
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0203
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0204
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0204
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0204
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0204
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0203
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0203
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0202
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0202
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0201
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0201
6512/6530 [============================>.] - ETA: 0s - loss: 0.0200
6530/6530 [==============================] - 1s 224us/step - loss: 0.0200 - val_loss: 0.0116
Epoch 15/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0117
 224/6530 [>.............................] - ETA: 1s - loss: 0.0196
 464/6530 [=>............................] - ETA: 1s - loss: 0.0183
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0183
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0186
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0192
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0193
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0187
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0185
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0187
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0186
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0187
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0187
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0188
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0189
2832/6530 [============>.................] - ETA: 1s - loss: 0.0188
3024/6530 [============>.................] - ETA: 0s - loss: 0.0187
3216/6530 [=============>................] - ETA: 0s - loss: 0.0188
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0189
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0192
3808/6530 [================>.............] - ETA: 0s - loss: 0.0190
4032/6530 [=================>............] - ETA: 0s - loss: 0.0193
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0192
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0193
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0192
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0192
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0192
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0190
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0190
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0189
6352/6530 [============================>.] - ETA: 0s - loss: 0.0189
6530/6530 [==============================] - 2s 252us/step - loss: 0.0188 - val_loss: 0.0139
Epoch 16/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0125
 240/6530 [>.............................] - ETA: 1s - loss: 0.0182
 464/6530 [=>............................] - ETA: 1s - loss: 0.0193
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0190
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0198
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0205
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0201
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0200
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0201
2016/6530 [========>.....................] - ETA: 1s - loss: 0.0201
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0199
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0200
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0201
2864/6530 [============>.................] - ETA: 0s - loss: 0.0199
3072/6530 [=============>................] - ETA: 0s - loss: 0.0198
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0199
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0200
3760/6530 [================>.............] - ETA: 0s - loss: 0.0200
3984/6530 [=================>............] - ETA: 0s - loss: 0.0200
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0200
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0199
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0199
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0198
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0198
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0198
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0198
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0198
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0198
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0198
6352/6530 [============================>.] - ETA: 0s - loss: 0.0199
6530/6530 [==============================] - 2s 247us/step - loss: 0.0198 - val_loss: 0.0160
Epoch 17/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0086
 224/6530 [>.............................] - ETA: 1s - loss: 0.0190
 432/6530 [>.............................] - ETA: 1s - loss: 0.0189
 656/6530 [==>...........................] - ETA: 1s - loss: 0.0186
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0194
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0198
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0199
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0195
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0193
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0191
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0190
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0190
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0193
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0193
2848/6530 [============>.................] - ETA: 0s - loss: 0.0192
3072/6530 [=============>................] - ETA: 0s - loss: 0.0190
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0190
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0190
3744/6530 [================>.............] - ETA: 0s - loss: 0.0190
3968/6530 [=================>............] - ETA: 0s - loss: 0.0193
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0192
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0192
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0193
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0193
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0193
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0193
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0191
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0191
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0191
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0190
6336/6530 [============================>.] - ETA: 0s - loss: 0.0189
6530/6530 [==============================] - 2s 251us/step - loss: 0.0189 - val_loss: 0.0161
Epoch 18/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0091
 288/6530 [>.............................] - ETA: 1s - loss: 0.0182
 560/6530 [=>............................] - ETA: 1s - loss: 0.0200
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0193
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0201
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0195
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0192
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0188
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0190
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0191
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0191
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0189
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0191
2992/6530 [============>.................] - ETA: 0s - loss: 0.0188
3168/6530 [=============>................] - ETA: 0s - loss: 0.0187
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0188
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0189
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0190
3872/6530 [================>.............] - ETA: 0s - loss: 0.0190
4064/6530 [=================>............] - ETA: 0s - loss: 0.0190
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0188
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0189
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0189
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0189
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0189
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0189
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0188
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0189
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0189
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0188
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0187
6432/6530 [============================>.] - ETA: 0s - loss: 0.0186
6530/6530 [==============================] - 2s 263us/step - loss: 0.0186 - val_loss: 0.0118
Epoch 19/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0088
 176/6530 [..............................] - ETA: 2s - loss: 0.0181
 320/6530 [>.............................] - ETA: 2s - loss: 0.0175
 480/6530 [=>............................] - ETA: 2s - loss: 0.0185
 640/6530 [=>............................] - ETA: 1s - loss: 0.0190
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0191
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0198
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0204
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0198
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0194
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0190
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0190
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0188
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0190
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0188
2976/6530 [============>.................] - ETA: 0s - loss: 0.0187
3136/6530 [=============>................] - ETA: 0s - loss: 0.0186
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0186
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0187
3728/6530 [================>.............] - ETA: 0s - loss: 0.0187
3936/6530 [=================>............] - ETA: 0s - loss: 0.0188
4128/6530 [=================>............] - ETA: 0s - loss: 0.0188
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0187
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0187
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0188
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0188
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0188
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0187
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0187
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0186
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0186
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0184
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0184
6512/6530 [============================>.] - ETA: 0s - loss: 0.0184
6530/6530 [==============================] - 2s 269us/step - loss: 0.0184 - val_loss: 0.0188

# training | RMSE: 0.1291, MAE: 0.1039
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3383217620683511}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.12913492323040437, 'rmse': 0.12913492323040437, 'mae': 0.10390819026874357, 'early_stop': True}
vggnet done  1
all of workers have been done
calculation finished
#0 epoch=27.0 loss={'loss': 0.12366451734901707, 'rmse': 0.12366451734901707, 'mae': 0.09930953835734008, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18421503775253176}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#2 epoch=27.0 loss={'loss': 0.09263230687982994, 'rmse': 0.09263230687982994, 'mae': 0.07096493459402949, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#1 epoch=27.0 loss={'loss': 0.12913492323040437, 'rmse': 0.12913492323040437, 'mae': 0.10390819026874357, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3383217620683511}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
get a list [results] of length 120
get a list [loss] of length 3
get a list [val_loss] of length 3
length of indices is [1 0 2]
length of indices is 3
length of T is 3
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3383217620683511}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]] 

*** 1.0 configurations x 81.0 iterations each

2 | Thu Sep 27 18:39:50 2018 | lowest loss so far: 0.0926 (run 2)

vggnet done  1
vggnet done  2
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  46 | activation: relu    | extras: dropout - rate: 33.8% 
layer 2 | size:  79 | activation: sigmoid | extras: None 
layer 3 | size:  69 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  52 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 6:30 - loss: 0.6694
 208/6530 [..............................] - ETA: 30s - loss: 0.4621 
 448/6530 [=>............................] - ETA: 14s - loss: 0.3048
 672/6530 [==>...........................] - ETA: 9s - loss: 0.2260 
 912/6530 [===>..........................] - ETA: 7s - loss: 0.1813
1152/6530 [====>.........................] - ETA: 5s - loss: 0.1561
1360/6530 [=====>........................] - ETA: 4s - loss: 0.1388
1568/6530 [======>.......................] - ETA: 4s - loss: 0.1269
1808/6530 [=======>......................] - ETA: 3s - loss: 0.1181
2064/6530 [========>.....................] - ETA: 3s - loss: 0.1100
2352/6530 [=========>....................] - ETA: 2s - loss: 0.1026
2640/6530 [===========>..................] - ETA: 2s - loss: 0.0976
2960/6530 [============>.................] - ETA: 1s - loss: 0.0923
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0873
3632/6530 [===============>..............] - ETA: 1s - loss: 0.0841
3936/6530 [=================>............] - ETA: 1s - loss: 0.0815
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0793
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0778
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0765
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0752
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0745
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0735
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0730
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0720
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0712
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0702
6352/6530 [============================>.] - ETA: 0s - loss: 0.0695
6530/6530 [==============================] - 2s 371us/step - loss: 0.0687 - val_loss: 0.0428
Epoch 2/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0367
 208/6530 [..............................] - ETA: 1s - loss: 0.0417
 400/6530 [>.............................] - ETA: 1s - loss: 0.0453
 576/6530 [=>............................] - ETA: 1s - loss: 0.0457
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0468
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0461
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0471
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0466
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0461
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0453
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0456
2016/6530 [========>.....................] - ETA: 1s - loss: 0.0453
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0455
2400/6530 [==========>...................] - ETA: 1s - loss: 0.0454
2592/6530 [==========>...................] - ETA: 1s - loss: 0.0461
2784/6530 [===========>..................] - ETA: 1s - loss: 0.0456
2992/6530 [============>.................] - ETA: 1s - loss: 0.0451
3216/6530 [=============>................] - ETA: 0s - loss: 0.0448
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0450
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0447
3904/6530 [================>.............] - ETA: 0s - loss: 0.0446
4080/6530 [=================>............] - ETA: 0s - loss: 0.0448
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0445
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0446
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0443
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0441
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0439
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0442
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0440
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0438
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0436
6352/6530 [============================>.] - ETA: 0s - loss: 0.0435
6530/6530 [==============================] - 2s 263us/step - loss: 0.0432 - val_loss: 0.0345
Epoch 3/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0361
 256/6530 [>.............................] - ETA: 1s - loss: 0.0399
 512/6530 [=>............................] - ETA: 1s - loss: 0.0405
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0399
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0386
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0393
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0384
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0380
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0371
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0369
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0366
2224/6530 [=========>....................] - ETA: 1s - loss: 0.0368
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0373
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0380
2832/6530 [============>.................] - ETA: 0s - loss: 0.0378
3024/6530 [============>.................] - ETA: 0s - loss: 0.0375
3216/6530 [=============>................] - ETA: 0s - loss: 0.0372
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0373
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0376
3824/6530 [================>.............] - ETA: 0s - loss: 0.0375
4032/6530 [=================>............] - ETA: 0s - loss: 0.0375
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0372
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0371
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0370
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0370
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0370
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0370
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0371
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0369
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0368
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0367
6416/6530 [============================>.] - ETA: 0s - loss: 0.0364
6530/6530 [==============================] - 2s 259us/step - loss: 0.0362 - val_loss: 0.0262
Epoch 4/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0349
 240/6530 [>.............................] - ETA: 1s - loss: 0.0334
 448/6530 [=>............................] - ETA: 1s - loss: 0.0330
 624/6530 [=>............................] - ETA: 1s - loss: 0.0327
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0334
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0335
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0342
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0331
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0329
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0326
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0326
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0320
2224/6530 [=========>....................] - ETA: 1s - loss: 0.0318
2416/6530 [==========>...................] - ETA: 1s - loss: 0.0318
2608/6530 [==========>...................] - ETA: 1s - loss: 0.0322
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0320
2976/6530 [============>.................] - ETA: 0s - loss: 0.0316
3152/6530 [=============>................] - ETA: 0s - loss: 0.0315
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0316
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0320
3712/6530 [================>.............] - ETA: 0s - loss: 0.0320
3936/6530 [=================>............] - ETA: 0s - loss: 0.0319
4128/6530 [=================>............] - ETA: 0s - loss: 0.0319
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0318
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0321
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0320
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0321
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0322
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0321
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0322
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0321
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0320
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0319
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0319
6464/6530 [============================>.] - ETA: 0s - loss: 0.0317
6530/6530 [==============================] - 2s 282us/step - loss: 0.0317 - val_loss: 0.0260
Epoch 5/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0226
 240/6530 [>.............................] - ETA: 1s - loss: 0.0273
 464/6530 [=>............................] - ETA: 1s - loss: 0.0285
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0285
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0286
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0298
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0292
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0286
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0291
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0294
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0296
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0299
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0301
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0301
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0300
3008/6530 [============>.................] - ETA: 0s - loss: 0.0295
3200/6530 [=============>................] - ETA: 0s - loss: 0.0292
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0291
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0295
3776/6530 [================>.............] - ETA: 0s - loss: 0.0293
3984/6530 [=================>............] - ETA: 0s - loss: 0.0294
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0292
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0292
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0293
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0293
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0293
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0293
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0293
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0293
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0294
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0292
6464/6530 [============================>.] - ETA: 0s - loss: 0.0291
6530/6530 [==============================] - 2s 256us/step - loss: 0.0291 - val_loss: 0.0215
Epoch 6/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0207
 208/6530 [..............................] - ETA: 1s - loss: 0.0255
 416/6530 [>.............................] - ETA: 1s - loss: 0.0252
 624/6530 [=>............................] - ETA: 1s - loss: 0.0257
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0264
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0289
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0284
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0275
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0277
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0276
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0278
2864/6530 [============>.................] - ETA: 0s - loss: 0.0272
3104/6530 [=============>................] - ETA: 0s - loss: 0.0270
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0271
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0275
3776/6530 [================>.............] - ETA: 0s - loss: 0.0272
4016/6530 [=================>............] - ETA: 0s - loss: 0.0272
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0271
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0271
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0270
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0268
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0268
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0268
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0265
6416/6530 [============================>.] - ETA: 0s - loss: 0.0265
6530/6530 [==============================] - 1s 197us/step - loss: 0.0266 - val_loss: 0.0331
Epoch 7/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0196
 288/6530 [>.............................] - ETA: 1s - loss: 0.0251
 560/6530 [=>............................] - ETA: 1s - loss: 0.0267
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0271
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0280
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0275
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0267
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0265
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0263
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0260
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0260
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0260
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0262
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0261
2976/6530 [============>.................] - ETA: 0s - loss: 0.0257
3152/6530 [=============>................] - ETA: 0s - loss: 0.0256
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0255
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0256
3760/6530 [================>.............] - ETA: 0s - loss: 0.0255
4000/6530 [=================>............] - ETA: 0s - loss: 0.0256
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0256
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0256
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0257
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0255
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0256
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0257
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0256
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0255
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0256
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0255
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0256
6416/6530 [============================>.] - ETA: 0s - loss: 0.0254
6530/6530 [==============================] - 2s 261us/step - loss: 0.0254 - val_loss: 0.0175
Epoch 8/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0140
 224/6530 [>.............................] - ETA: 1s - loss: 0.0243
 432/6530 [>.............................] - ETA: 1s - loss: 0.0234
 608/6530 [=>............................] - ETA: 1s - loss: 0.0246
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0241
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0245
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0246
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0242
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0234
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0236
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0233
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0239
2400/6530 [==========>...................] - ETA: 1s - loss: 0.0234
2576/6530 [==========>...................] - ETA: 1s - loss: 0.0233
2752/6530 [===========>..................] - ETA: 1s - loss: 0.0235
2944/6530 [============>.................] - ETA: 0s - loss: 0.0236
3120/6530 [=============>................] - ETA: 0s - loss: 0.0235
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0236
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0238
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0240
3856/6530 [================>.............] - ETA: 0s - loss: 0.0240
4064/6530 [=================>............] - ETA: 0s - loss: 0.0240
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0239
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0242
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0241
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0240
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0240
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0241
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0241
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0241
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0239
6384/6530 [============================>.] - ETA: 0s - loss: 0.0239
6530/6530 [==============================] - 2s 260us/step - loss: 0.0239 - val_loss: 0.0152
Epoch 9/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0191
 192/6530 [..............................] - ETA: 1s - loss: 0.0263
 384/6530 [>.............................] - ETA: 1s - loss: 0.0231
 592/6530 [=>............................] - ETA: 1s - loss: 0.0249
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0254
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0261
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0260
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0251
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0249
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0247
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0246
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0242
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0241
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0238
2928/6530 [============>.................] - ETA: 0s - loss: 0.0237
3104/6530 [=============>................] - ETA: 0s - loss: 0.0235
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0234
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0236
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0237
3824/6530 [================>.............] - ETA: 0s - loss: 0.0235
4016/6530 [=================>............] - ETA: 0s - loss: 0.0234
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0234
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0233
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0233
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0233
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0233
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0232
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0231
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0232
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0233
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0233
6432/6530 [============================>.] - ETA: 0s - loss: 0.0232
6530/6530 [==============================] - 2s 259us/step - loss: 0.0233 - val_loss: 0.0141
Epoch 10/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0236
 208/6530 [..............................] - ETA: 1s - loss: 0.0226
 384/6530 [>.............................] - ETA: 1s - loss: 0.0221
 576/6530 [=>............................] - ETA: 1s - loss: 0.0225
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0229
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0229
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0237
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0233
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0227
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0227
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0225
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0226
2416/6530 [==========>...................] - ETA: 1s - loss: 0.0227
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0229
2912/6530 [============>.................] - ETA: 0s - loss: 0.0229
3152/6530 [=============>................] - ETA: 0s - loss: 0.0227
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0225
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0229
3920/6530 [=================>............] - ETA: 0s - loss: 0.0229
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0229
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0227
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0230
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0230
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0228
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0228
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0227
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0228
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0227
6352/6530 [============================>.] - ETA: 0s - loss: 0.0226
6530/6530 [==============================] - 2s 240us/step - loss: 0.0227 - val_loss: 0.0137
Epoch 11/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0172
 208/6530 [..............................] - ETA: 1s - loss: 0.0205
 432/6530 [>.............................] - ETA: 1s - loss: 0.0193
 656/6530 [==>...........................] - ETA: 1s - loss: 0.0208
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0209
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0217
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0214
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0210
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0211
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0211
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0211
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0214
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0215
2912/6530 [============>.................] - ETA: 0s - loss: 0.0215
3104/6530 [=============>................] - ETA: 0s - loss: 0.0214
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0213
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0215
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0217
3840/6530 [================>.............] - ETA: 0s - loss: 0.0217
4016/6530 [=================>............] - ETA: 0s - loss: 0.0216
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0217
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0215
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0217
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0217
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0218
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0217
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0217
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0217
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0216
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0216
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0216
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0215
6384/6530 [============================>.] - ETA: 0s - loss: 0.0214
6530/6530 [==============================] - 2s 270us/step - loss: 0.0214 - val_loss: 0.0138
Epoch 12/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0162
 224/6530 [>.............................] - ETA: 1s - loss: 0.0215
 432/6530 [>.............................] - ETA: 1s - loss: 0.0220
 640/6530 [=>............................] - ETA: 1s - loss: 0.0213
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0213
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0207
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0211
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0206
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0202
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0206
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0207
2224/6530 [=========>....................] - ETA: 1s - loss: 0.0207
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0206
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0204
3040/6530 [============>.................] - ETA: 0s - loss: 0.0204
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0204
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0207
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0209
3856/6530 [================>.............] - ETA: 0s - loss: 0.0210
4032/6530 [=================>............] - ETA: 0s - loss: 0.0211
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0211
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0209
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0211
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0211
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0211
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0211
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0211
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0212
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0210
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0210
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0210
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0210
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0209
6464/6530 [============================>.] - ETA: 0s - loss: 0.0208
6530/6530 [==============================] - 2s 277us/step - loss: 0.0208 - val_loss: 0.0122
Epoch 13/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0116
 192/6530 [..............................] - ETA: 1s - loss: 0.0206
 352/6530 [>.............................] - ETA: 1s - loss: 0.0191
 544/6530 [=>............................] - ETA: 1s - loss: 0.0213
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0205
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0209
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0213
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0210
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0204
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0200
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0200
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0197
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0198
2544/6530 [==========>...................] - ETA: 1s - loss: 0.0202
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0201
3104/6530 [=============>................] - ETA: 0s - loss: 0.0199
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0198
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0200
3904/6530 [================>.............] - ETA: 0s - loss: 0.0201
4128/6530 [=================>............] - ETA: 0s - loss: 0.0203
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0201
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0203
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0204
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0204
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0204
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0204
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0204
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0204
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0204
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0204
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0204
6432/6530 [============================>.] - ETA: 0s - loss: 0.0202
6530/6530 [==============================] - 2s 260us/step - loss: 0.0203 - val_loss: 0.0189
Epoch 14/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0134
 192/6530 [..............................] - ETA: 1s - loss: 0.0198
 368/6530 [>.............................] - ETA: 1s - loss: 0.0180
 544/6530 [=>............................] - ETA: 1s - loss: 0.0196
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0198
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0200
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0210
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0207
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0203
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0201
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0197
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0197
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0199
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0202
2720/6530 [===========>..................] - ETA: 1s - loss: 0.0203
2928/6530 [============>.................] - ETA: 0s - loss: 0.0202
3152/6530 [=============>................] - ETA: 0s - loss: 0.0199
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0199
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0202
3712/6530 [================>.............] - ETA: 0s - loss: 0.0203
3920/6530 [=================>............] - ETA: 0s - loss: 0.0204
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0203
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0202
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0204
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0205
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0204
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0204
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0204
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0203
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0202
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0201
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0201
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0201
6464/6530 [============================>.] - ETA: 0s - loss: 0.0200
6530/6530 [==============================] - 2s 276us/step - loss: 0.0200 - val_loss: 0.0116
Epoch 15/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0116
 240/6530 [>.............................] - ETA: 1s - loss: 0.0191
 496/6530 [=>............................] - ETA: 1s - loss: 0.0181
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0183
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0186
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0192
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0192
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0186
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0185
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0186
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0187
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0187
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0188
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0189
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0188
2992/6530 [============>.................] - ETA: 0s - loss: 0.0188
3200/6530 [=============>................] - ETA: 0s - loss: 0.0187
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0189
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0191
3840/6530 [================>.............] - ETA: 0s - loss: 0.0192
4064/6530 [=================>............] - ETA: 0s - loss: 0.0193
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0193
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0192
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0192
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0192
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0192
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0190
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0191
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0189
6384/6530 [============================>.] - ETA: 0s - loss: 0.0188
6530/6530 [==============================] - 2s 242us/step - loss: 0.0188 - val_loss: 0.0139
Epoch 16/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0124
 208/6530 [..............................] - ETA: 1s - loss: 0.0194
 416/6530 [>.............................] - ETA: 1s - loss: 0.0187
 624/6530 [=>............................] - ETA: 1s - loss: 0.0189
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0190
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0203
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0204
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0201
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0202
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0201
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0201
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0199
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0200
2848/6530 [============>.................] - ETA: 0s - loss: 0.0200
3088/6530 [=============>................] - ETA: 0s - loss: 0.0199
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0199
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0201
3856/6530 [================>.............] - ETA: 0s - loss: 0.0201
4064/6530 [=================>............] - ETA: 0s - loss: 0.0201
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0200
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0199
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0200
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0198
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0198
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0198
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0198
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0198
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0199
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0198
6336/6530 [============================>.] - ETA: 0s - loss: 0.0199
6530/6530 [==============================] - 2s 246us/step - loss: 0.0198 - val_loss: 0.0160
Epoch 17/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0086
 240/6530 [>.............................] - ETA: 1s - loss: 0.0181
 464/6530 [=>............................] - ETA: 1s - loss: 0.0190
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0184
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0194
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0198
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0200
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0195
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0194
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0193
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0190
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0191
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0192
2560/6530 [==========>...................] - ETA: 1s - loss: 0.0194
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0192
3008/6530 [============>.................] - ETA: 0s - loss: 0.0190
3232/6530 [=============>................] - ETA: 0s - loss: 0.0190
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0190
3744/6530 [================>.............] - ETA: 0s - loss: 0.0190
4000/6530 [=================>............] - ETA: 0s - loss: 0.0193
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0192
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0193
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0193
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0194
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0193
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0193
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0191
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0191
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0190
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0190
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0189
6432/6530 [============================>.] - ETA: 0s - loss: 0.0189
6530/6530 [==============================] - 2s 257us/step - loss: 0.0189 - val_loss: 0.0163
Epoch 18/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0090
 208/6530 [..............................] - ETA: 1s - loss: 0.0194
 352/6530 [>.............................] - ETA: 1s - loss: 0.0188
 528/6530 [=>............................] - ETA: 1s - loss: 0.0205
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0201
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0192
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0199
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0198
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0193
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0189
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0192
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0190
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0189
2928/6530 [============>.................] - ETA: 0s - loss: 0.0189
3152/6530 [=============>................] - ETA: 0s - loss: 0.0188
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0188
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0190
3888/6530 [================>.............] - ETA: 0s - loss: 0.0190
4096/6530 [=================>............] - ETA: 0s - loss: 0.0190
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0189
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0188
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0189
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0190
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0190
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0188
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0189
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0188
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0187
6400/6530 [============================>.] - ETA: 0s - loss: 0.0186
6530/6530 [==============================] - 2s 238us/step - loss: 0.0186 - val_loss: 0.0118
Epoch 19/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0088
 192/6530 [..............................] - ETA: 1s - loss: 0.0187
 384/6530 [>.............................] - ETA: 1s - loss: 0.0183
 560/6530 [=>............................] - ETA: 1s - loss: 0.0182
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0189
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0195
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0202
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0200
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0196
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0195
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0192
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0190
2288/6530 [=========>....................] - ETA: 1s - loss: 0.0189
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0188
2672/6530 [===========>..................] - ETA: 1s - loss: 0.0190
2880/6530 [============>.................] - ETA: 1s - loss: 0.0187
3104/6530 [=============>................] - ETA: 0s - loss: 0.0186
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0186
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0187
3728/6530 [================>.............] - ETA: 0s - loss: 0.0187
3952/6530 [=================>............] - ETA: 0s - loss: 0.0188
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0188
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0187
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0188
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0188
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0188
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0188
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0188
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0186
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0186
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0185
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0184
6464/6530 [============================>.] - ETA: 0s - loss: 0.0184
6530/6530 [==============================] - 2s 268us/step - loss: 0.0184 - val_loss: 0.0186

# training | RMSE: 0.1285, MAE: 0.1032
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3383217620683511}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.1285425468127502, 'rmse': 0.1285425468127502, 'mae': 0.10323235739723023, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=81.0 loss={'loss': 0.1285425468127502, 'rmse': 0.1285425468127502, 'mae': 0.10323235739723023, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3383217620683511}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
get a list [results] of length 121
get a list [loss] of length 1
get a list [val_loss] of length 1
length of indices is [0]
length of indices is 1
length of T is 1
s=3
T is of size 34
T=[{'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42143477994447576}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.31223754765409084}, 'layer_2_size': 96, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23424407254280194}, 'layer_3_size': 81, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 88, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.36965973742492475}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1606854821149849}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.300135921897431}, 'layer_3_size': 11, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4013607522402225}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22344854801017078}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.43379468411831923}, 'layer_2_size': 88, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.237543168500524}, 'layer_4_size': 70, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3336391431416651}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 37, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1985387424526034}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11082940352862432}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4774286097069397}, 'layer_4_size': 27, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22115319953683002}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24260494912330344}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.245751806831647}, 'layer_3_size': 72, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3313486863097612}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1400219191605232}, 'layer_2_size': 24, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21573621650431832}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22573404172252914}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16328226150226122}, 'layer_1_size': 65, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 68, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2724334792774713}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 6, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42656591904286645}, 'layer_2_size': 53, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43977270884349884}, 'layer_3_size': 74, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21694647959011945}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.25455619904572774}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 58, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 5, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23440243735423638}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 62, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13291746662379664}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10896181943545252}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4276091076951002}, 'layer_3_size': 63, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.29307793174816765}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19464477406657954}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 32, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11718869715812273}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47263121756876636}, 'layer_2_size': 48, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4103126608856419}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 23, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15288538110580896}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 83, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 18, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 82, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.29969934092088746}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 3, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14248100539073072}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1941426640196279}, 'layer_2_size': 94, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30708224429563385}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20554577126203855}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.38782014643625917}, 'layer_4_size': 99, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34751031124990506}, 'layer_1_size': 61, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16053681216323856}, 'layer_2_size': 41, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2537283937797355}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4229999232812024}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17166616289134531}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4844414424363912}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21146430495516638}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3583376083280042}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4770486714254554}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3323045329803257}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 80, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 86, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42019757421631054}, 'layer_4_size': 3, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15723407789772303}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10091696738640682}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3541840405276866}, 'layer_4_size': 12, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13378420333817492}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14588882713852366}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39239340644363085}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11000700240545346}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4799552058691362}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20598381938009017}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21312643339710788}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22227484043894946}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13736846942029257}, 'layer_1_size': 20, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.33701832872406634}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45553820744359885}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31686787298517216}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]
newly formed T structure is:[[0, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42143477994447576}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.31223754765409084}, 'layer_2_size': 96, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23424407254280194}, 'layer_3_size': 81, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 88, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.36965973742492475}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [1, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1606854821149849}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.300135921897431}, 'layer_3_size': 11, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4013607522402225}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [2, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22344854801017078}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.43379468411831923}, 'layer_2_size': 88, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.237543168500524}, 'layer_4_size': 70, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [3, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3336391431416651}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 37, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1985387424526034}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [4, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11082940352862432}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4774286097069397}, 'layer_4_size': 27, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22115319953683002}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [5, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24260494912330344}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.245751806831647}, 'layer_3_size': 72, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3313486863097612}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [6, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1400219191605232}, 'layer_2_size': 24, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21573621650431832}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [7, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22573404172252914}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [8, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16328226150226122}, 'layer_1_size': 65, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 68, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [9, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2724334792774713}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [10, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 6, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42656591904286645}, 'layer_2_size': 53, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43977270884349884}, 'layer_3_size': 74, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21694647959011945}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [11, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.25455619904572774}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 58, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 5, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [12, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23440243735423638}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 62, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13291746662379664}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [13, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10896181943545252}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [14, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4276091076951002}, 'layer_3_size': 63, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.29307793174816765}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19464477406657954}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [15, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [16, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [17, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 32, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11718869715812273}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [18, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47263121756876636}, 'layer_2_size': 48, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4103126608856419}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 23, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [19, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15288538110580896}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 83, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 18, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [20, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 82, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.29969934092088746}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 3, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [21, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14248100539073072}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [22, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1941426640196279}, 'layer_2_size': 94, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [23, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30708224429563385}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20554577126203855}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.38782014643625917}, 'layer_4_size': 99, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [24, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34751031124990506}, 'layer_1_size': 61, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16053681216323856}, 'layer_2_size': 41, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2537283937797355}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4229999232812024}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [25, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17166616289134531}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4844414424363912}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21146430495516638}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3583376083280042}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [26, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4770486714254554}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3323045329803257}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [27, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 80, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 86, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42019757421631054}, 'layer_4_size': 3, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [28, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15723407789772303}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10091696738640682}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3541840405276866}, 'layer_4_size': 12, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13378420333817492}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [29, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14588882713852366}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [30, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39239340644363085}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11000700240545346}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [31, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4799552058691362}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [32, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20598381938009017}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21312643339710788}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22227484043894946}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [33, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13736846942029257}, 'layer_1_size': 20, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.33701832872406634}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45553820744359885}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31686787298517216}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]] 

*** 34 configurations x 3.0 iterations each

1 | Thu Sep 27 18:40:24 2018 | lowest loss so far: 0.0926 (run 2)

{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  84 | activation: sigmoid | extras: dropout - rate: 22.3% 
layer 2 | size:  88 | activation: tanh    | extras: dropout - rate: 43.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 5:53 - loss: 1.4023
 320/6530 [>.............................] - ETA: 17s - loss: 0.5121 
 656/6530 [==>...........................] - ETA: 8s - loss: 0.4426 
1072/6530 [===>..........................] - ETA: 5s - loss: 0.4028
1488/6530 [=====>........................] - ETA: 3s - loss: 0.3846{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  89 | activation: tanh    | extras: dropout - rate: 42.1% 
layer 2 | size:  96 | activation: sigmoid | extras: dropout - rate: 31.2% 
layer 3 | size:  81 | activation: sigmoid | extras: dropout - rate: 23.4% 
layer 4 | size:  88 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 1:31 - loss: 0.9362
1936/6530 [=======>......................] - ETA: 2s - loss: 0.3612
1728/6530 [======>.......................] - ETA: 2s - loss: 0.1690  
2400/6530 [==========>...................] - ETA: 2s - loss: 0.3469
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1341
2864/6530 [============>.................] - ETA: 1s - loss: 0.3335
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1154
3328/6530 [==============>...............] - ETA: 1s - loss: 0.3184
3808/6530 [================>.............] - ETA: 0s - loss: 0.3070
6530/6530 [==============================] - 1s 175us/step - loss: 0.1042 - val_loss: 0.0468
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0617
4304/6530 [==================>...........] - ETA: 0s - loss: 0.2963
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0610
4752/6530 [====================>.........] - ETA: 0s - loss: 0.2913
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0603
5232/6530 [=======================>......] - ETA: 0s - loss: 0.2849
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0582
5728/6530 [=========================>....] - ETA: 0s - loss: 0.2785{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  67 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  48 | activation: relu    | extras: dropout - rate: 16.1% 
layer 3 | size:  11 | activation: sigmoid | extras: dropout - rate: 30.0% 
layer 4 | size:  89 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 6:41 - loss: 0.2339
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0571
6192/6530 [===========================>..] - ETA: 0s - loss: 0.2737
 256/6530 [>.............................] - ETA: 25s - loss: 0.2334 
6530/6530 [==============================] - 0s 44us/step - loss: 0.0551 - val_loss: 0.0684
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1011
 528/6530 [=>............................] - ETA: 12s - loss: 0.1870
6530/6530 [==============================] - 2s 256us/step - loss: 0.2702 - val_loss: 0.1710
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1942
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0491
 768/6530 [==>...........................] - ETA: 8s - loss: 0.1605 
 464/6530 [=>............................] - ETA: 0s - loss: 0.2054
2880/6530 [============>.................] - ETA: 0s - loss: 0.0504
1008/6530 [===>..........................] - ETA: 6s - loss: 0.1439
 928/6530 [===>..........................] - ETA: 0s - loss: 0.2144
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0500
1280/6530 [====>.........................] - ETA: 5s - loss: 0.1305
1328/6530 [=====>........................] - ETA: 0s - loss: 0.2105
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0499
1536/6530 [======>.......................] - ETA: 4s - loss: 0.1204
6530/6530 [==============================] - 0s 35us/step - loss: 0.0499 - val_loss: 0.0907

1808/6530 [=======>......................] - ETA: 0s - loss: 0.2085
1792/6530 [=======>......................] - ETA: 3s - loss: 0.1124
2272/6530 [=========>....................] - ETA: 0s - loss: 0.2085
2048/6530 [========>.....................] - ETA: 3s - loss: 0.1061
2752/6530 [===========>..................] - ETA: 0s - loss: 0.2043
2320/6530 [=========>....................] - ETA: 2s - loss: 0.1018
3216/6530 [=============>................] - ETA: 0s - loss: 0.2020
2560/6530 [==========>...................] - ETA: 2s - loss: 0.0978
3712/6530 [================>.............] - ETA: 0s - loss: 0.2032
2816/6530 [===========>..................] - ETA: 2s - loss: 0.0950
4192/6530 [==================>...........] - ETA: 0s - loss: 0.2011
3024/6530 [============>.................] - ETA: 1s - loss: 0.0924
4592/6530 [====================>.........] - ETA: 0s - loss: 0.2014
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0899
5072/6530 [======================>.......] - ETA: 0s - loss: 0.2001
3536/6530 [===============>..............] - ETA: 1s - loss: 0.0876
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1987
3792/6530 [================>.............] - ETA: 1s - loss: 0.0852
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1990
4048/6530 [=================>............] - ETA: 1s - loss: 0.0833
6512/6530 [============================>.] - ETA: 0s - loss: 0.1973
6530/6530 [==============================] - 1s 114us/step - loss: 0.1972 - val_loss: 0.2194
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.2039
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0814
 496/6530 [=>............................] - ETA: 0s - loss: 0.1830
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0800
 976/6530 [===>..........................] - ETA: 0s - loss: 0.1869
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0790
1456/6530 [=====>........................] - ETA: 0s - loss: 0.1844
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0772
1936/6530 [=======>......................] - ETA: 0s - loss: 0.1859
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0758
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1832
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0746
# training | RMSE: 0.2998, MAE: 0.2547
worker 0  xfile  [0, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42143477994447576}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.31223754765409084}, 'layer_2_size': 96, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23424407254280194}, 'layer_3_size': 81, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 88, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.36965973742492475}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.29976423699273, 'rmse': 0.29976423699273, 'mae': 0.2547234262770374, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  29 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  52 | activation: sigmoid | extras: dropout - rate: 33.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e913280f0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 34s - loss: 0.4814
2928/6530 [============>.................] - ETA: 0s - loss: 0.1833
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0736
1216/6530 [====>.........................] - ETA: 1s - loss: 0.2836 
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1823
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0725
2560/6530 [==========>...................] - ETA: 0s - loss: 0.2240
3808/6530 [================>.............] - ETA: 0s - loss: 0.1824
6448/6530 [============================>.] - ETA: 0s - loss: 0.0716
3904/6530 [================>.............] - ETA: 0s - loss: 0.2033
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1822
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1937
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1840
6530/6530 [==============================] - 2s 363us/step - loss: 0.0713 - val_loss: 0.1040
Epoch 2/3

  16/6530 [..............................] - ETA: 2s - loss: 0.0754
6464/6530 [============================>.] - ETA: 0s - loss: 0.1885
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1837
 256/6530 [>.............................] - ETA: 1s - loss: 0.0458
6530/6530 [==============================] - 1s 98us/step - loss: 0.1882 - val_loss: 0.1764
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1558
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1835
 528/6530 [=>............................] - ETA: 1s - loss: 0.0495
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1659
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1833
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0483
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1633
6464/6530 [============================>.] - ETA: 0s - loss: 0.1823
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0490
3904/6530 [================>.............] - ETA: 0s - loss: 0.1641
6530/6530 [==============================] - 1s 115us/step - loss: 0.1821 - val_loss: 0.2753

1296/6530 [====>.........................] - ETA: 1s - loss: 0.0498
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1626
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0494
6528/6530 [============================>.] - ETA: 0s - loss: 0.1633
6530/6530 [==============================] - 0s 42us/step - loss: 0.1633 - val_loss: 0.1777
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1687
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0493
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1635
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0486
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1653
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0487
4096/6530 [=================>............] - ETA: 0s - loss: 0.1621
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0488
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1610
2848/6530 [============>.................] - ETA: 0s - loss: 0.0487
6530/6530 [==============================] - 0s 39us/step - loss: 0.1605 - val_loss: 0.1928

3120/6530 [=============>................] - ETA: 0s - loss: 0.0482
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0475
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0473
3920/6530 [=================>............] - ETA: 0s - loss: 0.0473
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0471
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0467
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0462
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0458
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0454
# training | RMSE: 0.3160, MAE: 0.2725
worker 2  xfile  [2, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22344854801017078}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.43379468411831923}, 'layer_2_size': 88, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.237543168500524}, 'layer_4_size': 70, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.3159868504141935, 'rmse': 0.3159868504141935, 'mae': 0.2724558964398283, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  91 | activation: tanh    | extras: batchnorm 
layer 2 | size:   7 | activation: tanh    | extras: dropout - rate: 11.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e912f8198>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:48 - loss: 0.8446
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0452
# training | RMSE: 0.2271, MAE: 0.1907
worker 0  xfile  [3, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3336391431416651}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 37, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1985387424526034}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.22709574956572226, 'rmse': 0.22709574956572226, 'mae': 0.19068296495552425, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  57 | activation: sigmoid | extras: dropout - rate: 24.3% 
layer 2 | size:  18 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e88147fd0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 20s - loss: 0.7348
 416/6530 [>.............................] - ETA: 4s - loss: 0.5587  
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0449
2176/6530 [========>.....................] - ETA: 0s - loss: 0.3993 
 800/6530 [==>...........................] - ETA: 2s - loss: 0.3934
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0446
4288/6530 [==================>...........] - ETA: 0s - loss: 0.3091
1200/6530 [====>.........................] - ETA: 1s - loss: 0.3008
6320/6530 [============================>.] - ETA: 0s - loss: 0.0442
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2793
1584/6530 [======>.......................] - ETA: 1s - loss: 0.2478
6530/6530 [==============================] - 0s 60us/step - loss: 0.2761 - val_loss: 0.2084
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2339
6530/6530 [==============================] - 1s 204us/step - loss: 0.0440 - val_loss: 0.0334

1968/6530 [========>.....................] - ETA: 1s - loss: 0.2122Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0616
2048/6530 [========>.....................] - ETA: 0s - loss: 0.2165
2352/6530 [=========>....................] - ETA: 1s - loss: 0.1876
 272/6530 [>.............................] - ETA: 1s - loss: 0.0410
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2111
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1711
 528/6530 [=>............................] - ETA: 1s - loss: 0.0412
6336/6530 [============================>.] - ETA: 0s - loss: 0.2092
6530/6530 [==============================] - 0s 26us/step - loss: 0.2088 - val_loss: 0.2007
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2247
3120/6530 [=============>................] - ETA: 0s - loss: 0.1551
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0426
2176/6530 [========>.....................] - ETA: 0s - loss: 0.2082
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1440
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0419
4288/6530 [==================>...........] - ETA: 0s - loss: 0.2025
3888/6530 [================>.............] - ETA: 0s - loss: 0.1351
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0422
6336/6530 [============================>.] - ETA: 0s - loss: 0.2010
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1265
6530/6530 [==============================] - 0s 25us/step - loss: 0.2005 - val_loss: 0.1903

1536/6530 [======>.......................] - ETA: 1s - loss: 0.0412
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1194
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0407
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1136
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0395
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1087
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0397
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1039
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0396
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0997
2864/6530 [============>.................] - ETA: 0s - loss: 0.0396
3136/6530 [=============>................] - ETA: 0s - loss: 0.0396
6530/6530 [==============================] - 1s 181us/step - loss: 0.0967 - val_loss: 0.0318
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0565
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0396
 432/6530 [>.............................] - ETA: 0s - loss: 0.0320
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0400
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0338
3936/6530 [=================>............] - ETA: 0s - loss: 0.0395
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0339
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0397
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0338
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0394
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0330
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0391
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0330
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0389
2848/6530 [============>.................] - ETA: 0s - loss: 0.0326
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0388
3248/6530 [=============>................] - ETA: 0s - loss: 0.0321
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0386
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0325
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0384
4080/6530 [=================>............] - ETA: 0s - loss: 0.0321
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0388
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0318
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0384
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0316
6528/6530 [============================>.] - ETA: 0s - loss: 0.0383
# training | RMSE: 0.2355, MAE: 0.1920
worker 0  xfile  [5, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24260494912330344}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.245751806831647}, 'layer_3_size': 72, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3313486863097612}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.23548609947100957, 'rmse': 0.23548609947100957, 'mae': 0.19199739700391313, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  11 | activation: relu    | extras: None 
layer 2 | size:  24 | activation: tanh    | extras: dropout - rate: 14.0% 
layer 3 | size:  86 | activation: relu    | extras: batchnorm 
layer 4 | size:  36 | activation: sigmoid | extras: dropout - rate: 21.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e881b30f0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 20s - loss: 0.5831
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0313
6530/6530 [==============================] - 1s 204us/step - loss: 0.0383 - val_loss: 0.0691

2688/6530 [===========>..................] - ETA: 0s - loss: 0.2646 
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0310
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1666
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0308
6528/6530 [============================>.] - ETA: 0s - loss: 0.0307
6530/6530 [==============================] - 1s 91us/step - loss: 0.1347 - val_loss: 0.0340
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0415
6530/6530 [==============================] - 1s 131us/step - loss: 0.0307 - val_loss: 0.0220
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0354
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0287
 400/6530 [>.............................] - ETA: 0s - loss: 0.0246
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0269
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0269
6530/6530 [==============================] - 0s 22us/step - loss: 0.0263 - val_loss: 0.0243
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0223
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0271
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0225
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0268
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0217
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0266
6530/6530 [==============================] - 0s 21us/step - loss: 0.0212 - val_loss: 0.0196

2336/6530 [=========>....................] - ETA: 0s - loss: 0.0263
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0265
3088/6530 [=============>................] - ETA: 0s - loss: 0.0260
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0265
3888/6530 [================>.............] - ETA: 0s - loss: 0.0263
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0261
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0260
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0260
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0259
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0257
# training | RMSE: 0.1368, MAE: 0.1042
worker 0  xfile  [6, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1400219191605232}, 'layer_2_size': 24, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21573621650431832}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.13680919489308585, 'rmse': 0.13680919489308585, 'mae': 0.10424354734439262, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  65 | activation: sigmoid | extras: dropout - rate: 16.3% 
layer 2 | size:   2 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e7b772630>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 11s - loss: 0.7875
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0256
3968/6530 [=================>............] - ETA: 0s - loss: 0.7288 
6530/6530 [==============================] - 1s 138us/step - loss: 0.0257 - val_loss: 0.0175

6530/6530 [==============================] - 0s 53us/step - loss: 0.7011 - val_loss: 0.6445
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.6414
4736/6530 [====================>.........] - ETA: 0s - loss: 0.5925
6530/6530 [==============================] - 0s 12us/step - loss: 0.5792 - val_loss: 0.5254
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.5108
4736/6530 [====================>.........] - ETA: 0s - loss: 0.4704
6530/6530 [==============================] - 0s 12us/step - loss: 0.4530 - val_loss: 0.3851

# training | RMSE: 0.2591, MAE: 0.2163
worker 1  xfile  [1, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1606854821149849}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.300135921897431}, 'layer_3_size': 11, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4013607522402225}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.25914226672437723, 'rmse': 0.25914226672437723, 'mae': 0.21634410071725704, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  17 | activation: tanh    | extras: dropout - rate: 22.6% 
layer 2 | size:  71 | activation: tanh    | extras: None 
layer 3 | size:  83 | activation: relu    | extras: batchnorm 
layer 4 | size:  58 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e912f8128>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:43 - loss: 1.4811
 672/6530 [==>...........................] - ETA: 4s - loss: 0.6325  
1312/6530 [=====>........................] - ETA: 2s - loss: 0.4304
1920/6530 [=======>......................] - ETA: 1s - loss: 0.3364
2528/6530 [==========>...................] - ETA: 1s - loss: 0.2795
3168/6530 [=============>................] - ETA: 0s - loss: 0.2426
3840/6530 [================>.............] - ETA: 0s - loss: 0.2139
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1962
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1820
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1708
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1623
6530/6530 [==============================] - 1s 171us/step - loss: 0.1577 - val_loss: 0.0458
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0869
# training | RMSE: 0.1317, MAE: 0.1037
worker 2  xfile  [4, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11082940352862432}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4774286097069397}, 'layer_4_size': 27, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22115319953683002}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.13169573105640692, 'rmse': 0.13169573105640692, 'mae': 0.10366174279884777, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  96 | activation: tanh    | extras: None 
layer 2 | size:  87 | activation: relu    | extras: None 
layer 3 | size:  13 | activation: tanh    | extras: dropout - rate: 27.2% 
layer 4 | size:  22 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e9061d4e0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 14s - loss: 0.7255
 512/6530 [=>............................] - ETA: 0s - loss: 0.0739
2944/6530 [============>.................] - ETA: 0s - loss: 0.6578 
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0721
5888/6530 [==========================>...] - ETA: 0s - loss: 0.4847
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0687
6530/6530 [==============================] - 0s 67us/step - loss: 0.4574 - val_loss: 0.1714
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1971
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0671
3072/6530 [=============>................] - ETA: 0s - loss: 0.1859
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0648
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1818
6530/6530 [==============================] - 0s 18us/step - loss: 0.1808 - val_loss: 0.1568
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1618
3232/6530 [=============>................] - ETA: 0s - loss: 0.0634
2944/6530 [============>.................] - ETA: 0s - loss: 0.1723
3776/6530 [================>.............] - ETA: 0s - loss: 0.0630
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1708
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0622
6530/6530 [==============================] - 0s 19us/step - loss: 0.1699 - val_loss: 0.1560

4864/6530 [=====================>........] - ETA: 0s - loss: 0.0618
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0613
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0604
6530/6530 [==============================] - 1s 96us/step - loss: 0.0594 - val_loss: 0.0358
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0366
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0541
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0516
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0503
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0497
# training | RMSE: 0.4479, MAE: 0.3773
worker 0  xfile  [8, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16328226150226122}, 'layer_1_size': 65, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 68, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.44785599814449295, 'rmse': 0.44785599814449295, 'mae': 0.3772598285111749, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:   6 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  53 | activation: tanh    | extras: dropout - rate: 42.7% 
layer 3 | size:  74 | activation: sigmoid | extras: dropout - rate: 44.0% 
layer 4 | size:  82 | activation: sigmoid | extras: batchnorm 
layer 5 | size:  73 | activation: tanh    | extras: dropout - rate: 21.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e780093c8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 2:35 - loss: 1.0066
3040/6530 [============>.................] - ETA: 0s - loss: 0.0496
 480/6530 [=>............................] - ETA: 10s - loss: 0.7395 
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0496
 928/6530 [===>..........................] - ETA: 5s - loss: 0.6146 
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0488
1408/6530 [=====>........................] - ETA: 3s - loss: 0.5187
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0490
1856/6530 [=======>......................] - ETA: 2s - loss: 0.4713
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0481
2336/6530 [=========>....................] - ETA: 1s - loss: 0.4377
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0478
2816/6530 [===========>..................] - ETA: 1s - loss: 0.4166
6496/6530 [============================>.] - ETA: 0s - loss: 0.0478
6530/6530 [==============================] - 1s 91us/step - loss: 0.0477 - val_loss: 0.0299

3264/6530 [=============>................] - ETA: 1s - loss: 0.4008
3680/6530 [===============>..............] - ETA: 0s - loss: 0.3889
4096/6530 [=================>............] - ETA: 0s - loss: 0.3801
4544/6530 [===================>..........] - ETA: 0s - loss: 0.3685
4992/6530 [=====================>........] - ETA: 0s - loss: 0.3607
5472/6530 [========================>.....] - ETA: 0s - loss: 0.3535
# training | RMSE: 0.1947, MAE: 0.1498
worker 2  xfile  [9, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2724334792774713}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.19467623784635188, 'rmse': 0.19467623784635188, 'mae': 0.14983309127929142, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  57 | activation: sigmoid | extras: dropout - rate: 25.5% 
layer 2 | size:  20 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  58 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e90005320>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 46s - loss: 1.1128
5824/6530 [=========================>....] - ETA: 0s - loss: 0.3491
1152/6530 [====>.........................] - ETA: 2s - loss: 0.8741 
6240/6530 [===========================>..] - ETA: 0s - loss: 0.3436
2368/6530 [=========>....................] - ETA: 0s - loss: 0.6657
3520/6530 [===============>..............] - ETA: 0s - loss: 0.5571
4672/6530 [====================>.........] - ETA: 0s - loss: 0.4911
6530/6530 [==============================] - 2s 247us/step - loss: 0.3403 - val_loss: 0.3310
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.3773
5888/6530 [==========================>...] - ETA: 0s - loss: 0.4460
 480/6530 [=>............................] - ETA: 0s - loss: 0.2766
 928/6530 [===>..........................] - ETA: 0s - loss: 0.2824
6530/6530 [==============================] - 1s 122us/step - loss: 0.4275 - val_loss: 0.2546
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2979
1344/6530 [=====>........................] - ETA: 0s - loss: 0.2737
1216/6530 [====>.........................] - ETA: 0s - loss: 0.2484
1792/6530 [=======>......................] - ETA: 0s - loss: 0.2671
2304/6530 [=========>....................] - ETA: 0s - loss: 0.2469
2240/6530 [=========>....................] - ETA: 0s - loss: 0.2653
3392/6530 [==============>...............] - ETA: 0s - loss: 0.2418
2592/6530 [==========>...................] - ETA: 0s - loss: 0.2640
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2381
3040/6530 [============>.................] - ETA: 0s - loss: 0.2607
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2351
3456/6530 [==============>...............] - ETA: 0s - loss: 0.2617
6530/6530 [==============================] - 0s 48us/step - loss: 0.2328 - val_loss: 0.2357
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2093
3872/6530 [================>.............] - ETA: 0s - loss: 0.2600
1088/6530 [===>..........................] - ETA: 0s - loss: 0.2220
4320/6530 [==================>...........] - ETA: 0s - loss: 0.2581
2304/6530 [=========>....................] - ETA: 0s - loss: 0.2133
4768/6530 [====================>.........] - ETA: 0s - loss: 0.2573
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2136
5216/6530 [======================>.......] - ETA: 0s - loss: 0.2557
4800/6530 [=====================>........] - ETA: 0s - loss: 0.2137
# training | RMSE: 0.1692, MAE: 0.1326
worker 1  xfile  [7, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22573404172252914}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1692494086166965, 'rmse': 0.1692494086166965, 'mae': 0.13256268895780549, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  79 | activation: tanh    | extras: batchnorm 
layer 2 | size:  78 | activation: tanh    | extras: dropout - rate: 23.4% 
layer 3 | size:  71 | activation: tanh    | extras: None 
layer 4 | size:  62 | activation: relu    | extras: None 
layer 5 | size:  48 | activation: tanh    | extras: dropout - rate: 13.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e4d665d68>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 10s - loss: 1.5600
5664/6530 [=========================>....] - ETA: 0s - loss: 0.2557
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2209 
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2122
6112/6530 [===========================>..] - ETA: 0s - loss: 0.2549
6530/6530 [==============================] - 0s 46us/step - loss: 0.2109 - val_loss: 0.2153

6530/6530 [==============================] - 1s 86us/step - loss: 0.1762 - val_loss: 0.0314
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0395
6528/6530 [============================>.] - ETA: 0s - loss: 0.2542
6530/6530 [==============================] - 1s 122us/step - loss: 0.2542 - val_loss: 0.2441
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2541
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0379
6530/6530 [==============================] - 0s 11us/step - loss: 0.0365 - val_loss: 0.0252
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0371
 480/6530 [=>............................] - ETA: 0s - loss: 0.2520
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0310
 928/6530 [===>..........................] - ETA: 0s - loss: 0.2507
6530/6530 [==============================] - 0s 11us/step - loss: 0.0309 - val_loss: 0.0212

1408/6530 [=====>........................] - ETA: 0s - loss: 0.2402
1856/6530 [=======>......................] - ETA: 0s - loss: 0.2365
2336/6530 [=========>....................] - ETA: 0s - loss: 0.2343
2816/6530 [===========>..................] - ETA: 0s - loss: 0.2343
3296/6530 [==============>...............] - ETA: 0s - loss: 0.2339
3744/6530 [================>.............] - ETA: 0s - loss: 0.2344
4256/6530 [==================>...........] - ETA: 0s - loss: 0.2343
4672/6530 [====================>.........] - ETA: 0s - loss: 0.2334
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2334
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2326
6016/6530 [==========================>...] - ETA: 0s - loss: 0.2321
6464/6530 [============================>.] - ETA: 0s - loss: 0.2316
6530/6530 [==============================] - 1s 117us/step - loss: 0.2314 - val_loss: 0.2269

# training | RMSE: 0.1308, MAE: 0.1026
worker 1  xfile  [12, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23440243735423638}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 62, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13291746662379664}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.13084272856136359, 'rmse': 0.13084272856136359, 'mae': 0.10264168519781527, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  31 | activation: sigmoid | extras: None 
layer 2 | size:  61 | activation: sigmoid | extras: None 
layer 3 | size:  63 | activation: relu    | extras: dropout - rate: 42.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e9012c748>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 30s - loss: 1.4615
1920/6530 [=======>......................] - ETA: 0s - loss: 0.4159 
# training | RMSE: 0.2580, MAE: 0.2149
worker 2  xfile  [11, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.25455619904572774}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 58, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 5, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2579716674798562, 'rmse': 0.2579716674798562, 'mae': 0.2148980508477704, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  25 | activation: relu    | extras: batchnorm 
layer 2 | size:  54 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e8013f828>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:44 - loss: 0.9507
3840/6530 [================>.............] - ETA: 0s - loss: 0.3123
 368/6530 [>.............................] - ETA: 7s - loss: 0.5080  
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2754
 736/6530 [==>...........................] - ETA: 4s - loss: 0.3564
6530/6530 [==============================] - 1s 79us/step - loss: 0.2630 - val_loss: 0.1854
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1947
1088/6530 [===>..........................] - ETA: 2s - loss: 0.2851
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1887
1440/6530 [=====>........................] - ETA: 2s - loss: 0.2449
3840/6530 [================>.............] - ETA: 0s - loss: 0.1827
1776/6530 [=======>......................] - ETA: 1s - loss: 0.2187
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1775
2128/6530 [========>.....................] - ETA: 1s - loss: 0.2026
6530/6530 [==============================] - 0s 29us/step - loss: 0.1749 - val_loss: 0.1735
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1609
2496/6530 [==========>...................] - ETA: 1s - loss: 0.1871
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1662
2848/6530 [============>.................] - ETA: 1s - loss: 0.1737
3840/6530 [================>.............] - ETA: 0s - loss: 0.1620
3184/6530 [=============>................] - ETA: 0s - loss: 0.1627
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1626
6530/6530 [==============================] - 0s 28us/step - loss: 0.1627 - val_loss: 0.1611

3552/6530 [===============>..............] - ETA: 0s - loss: 0.1536
3920/6530 [=================>............] - ETA: 0s - loss: 0.1458
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1388
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1331
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1282
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1230
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1190
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1153
6432/6530 [============================>.] - ETA: 0s - loss: 0.1117
# training | RMSE: 0.2850, MAE: 0.2219
worker 0  xfile  [10, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 6, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42656591904286645}, 'layer_2_size': 53, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43977270884349884}, 'layer_3_size': 74, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21694647959011945}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2850121847164537, 'rmse': 0.2850121847164537, 'mae': 0.22194217613261033, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  25 | activation: tanh    | extras: batchnorm 
layer 2 | size:  19 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e686643c8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 10s - loss: 0.7355
5888/6530 [==========================>...] - ETA: 0s - loss: 0.4438 
6530/6530 [==============================] - 1s 216us/step - loss: 0.1108 - val_loss: 0.0468
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0465
# training | RMSE: 0.2009, MAE: 0.1597
worker 1  xfile  [14, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4276091076951002}, 'layer_3_size': 63, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.29307793174816765}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19464477406657954}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.2009208564602402, 'rmse': 0.2009208564602402, 'mae': 0.15965067674809508, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  88 | activation: relu    | extras: None 
layer 2 | size:  48 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e4428c3c8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:41 - loss: 0.5879
6530/6530 [==============================] - 1s 86us/step - loss: 0.4286 - val_loss: 0.2714
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2752
 336/6530 [>.............................] - ETA: 0s - loss: 0.0414
 576/6530 [=>............................] - ETA: 3s - loss: 0.3075  
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2446
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0484
6530/6530 [==============================] - 0s 10us/step - loss: 0.2425 - val_loss: 0.2168
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2231
1120/6530 [====>.........................] - ETA: 1s - loss: 0.2537
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0465
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2088
6530/6530 [==============================] - 0s 10us/step - loss: 0.2079 - val_loss: 0.1999

1664/6530 [======>.......................] - ETA: 1s - loss: 0.2246
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0477
2192/6530 [=========>....................] - ETA: 0s - loss: 0.2091
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0488
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1997
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0478
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1907
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0475
3840/6530 [================>.............] - ETA: 0s - loss: 0.1839
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0474
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1784
3104/6530 [=============>................] - ETA: 0s - loss: 0.0475
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1739
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0480
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1699
3808/6530 [================>.............] - ETA: 0s - loss: 0.0479
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1669
4128/6530 [=================>............] - ETA: 0s - loss: 0.0470
6530/6530 [==============================] - 1s 137us/step - loss: 0.1645 - val_loss: 0.1402
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0999
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0466
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0459
 592/6530 [=>............................] - ETA: 0s - loss: 0.1247
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1242
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0457
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1244
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0454
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1238
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0450
2880/6530 [============>.................] - ETA: 0s - loss: 0.1237
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0450
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1226
6530/6530 [==============================] - 1s 153us/step - loss: 0.0445 - val_loss: 0.0352
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0433
3984/6530 [=================>............] - ETA: 0s - loss: 0.1214
 368/6530 [>.............................] - ETA: 0s - loss: 0.0417
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1204
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0407
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1203
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0389
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1189
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0393
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1188
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0389
6530/6530 [==============================] - 1s 95us/step - loss: 0.1183 - val_loss: 0.1215
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0785
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0390
 560/6530 [=>............................] - ETA: 0s - loss: 0.1012
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0376
1168/6530 [====>.........................] - ETA: 0s - loss: 0.1041
2848/6530 [============>.................] - ETA: 0s - loss: 0.0378
1744/6530 [=======>......................] - ETA: 0s - loss: 0.1039
3200/6530 [=============>................] - ETA: 0s - loss: 0.0373
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1044
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0375
2896/6530 [============>.................] - ETA: 0s - loss: 0.1042
3920/6530 [=================>............] - ETA: 0s - loss: 0.0375
3408/6530 [==============>...............] - ETA: 0s - loss: 0.1034
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0367
4000/6530 [=================>............] - ETA: 0s - loss: 0.1030
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0369
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1026
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0365
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1032
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0362
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1023
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0359
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1022
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0358
6530/6530 [==============================] - 1s 93us/step - loss: 0.1019 - val_loss: 0.1095

6384/6530 [============================>.] - ETA: 0s - loss: 0.0351
6530/6530 [==============================] - 1s 150us/step - loss: 0.0350 - val_loss: 0.0261

# training | RMSE: 0.2579, MAE: 0.1958
worker 0  xfile  [15, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2579019752598471, 'rmse': 0.2579019752598471, 'mae': 0.19582808156554082, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  32 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  88 | activation: sigmoid | extras: dropout - rate: 11.7% 
layer 3 | size:  89 | activation: tanh    | extras: None 
layer 4 | size:  20 | activation: sigmoid | extras: batchnorm 
layer 5 | size:  36 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4d26b240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 42s - loss: 0.9858
1792/6530 [=======>......................] - ETA: 2s - loss: 0.6264 
3456/6530 [==============>...............] - ETA: 0s - loss: 0.5099
5248/6530 [=======================>......] - ETA: 0s - loss: 0.4238
6530/6530 [==============================] - 1s 174us/step - loss: 0.3729 - val_loss: 0.1184
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1440
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1080
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0919
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0797
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0717
6530/6530 [==============================] - 0s 39us/step - loss: 0.0693 - val_loss: 0.3192
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0436
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0504
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0471
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0445
6530/6530 [==============================] - 0s 33us/step - loss: 0.0433 - val_loss: 0.0494

# training | RMSE: 0.1484, MAE: 0.1154
worker 2  xfile  [13, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10896181943545252}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.14835305010725877, 'rmse': 0.14835305010725877, 'mae': 0.11539131942927915, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  34 | activation: tanh    | extras: dropout - rate: 15.3% 
layer 2 | size:  83 | activation: sigmoid | extras: None 
layer 3 | size:  86 | activation: relu    | extras: batchnorm 
layer 4 | size:  18 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e887094a8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 12s - loss: 1.8126
4352/6530 [==================>...........] - ETA: 0s - loss: 1.5712 
6530/6530 [==============================] - 1s 97us/step - loss: 1.4382 - val_loss: 0.7342
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 1.0090
4608/6530 [====================>.........] - ETA: 0s - loss: 0.7424
6530/6530 [==============================] - 0s 13us/step - loss: 0.6402 - val_loss: 0.2509
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.3246
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2252
6530/6530 [==============================] - 0s 14us/step - loss: 0.2061 - val_loss: 0.3891

# training | RMSE: 0.1372, MAE: 0.1042
worker 1  xfile  [16, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.13718221781550244, 'rmse': 0.13718221781550244, 'mae': 0.10416280010118147, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  23 | activation: relu    | extras: batchnorm 
layer 2 | size:  48 | activation: tanh    | extras: dropout - rate: 47.3% 
layer 3 | size:  40 | activation: tanh    | extras: dropout - rate: 41.0% 
layer 4 | size:  23 | activation: relu    | extras: batchnorm 
layer 5 | size:  64 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4c5bf7b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 2:34 - loss: 1.4732
 448/6530 [=>............................] - ETA: 10s - loss: 1.1325 
# training | RMSE: 0.2163, MAE: 0.1740
worker 0  xfile  [17, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 32, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11718869715812273}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.21628336980707738, 'rmse': 0.21628336980707738, 'mae': 0.17397033857474445, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  82 | activation: sigmoid | extras: None 
layer 2 | size:  75 | activation: tanh    | extras: None 
layer 3 | size:  15 | activation: sigmoid | extras: dropout - rate: 30.0% 
layer 4 | size:   3 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4ca70ef0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 3:27 - loss: 0.6653
 928/6530 [===>..........................] - ETA: 5s - loss: 0.7839 
 416/6530 [>.............................] - ETA: 8s - loss: 0.4175  
1376/6530 [=====>........................] - ETA: 3s - loss: 0.6311
 880/6530 [===>..........................] - ETA: 3s - loss: 0.3222
1856/6530 [=======>......................] - ETA: 2s - loss: 0.5388
1360/6530 [=====>........................] - ETA: 2s - loss: 0.2921
2336/6530 [=========>....................] - ETA: 1s - loss: 0.4804
1824/6530 [=======>......................] - ETA: 1s - loss: 0.2714
2784/6530 [===========>..................] - ETA: 1s - loss: 0.4401
2272/6530 [=========>....................] - ETA: 1s - loss: 0.2599
3200/6530 [=============>................] - ETA: 1s - loss: 0.4099
2720/6530 [===========>..................] - ETA: 1s - loss: 0.2501
3552/6530 [===============>..............] - ETA: 0s - loss: 0.3911
3120/6530 [=============>................] - ETA: 0s - loss: 0.2446
4000/6530 [=================>............] - ETA: 0s - loss: 0.3727
3552/6530 [===============>..............] - ETA: 0s - loss: 0.2384
4416/6530 [===================>..........] - ETA: 0s - loss: 0.3572
3952/6530 [=================>............] - ETA: 0s - loss: 0.2344
4832/6530 [=====================>........] - ETA: 0s - loss: 0.3447
4384/6530 [===================>..........] - ETA: 0s - loss: 0.2301
5280/6530 [=======================>......] - ETA: 0s - loss: 0.3335
4800/6530 [=====================>........] - ETA: 0s - loss: 0.2259
# training | RMSE: 0.4335, MAE: 0.3755
worker 2  xfile  [19, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15288538110580896}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 83, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 18, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.4334609814366345, 'rmse': 0.4334609814366345, 'mae': 0.3755348387867074, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  66 | activation: tanh    | extras: None 
layer 2 | size:  41 | activation: sigmoid | extras: dropout - rate: 14.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e709c0828>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:05 - loss: 0.4162
5728/6530 [=========================>....] - ETA: 0s - loss: 0.3240
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1171  
5216/6530 [======================>.......] - ETA: 0s - loss: 0.2228
6176/6530 [===========================>..] - ETA: 0s - loss: 0.3154
5696/6530 [=========================>....] - ETA: 0s - loss: 0.2203
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0796
3264/6530 [=============>................] - ETA: 0s - loss: 0.0690
6144/6530 [===========================>..] - ETA: 0s - loss: 0.2169
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0623
6530/6530 [==============================] - 2s 248us/step - loss: 0.3088 - val_loss: 0.1846
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2057
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0579
 512/6530 [=>............................] - ETA: 0s - loss: 0.2062
6530/6530 [==============================] - 1s 208us/step - loss: 0.2146 - val_loss: 0.1704
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.2030
 960/6530 [===>..........................] - ETA: 0s - loss: 0.2073
 480/6530 [=>............................] - ETA: 0s - loss: 0.1748
6530/6530 [==============================] - 1s 103us/step - loss: 0.0553 - val_loss: 0.0419
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0460
1408/6530 [=====>........................] - ETA: 0s - loss: 0.2032
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1774
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0421
1856/6530 [=======>......................] - ETA: 0s - loss: 0.2003
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1764
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0422
2304/6530 [=========>....................] - ETA: 0s - loss: 0.2002
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1795
3200/6530 [=============>................] - ETA: 0s - loss: 0.0417
2720/6530 [===========>..................] - ETA: 0s - loss: 0.2009
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1775
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0419
3168/6530 [=============>................] - ETA: 0s - loss: 0.2006
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1778
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0413
3616/6530 [===============>..............] - ETA: 0s - loss: 0.2006
3120/6530 [=============>................] - ETA: 0s - loss: 0.1765
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0407
6530/6530 [==============================] - 0s 51us/step - loss: 0.0408 - val_loss: 0.0401
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0331
4064/6530 [=================>............] - ETA: 0s - loss: 0.2001
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1770
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0379
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1998
3968/6530 [=================>............] - ETA: 0s - loss: 0.1780
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0381
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1990
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1770
3264/6530 [=============>................] - ETA: 0s - loss: 0.0375
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1988
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1770
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0375
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1987
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1763
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0381
6368/6530 [============================>.] - ETA: 0s - loss: 0.1980
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1758
6496/6530 [============================>.] - ETA: 0s - loss: 0.0379
6530/6530 [==============================] - 1s 118us/step - loss: 0.1978 - val_loss: 0.1766
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2023
6530/6530 [==============================] - 0s 49us/step - loss: 0.0379 - val_loss: 0.0371

6256/6530 [===========================>..] - ETA: 0s - loss: 0.1756
 480/6530 [=>............................] - ETA: 0s - loss: 0.1959
6530/6530 [==============================] - 1s 121us/step - loss: 0.1757 - val_loss: 0.1589
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1817
 928/6530 [===>..........................] - ETA: 0s - loss: 0.2003
 464/6530 [=>............................] - ETA: 0s - loss: 0.1729
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1967
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1742
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1940
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1704
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1939
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1704
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1930
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1721
3264/6530 [=============>................] - ETA: 0s - loss: 0.1923
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1723
3712/6530 [================>.............] - ETA: 0s - loss: 0.1916
3152/6530 [=============>................] - ETA: 0s - loss: 0.1704
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1911
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1706
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1903
4112/6530 [=================>............] - ETA: 0s - loss: 0.1700
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1896
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1699
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1893
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1699
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1897
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1690
6400/6530 [============================>.] - ETA: 0s - loss: 0.1889
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1697
6530/6530 [==============================] - 1s 119us/step - loss: 0.1888 - val_loss: 0.1652

6496/6530 [============================>.] - ETA: 0s - loss: 0.1690
6530/6530 [==============================] - 1s 115us/step - loss: 0.1690 - val_loss: 0.1590

# training | RMSE: 0.2015, MAE: 0.1582
worker 1  xfile  [18, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47263121756876636}, 'layer_2_size': 48, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4103126608856419}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 23, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2015307856271881, 'rmse': 0.2015307856271881, 'mae': 0.15819558806805448, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  18 | activation: tanh    | extras: dropout - rate: 30.7% 
layer 2 | size:  33 | activation: tanh    | extras: dropout - rate: 20.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4c3aaa58>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:12 - loss: 0.5670
# training | RMSE: 0.1922, MAE: 0.1547
worker 2  xfile  [21, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14248100539073072}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.19218339267738882, 'rmse': 0.19218339267738882, 'mae': 0.15472147284014903, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  63 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  94 | activation: tanh    | extras: dropout - rate: 19.4% 
layer 3 | size:  38 | activation: tanh    | extras: batchnorm 
layer 4 | size:  32 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e8007f668>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 1:26 - loss: 0.7221
 704/6530 [==>...........................] - ETA: 3s - loss: 0.3737  
 768/6530 [==>...........................] - ETA: 6s - loss: 0.5119  
1376/6530 [=====>........................] - ETA: 1s - loss: 0.2396
1536/6530 [======>.......................] - ETA: 3s - loss: 0.3464
2144/6530 [========>.....................] - ETA: 1s - loss: 0.1843
2368/6530 [=========>....................] - ETA: 1s - loss: 0.2473
2944/6530 [============>.................] - ETA: 0s - loss: 0.1534
# training | RMSE: 0.1950, MAE: 0.1558
worker 0  xfile  [20, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 82, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.29969934092088746}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 3, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.19500985521978548, 'rmse': 0.19500985521978548, 'mae': 0.15580471171176674, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  61 | activation: tanh    | extras: dropout - rate: 34.8% 
layer 2 | size:  41 | activation: relu    | extras: dropout - rate: 16.1% 
layer 3 | size:   4 | activation: relu    | extras: dropout - rate: 25.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e44251588>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 3:30 - loss: 1.4260
3264/6530 [=============>................] - ETA: 1s - loss: 0.1903
3808/6530 [================>.............] - ETA: 0s - loss: 0.1332
 416/6530 [>.............................] - ETA: 8s - loss: 0.6071  
4096/6530 [=================>............] - ETA: 0s - loss: 0.1605
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1196
 784/6530 [==>...........................] - ETA: 4s - loss: 0.4595
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1426
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1103
1168/6530 [====>.........................] - ETA: 3s - loss: 0.3830
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1258
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1017
1568/6530 [======>.......................] - ETA: 2s - loss: 0.3244
1968/6530 [========>.....................] - ETA: 1s - loss: 0.2881
6530/6530 [==============================] - 1s 130us/step - loss: 0.0989 - val_loss: 0.0519
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0563
6530/6530 [==============================] - 1s 206us/step - loss: 0.1167 - val_loss: 0.0417
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0324
2352/6530 [=========>....................] - ETA: 1s - loss: 0.2600
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0503
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0419
2736/6530 [===========>..................] - ETA: 1s - loss: 0.2426
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0469
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0403
3104/6530 [=============>................] - ETA: 1s - loss: 0.2258
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0475
2944/6530 [============>.................] - ETA: 0s - loss: 0.0403
3504/6530 [===============>..............] - ETA: 0s - loss: 0.2121
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0462
3904/6530 [================>.............] - ETA: 0s - loss: 0.0410
3888/6530 [================>.............] - ETA: 0s - loss: 0.2013
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0457
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0407
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1925
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0457
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0410
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1847
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0455
6530/6530 [==============================] - 0s 57us/step - loss: 0.0414 - val_loss: 0.0410

5008/6530 [======================>.......] - ETA: 0s - loss: 0.1769Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0419
6530/6530 [==============================] - 0s 62us/step - loss: 0.0450 - val_loss: 0.0435
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0502
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1713
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0405
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0440
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1651
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0412
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0425
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1597
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0411
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0434
3776/6530 [================>.............] - ETA: 0s - loss: 0.0416
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0426
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0414
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0428
6530/6530 [==============================] - 1s 226us/step - loss: 0.1551 - val_loss: 0.0725
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0873
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0416
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0430
 416/6530 [>.............................] - ETA: 0s - loss: 0.0854
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0429
6530/6530 [==============================] - 0s 57us/step - loss: 0.0411 - val_loss: 0.0413

6530/6530 [==============================] - 0s 61us/step - loss: 0.0427 - val_loss: 0.0476

 800/6530 [==>...........................] - ETA: 0s - loss: 0.0832
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0795
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0772
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0770
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0766
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0750
3168/6530 [=============>................] - ETA: 0s - loss: 0.0743
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0741
3936/6530 [=================>............] - ETA: 0s - loss: 0.0743
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0730
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0721
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0717
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0708
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0700
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0698
6530/6530 [==============================] - 1s 137us/step - loss: 0.0694 - val_loss: 0.0631
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0971
 352/6530 [>.............................] - ETA: 0s - loss: 0.0620
# training | RMSE: 0.2174, MAE: 0.1801
worker 1  xfile  [23, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30708224429563385}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20554577126203855}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.38782014643625917}, 'layer_4_size': 99, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.21739937939453108, 'rmse': 0.21739937939453108, 'mae': 0.18012234723886728, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  11 | activation: relu    | extras: None 
layer 2 | size:   8 | activation: tanh    | extras: dropout - rate: 47.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4c59b668>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 9s - loss: 0.7272
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0642
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0673
6530/6530 [==============================] - 0s 71us/step - loss: 0.6297 - val_loss: 0.2467
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2401
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0634
6530/6530 [==============================] - 0s 7us/step - loss: 0.2303 - val_loss: 0.2184
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2185
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0627
6530/6530 [==============================] - 0s 7us/step - loss: 0.2104 - val_loss: 0.1967

# training | RMSE: 0.2007, MAE: 0.1622
worker 2  xfile  [22, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1941426640196279}, 'layer_2_size': 94, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.20072150778951792, 'rmse': 0.20072150778951792, 'mae': 0.16221537033327407, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  71 | activation: tanh    | extras: dropout - rate: 17.2% 
layer 2 | size:  98 | activation: relu    | extras: dropout - rate: 48.4% 
layer 3 | size:  70 | activation: sigmoid | extras: None 
layer 4 | size:  37 | activation: relu    | extras: dropout - rate: 21.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e70efc780>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 52s - loss: 0.5240
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0621
1408/6530 [=====>........................] - ETA: 2s - loss: 0.2937 
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0617
2624/6530 [===========>..................] - ETA: 0s - loss: 0.2523
3024/6530 [============>.................] - ETA: 0s - loss: 0.0606
4096/6530 [=================>............] - ETA: 0s - loss: 0.2280
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0610
5440/6530 [=======================>......] - ETA: 0s - loss: 0.2165
3840/6530 [================>.............] - ETA: 0s - loss: 0.0608
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0606
6530/6530 [==============================] - 1s 128us/step - loss: 0.2086 - val_loss: 0.1453

4608/6530 [====================>.........] - ETA: 0s - loss: 0.0604Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1694
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1710
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0599
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1675
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0598
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0593
4096/6530 [=================>............] - ETA: 0s - loss: 0.1645
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0590
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1639
6530/6530 [==============================] - 0s 40us/step - loss: 0.1618 - val_loss: 0.1259
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1437
6530/6530 [==============================] - 1s 137us/step - loss: 0.0586 - val_loss: 0.0567

1472/6530 [=====>........................] - ETA: 0s - loss: 0.1519
2880/6530 [============>.................] - ETA: 0s - loss: 0.1508
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1514
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1514
6530/6530 [==============================] - 0s 39us/step - loss: 0.1509 - val_loss: 0.1209

# training | RMSE: 0.2431, MAE: 0.1977
worker 1  xfile  [26, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4770486714254554}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3323045329803257}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.243087361046285, 'rmse': 0.243087361046285, 'mae': 0.19772493787391973, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  80 | activation: tanh    | extras: None 
layer 2 | size:  86 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  49 | activation: relu    | extras: None 
layer 4 | size:   3 | activation: relu    | extras: dropout - rate: 42.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b45d1ef98>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 38s - loss: 0.7978
1408/6530 [=====>........................] - ETA: 2s - loss: 0.4611 
2816/6530 [===========>..................] - ETA: 1s - loss: 0.3400
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2806
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2505
# training | RMSE: 0.2372, MAE: 0.1959
worker 0  xfile  [24, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34751031124990506}, 'layer_1_size': 61, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16053681216323856}, 'layer_2_size': 41, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2537283937797355}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4229999232812024}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.23724547047343728, 'rmse': 0.23724547047343728, 'mae': 0.19591630644914043, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  89 | activation: relu    | extras: dropout - rate: 15.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4cabd898>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:39 - loss: 0.4000
1120/6530 [====>.........................] - ETA: 2s - loss: 0.1528  
6530/6530 [==============================] - 1s 165us/step - loss: 0.2417 - val_loss: 0.2151
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1823
2240/6530 [=========>....................] - ETA: 1s - loss: 0.1074
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1531
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0893
3968/6530 [=================>............] - ETA: 0s - loss: 0.1534
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0790
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1501
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0723
6530/6530 [==============================] - 0s 28us/step - loss: 0.1490 - val_loss: 0.1446
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1682
# training | RMSE: 0.1493, MAE: 0.1151
worker 2  xfile  [25, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17166616289134531}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4844414424363912}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21146430495516638}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3583376083280042}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.14932455007251005, 'rmse': 0.14932455007251005, 'mae': 0.11509835594746279, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  62 | activation: relu    | extras: None 
layer 2 | size:  14 | activation: sigmoid | extras: dropout - rate: 14.6% 
layer 3 | size:  62 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e70168908>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 11s - loss: 2.3449
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1395
6530/6530 [==============================] - 1s 130us/step - loss: 0.0692 - val_loss: 0.0426
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0467
3968/6530 [=================>............] - ETA: 0s - loss: 0.1376
6530/6530 [==============================] - 1s 88us/step - loss: 1.3033 - val_loss: 0.4590
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.4519
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0400
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1352
6530/6530 [==============================] - 0s 28us/step - loss: 0.1344 - val_loss: 0.2627

6530/6530 [==============================] - 0s 8us/step - loss: 0.2703 - val_loss: 0.2205
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2253
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0400
6530/6530 [==============================] - 0s 8us/step - loss: 0.2318 - val_loss: 0.2130

3680/6530 [===============>..............] - ETA: 0s - loss: 0.0390
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0392
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0391
6530/6530 [==============================] - 0s 45us/step - loss: 0.0390 - val_loss: 0.0382
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0371
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0348
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0346
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0347
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0347
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0345
6530/6530 [==============================] - 0s 44us/step - loss: 0.0346 - val_loss: 0.0349

# training | RMSE: 0.2646, MAE: 0.2166
worker 2  xfile  [29, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14588882713852366}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.26458587220129376, 'rmse': 0.26458587220129376, 'mae': 0.2165919837375706, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  69 | activation: tanh    | extras: None 
layer 2 | size:  61 | activation: relu    | extras: None 
layer 3 | size:  41 | activation: relu    | extras: dropout - rate: 48.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e63cb57f0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:49 - loss: 0.6960
 608/6530 [=>............................] - ETA: 5s - loss: 0.7020  
1376/6530 [=====>........................] - ETA: 2s - loss: 0.6022
2176/6530 [========>.....................] - ETA: 1s - loss: 0.4523
2912/6530 [============>.................] - ETA: 0s - loss: 0.3809
3744/6530 [================>.............] - ETA: 0s - loss: 0.3327
4608/6530 [====================>.........] - ETA: 0s - loss: 0.3013
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2791
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2653
# training | RMSE: 0.3123, MAE: 0.2587
worker 1  xfile  [27, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 80, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 86, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42019757421631054}, 'layer_4_size': 3, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.3123204646610663, 'rmse': 0.3123204646610663, 'mae': 0.25869320669554774, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  26 | activation: tanh    | extras: dropout - rate: 39.2% 
layer 2 | size:   6 | activation: relu    | extras: batchnorm 
layer 3 | size:  54 | activation: relu    | extras: dropout - rate: 11.0% 
layer 4 | size:  12 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b459f81d0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 1:08 - loss: 0.7547
 832/6530 [==>...........................] - ETA: 5s - loss: 0.6896  
6530/6530 [==============================] - 1s 160us/step - loss: 0.2610 - val_loss: 0.1675
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1738
1856/6530 [=======>......................] - ETA: 1s - loss: 0.6366
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1673
2944/6530 [============>.................] - ETA: 1s - loss: 0.5402
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1620
4032/6530 [=================>............] - ETA: 0s - loss: 0.4651
# training | RMSE: 0.1790, MAE: 0.1426
worker 0  xfile  [28, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15723407789772303}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10091696738640682}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3541840405276866}, 'layer_4_size': 12, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13378420333817492}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.17903848822243953, 'rmse': 0.17903848822243953, 'mae': 0.1426274712488588, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  96 | activation: relu    | extras: None 
layer 2 | size:  88 | activation: sigmoid | extras: dropout - rate: 20.6% 
layer 3 | size:  52 | activation: tanh    | extras: None 
layer 4 | size:  26 | activation: tanh    | extras: dropout - rate: 21.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5b4c487b70>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 2:06 - loss: 0.7483
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1628
5184/6530 [======================>.......] - ETA: 0s - loss: 0.4150
 768/6530 [==>...........................] - ETA: 5s - loss: 0.3919  
3200/6530 [=============>................] - ETA: 0s - loss: 0.1610
6400/6530 [============================>.] - ETA: 0s - loss: 0.3764
1472/6530 [=====>........................] - ETA: 2s - loss: 0.3210
4064/6530 [=================>............] - ETA: 0s - loss: 0.1621
2208/6530 [=========>....................] - ETA: 1s - loss: 0.2798
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1617
6530/6530 [==============================] - 1s 166us/step - loss: 0.3728 - val_loss: 0.2068
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2007
2912/6530 [============>.................] - ETA: 1s - loss: 0.2579
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1611
1216/6530 [====>.........................] - ETA: 0s - loss: 0.2002
3616/6530 [===============>..............] - ETA: 0s - loss: 0.2426
6530/6530 [==============================] - 0s 65us/step - loss: 0.1601 - val_loss: 0.1507
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1480
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1893
4320/6530 [==================>...........] - ETA: 0s - loss: 0.2308
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1487
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1875
5024/6530 [======================>.......] - ETA: 0s - loss: 0.2226
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1447
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1835
5728/6530 [=========================>....] - ETA: 0s - loss: 0.2155
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1448
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1826
6464/6530 [============================>.] - ETA: 0s - loss: 0.2099
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1428
6530/6530 [==============================] - 0s 47us/step - loss: 0.1803 - val_loss: 0.1633
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1874
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1410
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1649
6530/6530 [==============================] - 1s 179us/step - loss: 0.2095 - val_loss: 0.1454
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1686
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1400
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1648
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1776
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1390
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1636
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1662
6530/6530 [==============================] - 0s 63us/step - loss: 0.1376 - val_loss: 0.1324

4608/6530 [====================>.........] - ETA: 0s - loss: 0.1654
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1606
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1666
2976/6530 [============>.................] - ETA: 0s - loss: 0.1564
6530/6530 [==============================] - 0s 47us/step - loss: 0.1672 - val_loss: 0.1602

3680/6530 [===============>..............] - ETA: 0s - loss: 0.1542
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1520
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1500
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1477
6496/6530 [============================>.] - ETA: 0s - loss: 0.1462
6530/6530 [==============================] - 0s 74us/step - loss: 0.1462 - val_loss: 0.1398
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1335
# training | RMSE: 0.2017, MAE: 0.1572
worker 1  xfile  [30, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39239340644363085}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11000700240545346}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.201674201383799, 'rmse': 0.201674201383799, 'mae': 0.15716755402677654, 'early_stop': False}
vggnet done  1

 800/6530 [==>...........................] - ETA: 0s - loss: 0.1390
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1313
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1291
3168/6530 [=============>................] - ETA: 0s - loss: 0.1263
3872/6530 [================>.............] - ETA: 0s - loss: 0.1253
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1244
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1241
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1235
6432/6530 [============================>.] - ETA: 0s - loss: 0.1229
6530/6530 [==============================] - 0s 75us/step - loss: 0.1227 - val_loss: 0.0996

# training | RMSE: 0.1592, MAE: 0.1270
worker 2  xfile  [31, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4799552058691362}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.15918746158748381, 'rmse': 0.15918746158748381, 'mae': 0.12704699668635658, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  20 | activation: tanh    | extras: dropout - rate: 13.7% 
layer 2 | size:  58 | activation: relu    | extras: dropout - rate: 33.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e701ae128>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 42s - loss: 2.1806
1920/6530 [=======>......................] - ETA: 1s - loss: 0.7862 
3904/6530 [================>.............] - ETA: 0s - loss: 0.4932
5952/6530 [==========================>...] - ETA: 0s - loss: 0.3625
# training | RMSE: 0.1266, MAE: 0.0951
worker 0  xfile  [32, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20598381938009017}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21312643339710788}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22227484043894946}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1265609198285173, 'rmse': 0.1265609198285173, 'mae': 0.09507368101282446, 'early_stop': False}
vggnet done  0

6530/6530 [==============================] - 1s 100us/step - loss: 0.3382 - val_loss: 0.0610
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0837
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0760
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0709
6530/6530 [==============================] - 0s 24us/step - loss: 0.0667 - val_loss: 0.0373
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0580
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0518
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0499
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0494
6530/6530 [==============================] - 0s 27us/step - loss: 0.0489 - val_loss: 0.0350

# training | RMSE: 0.1845, MAE: 0.1441
worker 2  xfile  [33, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13736846942029257}, 'layer_1_size': 20, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.33701832872406634}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45553820744359885}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31686787298517216}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.18445512810221498, 'rmse': 0.18445512810221498, 'mae': 0.14408062675362865, 'early_stop': False}
vggnet done  2
all of workers have been done
calculation finished
#0 epoch=3.0 loss={'loss': 0.29976423699273, 'rmse': 0.29976423699273, 'mae': 0.2547234262770374, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42143477994447576}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.31223754765409084}, 'layer_2_size': 96, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23424407254280194}, 'layer_3_size': 81, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 88, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.36965973742492475}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#2 epoch=3.0 loss={'loss': 0.3159868504141935, 'rmse': 0.3159868504141935, 'mae': 0.2724558964398283, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22344854801017078}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.43379468411831923}, 'layer_2_size': 88, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.237543168500524}, 'layer_4_size': 70, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#3 epoch=3.0 loss={'loss': 0.22709574956572226, 'rmse': 0.22709574956572226, 'mae': 0.19068296495552425, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3336391431416651}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 37, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1985387424526034}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#5 epoch=3.0 loss={'loss': 0.23548609947100957, 'rmse': 0.23548609947100957, 'mae': 0.19199739700391313, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24260494912330344}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.245751806831647}, 'layer_3_size': 72, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3313486863097612}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#1 epoch=3.0 loss={'loss': 0.25914226672437723, 'rmse': 0.25914226672437723, 'mae': 0.21634410071725704, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1606854821149849}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.300135921897431}, 'layer_3_size': 11, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4013607522402225}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#6 epoch=3.0 loss={'loss': 0.13680919489308585, 'rmse': 0.13680919489308585, 'mae': 0.10424354734439262, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1400219191605232}, 'layer_2_size': 24, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21573621650431832}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#4 epoch=3.0 loss={'loss': 0.13169573105640692, 'rmse': 0.13169573105640692, 'mae': 0.10366174279884777, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11082940352862432}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4774286097069397}, 'layer_4_size': 27, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22115319953683002}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#8 epoch=3.0 loss={'loss': 0.44785599814449295, 'rmse': 0.44785599814449295, 'mae': 0.3772598285111749, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16328226150226122}, 'layer_1_size': 65, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 68, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#9 epoch=3.0 loss={'loss': 0.19467623784635188, 'rmse': 0.19467623784635188, 'mae': 0.14983309127929142, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2724334792774713}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#7 epoch=3.0 loss={'loss': 0.1692494086166965, 'rmse': 0.1692494086166965, 'mae': 0.13256268895780549, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22573404172252914}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#11 epoch=3.0 loss={'loss': 0.2579716674798562, 'rmse': 0.2579716674798562, 'mae': 0.2148980508477704, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.25455619904572774}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 58, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 5, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#12 epoch=3.0 loss={'loss': 0.13084272856136359, 'rmse': 0.13084272856136359, 'mae': 0.10264168519781527, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23440243735423638}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 62, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13291746662379664}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#10 epoch=3.0 loss={'loss': 0.2850121847164537, 'rmse': 0.2850121847164537, 'mae': 0.22194217613261033, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 6, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42656591904286645}, 'layer_2_size': 53, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43977270884349884}, 'layer_3_size': 74, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21694647959011945}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#14 epoch=3.0 loss={'loss': 0.2009208564602402, 'rmse': 0.2009208564602402, 'mae': 0.15965067674809508, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4276091076951002}, 'layer_3_size': 63, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.29307793174816765}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19464477406657954}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
#15 epoch=3.0 loss={'loss': 0.2579019752598471, 'rmse': 0.2579019752598471, 'mae': 0.19582808156554082, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#16 epoch=3.0 loss={'loss': 0.13718221781550244, 'rmse': 0.13718221781550244, 'mae': 0.10416280010118147, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#13 epoch=3.0 loss={'loss': 0.14835305010725877, 'rmse': 0.14835305010725877, 'mae': 0.11539131942927915, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10896181943545252}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#17 epoch=3.0 loss={'loss': 0.21628336980707738, 'rmse': 0.21628336980707738, 'mae': 0.17397033857474445, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 32, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11718869715812273}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#19 epoch=3.0 loss={'loss': 0.4334609814366345, 'rmse': 0.4334609814366345, 'mae': 0.3755348387867074, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15288538110580896}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 83, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 18, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#21 epoch=3.0 loss={'loss': 0.19218339267738882, 'rmse': 0.19218339267738882, 'mae': 0.15472147284014903, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14248100539073072}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#18 epoch=3.0 loss={'loss': 0.2015307856271881, 'rmse': 0.2015307856271881, 'mae': 0.15819558806805448, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47263121756876636}, 'layer_2_size': 48, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4103126608856419}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 23, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#20 epoch=3.0 loss={'loss': 0.19500985521978548, 'rmse': 0.19500985521978548, 'mae': 0.15580471171176674, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 82, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.29969934092088746}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 3, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#22 epoch=3.0 loss={'loss': 0.20072150778951792, 'rmse': 0.20072150778951792, 'mae': 0.16221537033327407, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1941426640196279}, 'layer_2_size': 94, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#23 epoch=3.0 loss={'loss': 0.21739937939453108, 'rmse': 0.21739937939453108, 'mae': 0.18012234723886728, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30708224429563385}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20554577126203855}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.38782014643625917}, 'layer_4_size': 99, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#26 epoch=3.0 loss={'loss': 0.243087361046285, 'rmse': 0.243087361046285, 'mae': 0.19772493787391973, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4770486714254554}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3323045329803257}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#24 epoch=3.0 loss={'loss': 0.23724547047343728, 'rmse': 0.23724547047343728, 'mae': 0.19591630644914043, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.34751031124990506}, 'layer_1_size': 61, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16053681216323856}, 'layer_2_size': 41, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2537283937797355}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4229999232812024}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#25 epoch=3.0 loss={'loss': 0.14932455007251005, 'rmse': 0.14932455007251005, 'mae': 0.11509835594746279, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17166616289134531}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4844414424363912}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21146430495516638}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3583376083280042}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#27 epoch=3.0 loss={'loss': 0.3123204646610663, 'rmse': 0.3123204646610663, 'mae': 0.25869320669554774, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 80, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 86, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42019757421631054}, 'layer_4_size': 3, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#29 epoch=3.0 loss={'loss': 0.26458587220129376, 'rmse': 0.26458587220129376, 'mae': 0.2165919837375706, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14588882713852366}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#28 epoch=3.0 loss={'loss': 0.17903848822243953, 'rmse': 0.17903848822243953, 'mae': 0.1426274712488588, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15723407789772303}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10091696738640682}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3541840405276866}, 'layer_4_size': 12, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13378420333817492}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#31 epoch=3.0 loss={'loss': 0.15918746158748381, 'rmse': 0.15918746158748381, 'mae': 0.12704699668635658, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4799552058691362}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#30 epoch=3.0 loss={'loss': 0.201674201383799, 'rmse': 0.201674201383799, 'mae': 0.15716755402677654, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39239340644363085}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11000700240545346}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#32 epoch=3.0 loss={'loss': 0.1265609198285173, 'rmse': 0.1265609198285173, 'mae': 0.09507368101282446, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20598381938009017}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21312643339710788}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22227484043894946}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#33 epoch=3.0 loss={'loss': 0.18445512810221498, 'rmse': 0.18445512810221498, 'mae': 0.14408062675362865, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13736846942029257}, 'layer_1_size': 20, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.33701832872406634}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45553820744359885}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31686787298517216}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
get a list [results] of length 155
get a list [loss] of length 34
get a list [val_loss] of length 34
length of indices is [32 11  6  5 15 16 26 30  9 29 33 19  8 21 22 13 20 31 17 23  2  3 25 24
 14 10  4 28 12  0 27  1 18  7]
length of indices is 34
length of T is 34
newly formed T structure is:[[0, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20598381938009017}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21312643339710788}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22227484043894946}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [1, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.25455619904572774}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 58, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 5, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [2, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1400219191605232}, 'layer_2_size': 24, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21573621650431832}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [3, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24260494912330344}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.245751806831647}, 'layer_3_size': 72, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3313486863097612}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [4, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [5, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [6, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4770486714254554}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3323045329803257}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [7, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39239340644363085}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11000700240545346}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [8, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2724334792774713}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [9, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14588882713852366}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [10, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13736846942029257}, 'layer_1_size': 20, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.33701832872406634}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45553820744359885}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31686787298517216}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]] 

*** 11.333333333333332 configurations x 9.0 iterations each

34 | Thu Sep 27 18:40:52 2018 | lowest loss so far: 0.0926 (run 2)

{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  96 | activation: relu    | extras: None 
layer 2 | size:  88 | activation: sigmoid | extras: dropout - rate: 20.6% 
layer 3 | size:  52 | activation: tanh    | extras: None 
layer 4 | size:  26 | activation: tanh    | extras: dropout - rate: 21.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e91314c50>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 3:04 - loss: 0.8214
 672/6530 [==>...........................] - ETA: 8s - loss: 0.3561  
1216/6530 [====>.........................] - ETA: 4s - loss: 0.3029
1760/6530 [=======>......................] - ETA: 2s - loss: 0.2727
2336/6530 [=========>....................] - ETA: 2s - loss: 0.2548{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  57 | activation: sigmoid | extras: dropout - rate: 25.5% 
layer 2 | size:  20 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  58 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e91314cc0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 1:37 - loss: 0.8266
2976/6530 [============>.................] - ETA: 1s - loss: 0.2392
1088/6530 [===>..........................] - ETA: 5s - loss: 0.5960  
3648/6530 [===============>..............] - ETA: 0s - loss: 0.2279
2240/6530 [=========>....................] - ETA: 2s - loss: 0.4715
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2181{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  11 | activation: relu    | extras: None 
layer 2 | size:  24 | activation: tanh    | extras: dropout - rate: 14.0% 
layer 3 | size:  86 | activation: relu    | extras: batchnorm 
layer 4 | size:  36 | activation: sigmoid | extras: dropout - rate: 21.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e91314d30>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 54s - loss: 0.7877
3392/6530 [==============>...............] - ETA: 1s - loss: 0.4144
5056/6530 [======================>.......] - ETA: 0s - loss: 0.2116
2560/6530 [==========>...................] - ETA: 1s - loss: 0.3681 
4544/6530 [===================>..........] - ETA: 0s - loss: 0.3789
5792/6530 [=========================>....] - ETA: 0s - loss: 0.2049
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2330
5760/6530 [=========================>....] - ETA: 0s - loss: 0.3544
6530/6530 [==============================] - 1s 196us/step - loss: 0.1827 - val_loss: 0.0557
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0242
6530/6530 [==============================] - 1s 224us/step - loss: 0.1996 - val_loss: 0.1316
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1476
6530/6530 [==============================] - 1s 200us/step - loss: 0.3417 - val_loss: 0.3041
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2537
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0280
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1524
1088/6530 [===>..........................] - ETA: 0s - loss: 0.2471
4096/6530 [=================>............] - ETA: 0s - loss: 0.0268
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1473
2240/6530 [=========>....................] - ETA: 0s - loss: 0.2422
6528/6530 [============================>.] - ETA: 0s - loss: 0.0255
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1440
6530/6530 [==============================] - 0s 25us/step - loss: 0.0255 - val_loss: 0.0310
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0213
3456/6530 [==============>...............] - ETA: 0s - loss: 0.2373
2912/6530 [============>.................] - ETA: 0s - loss: 0.1438
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0211
4736/6530 [====================>.........] - ETA: 0s - loss: 0.2338
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1420
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0210
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2310
6530/6530 [==============================] - 0s 22us/step - loss: 0.0206 - val_loss: 0.0214
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0219
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1416
6530/6530 [==============================] - 0s 46us/step - loss: 0.2304 - val_loss: 0.2358
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2327
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0180
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1413
1280/6530 [====>.........................] - ETA: 0s - loss: 0.2228
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0185
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1401
2432/6530 [==========>...................] - ETA: 0s - loss: 0.2181
6530/6530 [==============================] - 0s 21us/step - loss: 0.0186 - val_loss: 0.0252
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0263
6530/6530 [==============================] - 0s 71us/step - loss: 0.1388 - val_loss: 0.1130
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1188
3712/6530 [================>.............] - ETA: 0s - loss: 0.2159
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0193
 800/6530 [==>...........................] - ETA: 0s - loss: 0.1254
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2140
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0176
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1275
6530/6530 [==============================] - 0s 21us/step - loss: 0.0173 - val_loss: 0.0190
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0169
6080/6530 [==========================>...] - ETA: 0s - loss: 0.2128
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1278
6530/6530 [==============================] - 0s 44us/step - loss: 0.2120 - val_loss: 0.2390
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2418
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0164
3104/6530 [=============>................] - ETA: 0s - loss: 0.1254
1280/6530 [====>.........................] - ETA: 0s - loss: 0.2071
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0157
3872/6530 [================>.............] - ETA: 0s - loss: 0.1245
2560/6530 [==========>...................] - ETA: 0s - loss: 0.2050
6530/6530 [==============================] - 0s 22us/step - loss: 0.0158 - val_loss: 0.0179
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0168
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1236
3776/6530 [================>.............] - ETA: 0s - loss: 0.2012
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0151
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1235
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2005
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0145
6530/6530 [==============================] - 0s 21us/step - loss: 0.0147 - val_loss: 0.0198
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0164
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1230
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2007
6530/6530 [==============================] - 0s 43us/step - loss: 0.1995 - val_loss: 0.2119
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2057
6530/6530 [==============================] - 0s 68us/step - loss: 0.1225 - val_loss: 0.1074
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1026
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0139
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1961
 800/6530 [==>...........................] - ETA: 0s - loss: 0.1216
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0140
6530/6530 [==============================] - 0s 21us/step - loss: 0.0139 - val_loss: 0.0157
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0141
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1936
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1202
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0142
3712/6530 [================>.............] - ETA: 0s - loss: 0.1915
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1197
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0135
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1915
3168/6530 [=============>................] - ETA: 0s - loss: 0.1164
6530/6530 [==============================] - 0s 23us/step - loss: 0.0135 - val_loss: 0.0205

6208/6530 [===========================>..] - ETA: 0s - loss: 0.1915
3968/6530 [=================>............] - ETA: 0s - loss: 0.1156
6530/6530 [==============================] - 0s 43us/step - loss: 0.1909 - val_loss: 0.1981
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1952
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1150
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1872
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1141
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1897
6400/6530 [============================>.] - ETA: 0s - loss: 0.1142
3904/6530 [================>.............] - ETA: 0s - loss: 0.1870
6530/6530 [==============================] - 0s 67us/step - loss: 0.1138 - val_loss: 0.0989
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1066
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1873
 800/6530 [==>...........................] - ETA: 0s - loss: 0.1045
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1862
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1064
6530/6530 [==============================] - 0s 43us/step - loss: 0.1858 - val_loss: 0.2130
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1858
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1054
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1827
3232/6530 [=============>................] - ETA: 0s - loss: 0.1042
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1819
4064/6530 [=================>............] - ETA: 0s - loss: 0.1034
3712/6530 [================>.............] - ETA: 0s - loss: 0.1822
# training | RMSE: 0.1340, MAE: 0.1015
worker 2  xfile  [2, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1400219191605232}, 'layer_2_size': 24, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21573621650431832}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.13395103236702113, 'rmse': 0.13395103236702113, 'mae': 0.10152556524408891, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  57 | activation: sigmoid | extras: dropout - rate: 24.3% 
layer 2 | size:  18 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e9011e0f0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 18s - loss: 0.6892
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1027
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1821
2240/6530 [=========>....................] - ETA: 0s - loss: 0.3367 
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1030
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1819
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2739
6530/6530 [==============================] - 0s 44us/step - loss: 0.1816 - val_loss: 0.2072
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1826
6528/6530 [============================>.] - ETA: 0s - loss: 0.1026
6530/6530 [==============================] - 0s 65us/step - loss: 0.1025 - val_loss: 0.0958
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0939
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1795
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0923
6530/6530 [==============================] - 0s 56us/step - loss: 0.2534 - val_loss: 0.2059
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2261
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1800
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0958
2176/6530 [========>.....................] - ETA: 0s - loss: 0.2137
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1798
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0955
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2084
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1796
3008/6530 [============>.................] - ETA: 0s - loss: 0.0945
6336/6530 [============================>.] - ETA: 0s - loss: 0.2064
6530/6530 [==============================] - 0s 26us/step - loss: 0.2059 - val_loss: 0.1966
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2277
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1788
3808/6530 [================>.............] - ETA: 0s - loss: 0.0946
6530/6530 [==============================] - 0s 45us/step - loss: 0.1778 - val_loss: 0.2027
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1783
2240/6530 [=========>....................] - ETA: 0s - loss: 0.2049
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0938
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1813
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1987
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0946
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1801
6336/6530 [============================>.] - ETA: 0s - loss: 0.1975
6530/6530 [==============================] - 0s 25us/step - loss: 0.1971 - val_loss: 0.1867
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2097
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0944
3712/6530 [================>.............] - ETA: 0s - loss: 0.1801
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1939
6530/6530 [==============================] - 0s 69us/step - loss: 0.0944 - val_loss: 0.0816
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0882
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1789
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1900
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0926
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1782
6530/6530 [==============================] - 0s 25us/step - loss: 0.1880 - val_loss: 0.1766
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2095
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0921
6530/6530 [==============================] - 0s 46us/step - loss: 0.1778 - val_loss: 0.1907

2112/6530 [========>.....................] - ETA: 0s - loss: 0.1832
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0902
3168/6530 [=============>................] - ETA: 0s - loss: 0.0898
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1798
6464/6530 [============================>.] - ETA: 0s - loss: 0.1788
3936/6530 [=================>............] - ETA: 0s - loss: 0.0898
6530/6530 [==============================] - 0s 25us/step - loss: 0.1787 - val_loss: 0.1683
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2032
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0895
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1787
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0891
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1762
6400/6530 [============================>.] - ETA: 0s - loss: 0.0890
6400/6530 [============================>.] - ETA: 0s - loss: 0.1762
6530/6530 [==============================] - 0s 25us/step - loss: 0.1757 - val_loss: 0.1659
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1956
6530/6530 [==============================] - 0s 66us/step - loss: 0.0888 - val_loss: 0.0816
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0996
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1731
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0888
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1730
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0889
6530/6530 [==============================] - 0s 24us/step - loss: 0.1717 - val_loss: 0.1636
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1798
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0885
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1734
3232/6530 [=============>................] - ETA: 0s - loss: 0.0871
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1709
4064/6530 [=================>............] - ETA: 0s - loss: 0.0871
6464/6530 [============================>.] - ETA: 0s - loss: 0.1708
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0866
6530/6530 [==============================] - 0s 25us/step - loss: 0.1707 - val_loss: 0.1623
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1874
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0869
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1699
6400/6530 [============================>.] - ETA: 0s - loss: 0.0872
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1694
6530/6530 [==============================] - 0s 66us/step - loss: 0.0872 - val_loss: 0.0904
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0968
# training | RMSE: 0.2241, MAE: 0.1867
worker 1  xfile  [1, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.25455619904572774}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 58, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 5, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2241470587953769, 'rmse': 0.2241470587953769, 'mae': 0.18668924289348746, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  25 | activation: tanh    | extras: batchnorm 
layer 2 | size:  19 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e91314f60>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 7s - loss: 0.9053
6530/6530 [==============================] - 0s 24us/step - loss: 0.1687 - val_loss: 0.1619

 800/6530 [==>...........................] - ETA: 0s - loss: 0.0879
6400/6530 [============================>.] - ETA: 0s - loss: 0.5448
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0856
6530/6530 [==============================] - 0s 60us/step - loss: 0.5406 - val_loss: 0.3444
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.3514
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0865
6400/6530 [============================>.] - ETA: 0s - loss: 0.2892
6530/6530 [==============================] - 0s 9us/step - loss: 0.2879 - val_loss: 0.2477
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2409
3200/6530 [=============>................] - ETA: 0s - loss: 0.0843
6530/6530 [==============================] - 0s 9us/step - loss: 0.2266 - val_loss: 0.2135
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1992
4000/6530 [=================>............] - ETA: 0s - loss: 0.0841
6144/6530 [===========================>..] - ETA: 0s - loss: 0.2010
6530/6530 [==============================] - 0s 9us/step - loss: 0.2005 - val_loss: 0.1985
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1794
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0827
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1861
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0829
6530/6530 [==============================] - 0s 10us/step - loss: 0.1860 - val_loss: 0.1904
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1681
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0827
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1748
6530/6530 [==============================] - 0s 10us/step - loss: 0.1762 - val_loss: 0.1845
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1605
6530/6530 [==============================] - 0s 68us/step - loss: 0.0825 - val_loss: 0.0765

# training | RMSE: 0.2038, MAE: 0.1619
worker 2  xfile  [3, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24260494912330344}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.245751806831647}, 'layer_3_size': 72, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3313486863097612}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20376001490477794, 'rmse': 0.20376001490477794, 'mae': 0.16189119595714527, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  88 | activation: relu    | extras: None 
layer 2 | size:  48 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e90117f98>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 1:10 - loss: 0.6908
6530/6530 [==============================] - 0s 9us/step - loss: 0.1686 - val_loss: 0.1786
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1545
 576/6530 [=>............................] - ETA: 2s - loss: 0.2895  
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1621
6530/6530 [==============================] - 0s 9us/step - loss: 0.1623 - val_loss: 0.1726
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1479
1136/6530 [====>.........................] - ETA: 1s - loss: 0.2419
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1567
6530/6530 [==============================] - 0s 9us/step - loss: 0.1568 - val_loss: 0.1674

1760/6530 [=======>......................] - ETA: 0s - loss: 0.2156
2272/6530 [=========>....................] - ETA: 0s - loss: 0.2035
2912/6530 [============>.................] - ETA: 0s - loss: 0.1944
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1873
4048/6530 [=================>............] - ETA: 0s - loss: 0.1830
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1787
# training | RMSE: 0.0913, MAE: 0.0700
worker 0  xfile  [0, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20598381938009017}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21312643339710788}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22227484043894946}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.0913323329164211, 'rmse': 0.0913323329164211, 'mae': 0.06995347172456212, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  11 | activation: relu    | extras: None 
layer 2 | size:   8 | activation: tanh    | extras: dropout - rate: 47.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 3s - loss: 0.7275
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1749
6530/6530 [==============================] - 0s 34us/step - loss: 0.6267 - val_loss: 0.2429
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2376
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1712
6530/6530 [==============================] - 0s 7us/step - loss: 0.2294 - val_loss: 0.2170
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2176
6416/6530 [============================>.] - ETA: 0s - loss: 0.1692
6530/6530 [==============================] - 0s 7us/step - loss: 0.2080 - val_loss: 0.1924
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1958
6530/6530 [==============================] - 1s 119us/step - loss: 0.1683 - val_loss: 0.1450
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1252
6530/6530 [==============================] - 0s 7us/step - loss: 0.1849 - val_loss: 0.1714
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1767
 592/6530 [=>............................] - ETA: 0s - loss: 0.1331
6530/6530 [==============================] - 0s 7us/step - loss: 0.1752 - val_loss: 0.1649
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1676
1072/6530 [===>..........................] - ETA: 0s - loss: 0.1349
6530/6530 [==============================] - 0s 7us/step - loss: 0.1695 - val_loss: 0.1621
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1630
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1314
6530/6530 [==============================] - 0s 7us/step - loss: 0.1664 - val_loss: 0.1605
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1589
2256/6530 [=========>....................] - ETA: 0s - loss: 0.1308
6530/6530 [==============================] - 0s 7us/step - loss: 0.1629 - val_loss: 0.1593
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1553
2832/6530 [============>.................] - ETA: 0s - loss: 0.1305
6530/6530 [==============================] - 0s 7us/step - loss: 0.1612 - val_loss: 0.1589

3360/6530 [==============>...............] - ETA: 0s - loss: 0.1294
3952/6530 [=================>............] - ETA: 0s - loss: 0.1291
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1284
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1273
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1269
6432/6530 [============================>.] - ETA: 0s - loss: 0.1269
6530/6530 [==============================] - 1s 90us/step - loss: 0.1266 - val_loss: 0.1372
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1200
# training | RMSE: 0.2030, MAE: 0.1572
worker 1  xfile  [4, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.20301598409709665, 'rmse': 0.20301598409709665, 'mae': 0.15724005197731328, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  26 | activation: tanh    | extras: dropout - rate: 39.2% 
layer 2 | size:   6 | activation: relu    | extras: batchnorm 
layer 3 | size:  54 | activation: relu    | extras: dropout - rate: 11.0% 
layer 4 | size:  12 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e90464630>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 45s - loss: 0.6957
 608/6530 [=>............................] - ETA: 0s - loss: 0.1143
1216/6530 [====>.........................] - ETA: 2s - loss: 0.6708 
1200/6530 [====>.........................] - ETA: 0s - loss: 0.1157
2432/6530 [==========>...................] - ETA: 0s - loss: 0.5882
1712/6530 [======>.......................] - ETA: 0s - loss: 0.1149
3392/6530 [==============>...............] - ETA: 0s - loss: 0.5025
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1142
4544/6530 [===================>..........] - ETA: 0s - loss: 0.4345
2880/6530 [============>.................] - ETA: 0s - loss: 0.1137
5760/6530 [=========================>....] - ETA: 0s - loss: 0.3884
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1129
6530/6530 [==============================] - 1s 122us/step - loss: 0.3672 - val_loss: 0.1954
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1897
4112/6530 [=================>............] - ETA: 0s - loss: 0.1114
# training | RMSE: 0.1976, MAE: 0.1567
worker 0  xfile  [6, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4770486714254554}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3323045329803257}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.19757518105954638, 'rmse': 0.19757518105954638, 'mae': 0.15672991998242194, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  96 | activation: tanh    | extras: None 
layer 2 | size:  87 | activation: relu    | extras: None 
layer 3 | size:  13 | activation: tanh    | extras: dropout - rate: 27.2% 
layer 4 | size:  22 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e901a9240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 13s - loss: 0.7271
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1990
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1108
3072/6530 [=============>................] - ETA: 0s - loss: 0.6735 
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1884
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1100
6272/6530 [===========================>..] - ETA: 0s - loss: 0.4988
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1813
6530/6530 [==============================] - 0s 65us/step - loss: 0.4872 - val_loss: 0.1852

5808/6530 [=========================>....] - ETA: 0s - loss: 0.1096Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.2209
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1768
6336/6530 [============================>.] - ETA: 0s - loss: 0.1095
3072/6530 [=============>................] - ETA: 0s - loss: 0.1837
6530/6530 [==============================] - 1s 92us/step - loss: 0.1089 - val_loss: 0.1161
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1002
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1726
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1802
6530/6530 [==============================] - 0s 18us/step - loss: 0.1798 - val_loss: 0.1589
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1813
6530/6530 [==============================] - 0s 45us/step - loss: 0.1711 - val_loss: 0.1471
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1809
 592/6530 [=>............................] - ETA: 0s - loss: 0.0988
2944/6530 [============>.................] - ETA: 0s - loss: 0.1728
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1591
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1007
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1700
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1561
6530/6530 [==============================] - 0s 18us/step - loss: 0.1690 - val_loss: 0.1533
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1789
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0985
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1521
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1608
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0977
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1494
6400/6530 [============================>.] - ETA: 0s - loss: 0.1590
2848/6530 [============>.................] - ETA: 0s - loss: 0.0973
6530/6530 [==============================] - 0s 17us/step - loss: 0.1586 - val_loss: 0.1440
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1574
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1484
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0963
3200/6530 [=============>................] - ETA: 0s - loss: 0.1536
6530/6530 [==============================] - 0s 45us/step - loss: 0.1477 - val_loss: 0.1361
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1166
3968/6530 [=================>............] - ETA: 0s - loss: 0.0957
6400/6530 [============================>.] - ETA: 0s - loss: 0.1495
6530/6530 [==============================] - 0s 18us/step - loss: 0.1490 - val_loss: 0.1278
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1456
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1397
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0955
3200/6530 [=============>................] - ETA: 0s - loss: 0.1356
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1407
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0951
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1343
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1390
6530/6530 [==============================] - 0s 18us/step - loss: 0.1337 - val_loss: 0.1142
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1378
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0950
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1386
3200/6530 [=============>................] - ETA: 0s - loss: 0.1224
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0951
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1389
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1219
6530/6530 [==============================] - 0s 18us/step - loss: 0.1216 - val_loss: 0.1118
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1362
6530/6530 [==============================] - 1s 93us/step - loss: 0.0948 - val_loss: 0.0953
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0823
6530/6530 [==============================] - 0s 46us/step - loss: 0.1392 - val_loss: 0.1257
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1276
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1220
 608/6530 [=>............................] - ETA: 0s - loss: 0.0863
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1355
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1187
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0882
6530/6530 [==============================] - 0s 18us/step - loss: 0.1183 - val_loss: 0.1045
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1160
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1341
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0869
2944/6530 [============>.................] - ETA: 0s - loss: 0.1132
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1343
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0863
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1122
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1355
6530/6530 [==============================] - 0s 19us/step - loss: 0.1121 - val_loss: 0.1057

2864/6530 [============>.................] - ETA: 0s - loss: 0.0865
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1343
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0861
6530/6530 [==============================] - 0s 47us/step - loss: 0.1341 - val_loss: 0.1190
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1329
4000/6530 [=================>............] - ETA: 0s - loss: 0.0858
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1269
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0857
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1292
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0861
3712/6530 [================>.............] - ETA: 0s - loss: 0.1281
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0862
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1281
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0862
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1283
6530/6530 [==============================] - 1s 94us/step - loss: 0.0862 - val_loss: 0.0917
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0729
6530/6530 [==============================] - 0s 44us/step - loss: 0.1282 - val_loss: 0.1123
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1093
 608/6530 [=>............................] - ETA: 0s - loss: 0.0834
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1187
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0836
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1231
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0817
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1237
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0815
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1253
# training | RMSE: 0.1303, MAE: 0.0983
worker 0  xfile  [8, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2724334792774713}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.13033785507471196, 'rmse': 0.13033785507471196, 'mae': 0.09829011707105527, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  62 | activation: relu    | extras: None 
layer 2 | size:  14 | activation: sigmoid | extras: dropout - rate: 14.6% 
layer 3 | size:  62 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e70705eb8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 7s - loss: 0.2643
2960/6530 [============>.................] - ETA: 0s - loss: 0.0810
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1246
6530/6530 [==============================] - 0s 46us/step - loss: 0.1246 - val_loss: 0.1105

3520/6530 [===============>..............] - ETA: 0s - loss: 0.0811Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1468
6530/6530 [==============================] - 0s 57us/step - loss: 0.2196 - val_loss: 0.2225
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2148
4064/6530 [=================>............] - ETA: 0s - loss: 0.0807
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1260
6530/6530 [==============================] - 0s 7us/step - loss: 0.2129 - val_loss: 0.2039
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2249
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0808
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1275
6530/6530 [==============================] - 0s 7us/step - loss: 0.2030 - val_loss: 0.1900
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2037
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0808
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1250
6530/6530 [==============================] - 0s 7us/step - loss: 0.1908 - val_loss: 0.1787
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1852
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0808
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1252
6530/6530 [==============================] - 0s 7us/step - loss: 0.1803 - val_loss: 0.1648
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1827
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0810
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1258
6530/6530 [==============================] - 0s 8us/step - loss: 0.1753 - val_loss: 0.1836
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1851
6530/6530 [==============================] - 1s 94us/step - loss: 0.0810 - val_loss: 0.0923
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0709
6530/6530 [==============================] - 0s 8us/step - loss: 0.1719 - val_loss: 0.1608

6530/6530 [==============================] - 0s 48us/step - loss: 0.1249 - val_loss: 0.1082
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1831Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1251
6530/6530 [==============================] - 0s 7us/step - loss: 0.1701 - val_loss: 0.1571

 576/6530 [=>............................] - ETA: 0s - loss: 0.0797Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1553
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1209
6530/6530 [==============================] - 0s 7us/step - loss: 0.1672 - val_loss: 0.1606

1136/6530 [====>.........................] - ETA: 0s - loss: 0.0801
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1228
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0780
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1245
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0777
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1229
2944/6530 [============>.................] - ETA: 0s - loss: 0.0769
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1223
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0766
6530/6530 [==============================] - 0s 46us/step - loss: 0.1215 - val_loss: 0.1070

4192/6530 [==================>...........] - ETA: 0s - loss: 0.0763
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0766
# training | RMSE: 0.1319, MAE: 0.1015
worker 1  xfile  [7, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39239340644363085}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11000700240545346}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.13188567177590627, 'rmse': 0.13188567177590627, 'mae': 0.10146566399151073, 'early_stop': False}
vggnet done  1

5440/6530 [=======================>......] - ETA: 0s - loss: 0.0767
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0767
6530/6530 [==============================] - 1s 87us/step - loss: 0.0769 - val_loss: 0.1025
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0734
# training | RMSE: 0.1987, MAE: 0.1606
worker 0  xfile  [9, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14588882713852366}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.19865639106214858, 'rmse': 0.19865639106214858, 'mae': 0.1605963233342126, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  20 | activation: tanh    | extras: dropout - rate: 13.7% 
layer 2 | size:  58 | activation: relu    | extras: dropout - rate: 33.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e5c1d0198>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 21s - loss: 1.5712
 608/6530 [=>............................] - ETA: 0s - loss: 0.0805
2368/6530 [=========>....................] - ETA: 0s - loss: 0.5731 
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0785
4608/6530 [====================>.........] - ETA: 0s - loss: 0.3727
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0763
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0761
6530/6530 [==============================] - 0s 60us/step - loss: 0.2982 - val_loss: 0.0660
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0989
3104/6530 [=============>................] - ETA: 0s - loss: 0.0740
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0925
3744/6530 [================>.............] - ETA: 0s - loss: 0.0748
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0829
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0743
6530/6530 [==============================] - 0s 23us/step - loss: 0.0784 - val_loss: 0.0535
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0832
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0745
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0621
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0748
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0593
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0749
6530/6530 [==============================] - 0s 24us/step - loss: 0.0588 - val_loss: 0.0419
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0360
6530/6530 [==============================] - 1s 87us/step - loss: 0.0751 - val_loss: 0.0934
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0689
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0513
 640/6530 [=>............................] - ETA: 0s - loss: 0.0855
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0511
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0796
6530/6530 [==============================] - 0s 24us/step - loss: 0.0496 - val_loss: 0.0403
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0417
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0766
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0453
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0760
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0441
3104/6530 [=============>................] - ETA: 0s - loss: 0.0738
6530/6530 [==============================] - 0s 23us/step - loss: 0.0442 - val_loss: 0.0382
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0448
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0741
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0427
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0733
4032/6530 [=================>............] - ETA: 0s - loss: 0.0407
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0733
6336/6530 [============================>.] - ETA: 0s - loss: 0.0400
6530/6530 [==============================] - 0s 25us/step - loss: 0.0401 - val_loss: 0.0338
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0348
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0735
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0370
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0732
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0370
6530/6530 [==============================] - 1s 88us/step - loss: 0.0737 - val_loss: 0.0940

6530/6530 [==============================] - 0s 24us/step - loss: 0.0365 - val_loss: 0.0303
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0333
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0345
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0349
# training | RMSE: 0.1136, MAE: 0.0888
worker 2  xfile  [5, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.11364946243313413, 'rmse': 0.11364946243313413, 'mae': 0.08876194943219047, 'early_stop': False}
vggnet done  2

6530/6530 [==============================] - 0s 23us/step - loss: 0.0347 - val_loss: 0.0274
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0421
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0332
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0327
6530/6530 [==============================] - 0s 22us/step - loss: 0.0317 - val_loss: 0.0284

# training | RMSE: 0.1608, MAE: 0.1280
worker 0  xfile  [10, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13736846942029257}, 'layer_1_size': 20, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.33701832872406634}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45553820744359885}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31686787298517216}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.16079260007806392, 'rmse': 0.16079260007806392, 'mae': 0.12801152710017155, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#2 epoch=9.0 loss={'loss': 0.13395103236702113, 'rmse': 0.13395103236702113, 'mae': 0.10152556524408891, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1400219191605232}, 'layer_2_size': 24, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21573621650431832}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#1 epoch=9.0 loss={'loss': 0.2241470587953769, 'rmse': 0.2241470587953769, 'mae': 0.18668924289348746, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.25455619904572774}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 58, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 5, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#3 epoch=9.0 loss={'loss': 0.20376001490477794, 'rmse': 0.20376001490477794, 'mae': 0.16189119595714527, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24260494912330344}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.245751806831647}, 'layer_3_size': 72, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3313486863097612}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#0 epoch=9.0 loss={'loss': 0.0913323329164211, 'rmse': 0.0913323329164211, 'mae': 0.06995347172456212, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20598381938009017}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21312643339710788}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22227484043894946}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#4 epoch=9.0 loss={'loss': 0.20301598409709665, 'rmse': 0.20301598409709665, 'mae': 0.15724005197731328, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#6 epoch=9.0 loss={'loss': 0.19757518105954638, 'rmse': 0.19757518105954638, 'mae': 0.15672991998242194, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4770486714254554}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3323045329803257}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#8 epoch=9.0 loss={'loss': 0.13033785507471196, 'rmse': 0.13033785507471196, 'mae': 0.09829011707105527, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2724334792774713}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#9 epoch=9.0 loss={'loss': 0.19865639106214858, 'rmse': 0.19865639106214858, 'mae': 0.1605963233342126, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14588882713852366}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#7 epoch=9.0 loss={'loss': 0.13188567177590627, 'rmse': 0.13188567177590627, 'mae': 0.10146566399151073, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39239340644363085}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11000700240545346}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#5 epoch=9.0 loss={'loss': 0.11364946243313413, 'rmse': 0.11364946243313413, 'mae': 0.08876194943219047, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#10 epoch=9.0 loss={'loss': 0.16079260007806392, 'rmse': 0.16079260007806392, 'mae': 0.12801152710017155, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13736846942029257}, 'layer_1_size': 20, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.33701832872406634}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45553820744359885}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31686787298517216}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
get a list [results] of length 166
get a list [loss] of length 11
get a list [val_loss] of length 11
length of indices is [ 3  9  6  8  0 10  5  7  4  2  1]
length of indices is 11
length of T is 11
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24260494912330344}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.245751806831647}, 'layer_3_size': 72, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3313486863097612}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [1, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14588882713852366}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [2, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4770486714254554}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3323045329803257}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]] 

*** 3.7777777777777777 configurations x 27.0 iterations each

11 | Thu Sep 27 18:41:03 2018 | lowest loss so far: 0.0913 (run 0)

{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  11 | activation: relu    | extras: None 
layer 2 | size:   8 | activation: tanh    | extras: dropout - rate: 47.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 256/6530 [>.............................] - ETA: 18s - loss: 0.7273
6530/6530 [==============================] - 1s 128us/step - loss: 0.6089 - val_loss: 0.2365
Epoch 2/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.2333{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  57 | activation: sigmoid | extras: dropout - rate: 24.3% 
layer 2 | size:  18 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  64/6530 [..............................] - ETA: 1:21 - loss: 0.7036{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  62 | activation: relu    | extras: None 
layer 2 | size:  14 | activation: sigmoid | extras: dropout - rate: 14.6% 
layer 3 | size:  62 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 256/6530 [>.............................] - ETA: 20s - loss: 0.3337
6530/6530 [==============================] - 0s 8us/step - loss: 0.2268 - val_loss: 0.2139
Epoch 3/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.2152
1664/6530 [======>.......................] - ETA: 2s - loss: 0.3529  
6530/6530 [==============================] - 1s 137us/step - loss: 0.2211 - val_loss: 0.2186
Epoch 2/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.2103
6530/6530 [==============================] - 0s 7us/step - loss: 0.2050 - val_loss: 0.1899
Epoch 4/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1936
3776/6530 [================>.............] - ETA: 0s - loss: 0.2749
6530/6530 [==============================] - 0s 7us/step - loss: 0.2133 - val_loss: 0.2030
Epoch 3/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.2012
6530/6530 [==============================] - 0s 7us/step - loss: 0.1827 - val_loss: 0.1701
Epoch 5/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1750
5696/6530 [=========================>....] - ETA: 0s - loss: 0.2529
6530/6530 [==============================] - 0s 8us/step - loss: 0.2045 - val_loss: 0.1957
Epoch 4/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.2058
6530/6530 [==============================] - 0s 8us/step - loss: 0.1754 - val_loss: 0.1647
Epoch 6/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1679
6530/6530 [==============================] - 1s 154us/step - loss: 0.2468 - val_loss: 0.2040
Epoch 2/27

  64/6530 [..............................] - ETA: 0s - loss: 0.2177
6530/6530 [==============================] - 0s 8us/step - loss: 0.1937 - val_loss: 0.1827

6530/6530 [==============================] - 0s 7us/step - loss: 0.1713 - val_loss: 0.1617
Epoch 5/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1922Epoch 7/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1626
2112/6530 [========>.....................] - ETA: 0s - loss: 0.2124
6530/6530 [==============================] - 0s 7us/step - loss: 0.1672 - val_loss: 0.1609
Epoch 8/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1599
6530/6530 [==============================] - 0s 7us/step - loss: 0.1835 - val_loss: 0.1750
Epoch 6/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1653
4288/6530 [==================>...........] - ETA: 0s - loss: 0.2063
6530/6530 [==============================] - 0s 7us/step - loss: 0.1631 - val_loss: 0.1609
Epoch 9/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1591
6530/6530 [==============================] - 0s 7us/step - loss: 0.1769 - val_loss: 0.1677
Epoch 7/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1694
6336/6530 [============================>.] - ETA: 0s - loss: 0.2048
6530/6530 [==============================] - 0s 26us/step - loss: 0.2042 - val_loss: 0.1939
Epoch 3/27

  64/6530 [..............................] - ETA: 0s - loss: 0.2236
6530/6530 [==============================] - 0s 7us/step - loss: 0.1631 - val_loss: 0.1620
Epoch 10/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1597
6530/6530 [==============================] - 0s 7us/step - loss: 0.1714 - val_loss: 0.1610
Epoch 8/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1574
6530/6530 [==============================] - 0s 7us/step - loss: 0.1614 - val_loss: 0.1575
Epoch 11/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1540
2304/6530 [=========>....................] - ETA: 0s - loss: 0.2035
6530/6530 [==============================] - 0s 7us/step - loss: 0.1695 - val_loss: 0.1652
Epoch 9/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1834
6530/6530 [==============================] - 0s 7us/step - loss: 0.1593 - val_loss: 0.1554
Epoch 12/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1533
6530/6530 [==============================] - 0s 7us/step - loss: 0.1691 - val_loss: 0.1618
Epoch 10/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1690
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1982
6530/6530 [==============================] - 0s 8us/step - loss: 0.1581 - val_loss: 0.1510

6530/6530 [==============================] - 0s 7us/step - loss: 0.1685 - val_loss: 0.1617
Epoch 13/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1484Epoch 11/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1777
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1967
6530/6530 [==============================] - 0s 27us/step - loss: 0.1953 - val_loss: 0.1826
Epoch 4/27

  64/6530 [..............................] - ETA: 0s - loss: 0.2139
6530/6530 [==============================] - 0s 7us/step - loss: 0.1560 - val_loss: 0.1502

6530/6530 [==============================] - 0s 7us/step - loss: 0.1669 - val_loss: 0.1589
Epoch 14/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1488Epoch 12/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1714
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1913
6530/6530 [==============================] - 0s 7us/step - loss: 0.1492 - val_loss: 0.1635

6530/6530 [==============================] - 0s 7us/step - loss: 0.1658 - val_loss: 0.1588
Epoch 15/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1619Epoch 13/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1600
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1869
6530/6530 [==============================] - 0s 7us/step - loss: 0.1543 - val_loss: 0.1501
Epoch 16/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1466
6530/6530 [==============================] - 0s 7us/step - loss: 0.1657 - val_loss: 0.1581
Epoch 14/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1673
6530/6530 [==============================] - 0s 7us/step - loss: 0.1515 - val_loss: 0.1580
Epoch 17/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1536
6530/6530 [==============================] - 0s 7us/step - loss: 0.1632 - val_loss: 0.1586
Epoch 15/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1687
6530/6530 [==============================] - 0s 25us/step - loss: 0.1846 - val_loss: 0.1704
Epoch 5/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1908
6530/6530 [==============================] - 0s 7us/step - loss: 0.1489 - val_loss: 0.1603
Epoch 18/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1554
6530/6530 [==============================] - 0s 7us/step - loss: 0.1633 - val_loss: 0.1569
Epoch 16/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1633
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1789
6530/6530 [==============================] - 0s 7us/step - loss: 0.1471 - val_loss: 0.1494
Epoch 19/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1432
6530/6530 [==============================] - 0s 7us/step - loss: 0.1625 - val_loss: 0.1707
Epoch 17/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1831
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1765
6530/6530 [==============================] - 0s 7us/step - loss: 0.1447 - val_loss: 0.1535
Epoch 20/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1473
6530/6530 [==============================] - 0s 7us/step - loss: 0.1608 - val_loss: 0.1615
Epoch 18/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1624
6400/6530 [============================>.] - ETA: 0s - loss: 0.1767
6530/6530 [==============================] - 0s 25us/step - loss: 0.1764 - val_loss: 0.1634
Epoch 6/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1777
6530/6530 [==============================] - 0s 7us/step - loss: 0.1434 - val_loss: 0.1597
Epoch 21/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1529
6530/6530 [==============================] - 0s 7us/step - loss: 0.1606 - val_loss: 0.1548
Epoch 19/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1518
6530/6530 [==============================] - 0s 7us/step - loss: 0.1421 - val_loss: 0.1593
Epoch 22/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1520
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1756
6530/6530 [==============================] - 0s 7us/step - loss: 0.1589 - val_loss: 0.1535
Epoch 20/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1536
6530/6530 [==============================] - 0s 7us/step - loss: 0.1407 - val_loss: 0.1528
Epoch 23/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1448
6530/6530 [==============================] - 0s 7us/step - loss: 0.1570 - val_loss: 0.1518

4224/6530 [==================>...........] - ETA: 0s - loss: 0.1735Epoch 21/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1519
6530/6530 [==============================] - 0s 7us/step - loss: 0.1399 - val_loss: 0.1498

6530/6530 [==============================] - 0s 8us/step - loss: 0.1571 - val_loss: 0.1501
Epoch 22/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1473
6336/6530 [============================>.] - ETA: 0s - loss: 0.1730
# training | RMSE: 0.1825, MAE: 0.1452
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4770486714254554}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3323045329803257}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.18247255692253303, 'rmse': 0.18247255692253303, 'mae': 0.14523697167230132, 'early_stop': True}
vggnet done  2

6530/6530 [==============================] - 0s 25us/step - loss: 0.1728 - val_loss: 0.1614
Epoch 7/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1814
6530/6530 [==============================] - 0s 7us/step - loss: 0.1552 - val_loss: 0.1487
Epoch 23/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1500
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1719
6530/6530 [==============================] - 0s 7us/step - loss: 0.1542 - val_loss: 0.1474
Epoch 24/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1552
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1718
6530/6530 [==============================] - 0s 6us/step - loss: 0.1525 - val_loss: 0.1516
Epoch 25/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1544
6530/6530 [==============================] - 0s 24us/step - loss: 0.1718 - val_loss: 0.1617
Epoch 8/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1835
6530/6530 [==============================] - 0s 7us/step - loss: 0.1516 - val_loss: 0.1481
Epoch 26/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1574
6530/6530 [==============================] - 0s 6us/step - loss: 0.1496 - val_loss: 0.1481
Epoch 27/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1539
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1738
6530/6530 [==============================] - 0s 7us/step - loss: 0.1491 - val_loss: 0.1425

4672/6530 [====================>.........] - ETA: 0s - loss: 0.1724
# training | RMSE: 0.1806, MAE: 0.1400
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14588882713852366}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.18064649467214788, 'rmse': 0.18064649467214788, 'mae': 0.1400342153699552, 'early_stop': False}
vggnet done  1

6530/6530 [==============================] - 0s 23us/step - loss: 0.1709 - val_loss: 0.1614
Epoch 9/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1812
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1716
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1704
6530/6530 [==============================] - 0s 22us/step - loss: 0.1697 - val_loss: 0.1613
Epoch 10/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1767
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1689
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1692
6528/6530 [============================>.] - ETA: 0s - loss: 0.1696
6530/6530 [==============================] - 0s 25us/step - loss: 0.1696 - val_loss: 0.1615
Epoch 11/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1876
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1698
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1679
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1696
6530/6530 [==============================] - 0s 32us/step - loss: 0.1690 - val_loss: 0.1615
Epoch 12/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1852
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1694
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1698
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1699
6530/6530 [==============================] - 0s 30us/step - loss: 0.1696 - val_loss: 0.1623
Epoch 13/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1873
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1693
4096/6530 [=================>............] - ETA: 0s - loss: 0.1697
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1692
6530/6530 [==============================] - 0s 26us/step - loss: 0.1688 - val_loss: 0.1617
Epoch 14/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1836
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1678
3968/6530 [=================>............] - ETA: 0s - loss: 0.1684
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1692
6530/6530 [==============================] - 0s 26us/step - loss: 0.1684 - val_loss: 0.1631

# training | RMSE: 0.2031, MAE: 0.1624
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24260494912330344}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.245751806831647}, 'layer_3_size': 72, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3313486863097612}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2030692618312103, 'rmse': 0.2030692618312103, 'mae': 0.16237617572986304, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#2 epoch=27.0 loss={'loss': 0.18247255692253303, 'rmse': 0.18247255692253303, 'mae': 0.14523697167230132, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4770486714254554}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3323045329803257}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#1 epoch=27.0 loss={'loss': 0.18064649467214788, 'rmse': 0.18064649467214788, 'mae': 0.1400342153699552, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14588882713852366}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#0 epoch=27.0 loss={'loss': 0.2030692618312103, 'rmse': 0.2030692618312103, 'mae': 0.16237617572986304, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24260494912330344}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.245751806831647}, 'layer_3_size': 72, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3313486863097612}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
get a list [results] of length 169
get a list [loss] of length 3
get a list [val_loss] of length 3
length of indices is [1 0 2]
length of indices is 3
length of T is 3
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14588882713852366}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]] 

*** 1.259259259259259 configurations x 81.0 iterations each

1 | Thu Sep 27 18:41:07 2018 | lowest loss so far: 0.0913 (run 0)

vggnet done  1
vggnet done  2
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  62 | activation: relu    | extras: None 
layer 2 | size:  14 | activation: sigmoid | extras: dropout - rate: 14.6% 
layer 3 | size:  62 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

 256/6530 [>.............................] - ETA: 16s - loss: 0.3337
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2217 
6530/6530 [==============================] - 1s 117us/step - loss: 0.2211 - val_loss: 0.2186
Epoch 2/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.2103
6530/6530 [==============================] - 0s 8us/step - loss: 0.2133 - val_loss: 0.2030
Epoch 3/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.2012
6530/6530 [==============================] - 0s 8us/step - loss: 0.2045 - val_loss: 0.1957
Epoch 4/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.2058
6530/6530 [==============================] - 0s 8us/step - loss: 0.1937 - val_loss: 0.1827
Epoch 5/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1922
6530/6530 [==============================] - 0s 8us/step - loss: 0.1835 - val_loss: 0.1750
Epoch 6/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1653
6530/6530 [==============================] - 0s 7us/step - loss: 0.1769 - val_loss: 0.1677
Epoch 7/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1694
6530/6530 [==============================] - 0s 8us/step - loss: 0.1714 - val_loss: 0.1610
Epoch 8/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1574
6530/6530 [==============================] - 0s 9us/step - loss: 0.1695 - val_loss: 0.1652
Epoch 9/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1834
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1702
6530/6530 [==============================] - 0s 11us/step - loss: 0.1691 - val_loss: 0.1618
Epoch 10/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1690
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1672
6530/6530 [==============================] - 0s 9us/step - loss: 0.1685 - val_loss: 0.1617
Epoch 11/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1777
6530/6530 [==============================] - 0s 7us/step - loss: 0.1669 - val_loss: 0.1589
Epoch 12/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1714
6530/6530 [==============================] - 0s 7us/step - loss: 0.1658 - val_loss: 0.1588
Epoch 13/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1600
6530/6530 [==============================] - 0s 6us/step - loss: 0.1657 - val_loss: 0.1581
Epoch 14/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1673
6530/6530 [==============================] - 0s 7us/step - loss: 0.1632 - val_loss: 0.1586
Epoch 15/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1687
6530/6530 [==============================] - 0s 8us/step - loss: 0.1633 - val_loss: 0.1569
Epoch 16/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1633
6530/6530 [==============================] - 0s 7us/step - loss: 0.1625 - val_loss: 0.1707
Epoch 17/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1831
6530/6530 [==============================] - 0s 6us/step - loss: 0.1608 - val_loss: 0.1615
Epoch 18/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1624
6530/6530 [==============================] - 0s 7us/step - loss: 0.1606 - val_loss: 0.1548
Epoch 19/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1518
6530/6530 [==============================] - 0s 6us/step - loss: 0.1589 - val_loss: 0.1535
Epoch 20/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1536
6530/6530 [==============================] - 0s 6us/step - loss: 0.1570 - val_loss: 0.1518
Epoch 21/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1519
6530/6530 [==============================] - 0s 6us/step - loss: 0.1571 - val_loss: 0.1501
Epoch 22/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1473
6530/6530 [==============================] - 0s 7us/step - loss: 0.1552 - val_loss: 0.1487
Epoch 23/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1500
6530/6530 [==============================] - 0s 7us/step - loss: 0.1542 - val_loss: 0.1474
Epoch 24/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1552
6530/6530 [==============================] - 0s 8us/step - loss: 0.1525 - val_loss: 0.1516
Epoch 25/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1544
6530/6530 [==============================] - 0s 8us/step - loss: 0.1516 - val_loss: 0.1481
Epoch 26/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1574
6530/6530 [==============================] - 0s 8us/step - loss: 0.1496 - val_loss: 0.1481
Epoch 27/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1539
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1486
6530/6530 [==============================] - 0s 12us/step - loss: 0.1491 - val_loss: 0.1425
Epoch 28/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1446
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1488
6530/6530 [==============================] - 0s 12us/step - loss: 0.1483 - val_loss: 0.1435
Epoch 29/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1475
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1466
6530/6530 [==============================] - 0s 11us/step - loss: 0.1476 - val_loss: 0.1392
Epoch 30/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1369
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1451
6530/6530 [==============================] - 0s 12us/step - loss: 0.1449 - val_loss: 0.1377
Epoch 31/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1489
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1435
6530/6530 [==============================] - 0s 11us/step - loss: 0.1434 - val_loss: 0.1402
Epoch 32/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1576
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1432
6530/6530 [==============================] - 0s 11us/step - loss: 0.1432 - val_loss: 0.1459
Epoch 33/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1475
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1417
6530/6530 [==============================] - 0s 9us/step - loss: 0.1416 - val_loss: 0.1366
Epoch 34/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1457
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1420
6530/6530 [==============================] - 0s 10us/step - loss: 0.1420 - val_loss: 0.1317
Epoch 35/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1411
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1386
6530/6530 [==============================] - 0s 10us/step - loss: 0.1389 - val_loss: 0.1325
Epoch 36/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1340
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1375
6530/6530 [==============================] - 0s 10us/step - loss: 0.1381 - val_loss: 0.1299
Epoch 37/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1518
6400/6530 [============================>.] - ETA: 0s - loss: 0.1377
6530/6530 [==============================] - 0s 9us/step - loss: 0.1376 - val_loss: 0.1288
Epoch 38/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1359
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1366
6530/6530 [==============================] - 0s 10us/step - loss: 0.1366 - val_loss: 0.1291
Epoch 39/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1318
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1350
6530/6530 [==============================] - 0s 11us/step - loss: 0.1355 - val_loss: 0.1342
Epoch 40/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1290
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1338
6530/6530 [==============================] - 0s 10us/step - loss: 0.1349 - val_loss: 0.1336
Epoch 41/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1471
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1343
6530/6530 [==============================] - 0s 9us/step - loss: 0.1348 - val_loss: 0.1315
Epoch 42/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1237
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1337
6530/6530 [==============================] - 0s 10us/step - loss: 0.1344 - val_loss: 0.1258
Epoch 43/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1250
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1319
6530/6530 [==============================] - 0s 13us/step - loss: 0.1319 - val_loss: 0.1353
Epoch 44/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1361
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1314
6530/6530 [==============================] - 0s 12us/step - loss: 0.1307 - val_loss: 0.1287
Epoch 45/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1461
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1319
6530/6530 [==============================] - 0s 11us/step - loss: 0.1312 - val_loss: 0.1235
Epoch 46/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1239
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1342
6530/6530 [==============================] - 0s 11us/step - loss: 0.1327 - val_loss: 0.1230
Epoch 47/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1297
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1312
6530/6530 [==============================] - 0s 10us/step - loss: 0.1316 - val_loss: 0.1249
Epoch 48/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1302
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1305
6530/6530 [==============================] - 0s 12us/step - loss: 0.1302 - val_loss: 0.1222
Epoch 49/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1291
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1310
6530/6530 [==============================] - 0s 13us/step - loss: 0.1298 - val_loss: 0.1263
Epoch 50/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1355
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1288
6530/6530 [==============================] - 0s 13us/step - loss: 0.1284 - val_loss: 0.1216
Epoch 51/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1311
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1278
6530/6530 [==============================] - 0s 14us/step - loss: 0.1307 - val_loss: 0.1254
Epoch 52/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1376
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1283
6530/6530 [==============================] - 0s 13us/step - loss: 0.1278 - val_loss: 0.1216
Epoch 53/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1435
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1291
6530/6530 [==============================] - 0s 12us/step - loss: 0.1288 - val_loss: 0.1325
Epoch 54/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1330
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1274
6530/6530 [==============================] - 0s 11us/step - loss: 0.1279 - val_loss: 0.1197
Epoch 55/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1310
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1264
6530/6530 [==============================] - 0s 12us/step - loss: 0.1275 - val_loss: 0.1246
Epoch 56/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1233
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1253
6530/6530 [==============================] - 0s 11us/step - loss: 0.1254 - val_loss: 0.1191
Epoch 57/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1193
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1259
6530/6530 [==============================] - 0s 13us/step - loss: 0.1253 - val_loss: 0.1227
Epoch 58/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1203
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1275
6530/6530 [==============================] - 0s 12us/step - loss: 0.1260 - val_loss: 0.1226
Epoch 59/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1265
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1240
6530/6530 [==============================] - 0s 10us/step - loss: 0.1247 - val_loss: 0.1235
Epoch 60/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1476
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1254
6530/6530 [==============================] - 0s 11us/step - loss: 0.1251 - val_loss: 0.1247
Epoch 61/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1226
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1248
6530/6530 [==============================] - 0s 11us/step - loss: 0.1248 - val_loss: 0.1209

# training | RMSE: 0.1501, MAE: 0.1165
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14588882713852366}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.15012329167517757, 'rmse': 0.15012329167517757, 'mae': 0.11650934641902655, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=81.0 loss={'loss': 0.15012329167517757, 'rmse': 0.15012329167517757, 'mae': 0.11650934641902655, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14588882713852366}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
get a list [results] of length 170
get a list [loss] of length 1
get a list [val_loss] of length 1
length of indices is [0]
length of indices is 1
length of T is 1
s=2
T is of size 15
T=[{'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27679844046668345}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39624344937280587}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.34191658154173177}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4539264204452951}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37851315931663887}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 82, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 34, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11389578672392409}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4949767012811116}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41779063623655877}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19073769699532994}, 'layer_1_size': 95, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1543574533743368}, 'layer_3_size': 11, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3721211091810761}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1817087321690729}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10939569932384074}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.326615848226966}, 'layer_3_size': 16, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42231207461314724}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14900012006333854}, 'layer_2_size': 48, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1937868315068102}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3474702838513902}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2881528219221712}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3636661996407462}, 'layer_3_size': 94, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10641354205355764}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4544402724480966}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.48327937943956323}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21638682894172123}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 70, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.37063437706228464}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4656791441692686}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 6, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.48200256294458677}, 'layer_2_size': 71, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18703635612869834}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20150315133458624}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3651487968819357}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4985968736993499}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.25901559042494454}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1235998511602289}, 'layer_3_size': 52, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]
newly formed T structure is:[[0, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27679844046668345}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39624344937280587}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.34191658154173177}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [1, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4539264204452951}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37851315931663887}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [2, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 82, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 34, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11389578672392409}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [3, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4949767012811116}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41779063623655877}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [4, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19073769699532994}, 'layer_1_size': 95, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1543574533743368}, 'layer_3_size': 11, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3721211091810761}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [5, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1817087321690729}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [6, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10939569932384074}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.326615848226966}, 'layer_3_size': 16, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42231207461314724}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [7, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14900012006333854}, 'layer_2_size': 48, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1937868315068102}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [8, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3474702838513902}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2881528219221712}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [9, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3636661996407462}, 'layer_3_size': 94, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10641354205355764}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [10, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4544402724480966}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.48327937943956323}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21638682894172123}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [11, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 70, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.37063437706228464}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4656791441692686}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [12, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 6, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.48200256294458677}, 'layer_2_size': 71, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18703635612869834}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [13, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20150315133458624}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3651487968819357}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4985968736993499}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [14, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.25901559042494454}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1235998511602289}, 'layer_3_size': 52, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]] 

*** 15 configurations x 9.0 iterations each

1 | Thu Sep 27 18:41:12 2018 | lowest loss so far: 0.0913 (run 0)

{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  64 | activation: relu    | extras: dropout - rate: 45.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 2:36 - loss: 0.6841{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  93 | activation: tanh    | extras: dropout - rate: 27.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 5:56 - loss: 1.3293
 768/6530 [==>...........................] - ETA: 6s - loss: 0.6646  
 400/6530 [>.............................] - ETA: 14s - loss: 0.8658 
1568/6530 [======>.......................] - ETA: 2s - loss: 0.5771
 880/6530 [===>..........................] - ETA: 6s - loss: 0.5565 
2592/6530 [==========>...................] - ETA: 1s - loss: 0.4659
1376/6530 [=====>........................] - ETA: 3s - loss: 0.4001
3616/6530 [===============>..............] - ETA: 0s - loss: 0.3847
1936/6530 [=======>......................] - ETA: 2s - loss: 0.3063
4736/6530 [====================>.........] - ETA: 0s - loss: 0.3338
2528/6530 [==========>...................] - ETA: 1s - loss: 0.2488
5856/6530 [=========================>....] - ETA: 0s - loss: 0.3008
3120/6530 [=============>................] - ETA: 1s - loss: 0.2119
6530/6530 [==============================] - 1s 173us/step - loss: 0.2867 - val_loss: 0.1606
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1746
3744/6530 [================>.............] - ETA: 0s - loss: 0.1864
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1592
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1677
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1557
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1540
3712/6530 [================>.............] - ETA: 0s - loss: 0.1533
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1429
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1506{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  46 | activation: tanh    | extras: None 
layer 2 | size:  82 | activation: tanh    | extras: batchnorm 
layer 3 | size:  34 | activation: relu    | extras: None 
layer 4 | size:  21 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 6:17 - loss: 0.7775
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1338
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1484
 304/6530 [>.............................] - ETA: 20s - loss: 0.3136 
6530/6530 [==============================] - 1s 228us/step - loss: 0.1303 - val_loss: 0.0519
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0538
6530/6530 [==============================] - 0s 44us/step - loss: 0.1475 - val_loss: 0.1386
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1455
 576/6530 [=>............................] - ETA: 10s - loss: 0.2286
 528/6530 [=>............................] - ETA: 0s - loss: 0.0477
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1327
 864/6530 [==>...........................] - ETA: 7s - loss: 0.1818 
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0487
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1331
1152/6530 [====>.........................] - ETA: 5s - loss: 0.1557
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0478
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1319
1456/6530 [=====>........................] - ETA: 4s - loss: 0.1388
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0477
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1302
1744/6530 [=======>......................] - ETA: 3s - loss: 0.1277
2912/6530 [============>.................] - ETA: 0s - loss: 0.0470
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1280
2032/6530 [========>.....................] - ETA: 2s - loss: 0.1181
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0464
6530/6530 [==============================] - 0s 45us/step - loss: 0.1263 - val_loss: 0.1221
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1391
2336/6530 [=========>....................] - ETA: 2s - loss: 0.1120
4096/6530 [=================>............] - ETA: 0s - loss: 0.0458
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1251
2640/6530 [===========>..................] - ETA: 2s - loss: 0.1052
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0454
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1177
2928/6530 [============>.................] - ETA: 1s - loss: 0.1004
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0452
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1132
3232/6530 [=============>................] - ETA: 1s - loss: 0.0959
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0451
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1120
3536/6530 [===============>..............] - ETA: 1s - loss: 0.0920
6432/6530 [============================>.] - ETA: 0s - loss: 0.0444
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1116
6530/6530 [==============================] - 0s 44us/step - loss: 0.1108 - val_loss: 0.1103
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0861
6530/6530 [==============================] - 1s 91us/step - loss: 0.0442 - val_loss: 0.0383
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0417
3792/6530 [================>.............] - ETA: 1s - loss: 0.0892
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1008
 624/6530 [=>............................] - ETA: 0s - loss: 0.0352
4096/6530 [=================>............] - ETA: 0s - loss: 0.0859
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1013
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0362
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0828
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1013
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0358
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0805
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1014
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0358
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0785
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1009
3024/6530 [============>.................] - ETA: 0s - loss: 0.0350
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0761
6530/6530 [==============================] - 0s 43us/step - loss: 0.1006 - val_loss: 0.1022
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0827
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0348
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0745
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0977
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0341
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0726
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0954
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0340
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0707
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0937
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0339
6464/6530 [============================>.] - ETA: 0s - loss: 0.0694
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0950
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0337
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0944
6530/6530 [==============================] - 2s 327us/step - loss: 0.0691 - val_loss: 0.0320
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0368
6530/6530 [==============================] - 0s 45us/step - loss: 0.0938 - val_loss: 0.0986
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0652
6530/6530 [==============================] - 1s 89us/step - loss: 0.0332 - val_loss: 0.0288
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0293
 288/6530 [>.............................] - ETA: 1s - loss: 0.0355
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0912
 608/6530 [=>............................] - ETA: 0s - loss: 0.0265
 576/6530 [=>............................] - ETA: 1s - loss: 0.0355
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0923
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0274
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0355
3744/6530 [================>.............] - ETA: 0s - loss: 0.0914
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0275
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0347
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0900
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0275
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0350
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0897
3072/6530 [=============>................] - ETA: 0s - loss: 0.0269
6530/6530 [==============================] - 0s 43us/step - loss: 0.0900 - val_loss: 0.0977
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0831
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0343
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0268
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0908
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0336
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0261
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0886
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0336
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0264
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0882
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0337
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0263
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0878
3008/6530 [============>.................] - ETA: 0s - loss: 0.0336
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0263
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0871
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0336
6530/6530 [==============================] - 0s 43us/step - loss: 0.0869 - val_loss: 0.0924
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0617
6530/6530 [==============================] - 1s 86us/step - loss: 0.0260 - val_loss: 0.0240
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0212
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0335
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0891
 640/6530 [=>............................] - ETA: 0s - loss: 0.0212
3904/6530 [================>.............] - ETA: 0s - loss: 0.0334
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0866
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0226
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0334
3840/6530 [================>.............] - ETA: 0s - loss: 0.0856
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0228
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0328
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0855
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0230
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0326
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0849
3088/6530 [=============>................] - ETA: 0s - loss: 0.0226
6530/6530 [==============================] - 0s 42us/step - loss: 0.0850 - val_loss: 0.0926

5088/6530 [======================>.......] - ETA: 0s - loss: 0.0323
3744/6530 [================>.............] - ETA: 0s - loss: 0.0225
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0320
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0221
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0318
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0224
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0315
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0224
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0314
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0225
6530/6530 [==============================] - 1s 176us/step - loss: 0.0313 - val_loss: 0.0234

6530/6530 [==============================] - 1s 86us/step - loss: 0.0222 - val_loss: 0.0216
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0165Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0197
 624/6530 [=>............................] - ETA: 0s - loss: 0.0190
 336/6530 [>.............................] - ETA: 1s - loss: 0.0311
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0201
 640/6530 [=>............................] - ETA: 0s - loss: 0.0269
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0202
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0269
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0207
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0274
3184/6530 [=============>................] - ETA: 0s - loss: 0.0202
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0272
3792/6530 [================>.............] - ETA: 0s - loss: 0.0202
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0267
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0198
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0271
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0202
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0271
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0202
2848/6530 [============>.................] - ETA: 0s - loss: 0.0271
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0202
3168/6530 [=============>................] - ETA: 0s - loss: 0.0269
6530/6530 [==============================] - 1s 84us/step - loss: 0.0200 - val_loss: 0.0201
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0133
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0265
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0174
3792/6530 [================>.............] - ETA: 0s - loss: 0.0266
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0185
4096/6530 [=================>............] - ETA: 0s - loss: 0.0265
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0187
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0264
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0191
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0266
# training | RMSE: 0.1170, MAE: 0.0845
worker 1  xfile  [1, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4539264204452951}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37851315931663887}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.11704046119317943, 'rmse': 0.11704046119317943, 'mae': 0.08451865616586347, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  11 | activation: relu    | extras: dropout - rate: 49.5% 
layer 2 | size:  47 | activation: tanh    | extras: batchnorm 
layer 3 | size:  35 | activation: tanh    | extras: dropout - rate: 41.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e91325b00>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 20s - loss: 0.7269
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0186
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0265
2560/6530 [==========>...................] - ETA: 0s - loss: 0.6909 
3872/6530 [================>.............] - ETA: 0s - loss: 0.0185
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0265
4992/6530 [=====================>........] - ETA: 0s - loss: 0.6059
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0183
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0264
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0185
6530/6530 [==============================] - 1s 90us/step - loss: 0.5233 - val_loss: 0.1897

5872/6530 [=========================>....] - ETA: 0s - loss: 0.0264Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1944
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0186
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1857
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0263
6432/6530 [============================>.] - ETA: 0s - loss: 0.0185
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1793
6480/6530 [============================>.] - ETA: 0s - loss: 0.0262
6530/6530 [==============================] - 1s 83us/step - loss: 0.0184 - val_loss: 0.0189
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0110
6530/6530 [==============================] - 0s 22us/step - loss: 0.1762 - val_loss: 0.1841

6530/6530 [==============================] - 1s 173us/step - loss: 0.0261 - val_loss: 0.0197
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1680Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0260
 640/6530 [=>............................] - ETA: 0s - loss: 0.0161
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1726
 304/6530 [>.............................] - ETA: 1s - loss: 0.0235
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0171
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1706
 608/6530 [=>............................] - ETA: 1s - loss: 0.0232
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0171
6530/6530 [==============================] - 0s 22us/step - loss: 0.1701 - val_loss: 0.1994
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1714
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0237
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0176
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1721
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0242
3104/6530 [=============>................] - ETA: 0s - loss: 0.0172
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1692
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0236
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0171
6530/6530 [==============================] - 0s 22us/step - loss: 0.1677 - val_loss: 0.1817
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1569
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0231
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0169
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1697
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0229
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0171
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1687
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0228
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0171
6530/6530 [==============================] - 0s 22us/step - loss: 0.1683 - val_loss: 0.2275
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1629
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0227
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0172
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1697
2960/6530 [============>.................] - ETA: 0s - loss: 0.0228
6530/6530 [==============================] - 1s 86us/step - loss: 0.0170 - val_loss: 0.0179
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0093
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1688
3216/6530 [=============>................] - ETA: 0s - loss: 0.0229
 592/6530 [=>............................] - ETA: 0s - loss: 0.0152
6530/6530 [==============================] - 0s 24us/step - loss: 0.1678 - val_loss: 0.2218
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1577
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0230
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0159
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1666
3792/6530 [================>.............] - ETA: 0s - loss: 0.0229
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0162
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1667
4096/6530 [=================>............] - ETA: 0s - loss: 0.0227
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0164
6530/6530 [==============================] - 0s 23us/step - loss: 0.1648 - val_loss: 0.1626
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1716
2992/6530 [============>.................] - ETA: 0s - loss: 0.0161
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0230
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1707
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0160
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0229
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1683
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0159
6530/6530 [==============================] - 0s 23us/step - loss: 0.1677 - val_loss: 0.1551
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1647
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0229
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0160
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0229
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1653
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0161
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0229
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1664
6530/6530 [==============================] - 0s 22us/step - loss: 0.1656 - val_loss: 0.1921

6080/6530 [==========================>...] - ETA: 0s - loss: 0.0161
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0227
6530/6530 [==============================] - 1s 87us/step - loss: 0.0159 - val_loss: 0.0170

6192/6530 [===========================>..] - ETA: 0s - loss: 0.0225
6496/6530 [============================>.] - ETA: 0s - loss: 0.0223
6530/6530 [==============================] - 1s 182us/step - loss: 0.0223 - val_loss: 0.0171
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0156
 304/6530 [>.............................] - ETA: 1s - loss: 0.0204
 624/6530 [=>............................] - ETA: 0s - loss: 0.0228
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0221
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0223
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0224
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0219
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0222
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0218
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0212
# training | RMSE: 0.2266, MAE: 0.1873
worker 1  xfile  [3, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4949767012811116}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41779063623655877}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.22660271680293043, 'rmse': 0.22660271680293043, 'mae': 0.18730488037305953, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  95 | activation: tanh    | extras: dropout - rate: 19.1% 
layer 2 | size:  34 | activation: sigmoid | extras: None 
layer 3 | size:  11 | activation: relu    | extras: dropout - rate: 15.4% 
layer 4 | size:  12 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e901dbb00>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 24s - loss: 0.4556
3056/6530 [=============>................] - ETA: 0s - loss: 0.0212
1664/6530 [======>.......................] - ETA: 0s - loss: 0.2475 
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0216
3264/6530 [=============>................] - ETA: 0s - loss: 0.2131
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0216
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1992
3984/6530 [=================>............] - ETA: 0s - loss: 0.0215
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0218
6530/6530 [==============================] - 0s 75us/step - loss: 0.1916 - val_loss: 0.1625
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1750
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0217
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1627
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0215
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1613
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0212
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1588
# training | RMSE: 0.1217, MAE: 0.0930
worker 0  xfile  [0, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27679844046668345}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39624344937280587}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.34191658154173177}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.12167330156076778, 'rmse': 0.12167330156076778, 'mae': 0.09297761909995811, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  86 | activation: relu    | extras: dropout - rate: 18.2% 
layer 2 | size:  11 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  58 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e91325710>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 1:17 - loss: 0.1954
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0211
6530/6530 [==============================] - 0s 31us/step - loss: 0.1570 - val_loss: 0.1553
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1480
 608/6530 [=>............................] - ETA: 4s - loss: 0.1947  
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0209
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1462
1152/6530 [====>.........................] - ETA: 2s - loss: 0.1837
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0208
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1472
1728/6530 [======>.......................] - ETA: 1s - loss: 0.1796
6480/6530 [============================>.] - ETA: 0s - loss: 0.0208
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1468
6530/6530 [==============================] - 1s 172us/step - loss: 0.0207 - val_loss: 0.0136
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0193
2304/6530 [=========>....................] - ETA: 1s - loss: 0.1745
6530/6530 [==============================] - 0s 31us/step - loss: 0.1458 - val_loss: 0.1550
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1401
 320/6530 [>.............................] - ETA: 1s - loss: 0.0200
2880/6530 [============>.................] - ETA: 0s - loss: 0.1694
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1364
 624/6530 [=>............................] - ETA: 1s - loss: 0.0194
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1664
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1385
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0188
4000/6530 [=================>............] - ETA: 0s - loss: 0.1644
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1384
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0189
6530/6530 [==============================] - 0s 31us/step - loss: 0.1379 - val_loss: 0.1393

4576/6530 [====================>.........] - ETA: 0s - loss: 0.1619Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1319
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0195
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1603
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1332
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0200
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1585
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1355
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0198
6336/6530 [============================>.] - ETA: 0s - loss: 0.1566
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1350
6530/6530 [==============================] - 0s 31us/step - loss: 0.1346 - val_loss: 0.1326

2432/6530 [==========>...................] - ETA: 0s - loss: 0.0200Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1264
6530/6530 [==============================] - 1s 156us/step - loss: 0.1563 - val_loss: 0.1543
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1453
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0196
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1278
 608/6530 [=>............................] - ETA: 0s - loss: 0.1480
3040/6530 [============>.................] - ETA: 0s - loss: 0.0195
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1303
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1483
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0194
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1302
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1461
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0195
6530/6530 [==============================] - 0s 32us/step - loss: 0.1293 - val_loss: 0.1193
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1245
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1447
3920/6530 [=================>............] - ETA: 0s - loss: 0.0195
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1231
2848/6530 [============>.................] - ETA: 0s - loss: 0.1424
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0195
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1265
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1396
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0195
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1259
4032/6530 [=================>............] - ETA: 0s - loss: 0.1388
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0195
6528/6530 [============================>.] - ETA: 0s - loss: 0.1250
6530/6530 [==============================] - 0s 33us/step - loss: 0.1250 - val_loss: 0.1227
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1271
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1379
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0193
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1206
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1366
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0191
3264/6530 [=============>................] - ETA: 0s - loss: 0.1230
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1358
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0190
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1219
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1345
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0191
6530/6530 [==============================] - 0s 32us/step - loss: 0.1215 - val_loss: 0.1139
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1284
6530/6530 [==============================] - 1s 95us/step - loss: 0.1341 - val_loss: 0.1223
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1225
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0190
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1176
 608/6530 [=>............................] - ETA: 0s - loss: 0.1236
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1193
6530/6530 [==============================] - 1s 177us/step - loss: 0.0190 - val_loss: 0.0146
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0199
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1276
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1180
 320/6530 [>.............................] - ETA: 1s - loss: 0.0190
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1271
6530/6530 [==============================] - 0s 31us/step - loss: 0.1175 - val_loss: 0.1064

 624/6530 [=>............................] - ETA: 1s - loss: 0.0182
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1264
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0178
2976/6530 [============>.................] - ETA: 0s - loss: 0.1255
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0182
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1265
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0181
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1264
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0179
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1261
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0185
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1250
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0186
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1240
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0184
6530/6530 [==============================] - 1s 89us/step - loss: 0.1233 - val_loss: 0.1667
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1501
3120/6530 [=============>................] - ETA: 0s - loss: 0.0186
 608/6530 [=>............................] - ETA: 0s - loss: 0.1229
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0184
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1201
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0183
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1181
4000/6530 [=================>............] - ETA: 0s - loss: 0.0183
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1181
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0181
2976/6530 [============>.................] - ETA: 0s - loss: 0.1174
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0180
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1189
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0182
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1199
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0180
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1189
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0180
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1192
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0181
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1175
# training | RMSE: 0.1291, MAE: 0.0992
worker 1  xfile  [4, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19073769699532994}, 'layer_1_size': 95, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1543574533743368}, 'layer_3_size': 11, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3721211091810761}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.129070569005715, 'rmse': 0.129070569005715, 'mae': 0.09923047408263394, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  62 | activation: tanh    | extras: dropout - rate: 10.9% 
layer 2 | size:  17 | activation: tanh    | extras: batchnorm 
layer 3 | size:  16 | activation: tanh    | extras: dropout - rate: 32.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e6b7dff60>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 18s - loss: 1.7285
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0182
6496/6530 [============================>.] - ETA: 0s - loss: 0.1168
6530/6530 [==============================] - 1s 91us/step - loss: 0.1166 - val_loss: 0.1127
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0908
2944/6530 [============>.................] - ETA: 0s - loss: 0.3553 
6416/6530 [============================>.] - ETA: 0s - loss: 0.0182
 544/6530 [=>............................] - ETA: 0s - loss: 0.1231
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2350
6530/6530 [==============================] - 1s 176us/step - loss: 0.0183 - val_loss: 0.0134
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0095
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1174
6530/6530 [==============================] - 1s 82us/step - loss: 0.2103 - val_loss: 0.0913

 320/6530 [>.............................] - ETA: 1s - loss: 0.0144Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1214
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1186
2944/6530 [============>.................] - ETA: 0s - loss: 0.0708
 624/6530 [=>............................] - ETA: 1s - loss: 0.0164
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1172
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0167
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0641
6530/6530 [==============================] - 0s 19us/step - loss: 0.0629 - val_loss: 0.0645
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0720
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1157
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0165
2944/6530 [============>.................] - ETA: 0s - loss: 0.0525
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1155
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0163
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0489
3936/6530 [=================>............] - ETA: 0s - loss: 0.1154
6530/6530 [==============================] - 0s 19us/step - loss: 0.0486 - val_loss: 0.0518
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0767
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0160
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1147
2944/6530 [============>.................] - ETA: 0s - loss: 0.0469
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0160
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1143
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0439
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0159
6530/6530 [==============================] - 0s 19us/step - loss: 0.0433 - val_loss: 0.0360
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0414
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1137
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0160
2944/6530 [============>.................] - ETA: 0s - loss: 0.0389
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1134
3008/6530 [============>.................] - ETA: 0s - loss: 0.0163
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0382
6530/6530 [==============================] - 1s 94us/step - loss: 0.1134 - val_loss: 0.2011
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1640
6530/6530 [==============================] - 0s 19us/step - loss: 0.0379 - val_loss: 0.0453
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0577
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0162
 608/6530 [=>............................] - ETA: 0s - loss: 0.1081
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0387
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0163
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1145
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0364
3920/6530 [=================>............] - ETA: 0s - loss: 0.0164
6530/6530 [==============================] - 0s 20us/step - loss: 0.0362 - val_loss: 0.0368
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0520
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1123
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0167
2944/6530 [============>.................] - ETA: 0s - loss: 0.0354
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1103
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0171
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0348
2944/6530 [============>.................] - ETA: 0s - loss: 0.1106
6530/6530 [==============================] - 0s 19us/step - loss: 0.0345 - val_loss: 0.0260
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0350
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0168
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1108
2944/6530 [============>.................] - ETA: 0s - loss: 0.0335
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0168
4096/6530 [=================>............] - ETA: 0s - loss: 0.1111
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0329
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0169
6530/6530 [==============================] - 0s 20us/step - loss: 0.0327 - val_loss: 0.0248
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0293
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1100
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0169
2944/6530 [============>.................] - ETA: 0s - loss: 0.0312
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1096
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0172
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0308
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1099
6530/6530 [==============================] - 0s 19us/step - loss: 0.0307 - val_loss: 0.0263

6320/6530 [============================>.] - ETA: 0s - loss: 0.0171
6432/6530 [============================>.] - ETA: 0s - loss: 0.1092
6530/6530 [==============================] - 1s 92us/step - loss: 0.1090 - val_loss: 0.1068
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1378
6530/6530 [==============================] - 1s 178us/step - loss: 0.0172 - val_loss: 0.0152
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0123
 608/6530 [=>............................] - ETA: 0s - loss: 0.1065
 336/6530 [>.............................] - ETA: 1s - loss: 0.0161
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1088
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0167
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1090
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0159
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1075
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0161
2880/6530 [============>.................] - ETA: 0s - loss: 0.1071
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0155
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1058
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0158
4064/6530 [=================>............] - ETA: 0s - loss: 0.1053
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0155
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1054
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0156
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1061
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0159
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1053
3024/6530 [============>.................] - ETA: 0s - loss: 0.0157
6528/6530 [============================>.] - ETA: 0s - loss: 0.1060
6530/6530 [==============================] - 1s 90us/step - loss: 0.1061 - val_loss: 0.1575
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1974
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0158
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1015
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0158
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1037
3952/6530 [=================>............] - ETA: 0s - loss: 0.0159
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1017
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0158
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1023
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0158
3008/6530 [============>.................] - ETA: 0s - loss: 0.1026
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0158
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1015
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0160
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1027
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0160
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1032
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0160
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1040
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0160
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1042
6400/6530 [============================>.] - ETA: 0s - loss: 0.0159
# training | RMSE: 0.1608, MAE: 0.1263
worker 1  xfile  [6, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10939569932384074}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.326615848226966}, 'layer_3_size': 16, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42231207461314724}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.1608428725312425, 'rmse': 0.1608428725312425, 'mae': 0.1263195657466459, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  18 | activation: sigmoid | extras: None 
layer 2 | size:  48 | activation: tanh    | extras: dropout - rate: 14.9% 
layer 3 | size:  76 | activation: relu    | extras: batchnorm 
layer 4 | size:  51 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e686b6748>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 26s - loss: 0.3594
6530/6530 [==============================] - 1s 88us/step - loss: 0.1039 - val_loss: 0.1291

6530/6530 [==============================] - 1s 174us/step - loss: 0.0159 - val_loss: 0.0117
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1243
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1998 
 640/6530 [=>............................] - ETA: 0s - loss: 0.0998
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1559
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1025
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1027
6530/6530 [==============================] - 1s 112us/step - loss: 0.1368 - val_loss: 0.0536
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0878
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1020
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0781
2944/6530 [============>.................] - ETA: 0s - loss: 0.1027
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0736
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1022
6530/6530 [==============================] - 0s 25us/step - loss: 0.0707 - val_loss: 0.0584
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0659
4128/6530 [=================>............] - ETA: 0s - loss: 0.1026
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0624
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1019
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0602
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1015
6530/6530 [==============================] - 0s 23us/step - loss: 0.0589 - val_loss: 0.0456
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0492
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1016
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0563
6368/6530 [============================>.] - ETA: 0s - loss: 0.1011
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0533
6530/6530 [==============================] - 1s 92us/step - loss: 0.1014 - val_loss: 0.1571

6530/6530 [==============================] - 0s 25us/step - loss: 0.0522 - val_loss: 0.0625
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0530
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0518
# training | RMSE: 0.0985, MAE: 0.0779
worker 2  xfile  [2, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 82, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 34, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11389578672392409}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.09849437653704658, 'rmse': 0.09849437653704658, 'mae': 0.07790336920751025, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  53 | activation: relu    | extras: dropout - rate: 34.7% 
layer 2 | size:  87 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e91325fd0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 10s - loss: 0.3018
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0509
3712/6530 [================>.............] - ETA: 0s - loss: 0.2239 
6530/6530 [==============================] - 0s 24us/step - loss: 0.0500 - val_loss: 0.0492
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0593
6530/6530 [==============================] - 0s 51us/step - loss: 0.2082 - val_loss: 0.1675

2432/6530 [==========>...................] - ETA: 0s - loss: 0.0501Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1980
3968/6530 [=================>............] - ETA: 0s - loss: 0.1740
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0486
6530/6530 [==============================] - 0s 14us/step - loss: 0.1722 - val_loss: 0.2046
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.2197
6530/6530 [==============================] - 0s 23us/step - loss: 0.0486 - val_loss: 0.0512
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0630
4096/6530 [=================>............] - ETA: 0s - loss: 0.1674
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0528
6530/6530 [==============================] - 0s 14us/step - loss: 0.1654 - val_loss: 0.1584
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1514
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0510
3968/6530 [=================>............] - ETA: 0s - loss: 0.1600
6530/6530 [==============================] - 0s 23us/step - loss: 0.0500 - val_loss: 0.0432
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0614
6530/6530 [==============================] - 0s 14us/step - loss: 0.1605 - val_loss: 0.2642
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.2762
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0476
3840/6530 [================>.............] - ETA: 0s - loss: 0.1621
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0456
6530/6530 [==============================] - 0s 15us/step - loss: 0.1606 - val_loss: 0.1549
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1646
6530/6530 [==============================] - 0s 24us/step - loss: 0.0460 - val_loss: 0.0592
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0379
3712/6530 [================>.............] - ETA: 0s - loss: 0.1566
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0487
6530/6530 [==============================] - 0s 15us/step - loss: 0.1560 - val_loss: 0.2584
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.2687
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0477
3840/6530 [================>.............] - ETA: 0s - loss: 0.1578
6530/6530 [==============================] - 0s 24us/step - loss: 0.0476 - val_loss: 0.0510

6530/6530 [==============================] - 0s 15us/step - loss: 0.1562 - val_loss: 0.1621
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1706
3840/6530 [================>.............] - ETA: 0s - loss: 0.1519
6530/6530 [==============================] - 0s 15us/step - loss: 0.1511 - val_loss: 0.1499
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1662
3072/6530 [=============>................] - ETA: 0s - loss: 0.1454
# training | RMSE: 0.1776, MAE: 0.1550
worker 0  xfile  [5, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1817087321690729}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.17762570168744332, 'rmse': 0.17762570168744332, 'mae': 0.15501948646663954, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  24 | activation: tanh    | extras: batchnorm 
layer 2 | size:  31 | activation: sigmoid | extras: None 
layer 3 | size:  94 | activation: relu    | extras: dropout - rate: 36.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e901e90f0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 2:37 - loss: 0.5388
6530/6530 [==============================] - 0s 16us/step - loss: 0.1463 - val_loss: 0.1378

 320/6530 [>.............................] - ETA: 8s - loss: 0.1977  
 624/6530 [=>............................] - ETA: 4s - loss: 0.1315
 928/6530 [===>..........................] - ETA: 3s - loss: 0.1043
1232/6530 [====>.........................] - ETA: 2s - loss: 0.0890
1536/6530 [======>.......................] - ETA: 2s - loss: 0.0789
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0736
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0699
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0667
2688/6530 [===========>..................] - ETA: 1s - loss: 0.0643
3008/6530 [============>.................] - ETA: 1s - loss: 0.0614
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0595
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0584
3968/6530 [=================>............] - ETA: 0s - loss: 0.0574
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0565
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0560
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0550
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0541
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0533
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0528
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0520
6432/6530 [============================>.] - ETA: 0s - loss: 0.0513
6530/6530 [==============================] - 2s 238us/step - loss: 0.0511 - val_loss: 0.0323
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0380
 304/6530 [>.............................] - ETA: 1s - loss: 0.0353
# training | RMSE: 0.2225, MAE: 0.1848
worker 1  xfile  [7, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14900012006333854}, 'layer_2_size': 48, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1937868315068102}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2224722216850548, 'rmse': 0.2224722216850548, 'mae': 0.18481315588467176, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  78 | activation: sigmoid | extras: None 
layer 2 | size:  21 | activation: sigmoid | extras: dropout - rate: 45.4% 
layer 3 | size:  15 | activation: relu    | extras: dropout - rate: 48.3% 
layer 4 | size:  44 | activation: tanh    | extras: batchnorm 
layer 5 | size:   8 | activation: tanh    | extras: dropout - rate: 21.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e4c6fb0f0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 4:15 - loss: 0.8848
 576/6530 [=>............................] - ETA: 1s - loss: 0.0373
 240/6530 [>.............................] - ETA: 17s - loss: 0.8615 
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0380
 480/6530 [=>............................] - ETA: 9s - loss: 0.7211 
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0381
 720/6530 [==>...........................] - ETA: 6s - loss: 0.6184
# training | RMSE: 0.1705, MAE: 0.1336
worker 2  xfile  [8, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3474702838513902}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2881528219221712}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.17051182483205785, 'rmse': 0.17051182483205785, 'mae': 0.13357409672152773, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  17 | activation: relu    | extras: batchnorm 
layer 2 | size:  70 | activation: sigmoid | extras: None 
layer 3 | size:  86 | activation: relu    | extras: batchnorm 
layer 4 | size:  30 | activation: relu    | extras: dropout - rate: 37.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e905c6828>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 2:02 - loss: 1.6841
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0372
 928/6530 [===>..........................] - ETA: 5s - loss: 0.5580
 512/6530 [=>............................] - ETA: 7s - loss: 0.6675  
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0362
1152/6530 [====>.........................] - ETA: 4s - loss: 0.5194
 960/6530 [===>..........................] - ETA: 4s - loss: 0.4648
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0362
1376/6530 [=====>........................] - ETA: 3s - loss: 0.4862
1376/6530 [=====>........................] - ETA: 2s - loss: 0.3824
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0363
1600/6530 [======>.......................] - ETA: 3s - loss: 0.4624
1824/6530 [=======>......................] - ETA: 2s - loss: 0.3253
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0369
1840/6530 [=======>......................] - ETA: 2s - loss: 0.4416
2304/6530 [=========>....................] - ETA: 1s - loss: 0.2834
2864/6530 [============>.................] - ETA: 0s - loss: 0.0358
2080/6530 [========>.....................] - ETA: 2s - loss: 0.4283
2752/6530 [===========>..................] - ETA: 1s - loss: 0.2568
3168/6530 [=============>................] - ETA: 0s - loss: 0.0351
2320/6530 [=========>....................] - ETA: 2s - loss: 0.4149
3232/6530 [=============>................] - ETA: 0s - loss: 0.2339
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0356
2544/6530 [==========>...................] - ETA: 1s - loss: 0.4014
3712/6530 [================>.............] - ETA: 0s - loss: 0.2185
3744/6530 [================>.............] - ETA: 0s - loss: 0.0353
2768/6530 [===========>..................] - ETA: 1s - loss: 0.3917
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2049
4048/6530 [=================>............] - ETA: 0s - loss: 0.0357
3008/6530 [============>.................] - ETA: 1s - loss: 0.3815
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1927
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0353
3248/6530 [=============>................] - ETA: 1s - loss: 0.3742
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1816
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0357
3488/6530 [===============>..............] - ETA: 1s - loss: 0.3683
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1728
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0356
3728/6530 [================>.............] - ETA: 1s - loss: 0.3626
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1651
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0353
3952/6530 [=================>............] - ETA: 0s - loss: 0.3574
6496/6530 [============================>.] - ETA: 0s - loss: 0.1584
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0352
4176/6530 [==================>...........] - ETA: 0s - loss: 0.3513
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0352
6530/6530 [==============================] - 1s 215us/step - loss: 0.1578 - val_loss: 0.0676
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1035
4416/6530 [===================>..........] - ETA: 0s - loss: 0.3455
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0350
 512/6530 [=>............................] - ETA: 0s - loss: 0.0704
4656/6530 [====================>.........] - ETA: 0s - loss: 0.3418
6464/6530 [============================>.] - ETA: 0s - loss: 0.0348
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0699
4896/6530 [=====================>........] - ETA: 0s - loss: 0.3373
6530/6530 [==============================] - 1s 181us/step - loss: 0.0349 - val_loss: 0.0271
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0307
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0665
5136/6530 [======================>.......] - ETA: 0s - loss: 0.3327
 304/6530 [>.............................] - ETA: 1s - loss: 0.0300
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0639
5360/6530 [=======================>......] - ETA: 0s - loss: 0.3307
 608/6530 [=>............................] - ETA: 1s - loss: 0.0323
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0624
5584/6530 [========================>.....] - ETA: 0s - loss: 0.3277
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0323
2880/6530 [============>.................] - ETA: 0s - loss: 0.0617
5824/6530 [=========================>....] - ETA: 0s - loss: 0.3242
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0330
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0602
6064/6530 [==========================>...] - ETA: 0s - loss: 0.3213
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0328
3840/6530 [================>.............] - ETA: 0s - loss: 0.0605
6304/6530 [===========================>..] - ETA: 0s - loss: 0.3190
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0323
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0603
6512/6530 [============================>.] - ETA: 0s - loss: 0.3164
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0319
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0591
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0324
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0584
6530/6530 [==============================] - 2s 334us/step - loss: 0.3161 - val_loss: 0.2195
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.2942
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0327
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0578
 240/6530 [>.............................] - ETA: 1s - loss: 0.2302
2960/6530 [============>.................] - ETA: 0s - loss: 0.0319
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0568
 464/6530 [=>............................] - ETA: 1s - loss: 0.2423
3232/6530 [=============>................] - ETA: 0s - loss: 0.0315
 688/6530 [==>...........................] - ETA: 1s - loss: 0.2474
6530/6530 [==============================] - 1s 114us/step - loss: 0.0564 - val_loss: 0.0501
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0651
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0317
 928/6530 [===>..........................] - ETA: 1s - loss: 0.2499
 512/6530 [=>............................] - ETA: 0s - loss: 0.0520
3840/6530 [================>.............] - ETA: 0s - loss: 0.0318
1168/6530 [====>.........................] - ETA: 1s - loss: 0.2490
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0513
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0319
1376/6530 [=====>........................] - ETA: 1s - loss: 0.2455
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0487
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0320
1616/6530 [======>.......................] - ETA: 1s - loss: 0.2446
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0470
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0322
1840/6530 [=======>......................] - ETA: 1s - loss: 0.2457
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0465
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0320
2064/6530 [========>.....................] - ETA: 0s - loss: 0.2439
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0457
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0320
2304/6530 [=========>....................] - ETA: 0s - loss: 0.2437
3264/6530 [=============>................] - ETA: 0s - loss: 0.0451
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0317
2544/6530 [==========>...................] - ETA: 0s - loss: 0.2423
3744/6530 [================>.............] - ETA: 0s - loss: 0.0454
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0316
2784/6530 [===========>..................] - ETA: 0s - loss: 0.2415
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0458
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0315
3024/6530 [============>.................] - ETA: 0s - loss: 0.2397
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0451
6530/6530 [==============================] - 1s 178us/step - loss: 0.0314 - val_loss: 0.0246
Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0266
3264/6530 [=============>................] - ETA: 0s - loss: 0.2400
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0448
 320/6530 [>.............................] - ETA: 1s - loss: 0.0267
3504/6530 [===============>..............] - ETA: 0s - loss: 0.2400
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0443
 608/6530 [=>............................] - ETA: 1s - loss: 0.0288
3744/6530 [================>.............] - ETA: 0s - loss: 0.2399
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0440
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0296
3984/6530 [=================>............] - ETA: 0s - loss: 0.2391
6530/6530 [==============================] - 1s 114us/step - loss: 0.0435 - val_loss: 0.0425
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0512
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0304
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2381
 512/6530 [=>............................] - ETA: 0s - loss: 0.0447
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0299
4464/6530 [===================>..........] - ETA: 0s - loss: 0.2381
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0432
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0299
4688/6530 [====================>.........] - ETA: 0s - loss: 0.2378
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0407
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0295
4912/6530 [=====================>........] - ETA: 0s - loss: 0.2381
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0395
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0297
5152/6530 [======================>.......] - ETA: 0s - loss: 0.2375
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0390
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0296
5392/6530 [=======================>......] - ETA: 0s - loss: 0.2377
2848/6530 [============>.................] - ETA: 0s - loss: 0.0389
3008/6530 [============>.................] - ETA: 0s - loss: 0.0288
5616/6530 [========================>.....] - ETA: 0s - loss: 0.2371
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0383
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0288
5856/6530 [=========================>....] - ETA: 0s - loss: 0.2365
3776/6530 [================>.............] - ETA: 0s - loss: 0.0382
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0290
6096/6530 [===========================>..] - ETA: 0s - loss: 0.2363
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0388
3920/6530 [=================>............] - ETA: 0s - loss: 0.0291
6336/6530 [============================>.] - ETA: 0s - loss: 0.2359
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0383
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0292
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0381
6530/6530 [==============================] - 1s 227us/step - loss: 0.2352 - val_loss: 0.2204
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.2323
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0296
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0379
 240/6530 [>.............................] - ETA: 1s - loss: 0.2297
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0294
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0376
 480/6530 [=>............................] - ETA: 1s - loss: 0.2352
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0292
 704/6530 [==>...........................] - ETA: 1s - loss: 0.2366
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0293
6530/6530 [==============================] - 1s 113us/step - loss: 0.0373 - val_loss: 0.0376
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0464
 944/6530 [===>..........................] - ETA: 1s - loss: 0.2377
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0290
 480/6530 [=>............................] - ETA: 0s - loss: 0.0403
1184/6530 [====>.........................] - ETA: 1s - loss: 0.2351
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0289
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0378
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0287
1408/6530 [=====>........................] - ETA: 1s - loss: 0.2345
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0355
1648/6530 [======>.......................] - ETA: 1s - loss: 0.2324
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0343
6530/6530 [==============================] - 1s 176us/step - loss: 0.0288 - val_loss: 0.0237
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0215
1856/6530 [=======>......................] - ETA: 1s - loss: 0.2325
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0344
 272/6530 [>.............................] - ETA: 1s - loss: 0.0252
2096/6530 [========>.....................] - ETA: 0s - loss: 0.2339
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0346
 576/6530 [=>............................] - ETA: 1s - loss: 0.0254
2336/6530 [=========>....................] - ETA: 0s - loss: 0.2342
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0341
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0269
2576/6530 [==========>...................] - ETA: 0s - loss: 0.2337
3776/6530 [================>.............] - ETA: 0s - loss: 0.0343
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0284
2816/6530 [===========>..................] - ETA: 0s - loss: 0.2335
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0346
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0279
3040/6530 [============>.................] - ETA: 0s - loss: 0.2307
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0344
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0275
3264/6530 [=============>................] - ETA: 0s - loss: 0.2305
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0343
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0272
3456/6530 [==============>...............] - ETA: 0s - loss: 0.2301
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0341
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0272
3696/6530 [===============>..............] - ETA: 0s - loss: 0.2299
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0339
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0272
3936/6530 [=================>............] - ETA: 0s - loss: 0.2297
2992/6530 [============>.................] - ETA: 0s - loss: 0.0265
6530/6530 [==============================] - 1s 116us/step - loss: 0.0336 - val_loss: 0.0345
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0399
4176/6530 [==================>...........] - ETA: 0s - loss: 0.2283
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0266
 480/6530 [=>............................] - ETA: 0s - loss: 0.0369
4416/6530 [===================>..........] - ETA: 0s - loss: 0.2283
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0268
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0345
4640/6530 [====================>.........] - ETA: 0s - loss: 0.2284
3872/6530 [================>.............] - ETA: 0s - loss: 0.0268
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0324
4880/6530 [=====================>........] - ETA: 0s - loss: 0.2282
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0269
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0315
5104/6530 [======================>.......] - ETA: 0s - loss: 0.2278
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0269
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0309
5344/6530 [=======================>......] - ETA: 0s - loss: 0.2280
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0270
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0313
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2272
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0268
3232/6530 [=============>................] - ETA: 0s - loss: 0.0310
5792/6530 [=========================>....] - ETA: 0s - loss: 0.2272
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0269
3712/6530 [================>.............] - ETA: 0s - loss: 0.0309
6032/6530 [==========================>...] - ETA: 0s - loss: 0.2267
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0265
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0312
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2268
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0264
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0311
6528/6530 [============================>.] - ETA: 0s - loss: 0.2258
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0264
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0311
6530/6530 [==============================] - 1s 229us/step - loss: 0.2258 - val_loss: 0.2325
Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1600
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0308
6530/6530 [==============================] - 1s 178us/step - loss: 0.0264 - val_loss: 0.0223
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0169
 256/6530 [>.............................] - ETA: 1s - loss: 0.2236
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0307
 320/6530 [>.............................] - ETA: 1s - loss: 0.0216
 496/6530 [=>............................] - ETA: 1s - loss: 0.2258
 624/6530 [=>............................] - ETA: 1s - loss: 0.0232
 720/6530 [==>...........................] - ETA: 1s - loss: 0.2290
6530/6530 [==============================] - 1s 114us/step - loss: 0.0304 - val_loss: 0.0312
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0339
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0247
 960/6530 [===>..........................] - ETA: 1s - loss: 0.2301
 512/6530 [=>............................] - ETA: 0s - loss: 0.0326
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0258
1200/6530 [====>.........................] - ETA: 1s - loss: 0.2273
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0316
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0253
1440/6530 [=====>........................] - ETA: 1s - loss: 0.2257
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0299
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0253
1680/6530 [======>.......................] - ETA: 1s - loss: 0.2243
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0286
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0249
1920/6530 [=======>......................] - ETA: 0s - loss: 0.2236
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0287
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0248
2160/6530 [========>.....................] - ETA: 0s - loss: 0.2234
2912/6530 [============>.................] - ETA: 0s - loss: 0.0285
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0247
2400/6530 [==========>...................] - ETA: 0s - loss: 0.2227
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0283
3024/6530 [============>.................] - ETA: 0s - loss: 0.0243
2640/6530 [===========>..................] - ETA: 0s - loss: 0.2226
3872/6530 [================>.............] - ETA: 0s - loss: 0.0286
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0244
2864/6530 [============>.................] - ETA: 0s - loss: 0.2219
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0287
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0247
3104/6530 [=============>................] - ETA: 0s - loss: 0.2205
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0287
3904/6530 [================>.............] - ETA: 0s - loss: 0.0246
3328/6530 [==============>...............] - ETA: 0s - loss: 0.2198
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0287
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0247
3568/6530 [===============>..............] - ETA: 0s - loss: 0.2199
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0285
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0249
3808/6530 [================>.............] - ETA: 0s - loss: 0.2194
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0283
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0248
4048/6530 [=================>............] - ETA: 0s - loss: 0.2187
6530/6530 [==============================] - 1s 114us/step - loss: 0.0280 - val_loss: 0.0286
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0303
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0245
4288/6530 [==================>...........] - ETA: 0s - loss: 0.2171
 480/6530 [=>............................] - ETA: 0s - loss: 0.0303
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0245
4512/6530 [===================>..........] - ETA: 0s - loss: 0.2178
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0292
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0244
4752/6530 [====================>.........] - ETA: 0s - loss: 0.2178
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0273
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0242
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2169
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0266
6336/6530 [============================>.] - ETA: 0s - loss: 0.0242
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2166
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0263
6530/6530 [==============================] - 1s 175us/step - loss: 0.0243 - val_loss: 0.0195

5472/6530 [========================>.....] - ETA: 0s - loss: 0.2168Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0152
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0264
5696/6530 [=========================>....] - ETA: 0s - loss: 0.2163
 320/6530 [>.............................] - ETA: 1s - loss: 0.0204
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0263
5936/6530 [==========================>...] - ETA: 0s - loss: 0.2160
 624/6530 [=>............................] - ETA: 1s - loss: 0.0218
3776/6530 [================>.............] - ETA: 0s - loss: 0.0263
6176/6530 [===========================>..] - ETA: 0s - loss: 0.2158
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0232
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0267
6416/6530 [============================>.] - ETA: 0s - loss: 0.2154
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0243
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0267
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0238
6530/6530 [==============================] - 1s 224us/step - loss: 0.2151 - val_loss: 0.2089
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.2791
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0267
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0240
 224/6530 [>.............................] - ETA: 1s - loss: 0.2111
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0266
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0234
 464/6530 [=>............................] - ETA: 1s - loss: 0.2141
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0264
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0233
 688/6530 [==>...........................] - ETA: 1s - loss: 0.2174
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0234
6530/6530 [==============================] - 1s 114us/step - loss: 0.0262 - val_loss: 0.0271
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0253
 928/6530 [===>..........................] - ETA: 1s - loss: 0.2183
3008/6530 [============>.................] - ETA: 0s - loss: 0.0230
 512/6530 [=>............................] - ETA: 0s - loss: 0.0281
1168/6530 [====>.........................] - ETA: 1s - loss: 0.2179
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0232
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0273
1408/6530 [=====>........................] - ETA: 1s - loss: 0.2169
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0233
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0260
1648/6530 [======>.......................] - ETA: 1s - loss: 0.2161
3920/6530 [=================>............] - ETA: 0s - loss: 0.0234
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0251
1872/6530 [=======>......................] - ETA: 1s - loss: 0.2139
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0235
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0249
2112/6530 [========>.....................] - ETA: 0s - loss: 0.2136
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0236
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0250
2352/6530 [=========>....................] - ETA: 0s - loss: 0.2139
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0236
3264/6530 [=============>................] - ETA: 0s - loss: 0.0249
2592/6530 [==========>...................] - ETA: 0s - loss: 0.2141
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0234
3744/6530 [================>.............] - ETA: 0s - loss: 0.0248
2832/6530 [============>.................] - ETA: 0s - loss: 0.2135
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0234
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0251
3072/6530 [=============>................] - ETA: 0s - loss: 0.2118
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0231
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0251
3296/6530 [==============>...............] - ETA: 0s - loss: 0.2115
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0231
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0252
3456/6530 [==============>...............] - ETA: 0s - loss: 0.2116
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0230
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0251
3664/6530 [===============>..............] - ETA: 0s - loss: 0.2110
6464/6530 [============================>.] - ETA: 0s - loss: 0.0230
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0250
3904/6530 [================>.............] - ETA: 0s - loss: 0.2112
6530/6530 [==============================] - 1s 181us/step - loss: 0.0232 - val_loss: 0.0173
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0147
4144/6530 [==================>...........] - ETA: 0s - loss: 0.2104
6530/6530 [==============================] - 1s 114us/step - loss: 0.0248 - val_loss: 0.0265

 320/6530 [>.............................] - ETA: 1s - loss: 0.0197
4400/6530 [===================>..........] - ETA: 0s - loss: 0.2100
 624/6530 [=>............................] - ETA: 0s - loss: 0.0210
4624/6530 [====================>.........] - ETA: 0s - loss: 0.2103
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0221
4832/6530 [=====================>........] - ETA: 0s - loss: 0.2106
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0234
5072/6530 [======================>.......] - ETA: 0s - loss: 0.2105
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0230
5312/6530 [=======================>......] - ETA: 0s - loss: 0.2104
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0230
5552/6530 [========================>.....] - ETA: 0s - loss: 0.2097
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0226
5792/6530 [=========================>....] - ETA: 0s - loss: 0.2101
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0225
6016/6530 [==========================>...] - ETA: 0s - loss: 0.2096
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0224
6256/6530 [===========================>..] - ETA: 0s - loss: 0.2092
3072/6530 [=============>................] - ETA: 0s - loss: 0.0221
6480/6530 [============================>.] - ETA: 0s - loss: 0.2087
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0223
6530/6530 [==============================] - 2s 231us/step - loss: 0.2085 - val_loss: 0.1919
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1862
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0225
 240/6530 [>.............................] - ETA: 1s - loss: 0.1900
3952/6530 [=================>............] - ETA: 0s - loss: 0.0225
 464/6530 [=>............................] - ETA: 1s - loss: 0.1971
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0226
 704/6530 [==>...........................] - ETA: 1s - loss: 0.2031
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0227
 928/6530 [===>..........................] - ETA: 1s - loss: 0.2059
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0226
1168/6530 [====>.........................] - ETA: 1s - loss: 0.2075
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0224
1376/6530 [=====>........................] - ETA: 1s - loss: 0.2053
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0222
1600/6530 [======>.......................] - ETA: 1s - loss: 0.2036
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0222
1824/6530 [=======>......................] - ETA: 1s - loss: 0.2037
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0222
2064/6530 [========>.....................] - ETA: 1s - loss: 0.2040
6432/6530 [============================>.] - ETA: 0s - loss: 0.0221
2304/6530 [=========>....................] - ETA: 0s - loss: 0.2043
6530/6530 [==============================] - 1s 173us/step - loss: 0.0223 - val_loss: 0.0159
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0144
2544/6530 [==========>...................] - ETA: 0s - loss: 0.2034
 320/6530 [>.............................] - ETA: 1s - loss: 0.0192
2784/6530 [===========>..................] - ETA: 0s - loss: 0.2024
 624/6530 [=>............................] - ETA: 0s - loss: 0.0204
3024/6530 [============>.................] - ETA: 0s - loss: 0.2013
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0215
# training | RMSE: 0.1467, MAE: 0.1152
worker 2  xfile  [11, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 70, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.37063437706228464}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4656791441692686}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.14665756223790988, 'rmse': 0.14665756223790988, 'mae': 0.11523611241904501, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:   6 | activation: tanh    | extras: None 
layer 2 | size:  71 | activation: relu    | extras: dropout - rate: 48.2% 
layer 3 | size:   3 | activation: tanh    | extras: batchnorm 
layer 4 | size:   9 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e48659780>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 25s - loss: 0.5906
3264/6530 [=============>................] - ETA: 0s - loss: 0.2019
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0226
2432/6530 [==========>...................] - ETA: 0s - loss: 0.5610 
3488/6530 [===============>..............] - ETA: 0s - loss: 0.2029
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0222
4736/6530 [====================>.........] - ETA: 0s - loss: 0.5428
3728/6530 [================>.............] - ETA: 0s - loss: 0.2025
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0222
3968/6530 [=================>............] - ETA: 0s - loss: 0.2020
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0218
6530/6530 [==============================] - 1s 110us/step - loss: 0.5165 - val_loss: 0.3506
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.4592
4192/6530 [==================>...........] - ETA: 0s - loss: 0.2016
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0217
2432/6530 [==========>...................] - ETA: 0s - loss: 0.3866
4432/6530 [===================>..........] - ETA: 0s - loss: 0.2013
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0217
4480/6530 [===================>..........] - ETA: 0s - loss: 0.3473
4656/6530 [====================>.........] - ETA: 0s - loss: 0.2013
3008/6530 [============>.................] - ETA: 0s - loss: 0.0215
6530/6530 [==============================] - 0s 24us/step - loss: 0.3090 - val_loss: 0.0420
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1949
4896/6530 [=====================>........] - ETA: 0s - loss: 0.2013
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0215
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1465
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2007
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0218
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1143
5360/6530 [=======================>......] - ETA: 0s - loss: 0.2011
3872/6530 [================>.............] - ETA: 0s - loss: 0.0218
6530/6530 [==============================] - 0s 24us/step - loss: 0.0924 - val_loss: 0.5813
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0356
5584/6530 [========================>.....] - ETA: 0s - loss: 0.2004
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0219
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0363
5808/6530 [=========================>....] - ETA: 0s - loss: 0.2004
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0218
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0356
6048/6530 [==========================>...] - ETA: 0s - loss: 0.2001
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0220
6530/6530 [==============================] - 0s 24us/step - loss: 0.0349 - val_loss: 0.5563
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0347
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2004
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0218
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0332
6512/6530 [============================>.] - ETA: 0s - loss: 0.1997
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0217
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0332
6530/6530 [==============================] - 1s 230us/step - loss: 0.1995 - val_loss: 0.1720
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.2225
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0215
6530/6530 [==============================] - 0s 24us/step - loss: 0.0335 - val_loss: 0.3293
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0397
 256/6530 [>.............................] - ETA: 1s - loss: 0.1928
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0215
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0345
 496/6530 [=>............................] - ETA: 1s - loss: 0.1978
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0216
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0340
 720/6530 [==>...........................] - ETA: 1s - loss: 0.2060
6530/6530 [==============================] - 1s 179us/step - loss: 0.0217 - val_loss: 0.0149

6530/6530 [==============================] - 0s 24us/step - loss: 0.0328 - val_loss: 0.1658
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0405
 960/6530 [===>..........................] - ETA: 1s - loss: 0.2053
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0339
1200/6530 [====>.........................] - ETA: 1s - loss: 0.2036
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0326
1440/6530 [=====>........................] - ETA: 1s - loss: 0.2018
6530/6530 [==============================] - 0s 25us/step - loss: 0.0333 - val_loss: 0.0938

1664/6530 [======>.......................] - ETA: 1s - loss: 0.1979
1904/6530 [=======>......................] - ETA: 1s - loss: 0.1966
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1974
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1975
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1979
2896/6530 [============>.................] - ETA: 0s - loss: 0.1966
3136/6530 [=============>................] - ETA: 0s - loss: 0.1948
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1954
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1953
# training | RMSE: 0.1160, MAE: 0.0907
worker 0  xfile  [9, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3636661996407462}, 'layer_3_size': 94, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10641354205355764}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.11603795588355889, 'rmse': 0.11603795588355889, 'mae': 0.09069249498145135, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  78 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e90adc470>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 22s - loss: 0.7016
3872/6530 [================>.............] - ETA: 0s - loss: 0.1951
2560/6530 [==========>...................] - ETA: 0s - loss: 0.5308 
4128/6530 [=================>............] - ETA: 0s - loss: 0.1942
4928/6530 [=====================>........] - ETA: 0s - loss: 0.3668
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1936
6530/6530 [==============================] - 0s 58us/step - loss: 0.3159 - val_loss: 0.1628
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1583
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1940
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1590
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1942
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1609
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1942
6530/6530 [==============================] - 0s 21us/step - loss: 0.1614 - val_loss: 0.1604
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1746
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1936
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1639
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1931
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1613
6530/6530 [==============================] - 0s 20us/step - loss: 0.1604 - val_loss: 0.1610
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1444
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1934
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1618
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1933
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1599
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1928
6530/6530 [==============================] - 0s 21us/step - loss: 0.1605 - val_loss: 0.1603
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1887
6480/6530 [============================>.] - ETA: 0s - loss: 0.1926
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1615
# training | RMSE: 0.3068, MAE: 0.2614
worker 2  xfile  [12, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 6, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.48200256294458677}, 'layer_2_size': 71, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18703635612869834}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.3068127301066356, 'rmse': 0.3068127301066356, 'mae': 0.26143476916839814, 'early_stop': True}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  23 | activation: tanh    | extras: batchnorm 
layer 2 | size:   7 | activation: tanh    | extras: dropout - rate: 25.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e4866df28>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 3:12 - loss: 1.1565
6530/6530 [==============================] - 1s 224us/step - loss: 0.1923 - val_loss: 0.1651
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.2179
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1603
 352/6530 [>.............................] - ETA: 9s - loss: 0.6404  
6530/6530 [==============================] - 0s 21us/step - loss: 0.1601 - val_loss: 0.1602

 256/6530 [>.............................] - ETA: 1s - loss: 0.1876Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1606
 704/6530 [==>...........................] - ETA: 4s - loss: 0.5295
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1636
 480/6530 [=>............................] - ETA: 1s - loss: 0.1998
1040/6530 [===>..........................] - ETA: 3s - loss: 0.4399
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1600
 704/6530 [==>...........................] - ETA: 1s - loss: 0.2006
1392/6530 [=====>........................] - ETA: 2s - loss: 0.3754
6530/6530 [==============================] - 0s 21us/step - loss: 0.1603 - val_loss: 0.1612
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1650
 928/6530 [===>..........................] - ETA: 1s - loss: 0.2000
1728/6530 [======>.......................] - ETA: 2s - loss: 0.3338
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1567
1168/6530 [====>.........................] - ETA: 1s - loss: 0.1999
2064/6530 [========>.....................] - ETA: 1s - loss: 0.2982
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1573
1408/6530 [=====>........................] - ETA: 1s - loss: 0.1972
6530/6530 [==============================] - 0s 21us/step - loss: 0.1594 - val_loss: 0.1600
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1483
2400/6530 [==========>...................] - ETA: 1s - loss: 0.2696
1648/6530 [======>.......................] - ETA: 1s - loss: 0.1952
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1576
2736/6530 [===========>..................] - ETA: 1s - loss: 0.2474
1888/6530 [=======>......................] - ETA: 1s - loss: 0.1941
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1590
3088/6530 [=============>................] - ETA: 1s - loss: 0.2280
6530/6530 [==============================] - 0s 21us/step - loss: 0.1593 - val_loss: 0.1598
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1455
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1948
3408/6530 [==============>...............] - ETA: 0s - loss: 0.2145
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1617
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1937
3744/6530 [================>.............] - ETA: 0s - loss: 0.2027
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1591
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1939
4096/6530 [=================>............] - ETA: 0s - loss: 0.1921
6530/6530 [==============================] - 0s 21us/step - loss: 0.1590 - val_loss: 0.1590

2816/6530 [===========>..................] - ETA: 0s - loss: 0.1936
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1832
3056/6530 [=============>................] - ETA: 0s - loss: 0.1914
# training | RMSE: 0.2010, MAE: 0.1584
worker 0  xfile  [13, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20150315133458624}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3651487968819357}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4985968736993499}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.20103929199942705, 'rmse': 0.20103929199942705, 'mae': 0.15837236246953074, 'early_stop': False}
vggnet done  0

4768/6530 [====================>.........] - ETA: 0s - loss: 0.1749
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1924
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1672
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1918
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1609
3792/6530 [================>.............] - ETA: 0s - loss: 0.1917
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1544
4048/6530 [=================>............] - ETA: 0s - loss: 0.1916
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1490
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1907
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1909
6530/6530 [==============================] - 2s 233us/step - loss: 0.1442 - val_loss: 0.0881
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0721
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1902
 368/6530 [>.............................] - ETA: 0s - loss: 0.0644
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1897
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0638
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1892
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0592
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1892
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0568
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1894
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0547
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1897
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0549
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1900
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0550
6530/6530 [==============================] - 1s 221us/step - loss: 0.1893 - val_loss: 0.1844

2848/6530 [============>.................] - ETA: 0s - loss: 0.0543Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1995
 256/6530 [>.............................] - ETA: 1s - loss: 0.1806
3216/6530 [=============>................] - ETA: 0s - loss: 0.0534
 512/6530 [=>............................] - ETA: 1s - loss: 0.1921
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0537
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1945
3984/6530 [=================>............] - ETA: 0s - loss: 0.0537
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1952
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0532
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0530
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1897
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0524
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1888
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0519
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1865
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0515
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1864
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0513
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1852
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1852
6530/6530 [==============================] - 1s 147us/step - loss: 0.0507 - val_loss: 0.0436
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0519
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1844
 400/6530 [>.............................] - ETA: 0s - loss: 0.0435
3072/6530 [=============>................] - ETA: 0s - loss: 0.1826
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0456
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1829
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0438
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1834
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0420
3824/6530 [================>.............] - ETA: 0s - loss: 0.1832
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0421
4048/6530 [=================>............] - ETA: 0s - loss: 0.1826
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0424
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1821
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0429
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1829
2960/6530 [============>.................] - ETA: 0s - loss: 0.0420
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1823
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0418
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1814
3712/6530 [================>.............] - ETA: 0s - loss: 0.0423
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1809
4080/6530 [=================>............] - ETA: 0s - loss: 0.0424
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1805
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0421
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1806
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0419
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1811
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0413
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1809
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0411
6512/6530 [============================>.] - ETA: 0s - loss: 0.1805
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0411
6530/6530 [==============================] - 1s 213us/step - loss: 0.1804 - val_loss: 0.1666

6288/6530 [===========================>..] - ETA: 0s - loss: 0.0411
6530/6530 [==============================] - 1s 143us/step - loss: 0.0407 - val_loss: 0.0318
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0396
 368/6530 [>.............................] - ETA: 0s - loss: 0.0354
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0377
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0373
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0359
# training | RMSE: 0.2105, MAE: 0.1674
worker 1  xfile  [10, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4544402724480966}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.48327937943956323}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21638682894172123}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.21053801532709296, 'rmse': 0.21053801532709296, 'mae': 0.16743819221134026, 'early_stop': False}
vggnet done  1

1824/6530 [=======>......................] - ETA: 0s - loss: 0.0361
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0360
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0364
2992/6530 [============>.................] - ETA: 0s - loss: 0.0354
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0354
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0360
3872/6530 [================>.............] - ETA: 0s - loss: 0.0361
4128/6530 [=================>............] - ETA: 0s - loss: 0.0359
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0354
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0358
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0357
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0353
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0351
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0350
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0350
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0349
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0348
6432/6530 [============================>.] - ETA: 0s - loss: 0.0346
6530/6530 [==============================] - 1s 184us/step - loss: 0.0345 - val_loss: 0.0241
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0289
 288/6530 [>.............................] - ETA: 1s - loss: 0.0321
 592/6530 [=>............................] - ETA: 1s - loss: 0.0312
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0324
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0321
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0314
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0312
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0312
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0315
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0316
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0317
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0311
2992/6530 [============>.................] - ETA: 0s - loss: 0.0310
3200/6530 [=============>................] - ETA: 0s - loss: 0.0310
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0318
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0317
3936/6530 [=================>............] - ETA: 0s - loss: 0.0318
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0315
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0314
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0315
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0312
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0311
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0310
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0310
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0309
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0307
6368/6530 [============================>.] - ETA: 0s - loss: 0.0306
6530/6530 [==============================] - 1s 221us/step - loss: 0.0306 - val_loss: 0.0218
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0248
 240/6530 [>.............................] - ETA: 1s - loss: 0.0282
 496/6530 [=>............................] - ETA: 1s - loss: 0.0289
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0298
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0301
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0295
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0286
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0289
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0285
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0287
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0291
2848/6530 [============>.................] - ETA: 0s - loss: 0.0287
3104/6530 [=============>................] - ETA: 0s - loss: 0.0286
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0292
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0294
3936/6530 [=================>............] - ETA: 0s - loss: 0.0294
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0291
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0290
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0292
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0291
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0289
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0288
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0288
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0287
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0285
6336/6530 [============================>.] - ETA: 0s - loss: 0.0285
6530/6530 [==============================] - 1s 215us/step - loss: 0.0284 - val_loss: 0.0210
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0230
 256/6530 [>.............................] - ETA: 1s - loss: 0.0268
 528/6530 [=>............................] - ETA: 1s - loss: 0.0267
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0281
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0288
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0274
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0271
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0270
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0273
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0275
2848/6530 [============>.................] - ETA: 0s - loss: 0.0272
3152/6530 [=============>................] - ETA: 0s - loss: 0.0272
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0279
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0279
3856/6530 [================>.............] - ETA: 0s - loss: 0.0279
4048/6530 [=================>............] - ETA: 0s - loss: 0.0278
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0276
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0276
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0277
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0278
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0276
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0274
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0274
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0273
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0273
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0271
6320/6530 [============================>.] - ETA: 0s - loss: 0.0271
6528/6530 [============================>.] - ETA: 0s - loss: 0.0270
6530/6530 [==============================] - 1s 222us/step - loss: 0.0270 - val_loss: 0.0204
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0210
 272/6530 [>.............................] - ETA: 1s - loss: 0.0254
 560/6530 [=>............................] - ETA: 1s - loss: 0.0251
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0266
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0275
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0263
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0260
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0258
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0263
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0263
2880/6530 [============>.................] - ETA: 0s - loss: 0.0260
3056/6530 [=============>................] - ETA: 0s - loss: 0.0261
3264/6530 [=============>................] - ETA: 0s - loss: 0.0264
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0269
3744/6530 [================>.............] - ETA: 0s - loss: 0.0267
4016/6530 [=================>............] - ETA: 0s - loss: 0.0266
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0265
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0266
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0266
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0266
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0265
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0264
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0263
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0262
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0263
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0262
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0261
6384/6530 [============================>.] - ETA: 0s - loss: 0.0260
6530/6530 [==============================] - 1s 228us/step - loss: 0.0260 - val_loss: 0.0197
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0188
 240/6530 [>.............................] - ETA: 1s - loss: 0.0239
 464/6530 [=>............................] - ETA: 1s - loss: 0.0251
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0252
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0254
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0266
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0262
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0256
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0254
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0251
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0251
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0250
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0253
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0254
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0255
2832/6530 [============>.................] - ETA: 1s - loss: 0.0254
3008/6530 [============>.................] - ETA: 0s - loss: 0.0253
3216/6530 [=============>................] - ETA: 0s - loss: 0.0254
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0260
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0259
3968/6530 [=================>............] - ETA: 0s - loss: 0.0259
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0257
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0258
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0258
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0255
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0254
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0254
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0253
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0253
6512/6530 [============================>.] - ETA: 0s - loss: 0.0252
6530/6530 [==============================] - 2s 240us/step - loss: 0.0252 - val_loss: 0.0191

# training | RMSE: 0.1315, MAE: 0.1011
worker 2  xfile  [14, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.25901559042494454}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1235998511602289}, 'layer_3_size': 52, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.1315319377330874, 'rmse': 0.1315319377330874, 'mae': 0.1011411892710986, 'early_stop': False}
vggnet done  2
all of workers have been done
calculation finished
#1 epoch=9.0 loss={'loss': 0.11704046119317943, 'rmse': 0.11704046119317943, 'mae': 0.08451865616586347, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4539264204452951}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37851315931663887}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#3 epoch=9.0 loss={'loss': 0.22660271680293043, 'rmse': 0.22660271680293043, 'mae': 0.18730488037305953, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4949767012811116}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41779063623655877}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#0 epoch=9.0 loss={'loss': 0.12167330156076778, 'rmse': 0.12167330156076778, 'mae': 0.09297761909995811, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27679844046668345}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39624344937280587}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.34191658154173177}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#4 epoch=9.0 loss={'loss': 0.129070569005715, 'rmse': 0.129070569005715, 'mae': 0.09923047408263394, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19073769699532994}, 'layer_1_size': 95, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1543574533743368}, 'layer_3_size': 11, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3721211091810761}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#6 epoch=9.0 loss={'loss': 0.1608428725312425, 'rmse': 0.1608428725312425, 'mae': 0.1263195657466459, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10939569932384074}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.326615848226966}, 'layer_3_size': 16, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42231207461314724}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#2 epoch=9.0 loss={'loss': 0.09849437653704658, 'rmse': 0.09849437653704658, 'mae': 0.07790336920751025, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 82, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 34, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11389578672392409}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#5 epoch=9.0 loss={'loss': 0.17762570168744332, 'rmse': 0.17762570168744332, 'mae': 0.15501948646663954, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1817087321690729}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#7 epoch=9.0 loss={'loss': 0.2224722216850548, 'rmse': 0.2224722216850548, 'mae': 0.18481315588467176, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14900012006333854}, 'layer_2_size': 48, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1937868315068102}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#8 epoch=9.0 loss={'loss': 0.17051182483205785, 'rmse': 0.17051182483205785, 'mae': 0.13357409672152773, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3474702838513902}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2881528219221712}, 'layer_5_size': 12, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#11 epoch=9.0 loss={'loss': 0.14665756223790988, 'rmse': 0.14665756223790988, 'mae': 0.11523611241904501, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 70, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.37063437706228464}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4656791441692686}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#9 epoch=9.0 loss={'loss': 0.11603795588355889, 'rmse': 0.11603795588355889, 'mae': 0.09069249498145135, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3636661996407462}, 'layer_3_size': 94, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10641354205355764}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#12 epoch=9.0 loss={'loss': 0.3068127301066356, 'rmse': 0.3068127301066356, 'mae': 0.26143476916839814, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 6, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.48200256294458677}, 'layer_2_size': 71, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18703635612869834}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#13 epoch=9.0 loss={'loss': 0.20103929199942705, 'rmse': 0.20103929199942705, 'mae': 0.15837236246953074, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20150315133458624}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3651487968819357}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4985968736993499}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#10 epoch=9.0 loss={'loss': 0.21053801532709296, 'rmse': 0.21053801532709296, 'mae': 0.16743819221134026, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4544402724480966}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.48327937943956323}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21638682894172123}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#14 epoch=9.0 loss={'loss': 0.1315319377330874, 'rmse': 0.1315319377330874, 'mae': 0.1011411892710986, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.25901559042494454}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1235998511602289}, 'layer_3_size': 52, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
get a list [results] of length 185
get a list [loss] of length 15
get a list [val_loss] of length 15
length of indices is [ 5 10  0  2  3 14  9  4  8  6 12 13  7  1 11]
length of indices is 15
length of T is 15
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1817087321690729}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [1, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4544402724480966}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.48327937943956323}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21638682894172123}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [2, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27679844046668345}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39624344937280587}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.34191658154173177}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [3, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 82, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 34, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11389578672392409}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [4, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4949767012811116}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41779063623655877}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]] 

*** 5.0 configurations x 27.0 iterations each

15 | Thu Sep 27 18:41:50 2018 | lowest loss so far: 0.0913 (run 0)

{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  93 | activation: tanh    | extras: dropout - rate: 27.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 5:25 - loss: 1.3293
 464/6530 [=>............................] - ETA: 11s - loss: 0.8054 
 912/6530 [===>..........................] - ETA: 5s - loss: 0.5437 
1472/6530 [=====>........................] - ETA: 3s - loss: 0.3786
2032/6530 [========>.....................] - ETA: 2s - loss: 0.2947
2544/6530 [==========>...................] - ETA: 1s - loss: 0.2476
3152/6530 [=============>................] - ETA: 1s - loss: 0.2103
3824/6530 [================>.............] - ETA: 0s - loss: 0.1839
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1653
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1520
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1414
6368/6530 [============================>.] - ETA: 0s - loss: 0.1325
6530/6530 [==============================] - 1s 215us/step - loss: 0.1303 - val_loss: 0.0519
Epoch 2/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0538{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  86 | activation: relu    | extras: dropout - rate: 18.2% 
layer 2 | size:  11 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  58 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 3:13 - loss: 0.2415
 608/6530 [=>............................] - ETA: 0s - loss: 0.0483
 544/6530 [=>............................] - ETA: 11s - loss: 0.1984 
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0484
1152/6530 [====>.........................] - ETA: 4s - loss: 0.1821 
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0474
1760/6530 [=======>......................] - ETA: 3s - loss: 0.1764
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0474
2368/6530 [=========>....................] - ETA: 2s - loss: 0.1719{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  78 | activation: sigmoid | extras: None 
layer 2 | size:  21 | activation: sigmoid | extras: dropout - rate: 45.4% 
layer 3 | size:  15 | activation: relu    | extras: dropout - rate: 48.3% 
layer 4 | size:  44 | activation: tanh    | extras: batchnorm 
layer 5 | size:   8 | activation: tanh    | extras: dropout - rate: 21.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 6:59 - loss: 1.0983
3136/6530 [=============>................] - ETA: 0s - loss: 0.0462
2976/6530 [============>.................] - ETA: 1s - loss: 0.1666
3776/6530 [================>.............] - ETA: 0s - loss: 0.0462
 240/6530 [>.............................] - ETA: 28s - loss: 0.9703 
3488/6530 [===============>..............] - ETA: 1s - loss: 0.1647
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0452
 480/6530 [=>............................] - ETA: 14s - loss: 0.8401
4064/6530 [=================>............] - ETA: 0s - loss: 0.1631
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0454
 720/6530 [==>...........................] - ETA: 9s - loss: 0.7411 
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1610
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0451
 960/6530 [===>..........................] - ETA: 7s - loss: 0.6759
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1591
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0447
1200/6530 [====>.........................] - ETA: 5s - loss: 0.6197
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1579
6530/6530 [==============================] - 1s 85us/step - loss: 0.0442 - val_loss: 0.0383
Epoch 3/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0417
1440/6530 [=====>........................] - ETA: 4s - loss: 0.5774
6368/6530 [============================>.] - ETA: 0s - loss: 0.1563
 640/6530 [=>............................] - ETA: 0s - loss: 0.0349
1680/6530 [======>.......................] - ETA: 4s - loss: 0.5455
6530/6530 [==============================] - 2s 244us/step - loss: 0.1557 - val_loss: 0.1849
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1684
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0361
1920/6530 [=======>......................] - ETA: 3s - loss: 0.5215
 608/6530 [=>............................] - ETA: 0s - loss: 0.1366
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0357
2160/6530 [========>.....................] - ETA: 3s - loss: 0.5042
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1356
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0358
2400/6530 [==========>...................] - ETA: 2s - loss: 0.4850
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1351
3040/6530 [============>.................] - ETA: 0s - loss: 0.0349
2624/6530 [===========>..................] - ETA: 2s - loss: 0.4707
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1367
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0348
2880/6530 [============>.................] - ETA: 2s - loss: 0.4583
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1364
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0340
3120/6530 [=============>................] - ETA: 1s - loss: 0.4486
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1359
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0340
3360/6530 [==============>...............] - ETA: 1s - loss: 0.4397
3968/6530 [=================>............] - ETA: 0s - loss: 0.1352
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0339
3600/6530 [===============>..............] - ETA: 1s - loss: 0.4296
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1355
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0337
3840/6530 [================>.............] - ETA: 1s - loss: 0.4226
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1355
6530/6530 [==============================] - 1s 87us/step - loss: 0.0332 - val_loss: 0.0288
Epoch 4/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0293
4080/6530 [=================>............] - ETA: 1s - loss: 0.4164
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1360
 640/6530 [=>............................] - ETA: 0s - loss: 0.0259
4320/6530 [==================>...........] - ETA: 1s - loss: 0.4098
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1355
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0274
4560/6530 [===================>..........] - ETA: 0s - loss: 0.4022
6530/6530 [==============================] - 1s 93us/step - loss: 0.1355 - val_loss: 0.1861
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1553
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0275
4800/6530 [=====================>........] - ETA: 0s - loss: 0.3955
 640/6530 [=>............................] - ETA: 0s - loss: 0.1313
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0274
5056/6530 [======================>.......] - ETA: 0s - loss: 0.3893
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1302
3120/6530 [=============>................] - ETA: 0s - loss: 0.0268
5296/6530 [=======================>......] - ETA: 0s - loss: 0.3846
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1332
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0268
5520/6530 [========================>.....] - ETA: 0s - loss: 0.3805
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1319
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0261
5744/6530 [=========================>....] - ETA: 0s - loss: 0.3753
2944/6530 [============>.................] - ETA: 0s - loss: 0.1312
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0264
5984/6530 [==========================>...] - ETA: 0s - loss: 0.3706
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1300
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0263
6224/6530 [===========================>..] - ETA: 0s - loss: 0.3667
4128/6530 [=================>............] - ETA: 0s - loss: 0.1294
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0263
6448/6530 [============================>.] - ETA: 0s - loss: 0.3626
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1285
6530/6530 [==============================] - 1s 86us/step - loss: 0.0260 - val_loss: 0.0240
Epoch 5/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0212
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1275
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0212
6530/6530 [==============================] - 3s 387us/step - loss: 0.3608 - val_loss: 0.2151
Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.3215
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1273
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0226
 256/6530 [>.............................] - ETA: 1s - loss: 0.2663
6464/6530 [============================>.] - ETA: 0s - loss: 0.1268
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0227
6530/6530 [==============================] - 1s 91us/step - loss: 0.1263 - val_loss: 0.1279
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1268
 496/6530 [=>............................] - ETA: 1s - loss: 0.2521
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0231
 608/6530 [=>............................] - ETA: 0s - loss: 0.1146
 736/6530 [==>...........................] - ETA: 1s - loss: 0.2595
3120/6530 [=============>................] - ETA: 0s - loss: 0.0226
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1179
 992/6530 [===>..........................] - ETA: 1s - loss: 0.2649
3744/6530 [================>.............] - ETA: 0s - loss: 0.0225
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1170
1232/6530 [====>.........................] - ETA: 1s - loss: 0.2657
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0221
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1170
1472/6530 [=====>........................] - ETA: 1s - loss: 0.2636
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0224
2912/6530 [============>.................] - ETA: 0s - loss: 0.1175
1712/6530 [======>.......................] - ETA: 1s - loss: 0.2599
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0225
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1149
1952/6530 [=======>......................] - ETA: 0s - loss: 0.2605
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0224
4096/6530 [=================>............] - ETA: 0s - loss: 0.1147
2176/6530 [========>.....................] - ETA: 0s - loss: 0.2589
6530/6530 [==============================] - 1s 84us/step - loss: 0.0222 - val_loss: 0.0216
Epoch 6/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0165
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1150
2384/6530 [=========>....................] - ETA: 0s - loss: 0.2581
 624/6530 [=>............................] - ETA: 0s - loss: 0.0190
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1152
2624/6530 [===========>..................] - ETA: 0s - loss: 0.2565
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0200
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1149
2864/6530 [============>.................] - ETA: 0s - loss: 0.2554
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0203
6400/6530 [============================>.] - ETA: 0s - loss: 0.1153
3120/6530 [=============>................] - ETA: 0s - loss: 0.2550
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0207
6530/6530 [==============================] - 1s 92us/step - loss: 0.1153 - val_loss: 0.1239
Epoch 5/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1033
3360/6530 [==============>...............] - ETA: 0s - loss: 0.2550
3040/6530 [============>.................] - ETA: 0s - loss: 0.0203
 640/6530 [=>............................] - ETA: 0s - loss: 0.1164
3600/6530 [===============>..............] - ETA: 0s - loss: 0.2543
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0203
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1131
3840/6530 [================>.............] - ETA: 0s - loss: 0.2540
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0200
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1101
4080/6530 [=================>............] - ETA: 0s - loss: 0.2533
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0201
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1101
4336/6530 [==================>...........] - ETA: 0s - loss: 0.2523
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0202
2976/6530 [============>.................] - ETA: 0s - loss: 0.1088
4592/6530 [====================>.........] - ETA: 0s - loss: 0.2525
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0202
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1100
4848/6530 [=====================>........] - ETA: 0s - loss: 0.2516
6530/6530 [==============================] - 1s 86us/step - loss: 0.0200 - val_loss: 0.0201
Epoch 7/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0133
4128/6530 [=================>............] - ETA: 0s - loss: 0.1099
5088/6530 [======================>.......] - ETA: 0s - loss: 0.2512
 640/6530 [=>............................] - ETA: 0s - loss: 0.0174
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1100
5312/6530 [=======================>......] - ETA: 0s - loss: 0.2511
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0184
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1089
5552/6530 [========================>.....] - ETA: 0s - loss: 0.2501
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0185
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1085
5792/6530 [=========================>....] - ETA: 0s - loss: 0.2489
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0189
6464/6530 [============================>.] - ETA: 0s - loss: 0.1087
6530/6530 [==============================] - 1s 90us/step - loss: 0.1085 - val_loss: 0.0989
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0972
6032/6530 [==========================>...] - ETA: 0s - loss: 0.2481
3104/6530 [=============>................] - ETA: 0s - loss: 0.0185
 608/6530 [=>............................] - ETA: 0s - loss: 0.1029
6288/6530 [===========================>..] - ETA: 0s - loss: 0.2482
3728/6530 [================>.............] - ETA: 0s - loss: 0.0185
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1066
6512/6530 [============================>.] - ETA: 0s - loss: 0.2473
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0182
6530/6530 [==============================] - 1s 220us/step - loss: 0.2471 - val_loss: 0.2156

1824/6530 [=======>......................] - ETA: 0s - loss: 0.1096Epoch 3/27

  16/6530 [..............................] - ETA: 3s - loss: 0.2829
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0185
 256/6530 [>.............................] - ETA: 1s - loss: 0.2370
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1092
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0185
 496/6530 [=>............................] - ETA: 1s - loss: 0.2410
3008/6530 [============>.................] - ETA: 0s - loss: 0.1082
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0186
 736/6530 [==>...........................] - ETA: 1s - loss: 0.2497
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1069
6530/6530 [==============================] - 1s 85us/step - loss: 0.0184 - val_loss: 0.0189
Epoch 8/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0110
 976/6530 [===>..........................] - ETA: 1s - loss: 0.2513
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1070
 640/6530 [=>............................] - ETA: 0s - loss: 0.0161
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1071
1232/6530 [====>.........................] - ETA: 1s - loss: 0.2459
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0171
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1063
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0172
1472/6530 [=====>........................] - ETA: 1s - loss: 0.2452
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1068
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0175
1712/6530 [======>.......................] - ETA: 1s - loss: 0.2436
6432/6530 [============================>.] - ETA: 0s - loss: 0.1070
3072/6530 [=============>................] - ETA: 0s - loss: 0.0172
1968/6530 [========>.....................] - ETA: 0s - loss: 0.2424
6530/6530 [==============================] - 1s 91us/step - loss: 0.1069 - val_loss: 0.0994
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1263
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0172
2208/6530 [=========>....................] - ETA: 0s - loss: 0.2415
 608/6530 [=>............................] - ETA: 0s - loss: 0.1048
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0169
2448/6530 [==========>...................] - ETA: 0s - loss: 0.2401
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1032
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0172
2656/6530 [===========>..................] - ETA: 0s - loss: 0.2400
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1029
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0171
2896/6530 [============>.................] - ETA: 0s - loss: 0.2382
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1025
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0172
3136/6530 [=============>................] - ETA: 0s - loss: 0.2371
2848/6530 [============>.................] - ETA: 0s - loss: 0.1024
6530/6530 [==============================] - 1s 87us/step - loss: 0.0170 - val_loss: 0.0179
Epoch 9/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0093
3376/6530 [==============>...............] - ETA: 0s - loss: 0.2370
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1037
 624/6530 [=>............................] - ETA: 0s - loss: 0.0151
3632/6530 [===============>..............] - ETA: 0s - loss: 0.2372
4032/6530 [=================>............] - ETA: 0s - loss: 0.1033
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0160
3872/6530 [================>.............] - ETA: 0s - loss: 0.2371
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1026
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0161
4112/6530 [=================>............] - ETA: 0s - loss: 0.2367
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1021
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0165
4368/6530 [===================>..........] - ETA: 0s - loss: 0.2361
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1024
3120/6530 [=============>................] - ETA: 0s - loss: 0.0160
4624/6530 [====================>.........] - ETA: 0s - loss: 0.2360
6336/6530 [============================>.] - ETA: 0s - loss: 0.1027
3712/6530 [================>.............] - ETA: 0s - loss: 0.0160
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2356
6530/6530 [==============================] - 1s 93us/step - loss: 0.1026 - val_loss: 0.1433
Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1132
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0158
5104/6530 [======================>.......] - ETA: 0s - loss: 0.2348
 608/6530 [=>............................] - ETA: 0s - loss: 0.1009
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0160
5344/6530 [=======================>......] - ETA: 0s - loss: 0.2350
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1008
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0160
5584/6530 [========================>.....] - ETA: 0s - loss: 0.2344
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0978
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0161
5824/6530 [=========================>....] - ETA: 0s - loss: 0.2349
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0998
6530/6530 [==============================] - 1s 85us/step - loss: 0.0159 - val_loss: 0.0170
Epoch 10/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0080
6032/6530 [==========================>...] - ETA: 0s - loss: 0.2344
2912/6530 [============>.................] - ETA: 0s - loss: 0.0997
 640/6530 [=>............................] - ETA: 0s - loss: 0.0142
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2348
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0998
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0151
6512/6530 [============================>.] - ETA: 0s - loss: 0.2342
4064/6530 [=================>............] - ETA: 0s - loss: 0.1000
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0152
6530/6530 [==============================] - 1s 221us/step - loss: 0.2342 - val_loss: 0.2316
Epoch 4/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1976
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1003
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0155
 272/6530 [>.............................] - ETA: 1s - loss: 0.2147
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1001
3104/6530 [=============>................] - ETA: 0s - loss: 0.0151
 512/6530 [=>............................] - ETA: 1s - loss: 0.2211
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0997
3728/6530 [================>.............] - ETA: 0s - loss: 0.0150
 752/6530 [==>...........................] - ETA: 1s - loss: 0.2284
6368/6530 [============================>.] - ETA: 0s - loss: 0.1001
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0148
6530/6530 [==============================] - 1s 92us/step - loss: 0.0999 - val_loss: 0.1037

 992/6530 [===>..........................] - ETA: 1s - loss: 0.2298Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1120
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0150
1232/6530 [====>.........................] - ETA: 1s - loss: 0.2301
 576/6530 [=>............................] - ETA: 0s - loss: 0.1026
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0151
1472/6530 [=====>........................] - ETA: 1s - loss: 0.2303
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0972
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0151
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0976
1696/6530 [======>.......................] - ETA: 1s - loss: 0.2306
6530/6530 [==============================] - 1s 85us/step - loss: 0.0150 - val_loss: 0.0162
Epoch 11/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0069
1936/6530 [=======>......................] - ETA: 0s - loss: 0.2297
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1016
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0135
2848/6530 [============>.................] - ETA: 0s - loss: 0.1004
2176/6530 [========>.....................] - ETA: 0s - loss: 0.2292
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0142
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0995
2400/6530 [==========>...................] - ETA: 0s - loss: 0.2293
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0144
2640/6530 [===========>..................] - ETA: 0s - loss: 0.2300
4032/6530 [=================>............] - ETA: 0s - loss: 0.0990
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0146
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0976
2864/6530 [============>.................] - ETA: 0s - loss: 0.2293
3024/6530 [============>.................] - ETA: 0s - loss: 0.0143
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0975
3104/6530 [=============>................] - ETA: 0s - loss: 0.2277
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0142
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0980
3344/6530 [==============>...............] - ETA: 0s - loss: 0.2279
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0141
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0971
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2278
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0142
6530/6530 [==============================] - 1s 93us/step - loss: 0.0972 - val_loss: 0.0921
Epoch 10/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1150
3808/6530 [================>.............] - ETA: 0s - loss: 0.2276
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0143
 608/6530 [=>............................] - ETA: 0s - loss: 0.0972
4048/6530 [=================>............] - ETA: 0s - loss: 0.2273
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0142
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0998
4272/6530 [==================>...........] - ETA: 0s - loss: 0.2257
6530/6530 [==============================] - 1s 88us/step - loss: 0.0142 - val_loss: 0.0155
Epoch 12/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0060
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0985
4512/6530 [===================>..........] - ETA: 0s - loss: 0.2261
 640/6530 [=>............................] - ETA: 0s - loss: 0.0128
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0990
4752/6530 [====================>.........] - ETA: 0s - loss: 0.2257
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0135
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0969
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2256
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0136
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0958
5232/6530 [=======================>......] - ETA: 0s - loss: 0.2255
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0139
4000/6530 [=================>............] - ETA: 0s - loss: 0.0971
5472/6530 [========================>.....] - ETA: 0s - loss: 0.2259
3168/6530 [=============>................] - ETA: 0s - loss: 0.0134
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0978
5712/6530 [=========================>....] - ETA: 0s - loss: 0.2258
3808/6530 [================>.............] - ETA: 0s - loss: 0.0134
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0967
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2256
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0133
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0974
6192/6530 [===========================>..] - ETA: 0s - loss: 0.2259
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0135
6336/6530 [============================>.] - ETA: 0s - loss: 0.0977
6432/6530 [============================>.] - ETA: 0s - loss: 0.2260
6530/6530 [==============================] - 1s 93us/step - loss: 0.0981 - val_loss: 0.0954
Epoch 11/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0969
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0135
6530/6530 [==============================] - 1s 222us/step - loss: 0.2256 - val_loss: 0.2513
Epoch 5/27

  16/6530 [..............................] - ETA: 1s - loss: 0.2648
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0135
 608/6530 [=>............................] - ETA: 0s - loss: 0.1044
 256/6530 [>.............................] - ETA: 1s - loss: 0.2291
6530/6530 [==============================] - 1s 84us/step - loss: 0.0134 - val_loss: 0.0148
Epoch 13/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0052
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0971
 496/6530 [=>............................] - ETA: 1s - loss: 0.2279
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0121
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0980
 736/6530 [==>...........................] - ETA: 1s - loss: 0.2344
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0129
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0978
 992/6530 [===>..........................] - ETA: 1s - loss: 0.2343
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0130
2912/6530 [============>.................] - ETA: 0s - loss: 0.0973
1232/6530 [====>.........................] - ETA: 1s - loss: 0.2299
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0133
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0975
1472/6530 [=====>........................] - ETA: 1s - loss: 0.2315
3184/6530 [=============>................] - ETA: 0s - loss: 0.0128
4064/6530 [=================>............] - ETA: 0s - loss: 0.0959
1712/6530 [======>.......................] - ETA: 1s - loss: 0.2310
3808/6530 [================>.............] - ETA: 0s - loss: 0.0128
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0962
1952/6530 [=======>......................] - ETA: 0s - loss: 0.2312
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0126
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0958
2192/6530 [=========>....................] - ETA: 0s - loss: 0.2290
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0128
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0960
2432/6530 [==========>...................] - ETA: 0s - loss: 0.2284
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0128
6336/6530 [============================>.] - ETA: 0s - loss: 0.0963
2672/6530 [===========>..................] - ETA: 0s - loss: 0.2281
6530/6530 [==============================] - 1s 92us/step - loss: 0.0966 - val_loss: 0.1056
Epoch 12/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1046
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0129
2912/6530 [============>.................] - ETA: 0s - loss: 0.2259
6530/6530 [==============================] - 1s 86us/step - loss: 0.0128 - val_loss: 0.0142
Epoch 14/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0047
 608/6530 [=>............................] - ETA: 0s - loss: 0.0937
3152/6530 [=============>................] - ETA: 0s - loss: 0.2253
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0116
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0966
3392/6530 [==============>...............] - ETA: 0s - loss: 0.2252
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0124
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0946
3632/6530 [===============>..............] - ETA: 0s - loss: 0.2249
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0124
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0950
3872/6530 [================>.............] - ETA: 0s - loss: 0.2244
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0126
2912/6530 [============>.................] - ETA: 0s - loss: 0.0961
4112/6530 [=================>............] - ETA: 0s - loss: 0.2240
3168/6530 [=============>................] - ETA: 0s - loss: 0.0122
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0959
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2228
3808/6530 [================>.............] - ETA: 0s - loss: 0.0122
4064/6530 [=================>............] - ETA: 0s - loss: 0.0956
4592/6530 [====================>.........] - ETA: 0s - loss: 0.2234
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0121
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0946
4832/6530 [=====================>........] - ETA: 0s - loss: 0.2229
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0122
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0948
5072/6530 [======================>.......] - ETA: 0s - loss: 0.2221
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0123
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0946
5328/6530 [=======================>......] - ETA: 0s - loss: 0.2220
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0123
6368/6530 [============================>.] - ETA: 0s - loss: 0.0943
6530/6530 [==============================] - 1s 92us/step - loss: 0.0941 - val_loss: 0.1170

6530/6530 [==============================] - 1s 85us/step - loss: 0.0122 - val_loss: 0.0137
Epoch 13/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0959Epoch 15/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0042
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2215
 560/6530 [=>............................] - ETA: 0s - loss: 0.0108
 608/6530 [=>............................] - ETA: 0s - loss: 0.0969
5792/6530 [=========================>....] - ETA: 0s - loss: 0.2217
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0118
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0962
6032/6530 [==========================>...] - ETA: 0s - loss: 0.2212
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0119
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0967
6256/6530 [===========================>..] - ETA: 0s - loss: 0.2215
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0121
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0966
6496/6530 [============================>.] - ETA: 0s - loss: 0.2209
6530/6530 [==============================] - 1s 220us/step - loss: 0.2208 - val_loss: 0.2046

3040/6530 [============>.................] - ETA: 0s - loss: 0.0118Epoch 6/27

  16/6530 [..............................] - ETA: 1s - loss: 0.2557
2912/6530 [============>.................] - ETA: 0s - loss: 0.0945
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0117
 256/6530 [>.............................] - ETA: 1s - loss: 0.2149
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0941
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0116
 496/6530 [=>............................] - ETA: 1s - loss: 0.2194
4096/6530 [=================>............] - ETA: 0s - loss: 0.0935
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0116
 736/6530 [==>...........................] - ETA: 1s - loss: 0.2231
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0926
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0117
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0926
 992/6530 [===>..........................] - ETA: 1s - loss: 0.2241
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0118
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0928
1248/6530 [====>.........................] - ETA: 1s - loss: 0.2232
1472/6530 [=====>........................] - ETA: 1s - loss: 0.2236
6368/6530 [============================>.] - ETA: 0s - loss: 0.0925
6530/6530 [==============================] - 1s 87us/step - loss: 0.0117 - val_loss: 0.0132
Epoch 16/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0039
6530/6530 [==============================] - 1s 92us/step - loss: 0.0923 - val_loss: 0.1008
Epoch 14/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0846
1712/6530 [======>.......................] - ETA: 1s - loss: 0.2225
 640/6530 [=>............................] - ETA: 0s - loss: 0.0106
 576/6530 [=>............................] - ETA: 0s - loss: 0.0943
1936/6530 [=======>......................] - ETA: 0s - loss: 0.2227
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0115
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0934
2176/6530 [========>.....................] - ETA: 0s - loss: 0.2219
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0115
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0915
2416/6530 [==========>...................] - ETA: 0s - loss: 0.2207
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0116
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0913
2656/6530 [===========>..................] - ETA: 0s - loss: 0.2207
3088/6530 [=============>................] - ETA: 0s - loss: 0.0113
2880/6530 [============>.................] - ETA: 0s - loss: 0.0918
2864/6530 [============>.................] - ETA: 0s - loss: 0.2196
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0112
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0899
3104/6530 [=============>................] - ETA: 0s - loss: 0.2178
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0112
4064/6530 [=================>............] - ETA: 0s - loss: 0.0891
3360/6530 [==============>...............] - ETA: 0s - loss: 0.2173
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0112
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0895
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0113
3616/6530 [===============>..............] - ETA: 0s - loss: 0.2163
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0897
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0113
3856/6530 [================>.............] - ETA: 0s - loss: 0.2156
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0892
6530/6530 [==============================] - 1s 86us/step - loss: 0.0112 - val_loss: 0.0127
Epoch 17/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0036
4096/6530 [=================>............] - ETA: 0s - loss: 0.2153
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0889
 640/6530 [=>............................] - ETA: 0s - loss: 0.0102
4320/6530 [==================>...........] - ETA: 0s - loss: 0.2142
6530/6530 [==============================] - 1s 94us/step - loss: 0.0890 - val_loss: 0.1417

1264/6530 [====>.........................] - ETA: 0s - loss: 0.0110
4560/6530 [===================>..........] - ETA: 0s - loss: 0.2146
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0110
4800/6530 [=====================>........] - ETA: 0s - loss: 0.2142
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0113
5040/6530 [======================>.......] - ETA: 0s - loss: 0.2133
3104/6530 [=============>................] - ETA: 0s - loss: 0.0109
5280/6530 [=======================>......] - ETA: 0s - loss: 0.2131
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0108
5520/6530 [========================>.....] - ETA: 0s - loss: 0.2122
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0108
5776/6530 [=========================>....] - ETA: 0s - loss: 0.2127
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0108
6032/6530 [==========================>...] - ETA: 0s - loss: 0.2123
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0109
6288/6530 [===========================>..] - ETA: 0s - loss: 0.2123
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0109
6512/6530 [============================>.] - ETA: 0s - loss: 0.2116
6530/6530 [==============================] - 1s 85us/step - loss: 0.0108 - val_loss: 0.0124
Epoch 18/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0035
6530/6530 [==============================] - 1s 220us/step - loss: 0.2114 - val_loss: 0.1793
Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.3073
 624/6530 [=>............................] - ETA: 0s - loss: 0.0099
 256/6530 [>.............................] - ETA: 1s - loss: 0.2047
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0106
 496/6530 [=>............................] - ETA: 1s - loss: 0.2099
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0108
 736/6530 [==>...........................] - ETA: 1s - loss: 0.2149
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0109
 992/6530 [===>..........................] - ETA: 1s - loss: 0.2145
3056/6530 [=============>................] - ETA: 0s - loss: 0.0106
1232/6530 [====>.........................] - ETA: 1s - loss: 0.2104
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0105
1472/6530 [=====>........................] - ETA: 1s - loss: 0.2117
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0104
1712/6530 [======>.......................] - ETA: 1s - loss: 0.2096
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0105
1952/6530 [=======>......................] - ETA: 0s - loss: 0.2100
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0105
2192/6530 [=========>....................] - ETA: 0s - loss: 0.2088
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0106
2432/6530 [==========>...................] - ETA: 0s - loss: 0.2090
6530/6530 [==============================] - 1s 84us/step - loss: 0.0105 - val_loss: 0.0120
Epoch 19/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0034
2672/6530 [===========>..................] - ETA: 0s - loss: 0.2088
 576/6530 [=>............................] - ETA: 0s - loss: 0.0096
# training | RMSE: 0.1585, MAE: 0.1388
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1817087321690729}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.15853453262602013, 'rmse': 0.15853453262602013, 'mae': 0.13881986000899282, 'early_stop': True}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  46 | activation: tanh    | extras: None 
layer 2 | size:  82 | activation: tanh    | extras: batchnorm 
layer 3 | size:  34 | activation: relu    | extras: None 
layer 4 | size:  21 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e91329978>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 2:48 - loss: 0.9988
2928/6530 [============>.................] - ETA: 0s - loss: 0.2077
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0104
 320/6530 [>.............................] - ETA: 9s - loss: 0.5231  
3168/6530 [=============>................] - ETA: 0s - loss: 0.2065
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0105
 592/6530 [=>............................] - ETA: 5s - loss: 0.3599
3424/6530 [==============>...............] - ETA: 0s - loss: 0.2073
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0106
 880/6530 [===>..........................] - ETA: 3s - loss: 0.3009
3664/6530 [===============>..............] - ETA: 0s - loss: 0.2065
3136/6530 [=============>................] - ETA: 0s - loss: 0.0102
1184/6530 [====>.........................] - ETA: 2s - loss: 0.2561
3904/6530 [================>.............] - ETA: 0s - loss: 0.2073
3744/6530 [================>.............] - ETA: 0s - loss: 0.0102
1472/6530 [=====>........................] - ETA: 2s - loss: 0.2255
4144/6530 [==================>...........] - ETA: 0s - loss: 0.2066
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0101
1744/6530 [=======>......................] - ETA: 1s - loss: 0.2029
4368/6530 [===================>..........] - ETA: 0s - loss: 0.2062
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0102
2016/6530 [========>.....................] - ETA: 1s - loss: 0.1851
4592/6530 [====================>.........] - ETA: 0s - loss: 0.2065
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0102
2320/6530 [=========>....................] - ETA: 1s - loss: 0.1709
4832/6530 [=====================>........] - ETA: 0s - loss: 0.2066
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0103
2624/6530 [===========>..................] - ETA: 1s - loss: 0.1592
5072/6530 [======================>.......] - ETA: 0s - loss: 0.2066
6530/6530 [==============================] - 1s 87us/step - loss: 0.0102 - val_loss: 0.0117
Epoch 20/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0033
2928/6530 [============>.................] - ETA: 1s - loss: 0.1496
5328/6530 [=======================>......] - ETA: 0s - loss: 0.2067
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0094
3232/6530 [=============>................] - ETA: 1s - loss: 0.1421
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2060
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0102
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1355
5808/6530 [=========================>....] - ETA: 0s - loss: 0.2058
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0101
3808/6530 [================>.............] - ETA: 0s - loss: 0.1297
6048/6530 [==========================>...] - ETA: 0s - loss: 0.2057
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0104
4096/6530 [=================>............] - ETA: 0s - loss: 0.1247
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2055
3136/6530 [=============>................] - ETA: 0s - loss: 0.0100
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1202
6480/6530 [============================>.] - ETA: 0s - loss: 0.2050
3728/6530 [================>.............] - ETA: 0s - loss: 0.0099
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1157
6530/6530 [==============================] - 1s 222us/step - loss: 0.2046 - val_loss: 0.2311
Epoch 8/27

  16/6530 [..............................] - ETA: 1s - loss: 0.2112
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0099
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1124
 256/6530 [>.............................] - ETA: 1s - loss: 0.1975
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0099
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1090
 496/6530 [=>............................] - ETA: 1s - loss: 0.2057
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0099
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1062
 736/6530 [==>...........................] - ETA: 1s - loss: 0.2060
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0100
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1033
 976/6530 [===>..........................] - ETA: 1s - loss: 0.2060
6530/6530 [==============================] - 1s 85us/step - loss: 0.0099 - val_loss: 0.0115
Epoch 21/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0032
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1007
1232/6530 [====>.........................] - ETA: 1s - loss: 0.2048
 624/6530 [=>............................] - ETA: 0s - loss: 0.0092
6448/6530 [============================>.] - ETA: 0s - loss: 0.0983
1472/6530 [=====>........................] - ETA: 1s - loss: 0.2048
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0099
1728/6530 [======>.......................] - ETA: 1s - loss: 0.2028
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 2s 250us/step - loss: 0.0977 - val_loss: 0.0412
Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0504
1968/6530 [========>.....................] - ETA: 0s - loss: 0.2046
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0101
 304/6530 [>.............................] - ETA: 1s - loss: 0.0614
2976/6530 [============>.................] - ETA: 0s - loss: 0.0098
2208/6530 [=========>....................] - ETA: 0s - loss: 0.2022
 608/6530 [=>............................] - ETA: 1s - loss: 0.0551
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0097
2432/6530 [==========>...................] - ETA: 0s - loss: 0.2015
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0515
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0097
2672/6530 [===========>..................] - ETA: 0s - loss: 0.2024
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0509
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0097
2928/6530 [============>.................] - ETA: 0s - loss: 0.2011
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0493
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0097
3168/6530 [=============>................] - ETA: 0s - loss: 0.1992
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0473
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0097
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1992
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0473
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1985
6530/6530 [==============================] - 1s 88us/step - loss: 0.0097 - val_loss: 0.0112
Epoch 22/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0032
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0475
3888/6530 [================>.............] - ETA: 0s - loss: 0.1991
 608/6530 [=>............................] - ETA: 0s - loss: 0.0089
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0467
4128/6530 [=================>............] - ETA: 0s - loss: 0.1983
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0097
2912/6530 [============>.................] - ETA: 0s - loss: 0.0462
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1976
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0098
3216/6530 [=============>................] - ETA: 0s - loss: 0.0454
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0099
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1972
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0448
3056/6530 [=============>................] - ETA: 0s - loss: 0.0096
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1964
3776/6530 [================>.............] - ETA: 0s - loss: 0.0444
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0095
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1962
4080/6530 [=================>............] - ETA: 0s - loss: 0.0445
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0095
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1959
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0443
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0094
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1957
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0439
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0095
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1957
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0439
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0095
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1955
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0437
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1955
6530/6530 [==============================] - 1s 88us/step - loss: 0.0095 - val_loss: 0.0110
Epoch 23/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0032
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0435
6496/6530 [============================>.] - ETA: 0s - loss: 0.1948
 592/6530 [=>............................] - ETA: 0s - loss: 0.0088
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0429
6530/6530 [==============================] - 1s 222us/step - loss: 0.1946 - val_loss: 0.1728
Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.2432
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0095
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0423
 256/6530 [>.............................] - ETA: 1s - loss: 0.1924
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0096
6384/6530 [============================>.] - ETA: 0s - loss: 0.0421
 512/6530 [=>............................] - ETA: 1s - loss: 0.2006
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0097
6530/6530 [==============================] - 1s 182us/step - loss: 0.0420 - val_loss: 0.0330
Epoch 3/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0187
 752/6530 [==>...........................] - ETA: 1s - loss: 0.2005
3072/6530 [=============>................] - ETA: 0s - loss: 0.0094
 320/6530 [>.............................] - ETA: 1s - loss: 0.0420
 992/6530 [===>..........................] - ETA: 1s - loss: 0.2012
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0093
 624/6530 [=>............................] - ETA: 1s - loss: 0.0375
1216/6530 [====>.........................] - ETA: 1s - loss: 0.1980
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0093
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0357
1360/6530 [=====>........................] - ETA: 1s - loss: 0.1963
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0092
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0351
1600/6530 [======>.......................] - ETA: 1s - loss: 0.1941
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0093
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0342
1840/6530 [=======>......................] - ETA: 1s - loss: 0.1940
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0093
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0344
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1916
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0345
6530/6530 [==============================] - 1s 88us/step - loss: 0.0093 - val_loss: 0.0108
Epoch 24/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0032
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1917
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0339
 640/6530 [=>............................] - ETA: 0s - loss: 0.0088
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1919
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0338
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0094
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1923
2864/6530 [============>.................] - ETA: 0s - loss: 0.0342
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0094
3008/6530 [============>.................] - ETA: 0s - loss: 0.1905
3168/6530 [=============>................] - ETA: 0s - loss: 0.0339
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0095
3264/6530 [=============>................] - ETA: 0s - loss: 0.1912
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0340
2992/6530 [============>.................] - ETA: 0s - loss: 0.0092
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1913
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0092
3760/6530 [================>.............] - ETA: 0s - loss: 0.0343
3744/6530 [================>.............] - ETA: 0s - loss: 0.1912
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0091
4064/6530 [=================>............] - ETA: 0s - loss: 0.0345
4000/6530 [=================>............] - ETA: 0s - loss: 0.1916
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0091
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0341
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1909
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0091
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0341
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1913
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0091
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0343
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1912
6530/6530 [==============================] - 1s 86us/step - loss: 0.0091 - val_loss: 0.0107

5216/6530 [======================>.......] - ETA: 0s - loss: 0.0340Epoch 25/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0032
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1912
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0335
 592/6530 [=>............................] - ETA: 0s - loss: 0.0086
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1910
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0094
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0334
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1908
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0093
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0334
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1906
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0093
6400/6530 [============================>.] - ETA: 0s - loss: 0.0333
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1909
3056/6530 [=============>................] - ETA: 0s - loss: 0.0090
6530/6530 [==============================] - 1s 183us/step - loss: 0.0335 - val_loss: 0.0269
Epoch 4/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0372
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1905
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0090
 304/6530 [>.............................] - ETA: 1s - loss: 0.0276
6384/6530 [============================>.] - ETA: 0s - loss: 0.1900
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0090
 592/6530 [=>............................] - ETA: 1s - loss: 0.0320
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0089
6530/6530 [==============================] - 1s 225us/step - loss: 0.1900 - val_loss: 0.1763
Epoch 10/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1694
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0324
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0089
 272/6530 [>.............................] - ETA: 1s - loss: 0.1889
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0328
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0089
 512/6530 [=>............................] - ETA: 1s - loss: 0.1969
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0330
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1998
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0329
6530/6530 [==============================] - 1s 89us/step - loss: 0.0089 - val_loss: 0.0105
Epoch 26/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0032
 992/6530 [===>..........................] - ETA: 1s - loss: 0.1977
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0327
 592/6530 [=>............................] - ETA: 0s - loss: 0.0085
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1955
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0091
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0321
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1953
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0091
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0312
1728/6530 [======>.......................] - ETA: 1s - loss: 0.1932
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0092
2928/6530 [============>.................] - ETA: 0s - loss: 0.0309
3024/6530 [============>.................] - ETA: 0s - loss: 0.0089
1968/6530 [========>.....................] - ETA: 0s - loss: 0.1937
3216/6530 [=============>................] - ETA: 0s - loss: 0.0305
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0088
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1924
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0304
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0088
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1918
3792/6530 [================>.............] - ETA: 0s - loss: 0.0300
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0087
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1915
4080/6530 [=================>............] - ETA: 0s - loss: 0.0298
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0087
2928/6530 [============>.................] - ETA: 0s - loss: 0.1903
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0297
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0088
3168/6530 [=============>................] - ETA: 0s - loss: 0.1897
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0293
6530/6530 [==============================] - 1s 86us/step - loss: 0.0088 - val_loss: 0.0103
Epoch 27/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0032
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0293
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1905
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0084
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0293
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1903
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0090
3904/6530 [================>.............] - ETA: 0s - loss: 0.1903
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0292
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0089
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0294
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1899
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0091
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0292
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1898
3088/6530 [=============>................] - ETA: 0s - loss: 0.0087
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1898
6448/6530 [============================>.] - ETA: 0s - loss: 0.0290
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0087
6530/6530 [==============================] - 1s 182us/step - loss: 0.0289 - val_loss: 0.0207

4864/6530 [=====================>........] - ETA: 0s - loss: 0.1891Epoch 5/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0396
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0086
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1889
 320/6530 [>.............................] - ETA: 1s - loss: 0.0260
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0086
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1897
 624/6530 [=>............................] - ETA: 1s - loss: 0.0268
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0086
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1889
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0289
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0086
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1890
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0276
6530/6530 [==============================] - 1s 85us/step - loss: 0.0086 - val_loss: 0.0102

1504/6530 [=====>........................] - ETA: 0s - loss: 0.0271
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1884
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1884
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0269
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0268
6530/6530 [==============================] - 1s 220us/step - loss: 0.1876 - val_loss: 0.1774
Epoch 11/27

  16/6530 [..............................] - ETA: 1s - loss: 0.2273
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0262
 272/6530 [>.............................] - ETA: 1s - loss: 0.1863
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0265
 512/6530 [=>............................] - ETA: 1s - loss: 0.1869
2960/6530 [============>.................] - ETA: 0s - loss: 0.0261
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1861
3264/6530 [=============>................] - ETA: 0s - loss: 0.0260
1008/6530 [===>..........................] - ETA: 1s - loss: 0.1936
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0261
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1904
3872/6530 [================>.............] - ETA: 0s - loss: 0.0257
1488/6530 [=====>........................] - ETA: 1s - loss: 0.1882
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0257
1712/6530 [======>.......................] - ETA: 1s - loss: 0.1844
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0254
1968/6530 [========>.....................] - ETA: 0s - loss: 0.1855
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0251
2224/6530 [=========>....................] - ETA: 0s - loss: 0.1843
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0251
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1854
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0251
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1857
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0253
2960/6530 [============>.................] - ETA: 0s - loss: 0.1852
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0255
3200/6530 [=============>................] - ETA: 0s - loss: 0.1840
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0255
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1845
6512/6530 [============================>.] - ETA: 0s - loss: 0.0257
6530/6530 [==============================] - 1s 180us/step - loss: 0.0257 - val_loss: 0.0289

3696/6530 [===============>..............] - ETA: 0s - loss: 0.1829Epoch 6/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0221
3952/6530 [=================>............] - ETA: 0s - loss: 0.1834
 320/6530 [>.............................] - ETA: 1s - loss: 0.0252
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1822
 624/6530 [=>............................] - ETA: 0s - loss: 0.0252
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0257
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1825
# training | RMSE: 0.0921, MAE: 0.0715
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27679844046668345}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39624344937280587}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.34191658154173177}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.09214133383985915, 'rmse': 0.09214133383985915, 'mae': 0.07145643890639126, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  11 | activation: relu    | extras: dropout - rate: 49.5% 
layer 2 | size:  47 | activation: tanh    | extras: batchnorm 
layer 3 | size:  35 | activation: tanh    | extras: dropout - rate: 41.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e9131a668>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 128/6530 [..............................] - ETA: 19s - loss: 0.7269
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0242
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1829
2688/6530 [===========>..................] - ETA: 0s - loss: 0.6895 
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0232
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1824
5120/6530 [======================>.......] - ETA: 0s - loss: 0.6000
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0234
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1825
6530/6530 [==============================] - 1s 86us/step - loss: 0.5245 - val_loss: 0.5778
Epoch 2/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2075
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0234
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1820
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1977
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0233
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1819
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1865
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0230
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1822
6530/6530 [==============================] - 0s 23us/step - loss: 0.1834 - val_loss: 0.1518
Epoch 3/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1692
3024/6530 [============>.................] - ETA: 0s - loss: 0.0228
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1816
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1717
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0228
6416/6530 [============================>.] - ETA: 0s - loss: 0.1820
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1696
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0228
6530/6530 [==============================] - 1s 215us/step - loss: 0.1819 - val_loss: 0.1734
Epoch 12/27

  16/6530 [..............................] - ETA: 1s - loss: 0.2139
6530/6530 [==============================] - 0s 23us/step - loss: 0.1691 - val_loss: 0.1523
Epoch 4/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1606
3872/6530 [================>.............] - ETA: 0s - loss: 0.0227
 240/6530 [>.............................] - ETA: 1s - loss: 0.1857
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1679
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0228
 480/6530 [=>............................] - ETA: 1s - loss: 0.1887
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1668
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0227
 720/6530 [==>...........................] - ETA: 1s - loss: 0.1871
6530/6530 [==============================] - 0s 23us/step - loss: 0.1656 - val_loss: 0.2334
Epoch 5/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1466
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0229
 944/6530 [===>..........................] - ETA: 1s - loss: 0.1857
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1642
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0229
1200/6530 [====>.........................] - ETA: 1s - loss: 0.1834
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1635
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0230
1456/6530 [=====>........................] - ETA: 1s - loss: 0.1837
6530/6530 [==============================] - 0s 24us/step - loss: 0.1626 - val_loss: 0.1570
Epoch 6/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1658
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0229
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1820
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1684
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0231
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1798
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1640
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0233
6530/6530 [==============================] - 0s 22us/step - loss: 0.1623 - val_loss: 0.1530
Epoch 7/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1558
2160/6530 [========>.....................] - ETA: 0s - loss: 0.1790
6480/6530 [============================>.] - ETA: 0s - loss: 0.0234
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1647
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1799
6530/6530 [==============================] - 1s 180us/step - loss: 0.0234 - val_loss: 0.0159
Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0267
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1623
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1810
 320/6530 [>.............................] - ETA: 1s - loss: 0.0230
6530/6530 [==============================] - 0s 22us/step - loss: 0.1617 - val_loss: 0.1966

2896/6530 [============>.................] - ETA: 0s - loss: 0.1797
 624/6530 [=>............................] - ETA: 0s - loss: 0.0240
3136/6530 [=============>................] - ETA: 0s - loss: 0.1781
# training | RMSE: 0.2317, MAE: 0.1922
worker 2  xfile  [4, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4949767012811116}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41779063623655877}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.23173598380059188, 'rmse': 0.23173598380059188, 'mae': 0.1921613566514584, 'early_stop': True}
vggnet done  2

 912/6530 [===>..........................] - ETA: 0s - loss: 0.0237
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1788
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0238
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1780
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0231
3904/6530 [================>.............] - ETA: 0s - loss: 0.1783
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0236
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1781
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0232
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1783
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0231
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1786
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0230
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1788
3104/6530 [=============>................] - ETA: 0s - loss: 0.0226
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1786
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0227
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1791
3744/6530 [================>.............] - ETA: 0s - loss: 0.0230
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1783
4064/6530 [=================>............] - ETA: 0s - loss: 0.0229
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1784
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0228
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1780
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0228
6448/6530 [============================>.] - ETA: 0s - loss: 0.1775
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0227
6530/6530 [==============================] - 1s 214us/step - loss: 0.1773 - val_loss: 0.1759
Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.2066
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0225
 256/6530 [>.............................] - ETA: 1s - loss: 0.1735
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0224
 496/6530 [=>............................] - ETA: 1s - loss: 0.1810
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0223
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1818
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0221
1008/6530 [===>..........................] - ETA: 1s - loss: 0.1824
6512/6530 [============================>.] - ETA: 0s - loss: 0.0221
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1804
6530/6530 [==============================] - 1s 171us/step - loss: 0.0223 - val_loss: 0.0172
Epoch 8/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0109
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1803
 304/6530 [>.............................] - ETA: 1s - loss: 0.0224
1728/6530 [======>.......................] - ETA: 1s - loss: 0.1779
 608/6530 [=>............................] - ETA: 1s - loss: 0.0252
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1788
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0238
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0230
2256/6530 [=========>....................] - ETA: 0s - loss: 0.1785
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0231
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1796
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0229
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1782
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0232
3024/6530 [============>.................] - ETA: 0s - loss: 0.1773
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0230
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1769
2848/6530 [============>.................] - ETA: 0s - loss: 0.0225
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1765
3792/6530 [================>.............] - ETA: 0s - loss: 0.1757
3184/6530 [=============>................] - ETA: 0s - loss: 0.0219
4048/6530 [=================>............] - ETA: 0s - loss: 0.1758
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0220
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1762
3824/6530 [================>.............] - ETA: 0s - loss: 0.0220
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1758
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0221
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1760
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0222
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1764
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0220
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0220
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1762
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1759
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0222
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0220
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1762
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0219
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1765
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0219
6320/6530 [============================>.] - ETA: 0s - loss: 0.1762
6530/6530 [==============================] - 1s 168us/step - loss: 0.0219 - val_loss: 0.0227
Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0321
6530/6530 [==============================] - 1s 210us/step - loss: 0.1757 - val_loss: 0.1546
Epoch 14/27

  16/6530 [..............................] - ETA: 1s - loss: 0.2301
 336/6530 [>.............................] - ETA: 0s - loss: 0.0249
 272/6530 [>.............................] - ETA: 1s - loss: 0.1686
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0232
 528/6530 [=>............................] - ETA: 1s - loss: 0.1728
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0215
 800/6530 [==>...........................] - ETA: 1s - loss: 0.1752
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0214
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1740
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0208
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1722
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0208
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1708
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0208
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1704
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0207
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1710
2848/6530 [============>.................] - ETA: 0s - loss: 0.0205
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1719
3168/6530 [=============>................] - ETA: 0s - loss: 0.0204
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1721
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0203
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1717
3792/6530 [================>.............] - ETA: 0s - loss: 0.0208
3040/6530 [============>.................] - ETA: 0s - loss: 0.1703
4096/6530 [=================>............] - ETA: 0s - loss: 0.0206
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1709
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0206
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1708
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0204
3808/6530 [================>.............] - ETA: 0s - loss: 0.1710
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0202
4064/6530 [=================>............] - ETA: 0s - loss: 0.1713
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0199
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1707
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0200
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1710
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0201
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1713
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0200
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1718
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1713
6530/6530 [==============================] - 1s 170us/step - loss: 0.0200 - val_loss: 0.0166
Epoch 10/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0133
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1707
 336/6530 [>.............................] - ETA: 1s - loss: 0.0186
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1712
 640/6530 [=>............................] - ETA: 0s - loss: 0.0197
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1714
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0192
6336/6530 [============================>.] - ETA: 0s - loss: 0.1708
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0186
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0183
6530/6530 [==============================] - 1s 208us/step - loss: 0.1705 - val_loss: 0.1526
Epoch 15/27

  16/6530 [..............................] - ETA: 1s - loss: 0.2341
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0180
 272/6530 [>.............................] - ETA: 1s - loss: 0.1761
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0178
 528/6530 [=>............................] - ETA: 1s - loss: 0.1783
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0185
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1763
2864/6530 [============>.................] - ETA: 0s - loss: 0.0183
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1752
3168/6530 [=============>................] - ETA: 0s - loss: 0.0188
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1723
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0188
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1699
3776/6530 [================>.............] - ETA: 0s - loss: 0.0191
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1695
4096/6530 [=================>............] - ETA: 0s - loss: 0.0192
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1683
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0192
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1680
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0190
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1693
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0189
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1687
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0190
3024/6530 [============>.................] - ETA: 0s - loss: 0.1675
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0191
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1669
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0190
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1669
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0191
3792/6530 [================>.............] - ETA: 0s - loss: 0.1660
4032/6530 [=================>............] - ETA: 0s - loss: 0.1655
6530/6530 [==============================] - 1s 169us/step - loss: 0.0192 - val_loss: 0.0152
Epoch 11/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0353
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1656
 336/6530 [>.............................] - ETA: 1s - loss: 0.0219
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1655
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0190
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1658
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0184
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1659
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0184
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1657
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0186
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1654
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0188
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1655
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0188
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1655
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0185
6336/6530 [============================>.] - ETA: 0s - loss: 0.1649
2832/6530 [============>.................] - ETA: 0s - loss: 0.0184
3152/6530 [=============>................] - ETA: 0s - loss: 0.0185
6530/6530 [==============================] - 1s 209us/step - loss: 0.1647 - val_loss: 0.1809
Epoch 16/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1978
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0182
 256/6530 [>.............................] - ETA: 1s - loss: 0.1637
3792/6530 [================>.............] - ETA: 0s - loss: 0.0181
 496/6530 [=>............................] - ETA: 1s - loss: 0.1646
4096/6530 [=================>............] - ETA: 0s - loss: 0.0181
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1672
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0183
1008/6530 [===>..........................] - ETA: 1s - loss: 0.1678
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0183
1264/6530 [====>.........................] - ETA: 1s - loss: 0.1671
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0182
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1650
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0180
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1641
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0180
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1646
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0181
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1642
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0181
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1645
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1646
6530/6530 [==============================] - 1s 168us/step - loss: 0.0180 - val_loss: 0.0152
Epoch 12/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0180
3040/6530 [============>.................] - ETA: 0s - loss: 0.1629
 304/6530 [>.............................] - ETA: 1s - loss: 0.0172
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1634
 576/6530 [=>............................] - ETA: 1s - loss: 0.0177
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1631
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0165
3792/6530 [================>.............] - ETA: 0s - loss: 0.1631
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0167
4048/6530 [=================>............] - ETA: 0s - loss: 0.1635
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0168
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1633
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0167
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1634
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0165
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1634
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0166
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1636
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0169
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1637
2992/6530 [============>.................] - ETA: 0s - loss: 0.0171
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1636
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0171
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1637
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0174
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1635
3952/6530 [=================>............] - ETA: 0s - loss: 0.0174
6320/6530 [============================>.] - ETA: 0s - loss: 0.1634
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0175
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0175
6530/6530 [==============================] - 1s 210us/step - loss: 0.1629 - val_loss: 0.2046
Epoch 17/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1893
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0176
 272/6530 [>.............................] - ETA: 1s - loss: 0.1639
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0176
 528/6530 [=>............................] - ETA: 1s - loss: 0.1652
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0174
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1626
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0172
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1630
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0174
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1615
6464/6530 [============================>.] - ETA: 0s - loss: 0.0174
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1599
6530/6530 [==============================] - 1s 172us/step - loss: 0.0174 - val_loss: 0.0114
Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0113
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1582
 336/6530 [>.............................] - ETA: 1s - loss: 0.0162
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1590
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0165
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1590
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0157
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1600
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0155
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1596
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0166
3072/6530 [=============>................] - ETA: 0s - loss: 0.1592
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0165
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1595
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0165
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1599
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0164
3792/6530 [================>.............] - ETA: 0s - loss: 0.1601
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0163
4064/6530 [=================>............] - ETA: 0s - loss: 0.1603
3120/6530 [=============>................] - ETA: 0s - loss: 0.0165
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1600
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0167
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1601
3776/6530 [================>.............] - ETA: 0s - loss: 0.0165
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1601
4096/6530 [=================>............] - ETA: 0s - loss: 0.0166
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1600
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0168
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1601
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0169
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1592
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0169
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1592
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0167
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1590
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0167
6368/6530 [============================>.] - ETA: 0s - loss: 0.1583
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0167
6530/6530 [==============================] - 1s 208us/step - loss: 0.1582 - val_loss: 0.1404

6288/6530 [===========================>..] - ETA: 0s - loss: 0.0166Epoch 18/27

  16/6530 [..............................] - ETA: 1s - loss: 0.2722
 272/6530 [>.............................] - ETA: 1s - loss: 0.1651
6530/6530 [==============================] - 1s 168us/step - loss: 0.0165 - val_loss: 0.0114
Epoch 14/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0243
 528/6530 [=>............................] - ETA: 1s - loss: 0.1594
 336/6530 [>.............................] - ETA: 0s - loss: 0.0157
 800/6530 [==>...........................] - ETA: 1s - loss: 0.1588
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0177
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1584
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0171
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1561
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0170
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1546
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0168
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1560
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0169
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1553
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0167
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1566
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0166
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1577
2832/6530 [============>.................] - ETA: 0s - loss: 0.0167
2848/6530 [============>.................] - ETA: 0s - loss: 0.1568
3152/6530 [=============>................] - ETA: 0s - loss: 0.0165
3104/6530 [=============>................] - ETA: 0s - loss: 0.1562
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0165
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1574
3792/6530 [================>.............] - ETA: 0s - loss: 0.0164
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1576
4096/6530 [=================>............] - ETA: 0s - loss: 0.0166
3840/6530 [================>.............] - ETA: 0s - loss: 0.1572
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0165
4080/6530 [=================>............] - ETA: 0s - loss: 0.1572
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0168
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1568
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0168
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1572
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0169
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1570
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0169
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1566
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0170
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1568
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0169
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1564
6512/6530 [============================>.] - ETA: 0s - loss: 0.0170
6530/6530 [==============================] - 1s 171us/step - loss: 0.0170 - val_loss: 0.0110

5824/6530 [=========================>....] - ETA: 0s - loss: 0.1564Epoch 15/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0051
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1561
 336/6530 [>.............................] - ETA: 1s - loss: 0.0161
6320/6530 [============================>.] - ETA: 0s - loss: 0.1563
 624/6530 [=>............................] - ETA: 0s - loss: 0.0168
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0163
6530/6530 [==============================] - 1s 209us/step - loss: 0.1557 - val_loss: 0.1564
Epoch 19/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1694
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0161
 256/6530 [>.............................] - ETA: 1s - loss: 0.1596
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0158
 512/6530 [=>............................] - ETA: 1s - loss: 0.1557
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0157
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1535
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0157
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1559
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0158
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1536
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0161
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1542
3136/6530 [=============>................] - ETA: 0s - loss: 0.0157
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1541
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0156
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1540
3712/6530 [================>.............] - ETA: 0s - loss: 0.0158
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1555
4032/6530 [=================>............] - ETA: 0s - loss: 0.0158
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1569
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0159
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1566
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0158
3056/6530 [=============>................] - ETA: 0s - loss: 0.1557
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0157
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1563
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0158
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1565
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0158
3824/6530 [================>.............] - ETA: 0s - loss: 0.1567
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0158
4080/6530 [=================>............] - ETA: 0s - loss: 0.1569
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0158
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1565
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1569
6530/6530 [==============================] - 1s 171us/step - loss: 0.0159 - val_loss: 0.0107
Epoch 16/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0297
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1563
 336/6530 [>.............................] - ETA: 1s - loss: 0.0154
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1563
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0154
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1562
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0149
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1561
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0154
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1563
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0149
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1559
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0149
6336/6530 [============================>.] - ETA: 0s - loss: 0.1558
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0150
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0150
6530/6530 [==============================] - 1s 209us/step - loss: 0.1554 - val_loss: 0.1608
Epoch 20/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1703
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0151
 256/6530 [>.............................] - ETA: 1s - loss: 0.1552
3088/6530 [=============>................] - ETA: 0s - loss: 0.0148
 512/6530 [=>............................] - ETA: 1s - loss: 0.1570
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0147
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1550
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0145
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1552
4016/6530 [=================>............] - ETA: 0s - loss: 0.0145
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1539
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0145
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1513
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0146
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1500
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0146
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1507
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0147
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1506
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0146
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1520
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0147
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1512
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0147
3056/6530 [=============>................] - ETA: 0s - loss: 0.1502
6528/6530 [============================>.] - ETA: 0s - loss: 0.0146
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1509
6530/6530 [==============================] - 1s 170us/step - loss: 0.0146 - val_loss: 0.0152
Epoch 17/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0038
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1515
 304/6530 [>.............................] - ETA: 1s - loss: 0.0140
3792/6530 [================>.............] - ETA: 0s - loss: 0.1518
 608/6530 [=>............................] - ETA: 1s - loss: 0.0140
4048/6530 [=================>............] - ETA: 0s - loss: 0.1523
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0142
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1521
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0144
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1517
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0145
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1513
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0138
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1517
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0140
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1518
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0140
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1516
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0142
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1522
3120/6530 [=============>................] - ETA: 0s - loss: 0.0141
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1521
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0142
6368/6530 [============================>.] - ETA: 0s - loss: 0.1519
3744/6530 [================>.............] - ETA: 0s - loss: 0.0142
4064/6530 [=================>............] - ETA: 0s - loss: 0.0141
6530/6530 [==============================] - 1s 208us/step - loss: 0.1518 - val_loss: 0.1472
Epoch 21/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1524
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0140
 272/6530 [>.............................] - ETA: 1s - loss: 0.1472
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0138
 544/6530 [=>............................] - ETA: 1s - loss: 0.1480
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0138
 800/6530 [==>...........................] - ETA: 1s - loss: 0.1512
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0140
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1513
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0142
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1518
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0141
1552/6530 [======>.......................] - ETA: 1s - loss: 0.1514
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0142
1808/6530 [=======>......................] - ETA: 0s - loss: 0.1522
2064/6530 [========>.....................] - ETA: 0s - loss: 0.1506
6530/6530 [==============================] - 1s 167us/step - loss: 0.0143 - val_loss: 0.0097
Epoch 18/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0124
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1514
 352/6530 [>.............................] - ETA: 0s - loss: 0.0147
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1527
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0140
2848/6530 [============>.................] - ETA: 0s - loss: 0.1511
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0138
3088/6530 [=============>................] - ETA: 0s - loss: 0.1502
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0146
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1501
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0142
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1505
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0144
3776/6530 [================>.............] - ETA: 0s - loss: 0.1500
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0145
4048/6530 [=================>............] - ETA: 0s - loss: 0.1501
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0147
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1501
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0150
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1499
3040/6530 [============>.................] - ETA: 0s - loss: 0.0147
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1497
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0147
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1495
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0146
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1498
4016/6530 [=================>............] - ETA: 0s - loss: 0.0144
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0144
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1502
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1506
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0144
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1503
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0144
6384/6530 [============================>.] - ETA: 0s - loss: 0.1501
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0142
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0142
6530/6530 [==============================] - 1s 208us/step - loss: 0.1500 - val_loss: 0.1415
Epoch 22/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1944
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0143
 272/6530 [>.............................] - ETA: 1s - loss: 0.1540
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0142
 512/6530 [=>............................] - ETA: 1s - loss: 0.1511
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1523
6530/6530 [==============================] - 1s 170us/step - loss: 0.0143 - val_loss: 0.0095
Epoch 19/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0246
 976/6530 [===>..........................] - ETA: 1s - loss: 0.1522
 272/6530 [>.............................] - ETA: 1s - loss: 0.0247
1200/6530 [====>.........................] - ETA: 1s - loss: 0.1524
 560/6530 [=>............................] - ETA: 1s - loss: 0.0201
1440/6530 [=====>........................] - ETA: 1s - loss: 0.1530
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0184
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1511
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0173
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1501
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0159
2160/6530 [========>.....................] - ETA: 0s - loss: 0.1486
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0157
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1496
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0158
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1506
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0157
2880/6530 [============>.................] - ETA: 0s - loss: 0.1501
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0157
3104/6530 [=============>................] - ETA: 0s - loss: 0.1497
2896/6530 [============>.................] - ETA: 0s - loss: 0.0154
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1502
3200/6530 [=============>................] - ETA: 0s - loss: 0.0152
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1501
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0150
3888/6530 [================>.............] - ETA: 0s - loss: 0.1504
3792/6530 [================>.............] - ETA: 0s - loss: 0.0151
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1503
4096/6530 [=================>............] - ETA: 0s - loss: 0.0150
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1500
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0151
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1500
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0149
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1497
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0148
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1494
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0148
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1498
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0147
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1494
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0147
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1492
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0147
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1488
6530/6530 [==============================] - 1s 177us/step - loss: 0.0148 - val_loss: 0.0119
Epoch 20/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0127
6384/6530 [============================>.] - ETA: 0s - loss: 0.1485
 320/6530 [>.............................] - ETA: 1s - loss: 0.0157
6530/6530 [==============================] - 1s 216us/step - loss: 0.1484 - val_loss: 0.1359
Epoch 23/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1801
 608/6530 [=>............................] - ETA: 1s - loss: 0.0151
 256/6530 [>.............................] - ETA: 1s - loss: 0.1505
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0146
 512/6530 [=>............................] - ETA: 1s - loss: 0.1567
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0140
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1528
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0136
 992/6530 [===>..........................] - ETA: 1s - loss: 0.1523
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0137
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1500
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0137
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1499
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0136
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1485
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0136
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1483
3088/6530 [=============>................] - ETA: 0s - loss: 0.0136
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1488
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0135
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1491
3728/6530 [================>.............] - ETA: 0s - loss: 0.0133
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1497
4032/6530 [=================>............] - ETA: 0s - loss: 0.0133
3040/6530 [============>.................] - ETA: 0s - loss: 0.1476
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0133
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1474
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0132
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1479
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0133
3824/6530 [================>.............] - ETA: 0s - loss: 0.1476
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0134
4032/6530 [=================>............] - ETA: 0s - loss: 0.1474
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0134
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1468
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0133
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1468
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0133
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1472
6416/6530 [============================>.] - ETA: 0s - loss: 0.0133
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1475
6530/6530 [==============================] - 1s 175us/step - loss: 0.0134 - val_loss: 0.0095
Epoch 21/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0166
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1475
 256/6530 [>.............................] - ETA: 1s - loss: 0.0150
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1478
 528/6530 [=>............................] - ETA: 1s - loss: 0.0151
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1471
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0139
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1471
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0137
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1475
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0130
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1474
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0129
6512/6530 [============================>.] - ETA: 0s - loss: 0.1470
6530/6530 [==============================] - 1s 219us/step - loss: 0.1471 - val_loss: 0.1373
Epoch 24/27

  16/6530 [..............................] - ETA: 1s - loss: 0.2166
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0130
 256/6530 [>.............................] - ETA: 1s - loss: 0.1482
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0129
 512/6530 [=>............................] - ETA: 1s - loss: 0.1485
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0131
2928/6530 [============>.................] - ETA: 0s - loss: 0.0129
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1477
3216/6530 [=============>................] - ETA: 0s - loss: 0.0129
 992/6530 [===>..........................] - ETA: 1s - loss: 0.1482
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1479
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0128
1488/6530 [=====>........................] - ETA: 1s - loss: 0.1480
3840/6530 [================>.............] - ETA: 0s - loss: 0.0128
1728/6530 [======>.......................] - ETA: 1s - loss: 0.1480
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0128
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1469
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0129
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0129
2224/6530 [=========>....................] - ETA: 0s - loss: 0.1481
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0129
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1467
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0131
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1474
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0132
2928/6530 [============>.................] - ETA: 0s - loss: 0.1458
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0132
3184/6530 [=============>................] - ETA: 0s - loss: 0.1454
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0133
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1469
3696/6530 [===============>..............] - ETA: 0s - loss: 0.1467
6530/6530 [==============================] - 1s 178us/step - loss: 0.0133 - val_loss: 0.0120
Epoch 22/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0122
3920/6530 [=================>............] - ETA: 0s - loss: 0.1472
 320/6530 [>.............................] - ETA: 1s - loss: 0.0122
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1469
 624/6530 [=>............................] - ETA: 0s - loss: 0.0134
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1468
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0137
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1471
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0131
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1465
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0128
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1462
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0126
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1465
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0127
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1461
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0126
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1460
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0127
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1461
3008/6530 [============>.................] - ETA: 0s - loss: 0.0127
6352/6530 [============================>.] - ETA: 0s - loss: 0.1458
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0127
6530/6530 [==============================] - 1s 216us/step - loss: 0.1453 - val_loss: 0.1438
Epoch 25/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1413
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0127
 272/6530 [>.............................] - ETA: 1s - loss: 0.1439
3904/6530 [================>.............] - ETA: 0s - loss: 0.0127
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0128
 528/6530 [=>............................] - ETA: 1s - loss: 0.1483
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0126
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1490
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0126
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1498
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0125
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1465
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0126
1552/6530 [======>.......................] - ETA: 1s - loss: 0.1465
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0125
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1462
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0124
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1458
6400/6530 [============================>.] - ETA: 0s - loss: 0.0125
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1458
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1467
6530/6530 [==============================] - 1s 174us/step - loss: 0.0125 - val_loss: 0.0145
Epoch 23/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0069
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1460
 336/6530 [>.............................] - ETA: 1s - loss: 0.0130
3040/6530 [============>.................] - ETA: 0s - loss: 0.1445
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0130
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1449
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0136
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1450
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0148
3808/6530 [================>.............] - ETA: 0s - loss: 0.1450
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0139
4064/6530 [=================>............] - ETA: 0s - loss: 0.1447
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0137
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1447
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0137
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1446
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0136
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1447
2848/6530 [============>.................] - ETA: 0s - loss: 0.0134
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1444
3168/6530 [=============>................] - ETA: 0s - loss: 0.0133
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1445
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0132
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1443
3760/6530 [================>.............] - ETA: 0s - loss: 0.0133
4064/6530 [=================>............] - ETA: 0s - loss: 0.0132
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1445
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0131
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1442
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0130
6368/6530 [============================>.] - ETA: 0s - loss: 0.1440
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0129
6530/6530 [==============================] - 1s 209us/step - loss: 0.1438 - val_loss: 0.1415
Epoch 26/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1342
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0129
 272/6530 [>.............................] - ETA: 1s - loss: 0.1416
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0128
 528/6530 [=>............................] - ETA: 1s - loss: 0.1422
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0127
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1416
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0126
1040/6530 [===>..........................] - ETA: 1s - loss: 0.1456
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1425
6530/6530 [==============================] - 1s 169us/step - loss: 0.0127 - val_loss: 0.0095
Epoch 24/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0104
1552/6530 [======>.......................] - ETA: 0s - loss: 0.1420
 320/6530 [>.............................] - ETA: 1s - loss: 0.0119
1808/6530 [=======>......................] - ETA: 0s - loss: 0.1426
 608/6530 [=>............................] - ETA: 1s - loss: 0.0124
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1417
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0123
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1419
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0126
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1430
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0120
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1423
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0121
3008/6530 [============>.................] - ETA: 0s - loss: 0.1418
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0120
3264/6530 [=============>................] - ETA: 0s - loss: 0.1419
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0120
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1432
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0119
3776/6530 [================>.............] - ETA: 0s - loss: 0.1427
3072/6530 [=============>................] - ETA: 0s - loss: 0.0117
4016/6530 [=================>............] - ETA: 0s - loss: 0.1423
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0116
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1422
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0116
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1422
3968/6530 [=================>............] - ETA: 0s - loss: 0.0116
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1422
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0117
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1418
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0116
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1418
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0117
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1421
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0118
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1419
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0118
5968/6530 [==========================>...] - ETA: 0s - loss: 0.1420
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0118
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1419
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0119
6464/6530 [============================>.] - ETA: 0s - loss: 0.1413
6400/6530 [============================>.] - ETA: 0s - loss: 0.0118
6530/6530 [==============================] - 1s 211us/step - loss: 0.1411 - val_loss: 0.1255
Epoch 27/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1611
6530/6530 [==============================] - 1s 173us/step - loss: 0.0118 - val_loss: 0.0097
Epoch 25/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0140
 256/6530 [>.............................] - ETA: 1s - loss: 0.1404
 336/6530 [>.............................] - ETA: 1s - loss: 0.0131
 512/6530 [=>............................] - ETA: 1s - loss: 0.1334
 640/6530 [=>............................] - ETA: 0s - loss: 0.0114
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1329
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0117
 992/6530 [===>..........................] - ETA: 1s - loss: 0.1360
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0122
1232/6530 [====>.........................] - ETA: 1s - loss: 0.1374
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0127
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1385
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0126
1712/6530 [======>.......................] - ETA: 1s - loss: 0.1397
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0124
1968/6530 [========>.....................] - ETA: 0s - loss: 0.1398
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0121
2224/6530 [=========>....................] - ETA: 0s - loss: 0.1399
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0122
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1403
3120/6530 [=============>................] - ETA: 0s - loss: 0.0120
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1402
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0120
2992/6530 [============>.................] - ETA: 0s - loss: 0.1387
3744/6530 [================>.............] - ETA: 0s - loss: 0.0122
3248/6530 [=============>................] - ETA: 0s - loss: 0.1393
4048/6530 [=================>............] - ETA: 0s - loss: 0.0122
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1400
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0123
3760/6530 [================>.............] - ETA: 0s - loss: 0.1400
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0124
4016/6530 [=================>............] - ETA: 0s - loss: 0.1399
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0122
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1402
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0123
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1402
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0123
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1399
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0124
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1395
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0123
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1391
6512/6530 [============================>.] - ETA: 0s - loss: 0.0122
6530/6530 [==============================] - 1s 171us/step - loss: 0.0122 - val_loss: 0.0129
Epoch 26/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0071
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1390
 320/6530 [>.............................] - ETA: 1s - loss: 0.0116
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1388
 608/6530 [=>............................] - ETA: 1s - loss: 0.0115
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1392
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0111
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1394
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0110
6480/6530 [============================>.] - ETA: 0s - loss: 0.1391
6530/6530 [==============================] - 1s 213us/step - loss: 0.1392 - val_loss: 0.1379

1552/6530 [======>.......................] - ETA: 0s - loss: 0.0112
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0112
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0111
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0110
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0110
# training | RMSE: 0.1698, MAE: 0.1346
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4544402724480966}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.48327937943956323}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21638682894172123}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.1697903275108488, 'rmse': 0.1697903275108488, 'mae': 0.13461562911580713, 'early_stop': False}
vggnet done  1

3104/6530 [=============>................] - ETA: 0s - loss: 0.0113
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0115
3744/6530 [================>.............] - ETA: 0s - loss: 0.0116
4048/6530 [=================>............] - ETA: 0s - loss: 0.0115
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0116
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0117
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0118
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0117
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0117
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0117
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0119
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0119
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0118
6432/6530 [============================>.] - ETA: 0s - loss: 0.0118
6530/6530 [==============================] - 1s 193us/step - loss: 0.0119 - val_loss: 0.0090
Epoch 27/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0070
 256/6530 [>.............................] - ETA: 1s - loss: 0.0098
 528/6530 [=>............................] - ETA: 1s - loss: 0.0107
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0110
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0110
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0107
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0107
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0108
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0110
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0111
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0112
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0113
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0113
2928/6530 [============>.................] - ETA: 0s - loss: 0.0111
3120/6530 [=============>................] - ETA: 0s - loss: 0.0114
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0113
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0114
3808/6530 [================>.............] - ETA: 0s - loss: 0.0114
4000/6530 [=================>............] - ETA: 0s - loss: 0.0113
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0113
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0113
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0113
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0112
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0112
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0112
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0112
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0112
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0112
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0111
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0110
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0111
6530/6530 [==============================] - 2s 254us/step - loss: 0.0112 - val_loss: 0.0103

# training | RMSE: 0.0873, MAE: 0.0678
worker 0  xfile  [3, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 82, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 34, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11389578672392409}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.08731740790076341, 'rmse': 0.08731740790076341, 'mae': 0.06781659247854839, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=27.0 loss={'loss': 0.15853453262602013, 'rmse': 0.15853453262602013, 'mae': 0.13881986000899282, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1817087321690729}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#2 epoch=27.0 loss={'loss': 0.09214133383985915, 'rmse': 0.09214133383985915, 'mae': 0.07145643890639126, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27679844046668345}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39624344937280587}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.34191658154173177}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#4 epoch=27.0 loss={'loss': 0.23173598380059188, 'rmse': 0.23173598380059188, 'mae': 0.1921613566514584, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4949767012811116}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41779063623655877}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#1 epoch=27.0 loss={'loss': 0.1697903275108488, 'rmse': 0.1697903275108488, 'mae': 0.13461562911580713, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4544402724480966}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.48327937943956323}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21638682894172123}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#3 epoch=27.0 loss={'loss': 0.08731740790076341, 'rmse': 0.08731740790076341, 'mae': 0.06781659247854839, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 82, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 34, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11389578672392409}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
get a list [results] of length 190
get a list [loss] of length 5
get a list [val_loss] of length 5
length of indices is [4 1 0 3 2]
length of indices is 5
length of T is 5
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4949767012811116}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41779063623655877}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]] 

*** 1.6666666666666665 configurations x 81.0 iterations each

4 | Thu Sep 27 18:42:33 2018 | lowest loss so far: 0.0873 (run 3)

vggnet done  1
vggnet done  2
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  11 | activation: relu    | extras: dropout - rate: 49.5% 
layer 2 | size:  47 | activation: tanh    | extras: batchnorm 
layer 3 | size:  35 | activation: tanh    | extras: dropout - rate: 41.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

 128/6530 [..............................] - ETA: 41s - loss: 0.7269
1536/6530 [======>.......................] - ETA: 2s - loss: 0.7002 
3072/6530 [=============>................] - ETA: 1s - loss: 0.6809
4608/6530 [====================>.........] - ETA: 0s - loss: 0.6271
6144/6530 [===========================>..] - ETA: 0s - loss: 0.5494
6530/6530 [==============================] - 1s 173us/step - loss: 0.5284 - val_loss: 0.1813
Epoch 2/81

 128/6530 [..............................] - ETA: 0s - loss: 0.2108
1280/6530 [====>.........................] - ETA: 0s - loss: 0.2180
2688/6530 [===========>..................] - ETA: 0s - loss: 0.2031
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1941
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1914
6530/6530 [==============================] - 0s 36us/step - loss: 0.1904 - val_loss: 0.6114
Epoch 3/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1670
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1725
3840/6530 [================>.............] - ETA: 0s - loss: 0.1723
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1702
6530/6530 [==============================] - 0s 32us/step - loss: 0.1695 - val_loss: 0.4133
Epoch 4/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1695
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1660
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1687
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1702
6530/6530 [==============================] - 0s 32us/step - loss: 0.1689 - val_loss: 0.2087
Epoch 5/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1581
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1689
3200/6530 [=============>................] - ETA: 0s - loss: 0.1683
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1704
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1695
6530/6530 [==============================] - 0s 38us/step - loss: 0.1693 - val_loss: 0.1911
Epoch 6/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1680
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1664
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1718
3968/6530 [=================>............] - ETA: 0s - loss: 0.1700
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1691
6530/6530 [==============================] - 0s 42us/step - loss: 0.1689 - val_loss: 0.1460
Epoch 7/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1749
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1662
2944/6530 [============>.................] - ETA: 0s - loss: 0.1688
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1687
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1674
6530/6530 [==============================] - 0s 35us/step - loss: 0.1668 - val_loss: 0.1436
Epoch 8/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1555
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1636
3072/6530 [=============>................] - ETA: 0s - loss: 0.1650
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1663
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1662
6530/6530 [==============================] - 0s 37us/step - loss: 0.1657 - val_loss: 0.1850
Epoch 9/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1733
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1687
3200/6530 [=============>................] - ETA: 0s - loss: 0.1675
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1674
6530/6530 [==============================] - 0s 33us/step - loss: 0.1658 - val_loss: 0.1710
Epoch 10/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1727
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1677
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1650
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1661
6530/6530 [==============================] - 0s 27us/step - loss: 0.1655 - val_loss: 0.1486
Epoch 11/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1612
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1692
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1656
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1655
6530/6530 [==============================] - 0s 30us/step - loss: 0.1649 - val_loss: 0.1497
Epoch 12/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1520
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1674
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1658
6400/6530 [============================>.] - ETA: 0s - loss: 0.1659
6530/6530 [==============================] - 0s 27us/step - loss: 0.1656 - val_loss: 0.1464

# training | RMSE: 0.1821, MAE: 0.1471
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4949767012811116}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41779063623655877}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.18206492984718767, 'rmse': 0.18206492984718767, 'mae': 0.14713763115346362, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=81.0 loss={'loss': 0.18206492984718767, 'rmse': 0.18206492984718767, 'mae': 0.14713763115346362, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4949767012811116}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41779063623655877}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
get a list [results] of length 191
get a list [loss] of length 1
get a list [val_loss] of length 1
length of indices is [0]
length of indices is 1
length of T is 1
s=1
T is of size 8
T=[{'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 71, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4206882665177676}, 'layer_2_size': 52, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 51, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4302515185040906}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 59, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 5, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 91, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 80, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 59, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 36, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 74, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4602646829994158}, 'layer_5_size': 63, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 51, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4985470203594613}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22878813023045885}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4932102540157758}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1274633279440146}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13763780045412988}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38028865807259427}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 30, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 2, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 31, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 71, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4206882665177676}, 'layer_2_size': 52, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 51, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [1, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4302515185040906}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 59, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 5, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 91, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [2, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [3, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 80, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 59, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 36, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 74, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4602646829994158}, 'layer_5_size': 63, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [4, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 51, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4985470203594613}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [5, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22878813023045885}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4932102540157758}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1274633279440146}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [6, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13763780045412988}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38028865807259427}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 30, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 2, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [7, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 31, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]] 

*** 8 configurations x 27.0 iterations each

1 | Thu Sep 27 18:42:37 2018 | lowest loss so far: 0.0873 (run 3)

{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  91 | activation: tanh    | extras: None 
layer 2 | size:  46 | activation: sigmoid | extras: None 
layer 3 | size:  99 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e91329c88>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 128/6530 [..............................] - ETA: 38s - loss: 0.2799
2816/6530 [===========>..................] - ETA: 1s - loss: 0.2030 
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1803{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  27 | activation: relu    | extras: dropout - rate: 43.0% 
layer 2 | size:  59 | activation: tanh    | extras: None 
layer 3 | size:   5 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  64/6530 [..............................] - ETA: 1:25 - loss: 0.2145
6530/6530 [==============================] - 1s 139us/step - loss: 0.1770 - val_loss: 0.1931
Epoch 2/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1825
1664/6530 [======>.......................] - ETA: 2s - loss: 0.1826  
3712/6530 [================>.............] - ETA: 0s - loss: 0.1494{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  71 | activation: relu    | extras: None 
layer 2 | size:  52 | activation: sigmoid | extras: dropout - rate: 42.1% 
layer 3 | size:  51 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 5:30 - loss: 2.0711
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1749
6530/6530 [==============================] - 0s 15us/step - loss: 0.1476 - val_loss: 0.1552
Epoch 3/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1565
 432/6530 [>.............................] - ETA: 12s - loss: 0.8212 
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1704
3968/6530 [=================>............] - ETA: 0s - loss: 0.1374
 848/6530 [==>...........................] - ETA: 6s - loss: 0.5849 
6530/6530 [==============================] - 1s 161us/step - loss: 0.1697 - val_loss: 0.1638
Epoch 2/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1463
6530/6530 [==============================] - 0s 14us/step - loss: 0.1335 - val_loss: 0.3974
Epoch 4/27

 128/6530 [..............................] - ETA: 0s - loss: 0.3849
1264/6530 [====>.........................] - ETA: 4s - loss: 0.4688
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1557
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1302
1680/6530 [======>.......................] - ETA: 2s - loss: 0.3957
6530/6530 [==============================] - 0s 13us/step - loss: 0.1301 - val_loss: 0.1752
Epoch 5/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1688
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1580
2096/6530 [========>.....................] - ETA: 2s - loss: 0.3464
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1224
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1592
6530/6530 [==============================] - 0s 27us/step - loss: 0.1592 - val_loss: 0.1472

6530/6530 [==============================] - 0s 13us/step - loss: 0.1206 - val_loss: 0.1889
Epoch 3/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1317Epoch 6/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1749
2512/6530 [==========>...................] - ETA: 1s - loss: 0.3085
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1504
4096/6530 [=================>............] - ETA: 0s - loss: 0.1184
2928/6530 [============>.................] - ETA: 1s - loss: 0.2806
6530/6530 [==============================] - 0s 13us/step - loss: 0.1187 - val_loss: 0.1275
Epoch 7/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1112
4096/6530 [=================>............] - ETA: 0s - loss: 0.1538
3360/6530 [==============>...............] - ETA: 1s - loss: 0.2577
4096/6530 [=================>............] - ETA: 0s - loss: 0.1127
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1540
3776/6530 [================>.............] - ETA: 0s - loss: 0.2398
6530/6530 [==============================] - 0s 13us/step - loss: 0.1135 - val_loss: 0.2244

6530/6530 [==============================] - 0s 27us/step - loss: 0.1541 - val_loss: 0.1471
Epoch 8/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2221Epoch 4/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1369
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2250
4096/6530 [=================>............] - ETA: 0s - loss: 0.1136
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1501
4576/6530 [====================>.........] - ETA: 0s - loss: 0.2115
6530/6530 [==============================] - 0s 14us/step - loss: 0.1115 - val_loss: 0.1232
Epoch 9/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1258
3968/6530 [=================>............] - ETA: 0s - loss: 0.1507
4928/6530 [=====================>........] - ETA: 0s - loss: 0.2023
3968/6530 [=================>............] - ETA: 0s - loss: 0.1116
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1513
6530/6530 [==============================] - 0s 27us/step - loss: 0.1508 - val_loss: 0.1410
Epoch 5/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1126
6530/6530 [==============================] - 0s 14us/step - loss: 0.1081 - val_loss: 0.1596
Epoch 10/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1640
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1928
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1477
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1090
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1847
6530/6530 [==============================] - 0s 14us/step - loss: 0.1083 - val_loss: 0.1207
Epoch 11/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1146
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1473
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1767
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1058
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1470
6530/6530 [==============================] - 0s 13us/step - loss: 0.1045 - val_loss: 0.1419
Epoch 12/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1507
6530/6530 [==============================] - 0s 27us/step - loss: 0.1476 - val_loss: 0.1404
Epoch 6/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1450
6530/6530 [==============================] - 2s 255us/step - loss: 0.1707 - val_loss: 0.0463
Epoch 2/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0769
4096/6530 [=================>............] - ETA: 0s - loss: 0.1046
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1474
 400/6530 [>.............................] - ETA: 0s - loss: 0.0589
6530/6530 [==============================] - 0s 13us/step - loss: 0.1020 - val_loss: 0.3275
Epoch 13/27

 128/6530 [..............................] - ETA: 0s - loss: 0.3250
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1470
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0627
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1067
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1463
6530/6530 [==============================] - 0s 13us/step - loss: 0.1050 - val_loss: 0.1698
Epoch 14/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1564
6530/6530 [==============================] - 0s 27us/step - loss: 0.1460 - val_loss: 0.1379
Epoch 7/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1342
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0627
3968/6530 [=================>............] - ETA: 0s - loss: 0.1005
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1431
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0624
6530/6530 [==============================] - 0s 14us/step - loss: 0.0998 - val_loss: 0.1133
Epoch 15/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1234
3776/6530 [================>.............] - ETA: 0s - loss: 0.1455
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0610
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0961
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1434
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0608
6530/6530 [==============================] - 0s 14us/step - loss: 0.0976 - val_loss: 0.1611
Epoch 16/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1552
6530/6530 [==============================] - 0s 29us/step - loss: 0.1433 - val_loss: 0.1342
Epoch 8/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1311
2960/6530 [============>.................] - ETA: 0s - loss: 0.0591
3968/6530 [=================>............] - ETA: 0s - loss: 0.0979
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1433
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0587
6530/6530 [==============================] - 0s 15us/step - loss: 0.0970 - val_loss: 0.1399
Epoch 17/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1318
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1426
3824/6530 [================>.............] - ETA: 0s - loss: 0.0582
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0948
6400/6530 [============================>.] - ETA: 0s - loss: 0.1424
6530/6530 [==============================] - 0s 25us/step - loss: 0.1425 - val_loss: 0.1322

6530/6530 [==============================] - 0s 13us/step - loss: 0.0947 - val_loss: 0.1171
Epoch 9/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1662Epoch 18/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1027
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0579
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1440
4096/6530 [=================>............] - ETA: 0s - loss: 0.0971
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0570
6530/6530 [==============================] - 0s 13us/step - loss: 0.0961 - val_loss: 0.2462
Epoch 19/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2342
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1426
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0569
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0950
6336/6530 [============================>.] - ETA: 0s - loss: 0.1418
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0569
6530/6530 [==============================] - 0s 26us/step - loss: 0.1418 - val_loss: 0.1309
Epoch 10/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1225
6530/6530 [==============================] - 0s 13us/step - loss: 0.0950 - val_loss: 0.1267

6048/6530 [==========================>...] - ETA: 0s - loss: 0.0562
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1394
6480/6530 [============================>.] - ETA: 0s - loss: 0.0562
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1379
6530/6530 [==============================] - 1s 122us/step - loss: 0.0562 - val_loss: 0.0401
Epoch 3/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0387
6530/6530 [==============================] - 0s 24us/step - loss: 0.1391 - val_loss: 0.1299
Epoch 11/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1371
 464/6530 [=>............................] - ETA: 0s - loss: 0.0482
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1389
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0498
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1389
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0498
6530/6530 [==============================] - 0s 25us/step - loss: 0.1383 - val_loss: 0.1302
Epoch 12/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1592
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0478
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1404
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0486
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1389
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0492
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1361
3168/6530 [=============>................] - ETA: 0s - loss: 0.0485
6530/6530 [==============================] - 0s 26us/step - loss: 0.1363 - val_loss: 0.1280
Epoch 13/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1490
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0483
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1401
4080/6530 [=================>............] - ETA: 0s - loss: 0.0484
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1377
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0481
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1379
6530/6530 [==============================] - 0s 27us/step - loss: 0.1369 - val_loss: 0.1271
Epoch 14/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1439
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0482
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1373
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0478
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1334
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0476
6400/6530 [============================>.] - ETA: 0s - loss: 0.1345
6530/6530 [==============================] - 0s 25us/step - loss: 0.1343 - val_loss: 0.1244
Epoch 15/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1399
6336/6530 [============================>.] - ETA: 0s - loss: 0.0477
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1329
6530/6530 [==============================] - 1s 118us/step - loss: 0.0475 - val_loss: 0.0394
Epoch 4/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0431
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1329
 432/6530 [>.............................] - ETA: 0s - loss: 0.0439
6464/6530 [============================>.] - ETA: 0s - loss: 0.1342
6530/6530 [==============================] - 0s 25us/step - loss: 0.1340 - val_loss: 0.1229
Epoch 16/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1160
 848/6530 [==>...........................] - ETA: 0s - loss: 0.0457
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1305
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0457
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1326
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0448
6528/6530 [============================>.] - ETA: 0s - loss: 0.1336
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0446
6530/6530 [==============================] - 0s 25us/step - loss: 0.1336 - val_loss: 0.1232
Epoch 17/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1131
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0438
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1350
3072/6530 [=============>................] - ETA: 0s - loss: 0.0439
4096/6530 [=================>............] - ETA: 0s - loss: 0.1343
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0435
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1336
6530/6530 [==============================] - 0s 26us/step - loss: 0.1332 - val_loss: 0.1211
Epoch 18/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1317
3968/6530 [=================>............] - ETA: 0s - loss: 0.0437
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1306
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0442
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1306
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0439
6530/6530 [==============================] - 0s 24us/step - loss: 0.1315 - val_loss: 0.1207
Epoch 19/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1256
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0440
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1335
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0435
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1330
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0430
6530/6530 [==============================] - 0s 24us/step - loss: 0.1305 - val_loss: 0.1201
Epoch 20/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1467
6530/6530 [==============================] - 1s 118us/step - loss: 0.0427 - val_loss: 0.0357
Epoch 5/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0304
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1324
 448/6530 [=>............................] - ETA: 0s - loss: 0.0397
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1300
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0412
6528/6530 [============================>.] - ETA: 0s - loss: 0.1291
6530/6530 [==============================] - 0s 25us/step - loss: 0.1291 - val_loss: 0.1191
Epoch 21/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1179
# training | RMSE: 0.1490, MAE: 0.1202
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.14899858177545067, 'rmse': 0.14899858177545067, 'mae': 0.1201859977809567, 'early_stop': True}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  80 | activation: tanh    | extras: batchnorm 
layer 2 | size:  59 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  36 | activation: relu    | extras: None 
layer 4 | size:  74 | activation: sigmoid | extras: batchnorm 
layer 5 | size:  63 | activation: sigmoid | extras: dropout - rate: 46.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e91329c18>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 4:58 - loss: 0.2805
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0401
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1295
 192/6530 [..............................] - ETA: 25s - loss: 0.1645 
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0399
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1294
 384/6530 [>.............................] - ETA: 13s - loss: 0.1212
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0392
6464/6530 [============================>.] - ETA: 0s - loss: 0.1301
6530/6530 [==============================] - 0s 25us/step - loss: 0.1303 - val_loss: 0.1191
Epoch 22/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1148
 576/6530 [=>............................] - ETA: 9s - loss: 0.1001 
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0400
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1293
 768/6530 [==>...........................] - ETA: 7s - loss: 0.0857
3072/6530 [=============>................] - ETA: 0s - loss: 0.0404
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1286
 960/6530 [===>..........................] - ETA: 5s - loss: 0.0782
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0403
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1283
3904/6530 [================>.............] - ETA: 0s - loss: 0.0396
6530/6530 [==============================] - 0s 26us/step - loss: 0.1283 - val_loss: 0.1184

1152/6530 [====>.........................] - ETA: 4s - loss: 0.0758Epoch 23/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1200
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0396
1328/6530 [=====>........................] - ETA: 4s - loss: 0.0710
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1305
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0394
4096/6530 [=================>............] - ETA: 0s - loss: 0.1311
1520/6530 [=====>........................] - ETA: 3s - loss: 0.0680
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0391
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1286
1696/6530 [======>.......................] - ETA: 3s - loss: 0.0651
6530/6530 [==============================] - 0s 26us/step - loss: 0.1283 - val_loss: 0.1192
Epoch 24/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1274
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0389
1872/6530 [=======>......................] - ETA: 3s - loss: 0.0626
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1258
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0388
2048/6530 [========>.....................] - ETA: 2s - loss: 0.0608
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1268
6464/6530 [============================>.] - ETA: 0s - loss: 0.0387
2240/6530 [=========>....................] - ETA: 2s - loss: 0.0583
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1277
6530/6530 [==============================] - 1s 123us/step - loss: 0.0386 - val_loss: 0.0382
Epoch 6/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0196
6530/6530 [==============================] - 0s 26us/step - loss: 0.1277 - val_loss: 0.1171
Epoch 25/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1428
2416/6530 [==========>...................] - ETA: 2s - loss: 0.0567
 480/6530 [=>............................] - ETA: 0s - loss: 0.0322
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1262
2608/6530 [==========>...................] - ETA: 2s - loss: 0.0555
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0320
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1271
2800/6530 [===========>..................] - ETA: 2s - loss: 0.0542
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0334
6400/6530 [============================>.] - ETA: 0s - loss: 0.1274
6530/6530 [==============================] - 0s 26us/step - loss: 0.1271 - val_loss: 0.1167
Epoch 26/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1328
2992/6530 [============>.................] - ETA: 1s - loss: 0.0533
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0353
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1286
3168/6530 [=============>................] - ETA: 1s - loss: 0.0528
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0356
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1272
3344/6530 [==============>...............] - ETA: 1s - loss: 0.0517
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0354
6400/6530 [============================>.] - ETA: 0s - loss: 0.1271
3536/6530 [===============>..............] - ETA: 1s - loss: 0.0509
6530/6530 [==============================] - 0s 25us/step - loss: 0.1271 - val_loss: 0.1167
Epoch 27/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1235
3040/6530 [============>.................] - ETA: 0s - loss: 0.0355
3712/6530 [================>.............] - ETA: 1s - loss: 0.0500
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1250
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0357
3888/6530 [================>.............] - ETA: 1s - loss: 0.0492
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1262
3888/6530 [================>.............] - ETA: 0s - loss: 0.0356
4080/6530 [=================>............] - ETA: 1s - loss: 0.0485
6336/6530 [============================>.] - ETA: 0s - loss: 0.1265
6530/6530 [==============================] - 0s 25us/step - loss: 0.1268 - val_loss: 0.1160

4336/6530 [==================>...........] - ETA: 0s - loss: 0.0354
4256/6530 [==================>...........] - ETA: 1s - loss: 0.0478
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0354
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0471
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0346
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0465
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0345
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0459
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0346
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0454
6528/6530 [============================>.] - ETA: 0s - loss: 0.0344
6530/6530 [==============================] - 1s 121us/step - loss: 0.0344 - val_loss: 0.0300
Epoch 7/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0491
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0449
 448/6530 [=>............................] - ETA: 0s - loss: 0.0339
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0445
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0314
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0440
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0320
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0437
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0308
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0432
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0308
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0426
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0308
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0424
3072/6530 [=============>................] - ETA: 0s - loss: 0.0315
6512/6530 [============================>.] - ETA: 0s - loss: 0.0419
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0314
3984/6530 [=================>............] - ETA: 0s - loss: 0.0313
6530/6530 [==============================] - 3s 408us/step - loss: 0.0419 - val_loss: 0.0230
Epoch 2/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0120
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0313
 208/6530 [..............................] - ETA: 1s - loss: 0.0302
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0313
 400/6530 [>.............................] - ETA: 1s - loss: 0.0308
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0313
 592/6530 [=>............................] - ETA: 1s - loss: 0.0289
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0310
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0271
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0312
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0264
6530/6530 [==============================] - 1s 120us/step - loss: 0.0311 - val_loss: 0.0277
Epoch 8/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0205
# training | RMSE: 0.1442, MAE: 0.1100
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4302515185040906}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 59, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 5, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 91, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.14417456804135026, 'rmse': 0.14417456804135026, 'mae': 0.11004002973650756, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  51 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  91 | activation: tanh    | extras: batchnorm 
layer 3 | size:  30 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e91329320>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 256/6530 [>.............................] - ETA: 9s - loss: 0.5832
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0264
 448/6530 [=>............................] - ETA: 0s - loss: 0.0310
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1376
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0267
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0315
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0266
6530/6530 [==============================] - 1s 80us/step - loss: 0.1164 - val_loss: 0.1444
Epoch 2/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0701
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0312
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0270
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0541
6530/6530 [==============================] - 0s 12us/step - loss: 0.0536 - val_loss: 0.3184
Epoch 3/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0535
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0300
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0270
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0491
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0296
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0271
6530/6530 [==============================] - 0s 12us/step - loss: 0.0488 - val_loss: 0.2796
Epoch 4/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0497
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0297
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0270
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0467
3072/6530 [=============>................] - ETA: 0s - loss: 0.0295
6530/6530 [==============================] - 0s 12us/step - loss: 0.0462 - val_loss: 0.4537
Epoch 5/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0508
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0271
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0301
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0452
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0273
6530/6530 [==============================] - 0s 12us/step - loss: 0.0440 - val_loss: 0.4119
Epoch 6/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0485
3952/6530 [=================>............] - ETA: 0s - loss: 0.0300
2832/6530 [============>.................] - ETA: 1s - loss: 0.0274
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0401
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0301
6530/6530 [==============================] - 0s 12us/step - loss: 0.0405 - val_loss: 0.4254

3024/6530 [============>.................] - ETA: 0s - loss: 0.0272
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0299
3200/6530 [=============>................] - ETA: 0s - loss: 0.0272
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0299
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0274
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0296
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0274
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0297
3776/6530 [================>.............] - ETA: 0s - loss: 0.0274
6528/6530 [============================>.] - ETA: 0s - loss: 0.0296
6530/6530 [==============================] - 1s 121us/step - loss: 0.0296 - val_loss: 0.0283
Epoch 9/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0302
3968/6530 [=================>............] - ETA: 0s - loss: 0.0274
 448/6530 [=>............................] - ETA: 0s - loss: 0.0326
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0276
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0290
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0275
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0286
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0273
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0282
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0271
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0281
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0270
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0285
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0268
3104/6530 [=============>................] - ETA: 0s - loss: 0.0278
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0268
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0274
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0268
4016/6530 [=================>............] - ETA: 0s - loss: 0.0278
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0269
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0275
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0268
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0273
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0268
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0277
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0267
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0274
6432/6530 [============================>.] - ETA: 0s - loss: 0.0266
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0272
6530/6530 [==============================] - 2s 283us/step - loss: 0.0268 - val_loss: 0.0183
Epoch 3/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0257
# training | RMSE: 0.6465, MAE: 0.6052
worker 1  xfile  [4, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 51, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4985470203594613}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.6465495679172245, 'rmse': 0.6465495679172245, 'mae': 0.6051848154537434, 'early_stop': True}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  84 | activation: tanh    | extras: batchnorm 
layer 2 | size:  69 | activation: sigmoid | extras: dropout - rate: 22.9% 
layer 3 | size:  20 | activation: sigmoid | extras: dropout - rate: 49.3% 
layer 4 | size:  36 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e90158f60>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 1:29 - loss: 1.1979
6530/6530 [==============================] - 1s 118us/step - loss: 0.0272 - val_loss: 0.0243
Epoch 10/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0422
 208/6530 [..............................] - ETA: 1s - loss: 0.0305
 640/6530 [=>............................] - ETA: 4s - loss: 0.6366  
 448/6530 [=>............................] - ETA: 0s - loss: 0.0269
 400/6530 [>.............................] - ETA: 1s - loss: 0.0262
1248/6530 [====>.........................] - ETA: 2s - loss: 0.5167
 880/6530 [===>..........................] - ETA: 0s - loss: 0.0276
 592/6530 [=>............................] - ETA: 1s - loss: 0.0262
1888/6530 [=======>......................] - ETA: 1s - loss: 0.4478
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0263
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0261
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0273
2528/6530 [==========>...................] - ETA: 1s - loss: 0.4072
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0261
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0272
3072/6530 [=============>................] - ETA: 0s - loss: 0.3775
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0271
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0270
3680/6530 [===============>..............] - ETA: 0s - loss: 0.3532
3024/6530 [============>.................] - ETA: 0s - loss: 0.0264
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0275
4288/6530 [==================>...........] - ETA: 0s - loss: 0.3344
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0263
4896/6530 [=====================>........] - ETA: 0s - loss: 0.3201
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0270
3840/6530 [================>.............] - ETA: 0s - loss: 0.0262
5472/6530 [========================>.....] - ETA: 0s - loss: 0.3092
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0262
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0263
6080/6530 [==========================>...] - ETA: 0s - loss: 0.2992
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0259
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0261
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0260
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0257
6530/6530 [==============================] - 1s 162us/step - loss: 0.2921 - val_loss: 0.1691
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1932
2256/6530 [=========>....................] - ETA: 1s - loss: 0.0261
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0257
 576/6530 [=>............................] - ETA: 0s - loss: 0.1982
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0265
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0260
1184/6530 [====>.........................] - ETA: 0s - loss: 0.2029
2608/6530 [==========>...................] - ETA: 1s - loss: 0.0263
6400/6530 [============================>.] - ETA: 0s - loss: 0.0259
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1987
2784/6530 [===========>..................] - ETA: 1s - loss: 0.0267
6530/6530 [==============================] - 1s 124us/step - loss: 0.0260 - val_loss: 0.0268
Epoch 11/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0149
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1980
2976/6530 [============>.................] - ETA: 1s - loss: 0.0265
 464/6530 [=>............................] - ETA: 0s - loss: 0.0288
3040/6530 [============>.................] - ETA: 0s - loss: 0.1951
3152/6530 [=============>................] - ETA: 0s - loss: 0.0265
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0280
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1943
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0261
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0277
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1918
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0260
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0273
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1916
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0261
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0274
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1912
3888/6530 [================>.............] - ETA: 0s - loss: 0.0262
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0269
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1904
4064/6530 [=================>............] - ETA: 0s - loss: 0.0260
3024/6530 [============>.................] - ETA: 0s - loss: 0.0267
6530/6530 [==============================] - 1s 89us/step - loss: 0.1894 - val_loss: 0.1604
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.2060
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0259
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0262
 608/6530 [=>............................] - ETA: 0s - loss: 0.1792
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0257
3872/6530 [================>.............] - ETA: 0s - loss: 0.0259
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1828
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0255
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0253
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1816
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0254
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0251
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1814
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0253
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0252
3072/6530 [=============>................] - ETA: 0s - loss: 0.1794
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0252
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0253
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1801
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0251
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0250
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1791
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0251
6496/6530 [============================>.] - ETA: 0s - loss: 0.0249
6530/6530 [==============================] - 1s 123us/step - loss: 0.0249 - val_loss: 0.0214

4928/6530 [=====================>........] - ETA: 0s - loss: 0.1795Epoch 12/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0154
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0248
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1791
 448/6530 [=>............................] - ETA: 0s - loss: 0.0242
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0248
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1789
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0240
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0247
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0238
6530/6530 [==============================] - 1s 87us/step - loss: 0.1780 - val_loss: 0.1554
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1982
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0247
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0238
 640/6530 [=>............................] - ETA: 0s - loss: 0.1810
6480/6530 [============================>.] - ETA: 0s - loss: 0.0246
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0239
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1749
6530/6530 [==============================] - 2s 292us/step - loss: 0.0247 - val_loss: 0.0161
Epoch 4/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0140
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0241
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1733
 192/6530 [..............................] - ETA: 1s - loss: 0.0282
3056/6530 [=============>................] - ETA: 0s - loss: 0.0241
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1749
 368/6530 [>.............................] - ETA: 1s - loss: 0.0269
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0242
3040/6530 [============>.................] - ETA: 0s - loss: 0.1726
 560/6530 [=>............................] - ETA: 1s - loss: 0.0255
3968/6530 [=================>............] - ETA: 0s - loss: 0.0240
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1721
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0249
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0239
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1717
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0233
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0237
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1720
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0226
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0236
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1713
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0220
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0235
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1718
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0225
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0232
6530/6530 [==============================] - 1s 88us/step - loss: 0.1713 - val_loss: 0.1511
Epoch 5/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1896
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0228
6530/6530 [==============================] - 1s 120us/step - loss: 0.0233 - val_loss: 0.0213

 640/6530 [=>............................] - ETA: 0s - loss: 0.1706Epoch 13/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0198
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0224
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1667
 464/6530 [=>............................] - ETA: 0s - loss: 0.0270
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0220
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1654
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0255
2224/6530 [=========>....................] - ETA: 1s - loss: 0.0222
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1657
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0242
2400/6530 [==========>...................] - ETA: 1s - loss: 0.0223
3008/6530 [============>.................] - ETA: 0s - loss: 0.1651
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0239
2560/6530 [==========>...................] - ETA: 1s - loss: 0.0221
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1649
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0239
2752/6530 [===========>..................] - ETA: 1s - loss: 0.0224
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1646
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0237
2944/6530 [============>.................] - ETA: 1s - loss: 0.0226
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1643
3008/6530 [============>.................] - ETA: 0s - loss: 0.0230
3120/6530 [=============>................] - ETA: 0s - loss: 0.0230
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1644
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0230
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0229
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1644
3904/6530 [================>.............] - ETA: 0s - loss: 0.0227
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0229
6530/6530 [==============================] - 1s 87us/step - loss: 0.1638 - val_loss: 0.1439

4336/6530 [==================>...........] - ETA: 0s - loss: 0.0228Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1759
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0228
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0227
 640/6530 [=>............................] - ETA: 0s - loss: 0.1638
3872/6530 [================>.............] - ETA: 0s - loss: 0.0225
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0228
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1599
4064/6530 [=================>............] - ETA: 0s - loss: 0.0225
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0227
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1587
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0226
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0226
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1622
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0227
3072/6530 [=============>................] - ETA: 0s - loss: 0.1608
6530/6530 [==============================] - 1s 122us/step - loss: 0.0223 - val_loss: 0.0197
Epoch 14/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0192
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0226
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1607
 448/6530 [=>............................] - ETA: 0s - loss: 0.0198
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0225
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1599
 880/6530 [===>..........................] - ETA: 0s - loss: 0.0216
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0223
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1608
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0219
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0224
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1609
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0224
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0223
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1610
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0218
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0223
6528/6530 [============================>.] - ETA: 0s - loss: 0.1600
6530/6530 [==============================] - 1s 90us/step - loss: 0.1600 - val_loss: 0.1386

2528/6530 [==========>...................] - ETA: 0s - loss: 0.0215Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1612
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0224
2976/6530 [============>.................] - ETA: 0s - loss: 0.0213
 640/6530 [=>............................] - ETA: 0s - loss: 0.1574
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0224
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0210
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1533
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0224
3856/6530 [================>.............] - ETA: 0s - loss: 0.0211
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1527
6336/6530 [============================>.] - ETA: 0s - loss: 0.0225
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0210
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1541
6512/6530 [============================>.] - ETA: 0s - loss: 0.0225
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0211
3008/6530 [============>.................] - ETA: 0s - loss: 0.1529
6530/6530 [==============================] - 2s 291us/step - loss: 0.0225 - val_loss: 0.0167
Epoch 5/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0117
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0209
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1529
 208/6530 [..............................] - ETA: 1s - loss: 0.0280
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0208
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1520
 400/6530 [>.............................] - ETA: 1s - loss: 0.0253
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0209
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1532
 592/6530 [=>............................] - ETA: 1s - loss: 0.0255
6480/6530 [============================>.] - ETA: 0s - loss: 0.0211
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1533
6530/6530 [==============================] - 1s 124us/step - loss: 0.0210 - val_loss: 0.0215
Epoch 15/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0333
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0266
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1540
 448/6530 [=>............................] - ETA: 0s - loss: 0.0223
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0257
6530/6530 [==============================] - 1s 87us/step - loss: 0.1536 - val_loss: 0.1346
Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1662
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0212
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0247
 608/6530 [=>............................] - ETA: 0s - loss: 0.1548
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0200
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0239
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1487
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0214
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0233
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1488
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0211
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0232
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1500
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0207
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0226
3040/6530 [============>.................] - ETA: 0s - loss: 0.1494
3088/6530 [=============>................] - ETA: 0s - loss: 0.0208
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0221
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1509
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0208
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0222
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1498
3936/6530 [=================>............] - ETA: 0s - loss: 0.0208
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0219
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1507
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0207
2608/6530 [==========>...................] - ETA: 1s - loss: 0.0218
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1503
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0204
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0217
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1506
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0202
2992/6530 [============>.................] - ETA: 0s - loss: 0.0215
6530/6530 [==============================] - 1s 87us/step - loss: 0.1503 - val_loss: 0.1313
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1513
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0201
3168/6530 [=============>................] - ETA: 0s - loss: 0.0217
 576/6530 [=>............................] - ETA: 0s - loss: 0.1441
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0199
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0216
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1442
6530/6530 [==============================] - 1s 119us/step - loss: 0.0199 - val_loss: 0.0180
Epoch 16/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0310
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0215
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1460
 448/6530 [=>............................] - ETA: 0s - loss: 0.0184
3728/6530 [================>.............] - ETA: 0s - loss: 0.0213
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1477
 880/6530 [===>..........................] - ETA: 0s - loss: 0.0196
3904/6530 [================>.............] - ETA: 0s - loss: 0.0215
3040/6530 [============>.................] - ETA: 0s - loss: 0.1472
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0195
4096/6530 [=================>............] - ETA: 0s - loss: 0.0217
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1476
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0198
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0218
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1476
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0194
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0218
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1482
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0193
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1480
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0223
3088/6530 [=============>................] - ETA: 0s - loss: 0.0189
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1479
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0223
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0192
6464/6530 [============================>.] - ETA: 0s - loss: 0.1475
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0223
6530/6530 [==============================] - 1s 91us/step - loss: 0.1474 - val_loss: 0.1291

3952/6530 [=================>............] - ETA: 0s - loss: 0.0190Epoch 10/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1435
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0221
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0193
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1453
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0221
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0195
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1449
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0220
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0195
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1442
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0220
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0194
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1460
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0220
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0193
3168/6530 [=============>................] - ETA: 0s - loss: 0.1445
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0220
3776/6530 [================>.............] - ETA: 0s - loss: 0.1443
6530/6530 [==============================] - 1s 120us/step - loss: 0.0194 - val_loss: 0.0182
Epoch 17/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0229
6336/6530 [============================>.] - ETA: 0s - loss: 0.0221
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1444
 448/6530 [=>............................] - ETA: 0s - loss: 0.0180
6528/6530 [============================>.] - ETA: 0s - loss: 0.0221
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1450
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0202
6530/6530 [==============================] - 2s 290us/step - loss: 0.0221 - val_loss: 0.0155
Epoch 6/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0061
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1452
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0202
 192/6530 [..............................] - ETA: 1s - loss: 0.0160
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1457
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0198
 384/6530 [>.............................] - ETA: 1s - loss: 0.0197
6530/6530 [==============================] - 1s 86us/step - loss: 0.1459 - val_loss: 0.1245
Epoch 11/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1737
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0196
 576/6530 [=>............................] - ETA: 1s - loss: 0.0215
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1482
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0195
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0217
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1462
3040/6530 [============>.................] - ETA: 0s - loss: 0.0195
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0224
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1427
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0190
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0232
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1436
3920/6530 [=================>............] - ETA: 0s - loss: 0.0190
3104/6530 [=============>................] - ETA: 0s - loss: 0.1408
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0226
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0190
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1410
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0225
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0190
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1417
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0228
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0190
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1425
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0226
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0188
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1420
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0227
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0186
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1425
2256/6530 [=========>....................] - ETA: 1s - loss: 0.0229
6480/6530 [============================>.] - ETA: 0s - loss: 0.0186
6530/6530 [==============================] - 1s 123us/step - loss: 0.0186 - val_loss: 0.0166
Epoch 18/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0119
6530/6530 [==============================] - 1s 88us/step - loss: 0.1418 - val_loss: 0.1218
Epoch 12/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1413
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0229
 448/6530 [=>............................] - ETA: 0s - loss: 0.0166
 640/6530 [=>............................] - ETA: 0s - loss: 0.1424
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0227
 880/6530 [===>..........................] - ETA: 0s - loss: 0.0182
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1418
2832/6530 [============>.................] - ETA: 1s - loss: 0.0224
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0178
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1387
3008/6530 [============>.................] - ETA: 0s - loss: 0.0221
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0183
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1398
3200/6530 [=============>................] - ETA: 0s - loss: 0.0219
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0179
2912/6530 [============>.................] - ETA: 0s - loss: 0.1403
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0218
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0181
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1397
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0218
2992/6530 [============>.................] - ETA: 0s - loss: 0.0179
4128/6530 [=================>............] - ETA: 0s - loss: 0.1393
3744/6530 [================>.............] - ETA: 0s - loss: 0.0217
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0177
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1400
3936/6530 [=================>............] - ETA: 0s - loss: 0.0214
3824/6530 [================>.............] - ETA: 0s - loss: 0.0176
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1394
4112/6530 [=================>............] - ETA: 0s - loss: 0.0214
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0178
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1396
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0213
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0180
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0213
6530/6530 [==============================] - 1s 89us/step - loss: 0.1392 - val_loss: 0.1193
Epoch 13/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1649
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0181
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0212
 640/6530 [=>............................] - ETA: 0s - loss: 0.1453
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0182
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0211
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1399
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0182
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0212
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1383
6368/6530 [============================>.] - ETA: 0s - loss: 0.0182
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0213
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1396
6530/6530 [==============================] - 1s 124us/step - loss: 0.0182 - val_loss: 0.0169
Epoch 19/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0159
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0212
2944/6530 [============>.................] - ETA: 0s - loss: 0.1398
 432/6530 [>.............................] - ETA: 0s - loss: 0.0178
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0210
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1390
 848/6530 [==>...........................] - ETA: 0s - loss: 0.0175
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0210
4128/6530 [=================>............] - ETA: 0s - loss: 0.1383
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0173
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0211
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1389
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0170
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0211
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1387
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0170
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1388
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0209
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0171
6480/6530 [============================>.] - ETA: 0s - loss: 0.0210
6464/6530 [============================>.] - ETA: 0s - loss: 0.1384
2928/6530 [============>.................] - ETA: 0s - loss: 0.0170
6530/6530 [==============================] - 1s 92us/step - loss: 0.1383 - val_loss: 0.1182
Epoch 14/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1545
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0169
6530/6530 [==============================] - 2s 291us/step - loss: 0.0209 - val_loss: 0.0141
Epoch 7/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0488
 608/6530 [=>............................] - ETA: 0s - loss: 0.1362
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0169
 192/6530 [..............................] - ETA: 1s - loss: 0.0202
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1342
4128/6530 [=================>............] - ETA: 0s - loss: 0.0172
 400/6530 [>.............................] - ETA: 1s - loss: 0.0188
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1350
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0172
 592/6530 [=>............................] - ETA: 1s - loss: 0.0183
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1353
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0172
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0187
3072/6530 [=============>................] - ETA: 0s - loss: 0.1337
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0172
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0196
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1334
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0172
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0195
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1337
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0171
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0194
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1342
6530/6530 [==============================] - 1s 125us/step - loss: 0.0172 - val_loss: 0.0158
Epoch 20/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0172
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1341
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0190
 464/6530 [=>............................] - ETA: 0s - loss: 0.0168
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1345
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0192
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0161
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0188
6530/6530 [==============================] - 1s 89us/step - loss: 0.1342 - val_loss: 0.1153
Epoch 15/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1448
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0168
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0186
 608/6530 [=>............................] - ETA: 0s - loss: 0.1330
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0172
2288/6530 [=========>....................] - ETA: 1s - loss: 0.0186
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1316
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0174
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0189
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1323
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0176
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0188
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1332
3088/6530 [=============>................] - ETA: 0s - loss: 0.0175
2944/6530 [============>.................] - ETA: 0s - loss: 0.1336
2848/6530 [============>.................] - ETA: 1s - loss: 0.0188
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0175
3024/6530 [============>.................] - ETA: 0s - loss: 0.0187
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1337
3904/6530 [================>.............] - ETA: 0s - loss: 0.0172
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1336
3216/6530 [=============>................] - ETA: 0s - loss: 0.0188
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0169
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1347
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0192
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0170
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1353
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0191
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0170
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1351
3792/6530 [================>.............] - ETA: 0s - loss: 0.0192
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0169
6496/6530 [============================>.] - ETA: 0s - loss: 0.1345
3968/6530 [=================>............] - ETA: 0s - loss: 0.0193
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0169
6530/6530 [==============================] - 1s 90us/step - loss: 0.1345 - val_loss: 0.1144
Epoch 16/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1414
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0195
6496/6530 [============================>.] - ETA: 0s - loss: 0.0168
 640/6530 [=>............................] - ETA: 0s - loss: 0.1368
6530/6530 [==============================] - 1s 122us/step - loss: 0.0167 - val_loss: 0.0146
Epoch 21/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0313
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0195
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1346
 464/6530 [=>............................] - ETA: 0s - loss: 0.0168
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0197
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1343
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0161
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0197
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1347
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0163
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0197
3040/6530 [============>.................] - ETA: 0s - loss: 0.1332
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0160
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0199
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1338
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0160
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0199
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1333
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0163
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0197
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1341
3120/6530 [=============>................] - ETA: 0s - loss: 0.0162
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0197
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1341
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0163
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1341
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0198
4016/6530 [=================>............] - ETA: 0s - loss: 0.0162
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0199
6530/6530 [==============================] - 1s 89us/step - loss: 0.1340 - val_loss: 0.1128

4464/6530 [===================>..........] - ETA: 0s - loss: 0.0162Epoch 17/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1633
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0199
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0161
 608/6530 [=>............................] - ETA: 0s - loss: 0.1320
6400/6530 [============================>.] - ETA: 0s - loss: 0.0199
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0160
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1299
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0159
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1311
6530/6530 [==============================] - 2s 291us/step - loss: 0.0200 - val_loss: 0.0171
Epoch 8/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0174
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0159
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1321
 192/6530 [..............................] - ETA: 1s - loss: 0.0196
3040/6530 [============>.................] - ETA: 0s - loss: 0.1313
6530/6530 [==============================] - 1s 120us/step - loss: 0.0158 - val_loss: 0.0145
Epoch 22/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0123
 368/6530 [>.............................] - ETA: 1s - loss: 0.0188
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1317
 352/6530 [>.............................] - ETA: 0s - loss: 0.0149
 544/6530 [=>............................] - ETA: 1s - loss: 0.0190
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1313
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0169
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0199
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1317
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0158
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0202
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1319
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0157
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0198
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1320
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0154
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0192
6530/6530 [==============================] - 1s 88us/step - loss: 0.1315 - val_loss: 0.1116

2560/6530 [==========>...................] - ETA: 0s - loss: 0.0152Epoch 18/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1443
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0193
3008/6530 [============>.................] - ETA: 0s - loss: 0.0150
 640/6530 [=>............................] - ETA: 0s - loss: 0.1309
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0198
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0150
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1287
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0202
3840/6530 [================>.............] - ETA: 0s - loss: 0.0150
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1286
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0198
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0151
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1292
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0196
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0154
3072/6530 [=============>................] - ETA: 0s - loss: 0.1285
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0195
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0154
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1290
2608/6530 [==========>...................] - ETA: 1s - loss: 0.0199
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0156
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1292
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0196
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0156
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1305
2992/6530 [============>.................] - ETA: 1s - loss: 0.0196
6416/6530 [============================>.] - ETA: 0s - loss: 0.0156
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1305
6530/6530 [==============================] - 1s 124us/step - loss: 0.0155 - val_loss: 0.0151
Epoch 23/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0134
3184/6530 [=============>................] - ETA: 0s - loss: 0.0199
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1312
 448/6530 [=>............................] - ETA: 0s - loss: 0.0144
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0201
6530/6530 [==============================] - 1s 88us/step - loss: 0.1309 - val_loss: 0.1090
Epoch 19/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1535
 880/6530 [===>..........................] - ETA: 0s - loss: 0.0149
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0202
 640/6530 [=>............................] - ETA: 0s - loss: 0.1335
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0145
3760/6530 [================>.............] - ETA: 0s - loss: 0.0203
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1272
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0146
3952/6530 [=================>............] - ETA: 0s - loss: 0.0202
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1280
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0149
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0202
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1281
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0149
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0199
3040/6530 [============>.................] - ETA: 0s - loss: 0.1268
3136/6530 [=============>................] - ETA: 0s - loss: 0.0149
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0199
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1271
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0151
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0200
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1264
4032/6530 [=================>............] - ETA: 0s - loss: 0.0152
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0201
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1270
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0151
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0203
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1273
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0150
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0201
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1273
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0149
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0202
6530/6530 [==============================] - 1s 88us/step - loss: 0.1272 - val_loss: 0.1132
Epoch 20/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1497
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0150
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0202
 608/6530 [=>............................] - ETA: 0s - loss: 0.1354
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0149
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0201
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1272
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0199
6530/6530 [==============================] - 1s 121us/step - loss: 0.0148 - val_loss: 0.0143
Epoch 24/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0215
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1287
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0200
 464/6530 [=>............................] - ETA: 0s - loss: 0.0164
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1289
6336/6530 [============================>.] - ETA: 0s - loss: 0.0199
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0168
3040/6530 [============>.................] - ETA: 0s - loss: 0.1278
6512/6530 [============================>.] - ETA: 0s - loss: 0.0199
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0162
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1279
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0157
6530/6530 [==============================] - 2s 289us/step - loss: 0.0200 - val_loss: 0.0157
Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0440
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1277
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0156
 192/6530 [..............................] - ETA: 1s - loss: 0.0276
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1282
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0154
 384/6530 [>.............................] - ETA: 1s - loss: 0.0257
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1284
3088/6530 [=============>................] - ETA: 0s - loss: 0.0151
 576/6530 [=>............................] - ETA: 1s - loss: 0.0249
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1280
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0148
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0230
6530/6530 [==============================] - 1s 91us/step - loss: 0.1276 - val_loss: 0.1094
Epoch 21/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1161
3968/6530 [=================>............] - ETA: 0s - loss: 0.0150
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0214
 640/6530 [=>............................] - ETA: 0s - loss: 0.1250
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0148
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0208
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1234
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0148
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0200
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1257
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0148
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0198
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1269
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0148
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0200
3040/6530 [============>.................] - ETA: 0s - loss: 0.1269
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0148
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0205
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1274
6530/6530 [==============================] - 1s 120us/step - loss: 0.0149 - val_loss: 0.0128

2080/6530 [========>.....................] - ETA: 1s - loss: 0.0205Epoch 25/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0157
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1264
 448/6530 [=>............................] - ETA: 0s - loss: 0.0158
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0206
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1269
 880/6530 [===>..........................] - ETA: 0s - loss: 0.0150
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0205
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1270
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0145
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0204
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1274
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0140
2816/6530 [===========>..................] - ETA: 1s - loss: 0.0203
6530/6530 [==============================] - 1s 89us/step - loss: 0.1271 - val_loss: 0.1078
Epoch 22/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1324
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0146
2992/6530 [============>.................] - ETA: 0s - loss: 0.0201
 640/6530 [=>............................] - ETA: 0s - loss: 0.1246
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0145
3184/6530 [=============>................] - ETA: 0s - loss: 0.0200
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1228
2976/6530 [============>.................] - ETA: 0s - loss: 0.0143
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0198
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1215
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0144
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0197
3856/6530 [================>.............] - ETA: 0s - loss: 0.0145
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1221
3760/6530 [================>.............] - ETA: 0s - loss: 0.0196
3072/6530 [=============>................] - ETA: 0s - loss: 0.1229
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0145
3952/6530 [=================>............] - ETA: 0s - loss: 0.0194
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0146
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1230
4128/6530 [=================>............] - ETA: 0s - loss: 0.0195
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0144
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1231
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0194
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0144
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1237
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0193
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0144
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1236
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0193
6464/6530 [============================>.] - ETA: 0s - loss: 0.0144
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1239
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0191
6530/6530 [==============================] - 1s 122us/step - loss: 0.0143 - val_loss: 0.0125
Epoch 26/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0287
6530/6530 [==============================] - 1s 88us/step - loss: 0.1238 - val_loss: 0.1061
Epoch 23/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1232
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0191
 464/6530 [=>............................] - ETA: 0s - loss: 0.0161
 640/6530 [=>............................] - ETA: 0s - loss: 0.1226
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0191
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0149
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1221
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0191
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0149
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1223
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0191
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0145
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1246
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0189
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0138
3072/6530 [=============>................] - ETA: 0s - loss: 0.1244
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0189
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0135
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1244
3088/6530 [=============>................] - ETA: 0s - loss: 0.0137
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0190
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1233
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0137
6384/6530 [============================>.] - ETA: 0s - loss: 0.0190
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1240
3968/6530 [=================>............] - ETA: 0s - loss: 0.0138
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1241
6530/6530 [==============================] - 2s 288us/step - loss: 0.0190 - val_loss: 0.0166
Epoch 10/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0206
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0137
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1243
 208/6530 [..............................] - ETA: 1s - loss: 0.0164
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0137
6530/6530 [==============================] - 1s 88us/step - loss: 0.1242 - val_loss: 0.1053
Epoch 24/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1288
 400/6530 [>.............................] - ETA: 1s - loss: 0.0205
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0136
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1268
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0136
 592/6530 [=>............................] - ETA: 1s - loss: 0.0209
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1273
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0136
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0204
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1264
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0201
6530/6530 [==============================] - 1s 120us/step - loss: 0.0136 - val_loss: 0.0128
Epoch 27/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0184
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1257
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0199
 464/6530 [=>............................] - ETA: 0s - loss: 0.0137
3008/6530 [============>.................] - ETA: 0s - loss: 0.1254
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0194
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0131
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1251
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0195
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0133
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1241
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0196
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0135
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1247
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0197
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0135
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1244
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0192
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0135
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1242
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0189
3088/6530 [=============>................] - ETA: 0s - loss: 0.0135
6530/6530 [==============================] - 1s 89us/step - loss: 0.1240 - val_loss: 0.1038
Epoch 25/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1280
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0134
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0188
 640/6530 [=>............................] - ETA: 0s - loss: 0.1202
3936/6530 [=================>............] - ETA: 0s - loss: 0.0135
2688/6530 [===========>..................] - ETA: 1s - loss: 0.0186
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1191
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0135
2864/6530 [============>.................] - ETA: 1s - loss: 0.0185
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1204
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0135
3056/6530 [=============>................] - ETA: 0s - loss: 0.0186
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1211
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0135
3248/6530 [=============>................] - ETA: 0s - loss: 0.0187
3072/6530 [=============>................] - ETA: 0s - loss: 0.1202
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0135
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0186
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1203
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0136
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0186
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1208
3792/6530 [================>.............] - ETA: 0s - loss: 0.0187
6530/6530 [==============================] - 1s 120us/step - loss: 0.0135 - val_loss: 0.0119

4928/6530 [=====================>........] - ETA: 0s - loss: 0.1219
3984/6530 [=================>............] - ETA: 0s - loss: 0.0187
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1220
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0187
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1225
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0187
6530/6530 [==============================] - 1s 87us/step - loss: 0.1224 - val_loss: 0.1026
Epoch 26/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1393
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0186
 608/6530 [=>............................] - ETA: 0s - loss: 0.1211
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0186
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1198
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0186
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1191
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0186
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1213
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0186
3008/6530 [============>.................] - ETA: 0s - loss: 0.1216
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0185
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1222
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0185
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1216
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0184
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1219
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0184
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1223
# training | RMSE: 0.1006, MAE: 0.0766
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 71, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4206882665177676}, 'layer_2_size': 52, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 51, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.10056720367223733, 'rmse': 0.10056720367223733, 'mae': 0.07656052305431331, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  83 | activation: relu    | extras: dropout - rate: 13.8% 
layer 2 | size:  45 | activation: relu    | extras: dropout - rate: 38.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e91329470>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 37s - loss: 0.8418
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0183
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1227
1056/6530 [===>..........................] - ETA: 1s - loss: 0.2347 
6384/6530 [============================>.] - ETA: 0s - loss: 0.0183
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1555
6530/6530 [==============================] - 1s 89us/step - loss: 0.1227 - val_loss: 0.1067
Epoch 27/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1452
3168/6530 [=============>................] - ETA: 0s - loss: 0.1288
 576/6530 [=>............................] - ETA: 0s - loss: 0.1281
6530/6530 [==============================] - 2s 290us/step - loss: 0.0182 - val_loss: 0.0145
Epoch 11/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0260
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1130
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1234
 192/6530 [..............................] - ETA: 1s - loss: 0.0176
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1018
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1226
 384/6530 [>.............................] - ETA: 1s - loss: 0.0204
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0950
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1238
 560/6530 [=>............................] - ETA: 1s - loss: 0.0192
6530/6530 [==============================] - 1s 82us/step - loss: 0.0933 - val_loss: 0.0434
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0505
3008/6530 [============>.................] - ETA: 0s - loss: 0.1231
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0182
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0507
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1232
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0176
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0488
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1231
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0174
3200/6530 [=============>................] - ETA: 0s - loss: 0.0487
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1235
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0182
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0485
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1232
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0182
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0477
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1228
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0181
6368/6530 [============================>.] - ETA: 0s - loss: 0.0471
6528/6530 [============================>.] - ETA: 0s - loss: 0.1223
6530/6530 [==============================] - 0s 51us/step - loss: 0.0469 - val_loss: 0.0336
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0385
6530/6530 [==============================] - 1s 91us/step - loss: 0.1222 - val_loss: 0.1020

1824/6530 [=======>......................] - ETA: 1s - loss: 0.0180
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0402
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0181
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0396
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0179
3040/6530 [============>.................] - ETA: 0s - loss: 0.0390
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0179
4096/6530 [=================>............] - ETA: 0s - loss: 0.0390
2544/6530 [==========>...................] - ETA: 1s - loss: 0.0179
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0387
2720/6530 [===========>..................] - ETA: 1s - loss: 0.0177
6400/6530 [============================>.] - ETA: 0s - loss: 0.0384
6530/6530 [==============================] - 0s 50us/step - loss: 0.0382 - val_loss: 0.0269
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0339
2880/6530 [============>.................] - ETA: 1s - loss: 0.0175
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0315
3072/6530 [=============>................] - ETA: 1s - loss: 0.0174
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0315
3264/6530 [=============>................] - ETA: 0s - loss: 0.0172
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0313
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0173
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0310
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0174
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0306
3824/6530 [================>.............] - ETA: 0s - loss: 0.0174
6530/6530 [==============================] - 0s 48us/step - loss: 0.0303 - val_loss: 0.0208
Epoch 5/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0265
4000/6530 [=================>............] - ETA: 0s - loss: 0.0175
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0269
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0175
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0176
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0262
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0174
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0261
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0174
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0255
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0253
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0174
6530/6530 [==============================] - 0s 48us/step - loss: 0.0250 - val_loss: 0.0175

5152/6530 [======================>.......] - ETA: 0s - loss: 0.0175Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0227
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0175
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0213
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0212
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0176
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0215
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0175
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0213
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0175
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0213
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0175
6530/6530 [==============================] - 0s 47us/step - loss: 0.0211 - val_loss: 0.0157
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0134
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0175
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0191
6496/6530 [============================>.] - ETA: 0s - loss: 0.0175
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0189
6530/6530 [==============================] - 2s 289us/step - loss: 0.0175 - val_loss: 0.0119
Epoch 12/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0085
# training | RMSE: 0.1272, MAE: 0.0963
worker 1  xfile  [5, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22878813023045885}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4932102540157758}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1274633279440146}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.12722278355735844, 'rmse': 0.12722278355735844, 'mae': 0.09633486459576326, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  79 | activation: tanh    | extras: batchnorm 
layer 2 | size:  53 | activation: tanh    | extras: None 
layer 3 | size:  31 | activation: tanh    | extras: None 
layer 4 | size:  19 | activation: sigmoid | extras: None 
layer 5 | size:  43 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e4d74f3c8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 3:37 - loss: 0.5296
3200/6530 [=============>................] - ETA: 0s - loss: 0.0189
 208/6530 [..............................] - ETA: 1s - loss: 0.0151
 288/6530 [>.............................] - ETA: 12s - loss: 0.5279 
4096/6530 [=================>............] - ETA: 0s - loss: 0.0189
 400/6530 [>.............................] - ETA: 1s - loss: 0.0157
 560/6530 [=>............................] - ETA: 6s - loss: 0.4127 
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0187
 576/6530 [=>............................] - ETA: 1s - loss: 0.0151
 832/6530 [==>...........................] - ETA: 4s - loss: 0.3092
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0186
1104/6530 [====>.........................] - ETA: 3s - loss: 0.2471
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0153
6530/6530 [==============================] - 0s 52us/step - loss: 0.0185 - val_loss: 0.0141
Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0192
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0158
1376/6530 [=====>........................] - ETA: 2s - loss: 0.2056
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0176
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0158
1648/6530 [======>.......................] - ETA: 2s - loss: 0.1783
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0175
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0167
1920/6530 [=======>......................] - ETA: 2s - loss: 0.1589
3136/6530 [=============>................] - ETA: 0s - loss: 0.0174
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0165
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1455
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0171
2448/6530 [==========>...................] - ETA: 1s - loss: 0.1344
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0167
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0170
2720/6530 [===========>..................] - ETA: 1s - loss: 0.1257
6400/6530 [============================>.] - ETA: 0s - loss: 0.0170
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0168
6530/6530 [==============================] - 0s 50us/step - loss: 0.0169 - val_loss: 0.0131
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0151
2992/6530 [============>.................] - ETA: 1s - loss: 0.1177
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0169
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0164
3232/6530 [=============>................] - ETA: 1s - loss: 0.1116
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0171
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0162
3504/6530 [===============>..............] - ETA: 1s - loss: 0.1064
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0171
3264/6530 [=============>................] - ETA: 0s - loss: 0.0157
3776/6530 [================>.............] - ETA: 0s - loss: 0.1016
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0173
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0156
4032/6530 [=================>............] - ETA: 0s - loss: 0.0979
2848/6530 [============>.................] - ETA: 1s - loss: 0.0173
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0156
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0940
3040/6530 [============>.................] - ETA: 0s - loss: 0.0172
6530/6530 [==============================] - 0s 49us/step - loss: 0.0156 - val_loss: 0.0129
Epoch 10/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0112
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0909
3232/6530 [=============>................] - ETA: 0s - loss: 0.0170
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0152
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0880
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0170
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0147
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0853
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0171
3200/6530 [=============>................] - ETA: 0s - loss: 0.0145
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0833
3792/6530 [================>.............] - ETA: 0s - loss: 0.0171
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0144
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0812
3968/6530 [=================>............] - ETA: 0s - loss: 0.0171
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0145
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0793
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0171
6400/6530 [============================>.] - ETA: 0s - loss: 0.0145
6530/6530 [==============================] - 0s 50us/step - loss: 0.0145 - val_loss: 0.0119
Epoch 11/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0103
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0777
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0171
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0149
6336/6530 [============================>.] - ETA: 0s - loss: 0.0758
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0170
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0141
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0171
3200/6530 [=============>................] - ETA: 0s - loss: 0.0138
6530/6530 [==============================] - 2s 290us/step - loss: 0.0747 - val_loss: 0.0337
Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0506
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0172
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0138
 288/6530 [>.............................] - ETA: 1s - loss: 0.0390
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0171
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0138
 560/6530 [=>............................] - ETA: 1s - loss: 0.0400
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0172
6400/6530 [============================>.] - ETA: 0s - loss: 0.0138
6530/6530 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0111
Epoch 12/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0077
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0392
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0172
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0126
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0382
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0172
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0128
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0369
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0170
3232/6530 [=============>................] - ETA: 0s - loss: 0.0126
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0361
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0170
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0127
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0356
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0171
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0126
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0355
6416/6530 [============================>.] - ETA: 0s - loss: 0.0170
6464/6530 [============================>.] - ETA: 0s - loss: 0.0126
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0356
6530/6530 [==============================] - 0s 49us/step - loss: 0.0126 - val_loss: 0.0103
Epoch 13/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0114
6530/6530 [==============================] - 2s 287us/step - loss: 0.0170 - val_loss: 0.0143
Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0187
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0357
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0130
 208/6530 [..............................] - ETA: 1s - loss: 0.0192
3008/6530 [============>.................] - ETA: 0s - loss: 0.0354
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0127
 400/6530 [>.............................] - ETA: 1s - loss: 0.0195
3264/6530 [=============>................] - ETA: 0s - loss: 0.0351
3136/6530 [=============>................] - ETA: 0s - loss: 0.0125
 576/6530 [=>............................] - ETA: 1s - loss: 0.0197
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0352
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0124
3760/6530 [================>.............] - ETA: 0s - loss: 0.0350
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0189
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0123
4016/6530 [=================>............] - ETA: 0s - loss: 0.0348
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0186
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0124
6530/6530 [==============================] - 0s 51us/step - loss: 0.0123 - val_loss: 0.0104
Epoch 14/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0110
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0345
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0180
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0132
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0347
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0178
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0124
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0346
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0176
3200/6530 [=============>................] - ETA: 0s - loss: 0.0120
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0343
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0171
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0118
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0344
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0169
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0118
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0339
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0170
6336/6530 [============================>.] - ETA: 0s - loss: 0.0119
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0338
6530/6530 [==============================] - 0s 50us/step - loss: 0.0119 - val_loss: 0.0101
Epoch 15/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0093
2224/6530 [=========>....................] - ETA: 1s - loss: 0.0174
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0334
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0125
2416/6530 [==========>...................] - ETA: 1s - loss: 0.0175
6432/6530 [============================>.] - ETA: 0s - loss: 0.0332
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0122
2608/6530 [==========>...................] - ETA: 1s - loss: 0.0175
3072/6530 [=============>................] - ETA: 0s - loss: 0.0117
6530/6530 [==============================] - 1s 199us/step - loss: 0.0331 - val_loss: 0.0259
Epoch 3/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0342
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0174
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0116
2976/6530 [============>.................] - ETA: 1s - loss: 0.0172
 288/6530 [>.............................] - ETA: 1s - loss: 0.0299
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0114
 560/6530 [=>............................] - ETA: 1s - loss: 0.0308
3168/6530 [=============>................] - ETA: 0s - loss: 0.0171
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0114
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0299
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0171
6530/6530 [==============================] - 0s 51us/step - loss: 0.0114 - val_loss: 0.0097
Epoch 16/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0071
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0293
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0170
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0110
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0284
3728/6530 [================>.............] - ETA: 0s - loss: 0.0170
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0112
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0278
3920/6530 [=================>............] - ETA: 0s - loss: 0.0169
3232/6530 [=============>................] - ETA: 0s - loss: 0.0109
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0272
4112/6530 [=================>............] - ETA: 0s - loss: 0.0167
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0107
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0271
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0166
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0109
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0270
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0166
6496/6530 [============================>.] - ETA: 0s - loss: 0.0108
6530/6530 [==============================] - 0s 49us/step - loss: 0.0108 - val_loss: 0.0090
Epoch 17/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0092
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0270
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0166
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0115
2944/6530 [============>.................] - ETA: 0s - loss: 0.0268
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0165
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0108
3216/6530 [=============>................] - ETA: 0s - loss: 0.0266
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0165
3264/6530 [=============>................] - ETA: 0s - loss: 0.0105
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0269
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0165
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0104
3760/6530 [================>.............] - ETA: 0s - loss: 0.0266
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0166
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0105
4032/6530 [=================>............] - ETA: 0s - loss: 0.0267
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0165
6528/6530 [============================>.] - ETA: 0s - loss: 0.0104
6530/6530 [==============================] - 0s 49us/step - loss: 0.0104 - val_loss: 0.0088
Epoch 18/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0095
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0265
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0166
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0107
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0266
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0166
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0106
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0265
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0166
3200/6530 [=============>................] - ETA: 0s - loss: 0.0102
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0262
6336/6530 [============================>.] - ETA: 0s - loss: 0.0166
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0102
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0261
6512/6530 [============================>.] - ETA: 0s - loss: 0.0166
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0102
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0258
6530/6530 [==============================] - 2s 290us/step - loss: 0.0166 - val_loss: 0.0114
Epoch 14/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0283
6368/6530 [============================>.] - ETA: 0s - loss: 0.0103
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0258
6530/6530 [==============================] - 0s 50us/step - loss: 0.0102 - val_loss: 0.0087
Epoch 19/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0097
 192/6530 [..............................] - ETA: 1s - loss: 0.0209
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0256
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0096
 368/6530 [>.............................] - ETA: 1s - loss: 0.0208
6464/6530 [============================>.] - ETA: 0s - loss: 0.0255
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0098
 544/6530 [=>............................] - ETA: 1s - loss: 0.0204
6530/6530 [==============================] - 1s 198us/step - loss: 0.0255 - val_loss: 0.0198
Epoch 4/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0180
3264/6530 [=============>................] - ETA: 0s - loss: 0.0097
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0200
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0098
 272/6530 [>.............................] - ETA: 1s - loss: 0.0237
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0195
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0098
 544/6530 [=>............................] - ETA: 1s - loss: 0.0234
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0197
6528/6530 [============================>.] - ETA: 0s - loss: 0.0098
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0230
6530/6530 [==============================] - 0s 49us/step - loss: 0.0098 - val_loss: 0.0088
Epoch 20/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0097
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0191
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0234
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0101
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0185
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0231
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0102
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0179
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0226
3136/6530 [=============>................] - ETA: 0s - loss: 0.0097
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0176
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0221
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0098
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0175
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0222
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0097
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0173
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0223
6400/6530 [============================>.] - ETA: 0s - loss: 0.0097
6530/6530 [==============================] - 0s 50us/step - loss: 0.0097 - val_loss: 0.0088

2432/6530 [==========>...................] - ETA: 1s - loss: 0.0176Epoch 21/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0097
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0222
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0102
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0176
2960/6530 [============>.................] - ETA: 0s - loss: 0.0222
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0099
2816/6530 [===========>..................] - ETA: 1s - loss: 0.0176
3232/6530 [=============>................] - ETA: 0s - loss: 0.0221
3264/6530 [=============>................] - ETA: 0s - loss: 0.0097
3008/6530 [============>.................] - ETA: 0s - loss: 0.0175
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0224
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0097
3184/6530 [=============>................] - ETA: 0s - loss: 0.0175
3776/6530 [================>.............] - ETA: 0s - loss: 0.0223
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0096
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0175
4064/6530 [=================>............] - ETA: 0s - loss: 0.0226
6528/6530 [============================>.] - ETA: 0s - loss: 0.0096
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0175
6530/6530 [==============================] - 0s 49us/step - loss: 0.0096 - val_loss: 0.0086
Epoch 22/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0104
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0225
3744/6530 [================>.............] - ETA: 0s - loss: 0.0172
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0100
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0228
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0098
3936/6530 [=================>............] - ETA: 0s - loss: 0.0171
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0226
3104/6530 [=============>................] - ETA: 0s - loss: 0.0093
4128/6530 [=================>............] - ETA: 0s - loss: 0.0171
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0225
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0095
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0172
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0224
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0093
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0172
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0222
6336/6530 [============================>.] - ETA: 0s - loss: 0.0093
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0172
6530/6530 [==============================] - 0s 50us/step - loss: 0.0093 - val_loss: 0.0084

5984/6530 [==========================>...] - ETA: 0s - loss: 0.0221Epoch 23/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0083
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0170
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0221
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0092
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0169
6512/6530 [============================>.] - ETA: 0s - loss: 0.0221
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0090
6530/6530 [==============================] - 1s 197us/step - loss: 0.0221 - val_loss: 0.0168
Epoch 5/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0141
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0168
3232/6530 [=============>................] - ETA: 0s - loss: 0.0088
 288/6530 [>.............................] - ETA: 1s - loss: 0.0205
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0167
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0089
 560/6530 [=>............................] - ETA: 1s - loss: 0.0206
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0166
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0090
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0208
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0165
6400/6530 [============================>.] - ETA: 0s - loss: 0.0090
6530/6530 [==============================] - 0s 50us/step - loss: 0.0090 - val_loss: 0.0083
Epoch 24/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0074
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0218
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0164
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0089
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0214
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0164
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0088
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0211
6384/6530 [============================>.] - ETA: 0s - loss: 0.0164
3168/6530 [=============>................] - ETA: 0s - loss: 0.0087
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0205
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0088
6530/6530 [==============================] - 2s 288us/step - loss: 0.0164 - val_loss: 0.0112
Epoch 15/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0119
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0206
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0088
 192/6530 [..............................] - ETA: 1s - loss: 0.0154
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0207
6400/6530 [============================>.] - ETA: 0s - loss: 0.0088
 384/6530 [>.............................] - ETA: 1s - loss: 0.0161
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0204
6530/6530 [==============================] - 0s 51us/step - loss: 0.0088 - val_loss: 0.0083
Epoch 25/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0108
2960/6530 [============>.................] - ETA: 0s - loss: 0.0205
 576/6530 [=>............................] - ETA: 1s - loss: 0.0162
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0094
3232/6530 [=============>................] - ETA: 0s - loss: 0.0205
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0088
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0165
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0208
3136/6530 [=============>................] - ETA: 0s - loss: 0.0086
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0158
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0088
3760/6530 [================>.............] - ETA: 0s - loss: 0.0208
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0157
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0088
4048/6530 [=================>............] - ETA: 0s - loss: 0.0211
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0156
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0087
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0210
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0155
6530/6530 [==============================] - 0s 51us/step - loss: 0.0087 - val_loss: 0.0079
Epoch 26/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0079
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0212
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0156
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0084
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0212
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0155
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0084
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0210
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0156
2944/6530 [============>.................] - ETA: 0s - loss: 0.0084
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0209
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0154
4032/6530 [=================>............] - ETA: 0s - loss: 0.0085
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0207
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0154
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0085
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0208
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0155
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0084
6530/6530 [==============================] - 0s 51us/step - loss: 0.0084 - val_loss: 0.0079
Epoch 27/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0061
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0206
2848/6530 [============>.................] - ETA: 1s - loss: 0.0153
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0088
6384/6530 [============================>.] - ETA: 0s - loss: 0.0205
3024/6530 [============>.................] - ETA: 0s - loss: 0.0154
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0084
6530/6530 [==============================] - 1s 199us/step - loss: 0.0207 - val_loss: 0.0151
Epoch 6/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0125
3216/6530 [=============>................] - ETA: 0s - loss: 0.0155
3264/6530 [=============>................] - ETA: 0s - loss: 0.0083
 288/6530 [>.............................] - ETA: 1s - loss: 0.0195
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0155
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0083
 560/6530 [=>............................] - ETA: 1s - loss: 0.0195
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0155
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0084
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0198
3776/6530 [================>.............] - ETA: 0s - loss: 0.0153
6432/6530 [============================>.] - ETA: 0s - loss: 0.0084
6530/6530 [==============================] - 0s 50us/step - loss: 0.0084 - val_loss: 0.0076

1104/6530 [====>.........................] - ETA: 1s - loss: 0.0207
3968/6530 [=================>............] - ETA: 0s - loss: 0.0156
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0205
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0155
# training | RMSE: 0.0762, MAE: 0.0587
worker 0  xfile  [6, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13763780045412988}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38028865807259427}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 30, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 2, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.0762380935269196, 'rmse': 0.0762380935269196, 'mae': 0.05866624932945352, 'early_stop': False}
vggnet done  0

1648/6530 [======>.......................] - ETA: 0s - loss: 0.0201
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0154
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0196
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0154
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0198
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0154
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0198
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0155
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0196
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0155
3024/6530 [============>.................] - ETA: 0s - loss: 0.0195
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0155
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0197
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0156
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0199
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0155
3840/6530 [================>.............] - ETA: 0s - loss: 0.0200
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0156
4112/6530 [=================>............] - ETA: 0s - loss: 0.0201
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0156
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0200
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0156
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0203
6416/6530 [============================>.] - ETA: 0s - loss: 0.0156
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0201
6530/6530 [==============================] - 2s 285us/step - loss: 0.0156 - val_loss: 0.0112
Epoch 16/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0113
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0199
 208/6530 [..............................] - ETA: 1s - loss: 0.0203
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0198
 416/6530 [>.............................] - ETA: 1s - loss: 0.0188
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0198
 608/6530 [=>............................] - ETA: 1s - loss: 0.0173
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0197
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0168
6368/6530 [============================>.] - ETA: 0s - loss: 0.0196
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0160
6530/6530 [==============================] - 1s 192us/step - loss: 0.0198 - val_loss: 0.0141
Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0121
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0159
 304/6530 [>.............................] - ETA: 1s - loss: 0.0186
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0158
 592/6530 [=>............................] - ETA: 1s - loss: 0.0191
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0155
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0190
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0155
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0199
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0154
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0199
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0154
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0194
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0156
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0189
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0156
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0190
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0156
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0191
2928/6530 [============>.................] - ETA: 0s - loss: 0.0155
2896/6530 [============>.................] - ETA: 0s - loss: 0.0190
3136/6530 [=============>................] - ETA: 0s - loss: 0.0153
3200/6530 [=============>................] - ETA: 0s - loss: 0.0189
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0152
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0192
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0152
3776/6530 [================>.............] - ETA: 0s - loss: 0.0191
3760/6530 [================>.............] - ETA: 0s - loss: 0.0152
4080/6530 [=================>............] - ETA: 0s - loss: 0.0195
3968/6530 [=================>............] - ETA: 0s - loss: 0.0150
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0194
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0150
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0195
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0150
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0194
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0149
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0192
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0149
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0190
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0149
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0191
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0148
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0190
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0148
6384/6530 [============================>.] - ETA: 0s - loss: 0.0189
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0148
6530/6530 [==============================] - 1s 184us/step - loss: 0.0191 - val_loss: 0.0134
Epoch 8/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0118
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0148
 256/6530 [>.............................] - ETA: 1s - loss: 0.0193
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0148
 512/6530 [=>............................] - ETA: 1s - loss: 0.0188
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0149
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0188
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0149
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0193
6416/6530 [============================>.] - ETA: 0s - loss: 0.0149
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0195
6530/6530 [==============================] - 2s 276us/step - loss: 0.0149 - val_loss: 0.0122

1632/6530 [======>.......................] - ETA: 0s - loss: 0.0191Epoch 17/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0108
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0185
 208/6530 [..............................] - ETA: 1s - loss: 0.0180
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0187
 400/6530 [>.............................] - ETA: 1s - loss: 0.0176
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0187
 576/6530 [=>............................] - ETA: 1s - loss: 0.0173
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0184
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0169
3056/6530 [=============>................] - ETA: 0s - loss: 0.0184
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0165
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0185
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0153
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0187
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0152
3920/6530 [=================>............] - ETA: 0s - loss: 0.0188
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0152
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0189
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0149
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0189
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0146
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0190
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0146
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0188
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0145
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0187
2544/6530 [==========>...................] - ETA: 1s - loss: 0.0146
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0185
2752/6530 [===========>..................] - ETA: 1s - loss: 0.0147
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0185
2960/6530 [============>.................] - ETA: 0s - loss: 0.0145
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0185
3152/6530 [=============>................] - ETA: 0s - loss: 0.0145
6480/6530 [============================>.] - ETA: 0s - loss: 0.0185
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0147
6530/6530 [==============================] - 1s 188us/step - loss: 0.0186 - val_loss: 0.0128
Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0112
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0148
 304/6530 [>.............................] - ETA: 1s - loss: 0.0177
3744/6530 [================>.............] - ETA: 0s - loss: 0.0146
 608/6530 [=>............................] - ETA: 1s - loss: 0.0185
3920/6530 [=================>............] - ETA: 0s - loss: 0.0147
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0181
4112/6530 [=================>............] - ETA: 0s - loss: 0.0146
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0190
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0146
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0189
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0146
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0184
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0145
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0180
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0146
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0181
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0146
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0182
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0146
2832/6530 [============>.................] - ETA: 0s - loss: 0.0181
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0146
3136/6530 [=============>................] - ETA: 0s - loss: 0.0179
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0147
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0181
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0147
3712/6530 [================>.............] - ETA: 0s - loss: 0.0182
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0147
3984/6530 [=================>............] - ETA: 0s - loss: 0.0184
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0146
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0185
6496/6530 [============================>.] - ETA: 0s - loss: 0.0145
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0186
6530/6530 [==============================] - 2s 272us/step - loss: 0.0145 - val_loss: 0.0116
Epoch 18/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0112
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0185
 208/6530 [..............................] - ETA: 1s - loss: 0.0169
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0183
 400/6530 [>.............................] - ETA: 1s - loss: 0.0157
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0182
 576/6530 [=>............................] - ETA: 1s - loss: 0.0164
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0181
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0170
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0181
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0167
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0181
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0170
6496/6530 [============================>.] - ETA: 0s - loss: 0.0181
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0166
6530/6530 [==============================] - 1s 191us/step - loss: 0.0182 - val_loss: 0.0124
Epoch 10/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0105
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0161
 288/6530 [>.............................] - ETA: 1s - loss: 0.0177
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0161
 560/6530 [=>............................] - ETA: 1s - loss: 0.0176
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0159
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0180
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0154
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0188
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0152
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0187
2416/6530 [==========>...................] - ETA: 1s - loss: 0.0149
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0183
2608/6530 [==========>...................] - ETA: 1s - loss: 0.0149
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0178
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0151
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0178
2992/6530 [============>.................] - ETA: 1s - loss: 0.0151
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0179
3200/6530 [=============>................] - ETA: 0s - loss: 0.0151
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0177
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0151
3088/6530 [=============>................] - ETA: 0s - loss: 0.0175
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0149
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0177
3776/6530 [================>.............] - ETA: 0s - loss: 0.0149
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0178
3968/6530 [=================>............] - ETA: 0s - loss: 0.0149
3920/6530 [=================>............] - ETA: 0s - loss: 0.0179
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0181
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0150
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0181
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0150
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0181
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0152
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0179
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0153
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0178
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0152
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0176
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0152
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0177
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0154
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0176
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0153
6400/6530 [============================>.] - ETA: 0s - loss: 0.0176
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0153
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0152
6530/6530 [==============================] - 1s 191us/step - loss: 0.0178 - val_loss: 0.0120
Epoch 11/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0099
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0152
 304/6530 [>.............................] - ETA: 1s - loss: 0.0169
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0152
 608/6530 [=>............................] - ETA: 1s - loss: 0.0177
6496/6530 [============================>.] - ETA: 0s - loss: 0.0152
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0174
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0183
6530/6530 [==============================] - 2s 282us/step - loss: 0.0152 - val_loss: 0.0128
Epoch 19/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0074
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0183
 208/6530 [..............................] - ETA: 1s - loss: 0.0147
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0178
 400/6530 [>.............................] - ETA: 1s - loss: 0.0143
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0173
 592/6530 [=>............................] - ETA: 1s - loss: 0.0134
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0174
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0132
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0175
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0131
2880/6530 [============>.................] - ETA: 0s - loss: 0.0172
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0131
3152/6530 [=============>................] - ETA: 0s - loss: 0.0172
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0130
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0174
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0130
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0174
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0132
3968/6530 [=================>............] - ETA: 0s - loss: 0.0176
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0130
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0177
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0132
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0178
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0135
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0177
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0135
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0175
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0134
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0174
2832/6530 [============>.................] - ETA: 1s - loss: 0.0137
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0172
3024/6530 [============>.................] - ETA: 0s - loss: 0.0137
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0172
3216/6530 [=============>................] - ETA: 0s - loss: 0.0139
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0172
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0141
6464/6530 [============================>.] - ETA: 0s - loss: 0.0173
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0142
6530/6530 [==============================] - 1s 190us/step - loss: 0.0174 - val_loss: 0.0116
Epoch 12/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0095
3776/6530 [================>.............] - ETA: 0s - loss: 0.0142
 304/6530 [>.............................] - ETA: 1s - loss: 0.0166
3968/6530 [=================>............] - ETA: 0s - loss: 0.0140
 592/6530 [=>............................] - ETA: 1s - loss: 0.0172
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0139
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0171
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0140
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0180
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0139
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0179
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0139
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0174
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0140
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0170
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0140
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0172
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0141
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0172
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0141
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0169
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0143
3072/6530 [=============>................] - ETA: 0s - loss: 0.0168
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0143
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0169
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0143
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0171
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0143
3904/6530 [================>.............] - ETA: 0s - loss: 0.0172
6448/6530 [============================>.] - ETA: 0s - loss: 0.0143
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0173
6530/6530 [==============================] - 2s 282us/step - loss: 0.0142 - val_loss: 0.0113

4464/6530 [===================>..........] - ETA: 0s - loss: 0.0173
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0174
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0172
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0171
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0169
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0169
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0168
# training | RMSE: 0.0903, MAE: 0.0703
worker 2  xfile  [3, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 80, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 59, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 36, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 74, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4602646829994158}, 'layer_5_size': 63, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.09034943811590751, 'rmse': 0.09034943811590751, 'mae': 0.07026809846161355, 'early_stop': True}
vggnet done  2

6416/6530 [============================>.] - ETA: 0s - loss: 0.0168
6530/6530 [==============================] - 1s 191us/step - loss: 0.0170 - val_loss: 0.0112
Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0093
 320/6530 [>.............................] - ETA: 1s - loss: 0.0159
 608/6530 [=>............................] - ETA: 1s - loss: 0.0170
 880/6530 [===>..........................] - ETA: 0s - loss: 0.0168
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0175
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0175
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0171
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0166
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0167
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0168
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0167
2880/6530 [============>.................] - ETA: 0s - loss: 0.0166
3088/6530 [=============>................] - ETA: 0s - loss: 0.0165
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0165
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0168
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0168
3904/6530 [================>.............] - ETA: 0s - loss: 0.0169
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0170
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0169
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0171
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0170
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0168
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0168
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0167
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0165
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0165
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0166
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0166
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0165
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0165
6512/6530 [============================>.] - ETA: 0s - loss: 0.0167
6530/6530 [==============================] - 2s 250us/step - loss: 0.0167 - val_loss: 0.0108
Epoch 14/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0092
 192/6530 [..............................] - ETA: 1s - loss: 0.0172
 384/6530 [>.............................] - ETA: 1s - loss: 0.0158
 592/6530 [=>............................] - ETA: 1s - loss: 0.0166
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0169
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0165
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0173
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0174
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0170
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0167
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0164
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0164
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0164
2416/6530 [==========>...................] - ETA: 1s - loss: 0.0165
2576/6530 [==========>...................] - ETA: 1s - loss: 0.0165
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0163
2896/6530 [============>.................] - ETA: 1s - loss: 0.0164
3056/6530 [=============>................] - ETA: 1s - loss: 0.0163
3216/6530 [=============>................] - ETA: 0s - loss: 0.0162
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0164
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0164
3728/6530 [================>.............] - ETA: 0s - loss: 0.0164
3904/6530 [================>.............] - ETA: 0s - loss: 0.0166
4128/6530 [=================>............] - ETA: 0s - loss: 0.0167
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0166
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0167
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0166
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0165
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0164
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0162
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0163
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0163
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0162
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0162
6336/6530 [============================>.] - ETA: 0s - loss: 0.0162
6512/6530 [============================>.] - ETA: 0s - loss: 0.0164
6530/6530 [==============================] - 2s 291us/step - loss: 0.0164 - val_loss: 0.0105
Epoch 15/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0090
 160/6530 [..............................] - ETA: 2s - loss: 0.0173
 320/6530 [>.............................] - ETA: 2s - loss: 0.0154
 464/6530 [=>............................] - ETA: 2s - loss: 0.0165
 608/6530 [=>............................] - ETA: 2s - loss: 0.0165
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0166
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0162
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0170
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0172
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0168
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0168
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0164
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0161
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0162
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0162
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0163
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0162
2784/6530 [===========>..................] - ETA: 1s - loss: 0.0160
2928/6530 [============>.................] - ETA: 1s - loss: 0.0160
3104/6530 [=============>................] - ETA: 1s - loss: 0.0159
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0159
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0162
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0162
3792/6530 [================>.............] - ETA: 0s - loss: 0.0161
3952/6530 [=================>............] - ETA: 0s - loss: 0.0163
4128/6530 [=================>............] - ETA: 0s - loss: 0.0164
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0163
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0163
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0164
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0164
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0162
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0162
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0161
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0160
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0159
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0160
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0160
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0159
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0160
6336/6530 [============================>.] - ETA: 0s - loss: 0.0159
6528/6530 [============================>.] - ETA: 0s - loss: 0.0161
6530/6530 [==============================] - 2s 330us/step - loss: 0.0161 - val_loss: 0.0103
Epoch 16/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0088
 192/6530 [..............................] - ETA: 1s - loss: 0.0165
 400/6530 [>.............................] - ETA: 1s - loss: 0.0155
 560/6530 [=>............................] - ETA: 1s - loss: 0.0158
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0165
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0160
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0164
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0166
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0168
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0165
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0164
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0161
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0157
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0158
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0158
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0160
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0158
2816/6530 [===========>..................] - ETA: 1s - loss: 0.0158
2992/6530 [============>.................] - ETA: 1s - loss: 0.0157
3184/6530 [=============>................] - ETA: 1s - loss: 0.0156
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0158
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0160
3824/6530 [================>.............] - ETA: 0s - loss: 0.0160
4064/6530 [=================>............] - ETA: 0s - loss: 0.0161
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0160
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0161
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0161
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0159
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0159
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0158
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0156
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0157
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0157
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0157
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0157
6336/6530 [============================>.] - ETA: 0s - loss: 0.0157
6480/6530 [============================>.] - ETA: 0s - loss: 0.0157
6530/6530 [==============================] - 2s 302us/step - loss: 0.0158 - val_loss: 0.0101
Epoch 17/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0086
 144/6530 [..............................] - ETA: 2s - loss: 0.0169
 272/6530 [>.............................] - ETA: 2s - loss: 0.0157
 400/6530 [>.............................] - ETA: 2s - loss: 0.0153
 560/6530 [=>............................] - ETA: 2s - loss: 0.0155
 688/6530 [==>...........................] - ETA: 2s - loss: 0.0159
 848/6530 [==>...........................] - ETA: 2s - loss: 0.0159
1008/6530 [===>..........................] - ETA: 2s - loss: 0.0157
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0162
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0165
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0161
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0159
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0157
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0153
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0156
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0156
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0157
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0156
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0155
2992/6530 [============>.................] - ETA: 1s - loss: 0.0154
3184/6530 [=============>................] - ETA: 1s - loss: 0.0153
3392/6530 [==============>...............] - ETA: 1s - loss: 0.0155
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0156
3824/6530 [================>.............] - ETA: 0s - loss: 0.0157
4016/6530 [=================>............] - ETA: 0s - loss: 0.0157
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0157
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0156
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0158
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0157
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0156
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0155
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0154
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0153
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0154
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0154
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0154
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0155
6368/6530 [============================>.] - ETA: 0s - loss: 0.0154
6496/6530 [============================>.] - ETA: 0s - loss: 0.0155
6530/6530 [==============================] - 2s 320us/step - loss: 0.0156 - val_loss: 0.0099
Epoch 18/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0083
 192/6530 [..............................] - ETA: 1s - loss: 0.0159
 384/6530 [>.............................] - ETA: 1s - loss: 0.0149
 528/6530 [=>............................] - ETA: 1s - loss: 0.0153
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0156
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0157
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0155
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0159
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0162
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0159
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0157
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0155
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0151
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0153
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0153
2512/6530 [==========>...................] - ETA: 1s - loss: 0.0154
2688/6530 [===========>..................] - ETA: 1s - loss: 0.0151
2864/6530 [============>.................] - ETA: 1s - loss: 0.0152
3024/6530 [============>.................] - ETA: 1s - loss: 0.0151
3184/6530 [=============>................] - ETA: 1s - loss: 0.0151
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0152
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0154
3808/6530 [================>.............] - ETA: 0s - loss: 0.0153
4032/6530 [=================>............] - ETA: 0s - loss: 0.0155
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0154
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0155
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0155
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0153
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0153
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0152
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0150
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0151
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0151
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0151
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0151
6400/6530 [============================>.] - ETA: 0s - loss: 0.0151
6530/6530 [==============================] - 2s 298us/step - loss: 0.0153 - val_loss: 0.0097
Epoch 19/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0079
 176/6530 [..............................] - ETA: 2s - loss: 0.0156
 320/6530 [>.............................] - ETA: 2s - loss: 0.0144
 464/6530 [=>............................] - ETA: 2s - loss: 0.0155
 608/6530 [=>............................] - ETA: 2s - loss: 0.0155
 768/6530 [==>...........................] - ETA: 2s - loss: 0.0156
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0154
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0159
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0159
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0157
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0155
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0152
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0148
2224/6530 [=========>....................] - ETA: 1s - loss: 0.0151
2416/6530 [==========>...................] - ETA: 1s - loss: 0.0151
2608/6530 [==========>...................] - ETA: 1s - loss: 0.0151
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0149
2912/6530 [============>.................] - ETA: 1s - loss: 0.0149
3072/6530 [=============>................] - ETA: 1s - loss: 0.0149
3248/6530 [=============>................] - ETA: 1s - loss: 0.0148
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0150
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0151
3824/6530 [================>.............] - ETA: 0s - loss: 0.0152
4080/6530 [=================>............] - ETA: 0s - loss: 0.0152
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0151
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0152
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0152
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0150
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0150
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0148
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0149
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0149
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0148
6336/6530 [============================>.] - ETA: 0s - loss: 0.0148
6530/6530 [==============================] - 2s 282us/step - loss: 0.0150 - val_loss: 0.0096
Epoch 20/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0078
 224/6530 [>.............................] - ETA: 1s - loss: 0.0152
 432/6530 [>.............................] - ETA: 1s - loss: 0.0153
 656/6530 [==>...........................] - ETA: 1s - loss: 0.0154
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0149
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0157
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0156
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0153
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0150
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0146
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0146
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0147
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0148
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0146
2864/6530 [============>.................] - ETA: 0s - loss: 0.0146
3040/6530 [============>.................] - ETA: 0s - loss: 0.0146
3232/6530 [=============>................] - ETA: 0s - loss: 0.0145
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0147
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0147
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0148
3888/6530 [================>.............] - ETA: 0s - loss: 0.0149
4064/6530 [=================>............] - ETA: 0s - loss: 0.0149
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0149
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0148
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0149
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0149
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0147
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0147
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0146
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0145
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0145
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0145
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0146
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0146
6400/6530 [============================>.] - ETA: 0s - loss: 0.0145
6530/6530 [==============================] - 2s 288us/step - loss: 0.0147 - val_loss: 0.0095
Epoch 21/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0074
 176/6530 [..............................] - ETA: 2s - loss: 0.0153
 400/6530 [>.............................] - ETA: 1s - loss: 0.0145
 560/6530 [=>............................] - ETA: 1s - loss: 0.0146
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0151
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0150
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0154
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0154
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0151
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0149
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0144
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0145
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0145
2592/6530 [==========>...................] - ETA: 1s - loss: 0.0145
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0143
2992/6530 [============>.................] - ETA: 0s - loss: 0.0143
3168/6530 [=============>................] - ETA: 0s - loss: 0.0143
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0144
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0145
3728/6530 [================>.............] - ETA: 0s - loss: 0.0145
3904/6530 [================>.............] - ETA: 0s - loss: 0.0146
4064/6530 [=================>............] - ETA: 0s - loss: 0.0147
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0146
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0145
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0146
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0146
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0146
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0145
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0144
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0143
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0143
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0143
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0143
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0143
6448/6530 [============================>.] - ETA: 0s - loss: 0.0143
6530/6530 [==============================] - 2s 285us/step - loss: 0.0145 - val_loss: 0.0094
Epoch 22/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0071
 144/6530 [..............................] - ETA: 2s - loss: 0.0157
 304/6530 [>.............................] - ETA: 2s - loss: 0.0139
 464/6530 [=>............................] - ETA: 2s - loss: 0.0148
 608/6530 [=>............................] - ETA: 2s - loss: 0.0147
 768/6530 [==>...........................] - ETA: 2s - loss: 0.0147
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0145
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0151
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0151
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0149
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0148
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0144
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0141
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0142
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0142
2560/6530 [==========>...................] - ETA: 1s - loss: 0.0143
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0141
2960/6530 [============>.................] - ETA: 1s - loss: 0.0141
3152/6530 [=============>................] - ETA: 1s - loss: 0.0140
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0141
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0142
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0142
3792/6530 [================>.............] - ETA: 0s - loss: 0.0142
3936/6530 [=================>............] - ETA: 0s - loss: 0.0143
4080/6530 [=================>............] - ETA: 0s - loss: 0.0144
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0143
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0142
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0144
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0143
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0143
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0142
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0141
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0140
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0139
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0140
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0140
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0140
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0140
6432/6530 [============================>.] - ETA: 0s - loss: 0.0141
6530/6530 [==============================] - 2s 322us/step - loss: 0.0142 - val_loss: 0.0093
Epoch 23/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0069
 176/6530 [..............................] - ETA: 2s - loss: 0.0146
 336/6530 [>.............................] - ETA: 2s - loss: 0.0135
 512/6530 [=>............................] - ETA: 1s - loss: 0.0143
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0146
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0144
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0147
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0149
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0145
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0145
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0142
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0139
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0139
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0140
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0139
2544/6530 [==========>...................] - ETA: 1s - loss: 0.0140
2688/6530 [===========>..................] - ETA: 1s - loss: 0.0138
2864/6530 [============>.................] - ETA: 1s - loss: 0.0138
3024/6530 [============>.................] - ETA: 1s - loss: 0.0138
3184/6530 [=============>................] - ETA: 1s - loss: 0.0137
3344/6530 [==============>...............] - ETA: 1s - loss: 0.0138
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0140
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0140
3792/6530 [================>.............] - ETA: 0s - loss: 0.0139
3952/6530 [=================>............] - ETA: 0s - loss: 0.0141
4112/6530 [=================>............] - ETA: 0s - loss: 0.0141
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0140
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0140
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0141
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0140
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0139
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0139
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0138
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0137
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0137
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0137
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0138
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0138
6320/6530 [============================>.] - ETA: 0s - loss: 0.0138
6448/6530 [============================>.] - ETA: 0s - loss: 0.0138
6530/6530 [==============================] - 2s 328us/step - loss: 0.0139 - val_loss: 0.0092
Epoch 24/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0067
 176/6530 [..............................] - ETA: 1s - loss: 0.0143
 352/6530 [>.............................] - ETA: 1s - loss: 0.0135
 528/6530 [=>............................] - ETA: 1s - loss: 0.0139
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0143
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0139
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0145
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0145
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0144
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0142
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0139
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0136
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0136
2288/6530 [=========>....................] - ETA: 1s - loss: 0.0136
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0137
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0136
2784/6530 [===========>..................] - ETA: 1s - loss: 0.0135
2944/6530 [============>.................] - ETA: 1s - loss: 0.0135
3104/6530 [=============>................] - ETA: 1s - loss: 0.0134
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0135
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0137
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0137
3840/6530 [================>.............] - ETA: 0s - loss: 0.0138
4048/6530 [=================>............] - ETA: 0s - loss: 0.0139
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0137
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0138
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0138
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0136
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0136
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0135
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0134
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0135
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0135
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0135
6352/6530 [============================>.] - ETA: 0s - loss: 0.0135
6512/6530 [============================>.] - ETA: 0s - loss: 0.0136
6530/6530 [==============================] - 2s 292us/step - loss: 0.0136 - val_loss: 0.0091
Epoch 25/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0065
 176/6530 [..............................] - ETA: 2s - loss: 0.0140
 336/6530 [>.............................] - ETA: 2s - loss: 0.0130
 512/6530 [=>............................] - ETA: 1s - loss: 0.0138
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0141
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0137
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0140
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0142
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0141
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0139
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0136
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0133
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0133
2288/6530 [=========>....................] - ETA: 1s - loss: 0.0134
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0135
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0134
2832/6530 [============>.................] - ETA: 1s - loss: 0.0134
3008/6530 [============>.................] - ETA: 1s - loss: 0.0133
3168/6530 [=============>................] - ETA: 1s - loss: 0.0132
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0133
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0135
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0135
3744/6530 [================>.............] - ETA: 0s - loss: 0.0134
3888/6530 [================>.............] - ETA: 0s - loss: 0.0135
4032/6530 [=================>............] - ETA: 0s - loss: 0.0136
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0135
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0134
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0135
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0135
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0134
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0134
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0133
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0133
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0131
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0131
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0132
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0132
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0131
6336/6530 [============================>.] - ETA: 0s - loss: 0.0132
6512/6530 [============================>.] - ETA: 0s - loss: 0.0133
6530/6530 [==============================] - 2s 327us/step - loss: 0.0133 - val_loss: 0.0090
Epoch 26/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0064
 192/6530 [..............................] - ETA: 1s - loss: 0.0137
 368/6530 [>.............................] - ETA: 1s - loss: 0.0129
 528/6530 [=>............................] - ETA: 1s - loss: 0.0134
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0138
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0136
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0134
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0138
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0140
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0138
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0137
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0134
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0131
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0130
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0132
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0132
2608/6530 [==========>...................] - ETA: 1s - loss: 0.0132
2752/6530 [===========>..................] - ETA: 1s - loss: 0.0131
2912/6530 [============>.................] - ETA: 1s - loss: 0.0131
3072/6530 [=============>................] - ETA: 1s - loss: 0.0130
3232/6530 [=============>................] - ETA: 1s - loss: 0.0130
3424/6530 [==============>...............] - ETA: 1s - loss: 0.0131
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0132
3808/6530 [================>.............] - ETA: 0s - loss: 0.0132
4000/6530 [=================>............] - ETA: 0s - loss: 0.0133
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0132
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0132
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0132
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0132
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0131
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0131
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0130
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0129
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0129
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0129
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0129
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0129
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0129
6432/6530 [============================>.] - ETA: 0s - loss: 0.0130
6530/6530 [==============================] - 2s 325us/step - loss: 0.0131 - val_loss: 0.0089
Epoch 27/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0062
 192/6530 [..............................] - ETA: 1s - loss: 0.0134
 384/6530 [>.............................] - ETA: 1s - loss: 0.0126
 544/6530 [=>............................] - ETA: 1s - loss: 0.0133
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0136
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0132
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0136
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0135
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0136
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0134
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0133
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0131
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0127
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0129
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0129
2512/6530 [==========>...................] - ETA: 1s - loss: 0.0130
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0129
2976/6530 [============>.................] - ETA: 1s - loss: 0.0128
3200/6530 [=============>................] - ETA: 0s - loss: 0.0128
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0129
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0130
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0129
3792/6530 [================>.............] - ETA: 0s - loss: 0.0129
3968/6530 [=================>............] - ETA: 0s - loss: 0.0130
4128/6530 [=================>............] - ETA: 0s - loss: 0.0130
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0129
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0129
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0129
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0129
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0129
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0128
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0128
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0127
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0126
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0127
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0127
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0127
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0127
6400/6530 [============================>.] - ETA: 0s - loss: 0.0127
6530/6530 [==============================] - 2s 325us/step - loss: 0.0128 - val_loss: 0.0089

# training | RMSE: 0.0874, MAE: 0.0678
worker 1  xfile  [7, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 31, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.08737676939435932, 'rmse': 0.08737676939435932, 'mae': 0.06777566456095843, 'early_stop': False}
vggnet done  1
all of workers have been done
calculation finished
#2 epoch=27.0 loss={'loss': 0.14899858177545067, 'rmse': 0.14899858177545067, 'mae': 0.1201859977809567, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#1 epoch=27.0 loss={'loss': 0.14417456804135026, 'rmse': 0.14417456804135026, 'mae': 0.11004002973650756, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4302515185040906}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 59, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 5, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 91, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#4 epoch=27.0 loss={'loss': 0.6465495679172245, 'rmse': 0.6465495679172245, 'mae': 0.6051848154537434, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 51, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4985470203594613}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#0 epoch=27.0 loss={'loss': 0.10056720367223733, 'rmse': 0.10056720367223733, 'mae': 0.07656052305431331, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 71, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4206882665177676}, 'layer_2_size': 52, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 51, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#5 epoch=27.0 loss={'loss': 0.12722278355735844, 'rmse': 0.12722278355735844, 'mae': 0.09633486459576326, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22878813023045885}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4932102540157758}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1274633279440146}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#6 epoch=27.0 loss={'loss': 0.0762380935269196, 'rmse': 0.0762380935269196, 'mae': 0.05866624932945352, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13763780045412988}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38028865807259427}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 30, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 2, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#3 epoch=27.0 loss={'loss': 0.09034943811590751, 'rmse': 0.09034943811590751, 'mae': 0.07026809846161355, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 80, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 59, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 36, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 74, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4602646829994158}, 'layer_5_size': 63, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#7 epoch=27.0 loss={'loss': 0.08737676939435932, 'rmse': 0.08737676939435932, 'mae': 0.06777566456095843, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 31, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
get a list [results] of length 199
get a list [loss] of length 8
get a list [val_loss] of length 8
length of indices is [5 7 6 3 4 1 0 2]
length of indices is 8
length of T is 8
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22878813023045885}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4932102540157758}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1274633279440146}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [1, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 31, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]] 

*** 2.6666666666666665 configurations x 81.0 iterations each

8 | Thu Sep 27 18:43:48 2018 | lowest loss so far: 0.0762 (run 6)

vggnet done  2
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  84 | activation: tanh    | extras: batchnorm 
layer 2 | size:  69 | activation: sigmoid | extras: dropout - rate: 22.9% 
layer 3 | size:  20 | activation: sigmoid | extras: dropout - rate: 49.3% 
layer 4 | size:  36 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  32/6530 [..............................] - ETA: 3:12 - loss: 1.0603
 416/6530 [>.............................] - ETA: 14s - loss: 0.5653 
 768/6530 [==>...........................] - ETA: 7s - loss: 0.4947 
1184/6530 [====>.........................] - ETA: 4s - loss: 0.4397
1632/6530 [======>.......................] - ETA: 3s - loss: 0.3976{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  79 | activation: tanh    | extras: batchnorm 
layer 2 | size:  53 | activation: tanh    | extras: None 
layer 3 | size:  31 | activation: tanh    | extras: None 
layer 4 | size:  19 | activation: sigmoid | extras: None 
layer 5 | size:  43 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 6:52 - loss: 0.5175
2048/6530 [========>.....................] - ETA: 2s - loss: 0.3690
 224/6530 [>.............................] - ETA: 30s - loss: 0.5423 
2560/6530 [==========>...................] - ETA: 1s - loss: 0.3426
 464/6530 [=>............................] - ETA: 14s - loss: 0.5022
3072/6530 [=============>................] - ETA: 1s - loss: 0.3216
 720/6530 [==>...........................] - ETA: 9s - loss: 0.4163 
3648/6530 [===============>..............] - ETA: 1s - loss: 0.3053
 976/6530 [===>..........................] - ETA: 6s - loss: 0.3333
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2923
1264/6530 [====>.........................] - ETA: 5s - loss: 0.2670
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2820
1552/6530 [======>.......................] - ETA: 4s - loss: 0.2246
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2722
1840/6530 [=======>......................] - ETA: 3s - loss: 0.1964
6112/6530 [===========================>..] - ETA: 0s - loss: 0.2648
2128/6530 [========>.....................] - ETA: 2s - loss: 0.1757
2416/6530 [==========>...................] - ETA: 2s - loss: 0.1601
6530/6530 [==============================] - 2s 254us/step - loss: 0.2603 - val_loss: 0.1638
Epoch 2/81

  32/6530 [..............................] - ETA: 0s - loss: 0.2041
2704/6530 [===========>..................] - ETA: 2s - loss: 0.1481
 640/6530 [=>............................] - ETA: 0s - loss: 0.1983
2992/6530 [============>.................] - ETA: 1s - loss: 0.1376
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1953
3280/6530 [==============>...............] - ETA: 1s - loss: 0.1290
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1941
3568/6530 [===============>..............] - ETA: 1s - loss: 0.1222
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1957
3856/6530 [================>.............] - ETA: 1s - loss: 0.1162
3200/6530 [=============>................] - ETA: 0s - loss: 0.1923
4144/6530 [==================>...........] - ETA: 1s - loss: 0.1109
3872/6530 [================>.............] - ETA: 0s - loss: 0.1917
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1063
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1914
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1022
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1910
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0987
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1913
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0957
6368/6530 [============================>.] - ETA: 0s - loss: 0.1900
6530/6530 [==============================] - 1s 83us/step - loss: 0.1893 - val_loss: 0.1621
Epoch 3/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1595
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0924
 640/6530 [=>............................] - ETA: 0s - loss: 0.1814
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0899
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1813
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0873
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1794
6416/6530 [============================>.] - ETA: 0s - loss: 0.0849
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1813
3168/6530 [=============>................] - ETA: 0s - loss: 0.1783
6530/6530 [==============================] - 2s 351us/step - loss: 0.0841 - val_loss: 0.0336
Epoch 2/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0501
3808/6530 [================>.............] - ETA: 0s - loss: 0.1778
 320/6530 [>.............................] - ETA: 1s - loss: 0.0377
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1785
 608/6530 [=>............................] - ETA: 1s - loss: 0.0412
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1784
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0387
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1788
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0377
6336/6530 [============================>.] - ETA: 0s - loss: 0.1788
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0373
6530/6530 [==============================] - 1s 83us/step - loss: 0.1786 - val_loss: 0.1573
Epoch 4/81

  32/6530 [..............................] - ETA: 0s - loss: 0.2020
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0362
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1837
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0361
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1757
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0361
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1740
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0363
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1743
2864/6530 [============>.................] - ETA: 0s - loss: 0.0359
3200/6530 [=============>................] - ETA: 0s - loss: 0.1711
3152/6530 [=============>................] - ETA: 0s - loss: 0.0354
3840/6530 [================>.............] - ETA: 0s - loss: 0.1720
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0356
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1721
3728/6530 [================>.............] - ETA: 0s - loss: 0.0356
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1717
4016/6530 [=================>............] - ETA: 0s - loss: 0.0353
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1720
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0350
6432/6530 [============================>.] - ETA: 0s - loss: 0.1715
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0352
6530/6530 [==============================] - 1s 83us/step - loss: 0.1712 - val_loss: 0.1531
Epoch 5/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1529
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0350
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1720
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0349
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1682
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0347
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1664
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0345
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1667
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0342
3200/6530 [=============>................] - ETA: 0s - loss: 0.1636
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0339
3872/6530 [================>.............] - ETA: 0s - loss: 0.1654
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1658
6530/6530 [==============================] - 1s 184us/step - loss: 0.0337 - val_loss: 0.0258
Epoch 3/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0355
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1654
 304/6530 [>.............................] - ETA: 1s - loss: 0.0296
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1662
 592/6530 [=>............................] - ETA: 1s - loss: 0.0313
6496/6530 [============================>.] - ETA: 0s - loss: 0.1653
 880/6530 [===>..........................] - ETA: 0s - loss: 0.0297
6530/6530 [==============================] - 1s 82us/step - loss: 0.1652 - val_loss: 0.1468
Epoch 6/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1847
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0288
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1665
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0282
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1613
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0274
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1607
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0272
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1617
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0272
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1619
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0273
4064/6530 [=================>............] - ETA: 0s - loss: 0.1613
2976/6530 [============>.................] - ETA: 0s - loss: 0.0269
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1610
3264/6530 [=============>................] - ETA: 0s - loss: 0.0268
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1611
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0269
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1611
3856/6530 [================>.............] - ETA: 0s - loss: 0.0268
6530/6530 [==============================] - 1s 81us/step - loss: 0.1603 - val_loss: 0.1398
Epoch 7/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1785
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0268
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1605
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0266
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1527
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0267
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1536
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0263
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1542
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0262
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1530
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0259
3968/6530 [=================>............] - ETA: 0s - loss: 0.1534
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0259
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1535
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0256
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1533
6448/6530 [============================>.] - ETA: 0s - loss: 0.0255
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1539
6530/6530 [==============================] - 1s 181us/step - loss: 0.0256 - val_loss: 0.0192
Epoch 4/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0190
6530/6530 [==============================] - 1s 82us/step - loss: 0.1535 - val_loss: 0.1368
Epoch 8/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1611
 304/6530 [>.............................] - ETA: 1s - loss: 0.0228
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1534
 576/6530 [=>............................] - ETA: 1s - loss: 0.0229
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1490
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0223
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1503
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0227
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1512
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0224
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1490
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0221
3936/6530 [=================>............] - ETA: 0s - loss: 0.1498
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0218
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1495
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0219
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1500
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0220
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1501
2864/6530 [============>.................] - ETA: 0s - loss: 0.0218
6528/6530 [============================>.] - ETA: 0s - loss: 0.1499
3168/6530 [=============>................] - ETA: 0s - loss: 0.0217
6530/6530 [==============================] - 1s 82us/step - loss: 0.1499 - val_loss: 0.1312
Epoch 9/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1602
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0221
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1531
3744/6530 [================>.............] - ETA: 0s - loss: 0.0220
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1500
4016/6530 [=================>............] - ETA: 0s - loss: 0.0221
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1485
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0222
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1501
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0223
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1485
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0222
3968/6530 [=================>............] - ETA: 0s - loss: 0.1486
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0221
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1486
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0218
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1486
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1488
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0219
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0217
6530/6530 [==============================] - 1s 81us/step - loss: 0.1475 - val_loss: 0.1304
Epoch 10/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1733
6368/6530 [============================>.] - ETA: 0s - loss: 0.0216
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1442
6530/6530 [==============================] - 1s 184us/step - loss: 0.0217 - val_loss: 0.0165
Epoch 5/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0143
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1437
 304/6530 [>.............................] - ETA: 1s - loss: 0.0204
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1433
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1450
 592/6530 [=>............................] - ETA: 1s - loss: 0.0207
3264/6530 [=============>................] - ETA: 0s - loss: 0.1441
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0203
3904/6530 [================>.............] - ETA: 0s - loss: 0.1435
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0211
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1436
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0209
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1441
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0206
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1441
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0204
6528/6530 [============================>.] - ETA: 0s - loss: 0.1431
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0204
6530/6530 [==============================] - 1s 81us/step - loss: 0.1431 - val_loss: 0.1258
Epoch 11/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1566
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0205
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1424
2912/6530 [============>.................] - ETA: 0s - loss: 0.0203
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1393
3200/6530 [=============>................] - ETA: 0s - loss: 0.0202
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1400
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0206
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1414
3776/6530 [================>.............] - ETA: 0s - loss: 0.0205
3232/6530 [=============>................] - ETA: 0s - loss: 0.1404
4064/6530 [=================>............] - ETA: 0s - loss: 0.0208
3904/6530 [================>.............] - ETA: 0s - loss: 0.1416
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0206
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1416
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0208
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1421
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0207
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1422
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0206
6464/6530 [============================>.] - ETA: 0s - loss: 0.1421
6530/6530 [==============================] - 1s 83us/step - loss: 0.1420 - val_loss: 0.1245
Epoch 12/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1569
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0204
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1431
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0204
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1418
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0203
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1409
6384/6530 [============================>.] - ETA: 0s - loss: 0.0202
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1420
6530/6530 [==============================] - 1s 182us/step - loss: 0.0203 - val_loss: 0.0151
Epoch 6/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0126
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1402
 320/6530 [>.............................] - ETA: 1s - loss: 0.0190
4000/6530 [=================>............] - ETA: 0s - loss: 0.1400
 624/6530 [=>............................] - ETA: 0s - loss: 0.0196
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1401
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0194
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1399
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0203
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1402
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0201
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0198
6530/6530 [==============================] - 1s 82us/step - loss: 0.1399 - val_loss: 0.1218
Epoch 13/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1652
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0196
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1404
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0196
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1370
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0197
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1353
2928/6530 [============>.................] - ETA: 0s - loss: 0.0194
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1370
3200/6530 [=============>................] - ETA: 0s - loss: 0.0194
3232/6530 [=============>................] - ETA: 0s - loss: 0.1361
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0198
3904/6530 [================>.............] - ETA: 0s - loss: 0.1374
3792/6530 [================>.............] - ETA: 0s - loss: 0.0197
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1375
4096/6530 [=================>............] - ETA: 0s - loss: 0.0200
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1375
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0198
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1375
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0199
6464/6530 [============================>.] - ETA: 0s - loss: 0.1372
6530/6530 [==============================] - 1s 82us/step - loss: 0.1370 - val_loss: 0.1191
Epoch 14/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1482
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0198
 640/6530 [=>............................] - ETA: 0s - loss: 0.1387
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0197
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1364
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0195
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1354
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0195
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1373
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0194
3264/6530 [=============>................] - ETA: 0s - loss: 0.1346
6416/6530 [============================>.] - ETA: 0s - loss: 0.0194
3936/6530 [=================>............] - ETA: 0s - loss: 0.1352
6530/6530 [==============================] - 1s 181us/step - loss: 0.0195 - val_loss: 0.0141
Epoch 7/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0117
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1347
 304/6530 [>.............................] - ETA: 1s - loss: 0.0189
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1352
 608/6530 [=>............................] - ETA: 1s - loss: 0.0191
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1350
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0187
6530/6530 [==============================] - 1s 80us/step - loss: 0.1349 - val_loss: 0.1146
Epoch 15/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1360
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0197
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1303
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0195
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1307
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0192
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1310
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0190
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1328
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0190
3200/6530 [=============>................] - ETA: 0s - loss: 0.1306
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0191
3712/6530 [================>.............] - ETA: 0s - loss: 0.1307
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0190
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1307
3088/6530 [=============>................] - ETA: 0s - loss: 0.0187
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1311
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0189
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1310
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0191
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1318
3968/6530 [=================>............] - ETA: 0s - loss: 0.0192
6530/6530 [==============================] - 1s 86us/step - loss: 0.1317 - val_loss: 0.1139
Epoch 16/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1492
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0192
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1329
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0193
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1289
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0192
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1298
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0190
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1317
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0190
3232/6530 [=============>................] - ETA: 0s - loss: 0.1304
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0188
3904/6530 [================>.............] - ETA: 0s - loss: 0.1315
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0188
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1308
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0188
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1314
6480/6530 [============================>.] - ETA: 0s - loss: 0.0188
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1318
6530/6530 [==============================] - 1s 187us/step - loss: 0.0189 - val_loss: 0.0134
Epoch 8/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0111
6496/6530 [============================>.] - ETA: 0s - loss: 0.1321
 288/6530 [>.............................] - ETA: 1s - loss: 0.0188
6530/6530 [==============================] - 1s 82us/step - loss: 0.1319 - val_loss: 0.1145
Epoch 17/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1376
 560/6530 [=>............................] - ETA: 1s - loss: 0.0181
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1313
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0184
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1295
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0194
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1311
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0195
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1314
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0188
2976/6530 [============>.................] - ETA: 0s - loss: 0.1308
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0187
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1311
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0185
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1309
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0186
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1311
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0183
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1309
3008/6530 [============>.................] - ETA: 0s - loss: 0.0183
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1311
6530/6530 [==============================] - 1s 86us/step - loss: 0.1309 - val_loss: 0.1125

3296/6530 [==============>...............] - ETA: 0s - loss: 0.0183Epoch 18/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1227
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1287
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0187
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1266
3872/6530 [================>.............] - ETA: 0s - loss: 0.0187
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1272
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0187
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1276
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0186
3200/6530 [=============>................] - ETA: 0s - loss: 0.1266
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0187
3840/6530 [================>.............] - ETA: 0s - loss: 0.1277
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0185
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1271
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0184
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1282
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0182
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1281
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0184
6368/6530 [============================>.] - ETA: 0s - loss: 0.1278
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0182
6530/6530 [==============================] - 1s 84us/step - loss: 0.1277 - val_loss: 0.1119
Epoch 19/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1287
6416/6530 [============================>.] - ETA: 0s - loss: 0.0182
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1278
6530/6530 [==============================] - 1s 190us/step - loss: 0.0184 - val_loss: 0.0129
Epoch 9/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0107
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1265
 304/6530 [>.............................] - ETA: 1s - loss: 0.0181
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1262
 592/6530 [=>............................] - ETA: 1s - loss: 0.0182
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1271
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0179
3264/6530 [=============>................] - ETA: 0s - loss: 0.1261
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0188
3904/6530 [================>.............] - ETA: 0s - loss: 0.1267
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0189
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1266
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0184
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1276
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0180
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1274
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0180
6464/6530 [============================>.] - ETA: 0s - loss: 0.1268
6530/6530 [==============================] - 1s 82us/step - loss: 0.1267 - val_loss: 0.1087
Epoch 20/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1554
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0182
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1258
2864/6530 [============>.................] - ETA: 0s - loss: 0.0180
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1241
3152/6530 [=============>................] - ETA: 0s - loss: 0.0179
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1235
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0181
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1253
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0181
3264/6530 [=============>................] - ETA: 0s - loss: 0.1237
4000/6530 [=================>............] - ETA: 0s - loss: 0.0182
3904/6530 [================>.............] - ETA: 0s - loss: 0.1243
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0182
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1243
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0183
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1248
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0182
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1251
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0180
6464/6530 [============================>.] - ETA: 0s - loss: 0.1252
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0178
6530/6530 [==============================] - 1s 83us/step - loss: 0.1252 - val_loss: 0.1072
Epoch 21/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1405
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0179
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1279
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0178
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1229
6368/6530 [============================>.] - ETA: 0s - loss: 0.0177
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1213
6530/6530 [==============================] - 1s 184us/step - loss: 0.0179 - val_loss: 0.0125
Epoch 10/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0105
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1235
 304/6530 [>.............................] - ETA: 1s - loss: 0.0177
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1228
 592/6530 [=>............................] - ETA: 1s - loss: 0.0179
3872/6530 [================>.............] - ETA: 0s - loss: 0.1232
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0176
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1228
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0185
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1235
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0186
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1235
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0179
6432/6530 [============================>.] - ETA: 0s - loss: 0.1239
6530/6530 [==============================] - 1s 83us/step - loss: 0.1238 - val_loss: 0.1075
Epoch 22/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1371
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0177
 640/6530 [=>............................] - ETA: 0s - loss: 0.1233
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0177
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1234
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0177
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1225
2912/6530 [============>.................] - ETA: 0s - loss: 0.0176
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1249
3200/6530 [=============>................] - ETA: 0s - loss: 0.0175
3232/6530 [=============>................] - ETA: 0s - loss: 0.1239
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0178
3904/6530 [================>.............] - ETA: 0s - loss: 0.1245
3776/6530 [================>.............] - ETA: 0s - loss: 0.0177
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1245
4048/6530 [=================>............] - ETA: 0s - loss: 0.0179
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1247
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0178
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1248
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0179
6464/6530 [============================>.] - ETA: 0s - loss: 0.1250
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0178
6530/6530 [==============================] - 1s 84us/step - loss: 0.1248 - val_loss: 0.1063
Epoch 23/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1288
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0176
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1217
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0175
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1197
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0174
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1205
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0174
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1219
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0174
3104/6530 [=============>................] - ETA: 0s - loss: 0.1215
6496/6530 [============================>.] - ETA: 0s - loss: 0.0174
3744/6530 [================>.............] - ETA: 0s - loss: 0.1211
6530/6530 [==============================] - 1s 188us/step - loss: 0.0175 - val_loss: 0.0123
Epoch 11/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0103
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1215
 288/6530 [>.............................] - ETA: 1s - loss: 0.0178
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1224
 560/6530 [=>............................] - ETA: 1s - loss: 0.0171
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1227
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0175
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1232
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0183
6530/6530 [==============================] - 1s 85us/step - loss: 0.1228 - val_loss: 0.1054
Epoch 24/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1374
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0182
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1268
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0177
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1244
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0172
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1227
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0172
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1225
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0174
3232/6530 [=============>................] - ETA: 0s - loss: 0.1222
2880/6530 [============>.................] - ETA: 0s - loss: 0.0171
3904/6530 [================>.............] - ETA: 0s - loss: 0.1226
3168/6530 [=============>................] - ETA: 0s - loss: 0.0171
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1227
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0174
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1231
3744/6530 [================>.............] - ETA: 0s - loss: 0.0173
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1230
4016/6530 [=================>............] - ETA: 0s - loss: 0.0174
6368/6530 [============================>.] - ETA: 0s - loss: 0.1237
6530/6530 [==============================] - 1s 84us/step - loss: 0.1236 - val_loss: 0.1041
Epoch 25/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1105
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0174
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1212
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0175
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1205
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0174
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1195
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0172
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1216
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0171
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1223
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0171
3936/6530 [=================>............] - ETA: 0s - loss: 0.1217
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0170
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1212
6320/6530 [============================>.] - ETA: 0s - loss: 0.0170
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1216
6530/6530 [==============================] - 1s 185us/step - loss: 0.0172 - val_loss: 0.0120
Epoch 12/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0101
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1223
 304/6530 [>.............................] - ETA: 1s - loss: 0.0171
6368/6530 [============================>.] - ETA: 0s - loss: 0.1224
 592/6530 [=>............................] - ETA: 1s - loss: 0.0172
6530/6530 [==============================] - 1s 83us/step - loss: 0.1221 - val_loss: 0.1046
Epoch 26/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1384
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0169
 640/6530 [=>............................] - ETA: 0s - loss: 0.1211
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0178
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1190
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0178
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1178
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0172
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1201
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0168
3136/6530 [=============>................] - ETA: 0s - loss: 0.1190
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0169
3744/6530 [================>.............] - ETA: 0s - loss: 0.1200
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0171
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1196
2864/6530 [============>.................] - ETA: 0s - loss: 0.0168
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1197
3152/6530 [=============>................] - ETA: 0s - loss: 0.0168
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1194
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0170
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1201
6530/6530 [==============================] - 1s 85us/step - loss: 0.1198 - val_loss: 0.1040
Epoch 27/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1326
3728/6530 [================>.............] - ETA: 0s - loss: 0.0170
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1196
4016/6530 [=================>............] - ETA: 0s - loss: 0.0171
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1184
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0170
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1180
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0172
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1196
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0171
3200/6530 [=============>................] - ETA: 0s - loss: 0.1202
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0169
3840/6530 [================>.............] - ETA: 0s - loss: 0.1207
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0167
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1206
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0167
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1205
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0167
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1201
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0167
6400/6530 [============================>.] - ETA: 0s - loss: 0.1205
6530/6530 [==============================] - 1s 83us/step - loss: 0.1203 - val_loss: 0.1039
Epoch 28/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1226
6530/6530 [==============================] - 1s 185us/step - loss: 0.0168 - val_loss: 0.0118
Epoch 13/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0098
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1215
 320/6530 [>.............................] - ETA: 1s - loss: 0.0163
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1205
 608/6530 [=>............................] - ETA: 1s - loss: 0.0171
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1191
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0169
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1209
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0174
3232/6530 [=============>................] - ETA: 0s - loss: 0.1206
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0173
3872/6530 [================>.............] - ETA: 0s - loss: 0.1210
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0169
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1206
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0166
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1207
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0166
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1206
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0166
6496/6530 [============================>.] - ETA: 0s - loss: 0.1203
2928/6530 [============>.................] - ETA: 0s - loss: 0.0164
6530/6530 [==============================] - 1s 82us/step - loss: 0.1202 - val_loss: 0.1022
Epoch 29/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1074
3232/6530 [=============>................] - ETA: 0s - loss: 0.0164
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1122
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0167
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1160
3824/6530 [================>.............] - ETA: 0s - loss: 0.0168
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1182
4096/6530 [=================>............] - ETA: 0s - loss: 0.0168
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1196
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0167
3168/6530 [=============>................] - ETA: 0s - loss: 0.1198
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0168
3776/6530 [================>.............] - ETA: 0s - loss: 0.1201
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0167
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1201
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0166
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1206
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0164
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1199
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0164
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1207
6530/6530 [==============================] - 1s 85us/step - loss: 0.1202 - val_loss: 0.1052
Epoch 30/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1339
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0164
 608/6530 [=>............................] - ETA: 0s - loss: 0.1218
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0164
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1199
6530/6530 [==============================] - 1s 186us/step - loss: 0.0165 - val_loss: 0.0116
Epoch 14/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0096
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1182
 288/6530 [>.............................] - ETA: 1s - loss: 0.0170
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1187
 576/6530 [=>............................] - ETA: 1s - loss: 0.0165
3200/6530 [=============>................] - ETA: 0s - loss: 0.1177
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0164
3840/6530 [================>.............] - ETA: 0s - loss: 0.1183
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0172
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1181
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0172
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1183
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0166
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1183
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0161
6400/6530 [============================>.] - ETA: 0s - loss: 0.1188
6530/6530 [==============================] - 1s 83us/step - loss: 0.1187 - val_loss: 0.1026
Epoch 31/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1303
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0162
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1164
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0164
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1161
2864/6530 [============>.................] - ETA: 0s - loss: 0.0162
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1166
3152/6530 [=============>................] - ETA: 0s - loss: 0.0162
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1169
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0163
3136/6530 [=============>................] - ETA: 0s - loss: 0.1168
3728/6530 [================>.............] - ETA: 0s - loss: 0.0164
3776/6530 [================>.............] - ETA: 0s - loss: 0.1174
4016/6530 [=================>............] - ETA: 0s - loss: 0.0164
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1176
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0164
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1176
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0165
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1176
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0164
6368/6530 [============================>.] - ETA: 0s - loss: 0.1182
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0162
6530/6530 [==============================] - 1s 83us/step - loss: 0.1179 - val_loss: 0.1036
Epoch 32/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1386
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0161
 608/6530 [=>............................] - ETA: 0s - loss: 0.1247
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0161
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1205
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0161
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1183
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0161
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1189
6512/6530 [============================>.] - ETA: 0s - loss: 0.0162
3104/6530 [=============>................] - ETA: 0s - loss: 0.1179
6530/6530 [==============================] - 1s 188us/step - loss: 0.0162 - val_loss: 0.0114
Epoch 15/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0095
3712/6530 [================>.............] - ETA: 0s - loss: 0.1176
 288/6530 [>.............................] - ETA: 1s - loss: 0.0167
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1163
 576/6530 [=>............................] - ETA: 1s - loss: 0.0163
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1163
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0163
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1163
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0170
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1168
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0169
6530/6530 [==============================] - 1s 86us/step - loss: 0.1165 - val_loss: 0.1023
Epoch 33/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1139
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0164
 608/6530 [=>............................] - ETA: 0s - loss: 0.1184
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0159
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1156
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0161
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1157
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0161
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1171
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0159
3104/6530 [=============>................] - ETA: 0s - loss: 0.1171
3072/6530 [=============>................] - ETA: 0s - loss: 0.0159
3744/6530 [================>.............] - ETA: 0s - loss: 0.1174
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0160
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1166
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0161
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1172
3920/6530 [=================>............] - ETA: 0s - loss: 0.0162
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1162
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0162
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1169
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0161
6530/6530 [==============================] - 1s 86us/step - loss: 0.1168 - val_loss: 0.1020
Epoch 34/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1267
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0162
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1133
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0160
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1169
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0159
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1155
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0157
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1166
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0158
3200/6530 [=============>................] - ETA: 0s - loss: 0.1158
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0158
3872/6530 [================>.............] - ETA: 0s - loss: 0.1168
6496/6530 [============================>.] - ETA: 0s - loss: 0.0158
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1163
6530/6530 [==============================] - 1s 187us/step - loss: 0.0159 - val_loss: 0.0112
Epoch 16/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0095
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1168
 288/6530 [>.............................] - ETA: 1s - loss: 0.0165
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1160
 560/6530 [=>............................] - ETA: 1s - loss: 0.0158
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1168
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0161
6530/6530 [==============================] - 1s 87us/step - loss: 0.1164 - val_loss: 0.0984
Epoch 35/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1313
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0167
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1159
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0166
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1140
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0161
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1153
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0156
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1161
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0156
3200/6530 [=============>................] - ETA: 0s - loss: 0.1155
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0158
3808/6530 [================>.............] - ETA: 0s - loss: 0.1165
2848/6530 [============>.................] - ETA: 0s - loss: 0.0157
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1161
3152/6530 [=============>................] - ETA: 0s - loss: 0.0156
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1166
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0159
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1166
3728/6530 [================>.............] - ETA: 0s - loss: 0.0158
6400/6530 [============================>.] - ETA: 0s - loss: 0.1169
4032/6530 [=================>............] - ETA: 0s - loss: 0.0160
6530/6530 [==============================] - 1s 83us/step - loss: 0.1168 - val_loss: 0.1024
Epoch 36/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1429
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0159
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1153
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0159
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1132
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0158
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1129
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0157
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1138
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0155
3072/6530 [=============>................] - ETA: 0s - loss: 0.1150
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0155
3712/6530 [================>.............] - ETA: 0s - loss: 0.1151
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0155
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1151
6336/6530 [============================>.] - ETA: 0s - loss: 0.0155
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1152
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1145
6530/6530 [==============================] - 1s 185us/step - loss: 0.0157 - val_loss: 0.0111
Epoch 17/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0094
6336/6530 [============================>.] - ETA: 0s - loss: 0.1153
 304/6530 [>.............................] - ETA: 1s - loss: 0.0158
6530/6530 [==============================] - 1s 84us/step - loss: 0.1151 - val_loss: 0.0996
Epoch 37/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0985
 592/6530 [=>............................] - ETA: 1s - loss: 0.0159
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1110
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0156
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1117
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0163
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1117
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0163
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1142
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0157
3264/6530 [=============>................] - ETA: 0s - loss: 0.1140
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0152
3936/6530 [=================>............] - ETA: 0s - loss: 0.1143
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0154
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1140
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0155
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1140
2864/6530 [============>.................] - ETA: 0s - loss: 0.0154
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1142
3152/6530 [=============>................] - ETA: 0s - loss: 0.0154
6464/6530 [============================>.] - ETA: 0s - loss: 0.1144
6530/6530 [==============================] - 1s 82us/step - loss: 0.1144 - val_loss: 0.0979

3424/6530 [==============>...............] - ETA: 0s - loss: 0.0155Epoch 38/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1433
3712/6530 [================>.............] - ETA: 0s - loss: 0.0156
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1136
4016/6530 [=================>............] - ETA: 0s - loss: 0.0156
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1114
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0156
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1126
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0157
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1141
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0155
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1142
3936/6530 [=================>............] - ETA: 0s - loss: 0.1152
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0154
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1149
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0152
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1147
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0153
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1150
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0152
6384/6530 [============================>.] - ETA: 0s - loss: 0.0152
6530/6530 [==============================] - 1s 81us/step - loss: 0.1148 - val_loss: 0.0979
Epoch 39/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1189
6530/6530 [==============================] - 1s 182us/step - loss: 0.0154 - val_loss: 0.0109
Epoch 18/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0092
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1136
 304/6530 [>.............................] - ETA: 1s - loss: 0.0156
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1140
 608/6530 [=>............................] - ETA: 1s - loss: 0.0159
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1142
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0153
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1150
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0160
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1146
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0158
4032/6530 [=================>............] - ETA: 0s - loss: 0.1143
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0154
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1137
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0149
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1141
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0152
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1142
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0151
6530/6530 [==============================] - 1s 80us/step - loss: 0.1143 - val_loss: 0.0976
Epoch 40/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1287
2976/6530 [============>.................] - ETA: 0s - loss: 0.0151
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1155
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0151
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1126
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0154
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1120
3872/6530 [================>.............] - ETA: 0s - loss: 0.0154
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1138
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0153
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1135
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0153
4000/6530 [=================>............] - ETA: 0s - loss: 0.1131
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0154
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1129
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0151
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1129
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0151
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1129
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0149
6530/6530 [==============================] - 1s 80us/step - loss: 0.1132 - val_loss: 0.0997
Epoch 41/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1313
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0150
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1168
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0150
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1128
6512/6530 [============================>.] - ETA: 0s - loss: 0.0151
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1123
6530/6530 [==============================] - 1s 179us/step - loss: 0.0151 - val_loss: 0.0108
Epoch 19/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0091
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1131
 320/6530 [>.............................] - ETA: 1s - loss: 0.0150
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1126
 624/6530 [=>............................] - ETA: 1s - loss: 0.0155
3968/6530 [=================>............] - ETA: 0s - loss: 0.1128
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0154
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1128
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0157
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1126
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0155
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1128
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0151
6530/6530 [==============================] - 1s 81us/step - loss: 0.1132 - val_loss: 0.0986
Epoch 42/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1278
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0147
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0149
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1121
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0149
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1113
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1093
2976/6530 [============>.................] - ETA: 0s - loss: 0.0148
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1103
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0149
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1107
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0152
3936/6530 [=================>............] - ETA: 0s - loss: 0.1113
3872/6530 [================>.............] - ETA: 0s - loss: 0.0151
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1123
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0151
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1122
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0150
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1120
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0151
6528/6530 [============================>.] - ETA: 0s - loss: 0.1123
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0148
6530/6530 [==============================] - 1s 81us/step - loss: 0.1123 - val_loss: 0.0968
Epoch 43/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1156
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0148
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1126
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0146
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1120
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0147
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1101
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0148
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1110
6512/6530 [============================>.] - ETA: 0s - loss: 0.0148
3264/6530 [=============>................] - ETA: 0s - loss: 0.1102
6530/6530 [==============================] - 1s 180us/step - loss: 0.0148 - val_loss: 0.0106
Epoch 20/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0089
3904/6530 [================>.............] - ETA: 0s - loss: 0.1108
 304/6530 [>.............................] - ETA: 1s - loss: 0.0151
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1113
 576/6530 [=>............................] - ETA: 1s - loss: 0.0152
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1117
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0149
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1118
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0154
6400/6530 [============================>.] - ETA: 0s - loss: 0.1125
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0154
6530/6530 [==============================] - 1s 83us/step - loss: 0.1125 - val_loss: 0.0966
Epoch 44/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1052
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0149
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1153
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0143
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1118
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0146
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1113
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0147
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1124
2880/6530 [============>.................] - ETA: 0s - loss: 0.0145
3200/6530 [=============>................] - ETA: 0s - loss: 0.1119
3152/6530 [=============>................] - ETA: 0s - loss: 0.0146
3840/6530 [================>.............] - ETA: 0s - loss: 0.1124
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0148
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1120
3744/6530 [================>.............] - ETA: 0s - loss: 0.0148
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1118
4048/6530 [=================>............] - ETA: 0s - loss: 0.0149
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1119
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0147
6464/6530 [============================>.] - ETA: 0s - loss: 0.1121
6530/6530 [==============================] - 1s 83us/step - loss: 0.1120 - val_loss: 0.0966
Epoch 45/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1221
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0148
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1115
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0146
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1099
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0145
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1105
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0144
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1119
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0144
3200/6530 [=============>................] - ETA: 0s - loss: 0.1112
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0144
3776/6530 [================>.............] - ETA: 0s - loss: 0.1114
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0144
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1115
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1114
6530/6530 [==============================] - 1s 186us/step - loss: 0.0146 - val_loss: 0.0105
Epoch 21/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0087
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1106
 304/6530 [>.............................] - ETA: 1s - loss: 0.0149
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1118
 592/6530 [=>............................] - ETA: 1s - loss: 0.0150
6530/6530 [==============================] - 1s 85us/step - loss: 0.1114 - val_loss: 0.0984
Epoch 46/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1441
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0145
 640/6530 [=>............................] - ETA: 0s - loss: 0.1127
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0151
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1095
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0150
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1080
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0145
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1103
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0140
3136/6530 [=============>................] - ETA: 0s - loss: 0.1093
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0143
3808/6530 [================>.............] - ETA: 0s - loss: 0.1095
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0144
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1087
2912/6530 [============>.................] - ETA: 0s - loss: 0.0143
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1094
3184/6530 [=============>................] - ETA: 0s - loss: 0.0143
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1095
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0146
6336/6530 [============================>.] - ETA: 0s - loss: 0.1100
3744/6530 [================>.............] - ETA: 0s - loss: 0.0145
6530/6530 [==============================] - 1s 84us/step - loss: 0.1098 - val_loss: 0.0982
Epoch 47/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1329
4032/6530 [=================>............] - ETA: 0s - loss: 0.0147
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1156
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0145
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1121
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0145
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1114
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0144
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1123
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0142
3200/6530 [=============>................] - ETA: 0s - loss: 0.1115
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0141
3808/6530 [================>.............] - ETA: 0s - loss: 0.1119
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0141
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1111
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0141
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1113
6336/6530 [============================>.] - ETA: 0s - loss: 0.0141
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1116
6530/6530 [==============================] - 1s 184us/step - loss: 0.0143 - val_loss: 0.0103

6368/6530 [============================>.] - ETA: 0s - loss: 0.1120Epoch 22/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0084
6530/6530 [==============================] - 1s 84us/step - loss: 0.1116 - val_loss: 0.0957
Epoch 48/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1312
 304/6530 [>.............................] - ETA: 1s - loss: 0.0146
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1138
 592/6530 [=>............................] - ETA: 1s - loss: 0.0147
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1113
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0142
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1104
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0148
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1113
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0147
3232/6530 [=============>................] - ETA: 0s - loss: 0.1105
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0143
3904/6530 [================>.............] - ETA: 0s - loss: 0.1111
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0139
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1111
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0140
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1114
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0141
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1116
2928/6530 [============>.................] - ETA: 0s - loss: 0.0140
6496/6530 [============================>.] - ETA: 0s - loss: 0.1115
3216/6530 [=============>................] - ETA: 0s - loss: 0.0141
6530/6530 [==============================] - 1s 82us/step - loss: 0.1114 - val_loss: 0.0978
Epoch 49/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1142
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0143
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1132
3776/6530 [================>.............] - ETA: 0s - loss: 0.0142
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1123
4064/6530 [=================>............] - ETA: 0s - loss: 0.0143
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1113
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0141
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1119
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0142
3200/6530 [=============>................] - ETA: 0s - loss: 0.1110
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0141
3840/6530 [================>.............] - ETA: 0s - loss: 0.1109
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0140
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1108
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0138
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1109
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0138
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1107
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0138
6432/6530 [============================>.] - ETA: 0s - loss: 0.1110
6530/6530 [==============================] - 1s 83us/step - loss: 0.1111 - val_loss: 0.0949
Epoch 50/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1363
6384/6530 [============================>.] - ETA: 0s - loss: 0.0138
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1144
6530/6530 [==============================] - 1s 182us/step - loss: 0.0140 - val_loss: 0.0102
Epoch 23/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0082
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1097
 320/6530 [>.............................] - ETA: 1s - loss: 0.0140
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1090
 608/6530 [=>............................] - ETA: 1s - loss: 0.0145
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1095
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0142
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1091
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0145
3936/6530 [=================>............] - ETA: 0s - loss: 0.1093
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0144
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1093
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0140
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1096
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0136
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1100
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0137
6530/6530 [==============================] - 1s 81us/step - loss: 0.1100 - val_loss: 0.0962
Epoch 51/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1250
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0138
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1104
2960/6530 [============>.................] - ETA: 0s - loss: 0.0137
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1071
3264/6530 [=============>................] - ETA: 0s - loss: 0.0138
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1060
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0141
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1079
3856/6530 [================>.............] - ETA: 0s - loss: 0.0141
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1078
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0140
4032/6530 [=================>............] - ETA: 0s - loss: 0.1081
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0139
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1077
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0139
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1070
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0137
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1074
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0137
6530/6530 [==============================] - 1s 81us/step - loss: 0.1078 - val_loss: 0.0960
Epoch 52/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1210
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0135
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1102
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0135
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1091
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0136
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1084
6528/6530 [============================>.] - ETA: 0s - loss: 0.0137
6530/6530 [==============================] - 1s 179us/step - loss: 0.0137 - val_loss: 0.0100
Epoch 24/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0080
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1093
 320/6530 [>.............................] - ETA: 1s - loss: 0.0138
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1099
 624/6530 [=>............................] - ETA: 1s - loss: 0.0141
3904/6530 [================>.............] - ETA: 0s - loss: 0.1110
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0139
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1106
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0142
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1100
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0140
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1097
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0137
6496/6530 [============================>.] - ETA: 0s - loss: 0.1099
6530/6530 [==============================] - 1s 82us/step - loss: 0.1098 - val_loss: 0.0946
Epoch 53/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1138
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0133
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1097
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0135
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1080
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0135
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1078
2960/6530 [============>.................] - ETA: 0s - loss: 0.0135
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1088
3248/6530 [=============>................] - ETA: 0s - loss: 0.0135
3232/6530 [=============>................] - ETA: 0s - loss: 0.1085
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0138
3808/6530 [================>.............] - ETA: 0s - loss: 0.1091
3824/6530 [================>.............] - ETA: 0s - loss: 0.0138
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1086
4128/6530 [=================>............] - ETA: 0s - loss: 0.0138
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1090
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0136
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1085
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0136
6464/6530 [============================>.] - ETA: 0s - loss: 0.1091
6530/6530 [==============================] - 1s 82us/step - loss: 0.1091 - val_loss: 0.0974
Epoch 54/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1205
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0134
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1066
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0134
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1070
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0132
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0133
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1079
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0133
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1090
6512/6530 [============================>.] - ETA: 0s - loss: 0.0134
3264/6530 [=============>................] - ETA: 0s - loss: 0.1082
6530/6530 [==============================] - 1s 180us/step - loss: 0.0134 - val_loss: 0.0099
Epoch 25/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0078
3840/6530 [================>.............] - ETA: 0s - loss: 0.1090
 288/6530 [>.............................] - ETA: 1s - loss: 0.0142
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1086
 560/6530 [=>............................] - ETA: 1s - loss: 0.0137
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1086
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0135
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1086
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0139
6400/6530 [============================>.] - ETA: 0s - loss: 0.1090
6530/6530 [==============================] - 1s 83us/step - loss: 0.1090 - val_loss: 0.0975
Epoch 55/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1350
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0138
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1056
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0134
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1066
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0130
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1058
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0132
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1077
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0133
3264/6530 [=============>................] - ETA: 0s - loss: 0.1077
2944/6530 [============>.................] - ETA: 0s - loss: 0.0132
3840/6530 [================>.............] - ETA: 0s - loss: 0.1081
3200/6530 [=============>................] - ETA: 0s - loss: 0.0133
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1078
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0135
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1073
3792/6530 [================>.............] - ETA: 0s - loss: 0.0134
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1073
4096/6530 [=================>............] - ETA: 0s - loss: 0.0135
6496/6530 [============================>.] - ETA: 0s - loss: 0.1076
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0133
6530/6530 [==============================] - 1s 83us/step - loss: 0.1076 - val_loss: 0.0941
Epoch 56/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1099
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0134
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1048
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0132
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1053
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0131
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1060
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0129
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1070
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0130
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1078
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0130
3936/6530 [=================>............] - ETA: 0s - loss: 0.1081
6464/6530 [============================>.] - ETA: 0s - loss: 0.0131
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1082
6530/6530 [==============================] - 1s 181us/step - loss: 0.0132 - val_loss: 0.0098
Epoch 26/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0075
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1082
 288/6530 [>.............................] - ETA: 1s - loss: 0.0139
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1083
 592/6530 [=>............................] - ETA: 1s - loss: 0.0136
6530/6530 [==============================] - 1s 81us/step - loss: 0.1085 - val_loss: 0.0971
Epoch 57/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1328
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0132
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1069
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0136
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1072
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0137
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1072
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0132
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1086
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0128
3168/6530 [=============>................] - ETA: 0s - loss: 0.1078
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0130
3840/6530 [================>.............] - ETA: 0s - loss: 0.1090
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0131
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1085
2912/6530 [============>.................] - ETA: 0s - loss: 0.0130
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1079
3216/6530 [=============>................] - ETA: 0s - loss: 0.0130
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1079
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0133
6464/6530 [============================>.] - ETA: 0s - loss: 0.1078
3776/6530 [================>.............] - ETA: 0s - loss: 0.0132
6530/6530 [==============================] - 1s 82us/step - loss: 0.1077 - val_loss: 0.0933
Epoch 58/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1116
4080/6530 [=================>............] - ETA: 0s - loss: 0.0132
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1092
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0131
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1081
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1067
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0131
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1069
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0129
3232/6530 [=============>................] - ETA: 0s - loss: 0.1069
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0128
3904/6530 [================>.............] - ETA: 0s - loss: 0.1068
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0126
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1068
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0128
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1066
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0128
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1067
6480/6530 [============================>.] - ETA: 0s - loss: 0.0128
6530/6530 [==============================] - 1s 181us/step - loss: 0.0129 - val_loss: 0.0096

6464/6530 [============================>.] - ETA: 0s - loss: 0.1073Epoch 27/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0073
6530/6530 [==============================] - 1s 81us/step - loss: 0.1073 - val_loss: 0.0980
Epoch 59/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1189
 304/6530 [>.............................] - ETA: 1s - loss: 0.0132
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1079
 592/6530 [=>............................] - ETA: 1s - loss: 0.0133
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1073
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0129
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1066
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0134
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1073
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0133
3232/6530 [=============>................] - ETA: 0s - loss: 0.1066
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0130
3904/6530 [================>.............] - ETA: 0s - loss: 0.1068
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0125
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1068
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0128
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1069
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0128
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1068
2928/6530 [============>.................] - ETA: 0s - loss: 0.0127
6496/6530 [============================>.] - ETA: 0s - loss: 0.1072
6530/6530 [==============================] - 1s 82us/step - loss: 0.1071 - val_loss: 0.0958
Epoch 60/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1065
3200/6530 [=============>................] - ETA: 0s - loss: 0.0128
 640/6530 [=>............................] - ETA: 0s - loss: 0.1094
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0130
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1066
3776/6530 [================>.............] - ETA: 0s - loss: 0.0129
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1052
4064/6530 [=================>............] - ETA: 0s - loss: 0.0130
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1062
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0128
3040/6530 [============>.................] - ETA: 0s - loss: 0.1058
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0128
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1070
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0127
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1064
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0126
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1060
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0124
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1061
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0125
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1072
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0125
6530/6530 [==============================] - 1s 85us/step - loss: 0.1069 - val_loss: 0.0988
Epoch 61/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1212
6336/6530 [============================>.] - ETA: 0s - loss: 0.0125
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1083
6530/6530 [==============================] - 1s 184us/step - loss: 0.0126 - val_loss: 0.0095
Epoch 28/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0071
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1063
 304/6530 [>.............................] - ETA: 1s - loss: 0.0130
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1054
 592/6530 [=>............................] - ETA: 1s - loss: 0.0131
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1060
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0127
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1052
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0131
3936/6530 [=================>............] - ETA: 0s - loss: 0.1057
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0130
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1052
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0127
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1049
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0123
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1054
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0125
6530/6530 [==============================] - 1s 81us/step - loss: 0.1057 - val_loss: 0.0944
Epoch 62/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1251
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0126
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1085
2944/6530 [============>.................] - ETA: 0s - loss: 0.0125
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1084
3232/6530 [=============>................] - ETA: 0s - loss: 0.0125
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1062
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0127
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1076
3840/6530 [================>.............] - ETA: 0s - loss: 0.0128
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1064
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0127
3936/6530 [=================>............] - ETA: 0s - loss: 0.1060
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0126
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1057
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0125
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1052
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0124
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1052
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0123
6464/6530 [============================>.] - ETA: 0s - loss: 0.1057
6530/6530 [==============================] - 1s 83us/step - loss: 0.1056 - val_loss: 0.0986

5584/6530 [========================>.....] - ETA: 0s - loss: 0.0121
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0122
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0122
# training | RMSE: 0.1259, MAE: 0.0956
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22878813023045885}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4932102540157758}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1274633279440146}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.12588820408791238, 'rmse': 0.12588820408791238, 'mae': 0.0955917510952807, 'early_stop': True}
vggnet done  0

6496/6530 [============================>.] - ETA: 0s - loss: 0.0123
6530/6530 [==============================] - 1s 181us/step - loss: 0.0124 - val_loss: 0.0094
Epoch 29/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0070
 336/6530 [>.............................] - ETA: 1s - loss: 0.0124
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0131
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0125
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0130
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0127
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0124
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0121
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0124
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0124
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0123
3024/6530 [============>.................] - ETA: 0s - loss: 0.0123
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0124
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0125
3888/6530 [================>.............] - ETA: 0s - loss: 0.0125
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0124
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0123
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0123
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0121
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0121
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0119
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0120
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0120
6432/6530 [============================>.] - ETA: 0s - loss: 0.0120
6530/6530 [==============================] - 1s 191us/step - loss: 0.0121 - val_loss: 0.0093
Epoch 30/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0069
 256/6530 [>.............................] - ETA: 1s - loss: 0.0134
 432/6530 [>.............................] - ETA: 1s - loss: 0.0130
 608/6530 [=>............................] - ETA: 1s - loss: 0.0127
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0127
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0123
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0127
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0127
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0126
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0124
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0122
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0120
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0118
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0122
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0121
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0122
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0121
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0121
2960/6530 [============>.................] - ETA: 1s - loss: 0.0121
3120/6530 [=============>................] - ETA: 1s - loss: 0.0121
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0121
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0123
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0122
3872/6530 [================>.............] - ETA: 0s - loss: 0.0122
4048/6530 [=================>............] - ETA: 0s - loss: 0.0122
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0122
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0121
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0121
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0120
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0120
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0119
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0118
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0118
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0116
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0117
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0118
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0117
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0118
6400/6530 [============================>.] - ETA: 0s - loss: 0.0117
6530/6530 [==============================] - 2s 324us/step - loss: 0.0119 - val_loss: 0.0093
Epoch 31/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0068
 192/6530 [..............................] - ETA: 1s - loss: 0.0131
 368/6530 [>.............................] - ETA: 1s - loss: 0.0120
 544/6530 [=>............................] - ETA: 1s - loss: 0.0123
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0127
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0122
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0119
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0124
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0124
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0122
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0120
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0119
2016/6530 [========>.....................] - ETA: 1s - loss: 0.0116
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0118
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0119
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0119
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0119
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0119
2896/6530 [============>.................] - ETA: 1s - loss: 0.0119
3024/6530 [============>.................] - ETA: 1s - loss: 0.0119
3152/6530 [=============>................] - ETA: 1s - loss: 0.0119
3280/6530 [==============>...............] - ETA: 1s - loss: 0.0119
3408/6530 [==============>...............] - ETA: 1s - loss: 0.0120
3536/6530 [===============>..............] - ETA: 1s - loss: 0.0120
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0120
3824/6530 [================>.............] - ETA: 0s - loss: 0.0120
3952/6530 [=================>............] - ETA: 0s - loss: 0.0120
4128/6530 [=================>............] - ETA: 0s - loss: 0.0120
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0118
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0118
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0118
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0118
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0117
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0116
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0116
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0114
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0115
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0115
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0115
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0115
6432/6530 [============================>.] - ETA: 0s - loss: 0.0115
6530/6530 [==============================] - 2s 336us/step - loss: 0.0116 - val_loss: 0.0092
Epoch 32/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0067
 192/6530 [..............................] - ETA: 1s - loss: 0.0127
 352/6530 [>.............................] - ETA: 1s - loss: 0.0118
 512/6530 [=>............................] - ETA: 1s - loss: 0.0122
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0122
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0120
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0117
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0121
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0123
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0121
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0119
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0118
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0115
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0115
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0116
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0117
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0117
2848/6530 [============>.................] - ETA: 1s - loss: 0.0116
3056/6530 [=============>................] - ETA: 1s - loss: 0.0117
3264/6530 [=============>................] - ETA: 1s - loss: 0.0117
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0118
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0118
3904/6530 [================>.............] - ETA: 0s - loss: 0.0117
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0117
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0116
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0115
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0115
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0114
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0114
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0112
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0112
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0113
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0112
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0113
6368/6530 [============================>.] - ETA: 0s - loss: 0.0113
6528/6530 [============================>.] - ETA: 0s - loss: 0.0114
6530/6530 [==============================] - 2s 292us/step - loss: 0.0114 - val_loss: 0.0092
Epoch 33/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0066
 208/6530 [..............................] - ETA: 1s - loss: 0.0120
 416/6530 [>.............................] - ETA: 1s - loss: 0.0118
 624/6530 [=>............................] - ETA: 1s - loss: 0.0119
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0119
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0115
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0119
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0120
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0119
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0117
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0115
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0113
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0112
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0114
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0115
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0115
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0115
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0114
2976/6530 [============>.................] - ETA: 1s - loss: 0.0114
3136/6530 [=============>................] - ETA: 1s - loss: 0.0114
3328/6530 [==============>...............] - ETA: 1s - loss: 0.0115
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0116
3712/6530 [================>.............] - ETA: 0s - loss: 0.0115
3904/6530 [================>.............] - ETA: 0s - loss: 0.0115
4112/6530 [=================>............] - ETA: 0s - loss: 0.0115
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0114
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0114
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0113
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0112
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0112
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0110
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0110
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0111
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0111
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0111
6480/6530 [============================>.] - ETA: 0s - loss: 0.0111
6530/6530 [==============================] - 2s 297us/step - loss: 0.0112 - val_loss: 0.0091
Epoch 34/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0066
 176/6530 [..............................] - ETA: 2s - loss: 0.0121
 352/6530 [>.............................] - ETA: 1s - loss: 0.0113
 512/6530 [=>............................] - ETA: 1s - loss: 0.0117
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0115
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0113
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0117
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0118
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0115
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0114
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0112
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0110
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0112
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0113
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0113
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0113
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0112
2928/6530 [============>.................] - ETA: 1s - loss: 0.0112
3088/6530 [=============>................] - ETA: 1s - loss: 0.0112
3248/6530 [=============>................] - ETA: 1s - loss: 0.0112
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0113
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0113
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0113
3824/6530 [================>.............] - ETA: 0s - loss: 0.0113
3968/6530 [=================>............] - ETA: 0s - loss: 0.0113
4112/6530 [=================>............] - ETA: 0s - loss: 0.0112
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0112
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0111
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0111
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0111
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0110
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0109
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0109
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0108
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0108
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0108
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0108
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0108
6464/6530 [============================>.] - ETA: 0s - loss: 0.0109
6530/6530 [==============================] - 2s 317us/step - loss: 0.0110 - val_loss: 0.0091
Epoch 35/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0065
 208/6530 [..............................] - ETA: 1s - loss: 0.0114
 384/6530 [>.............................] - ETA: 1s - loss: 0.0110
 592/6530 [=>............................] - ETA: 1s - loss: 0.0113
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0115
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0112
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0116
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0114
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0112
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0110
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0109
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0109
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0110
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0110
2512/6530 [==========>...................] - ETA: 1s - loss: 0.0111
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0111
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0110
2944/6530 [============>.................] - ETA: 1s - loss: 0.0110
3088/6530 [=============>................] - ETA: 1s - loss: 0.0110
3232/6530 [=============>................] - ETA: 1s - loss: 0.0110
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0111
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0112
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0111
3808/6530 [================>.............] - ETA: 0s - loss: 0.0111
3952/6530 [=================>............] - ETA: 0s - loss: 0.0111
4128/6530 [=================>............] - ETA: 0s - loss: 0.0111
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0109
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0109
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0109
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0107
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0107
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0106
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0105
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0106
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0106
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0106
6368/6530 [============================>.] - ETA: 0s - loss: 0.0106
6528/6530 [============================>.] - ETA: 0s - loss: 0.0108
6530/6530 [==============================] - 2s 306us/step - loss: 0.0108 - val_loss: 0.0091
Epoch 36/81

  16/6530 [..............................] - ETA: 3s - loss: 0.0064
 208/6530 [..............................] - ETA: 1s - loss: 0.0111
 368/6530 [>.............................] - ETA: 1s - loss: 0.0107
 528/6530 [=>............................] - ETA: 1s - loss: 0.0111
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0111
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0110
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0107
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0113
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0113
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0111
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0109
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0108
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0107
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0108
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0109
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0109
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0109
2816/6530 [===========>..................] - ETA: 1s - loss: 0.0109
2976/6530 [============>.................] - ETA: 1s - loss: 0.0108
3136/6530 [=============>................] - ETA: 1s - loss: 0.0108
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0108
3440/6530 [==============>...............] - ETA: 1s - loss: 0.0109
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0109
3728/6530 [================>.............] - ETA: 0s - loss: 0.0109
3888/6530 [================>.............] - ETA: 0s - loss: 0.0109
4048/6530 [=================>............] - ETA: 0s - loss: 0.0109
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0108
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0107
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0107
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0107
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0106
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0105
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0105
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0103
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0104
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0104
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0104
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0105
6368/6530 [============================>.] - ETA: 0s - loss: 0.0104
6528/6530 [============================>.] - ETA: 0s - loss: 0.0105
6530/6530 [==============================] - 2s 326us/step - loss: 0.0105 - val_loss: 0.0091
Epoch 37/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0065
 208/6530 [..............................] - ETA: 1s - loss: 0.0108
 384/6530 [>.............................] - ETA: 1s - loss: 0.0105
 544/6530 [=>............................] - ETA: 1s - loss: 0.0108
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0108
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0108
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0105
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0109
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0110
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0108
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0107
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0104
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0106
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0106
2560/6530 [==========>...................] - ETA: 1s - loss: 0.0107
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0106
2896/6530 [============>.................] - ETA: 1s - loss: 0.0106
3072/6530 [=============>................] - ETA: 1s - loss: 0.0106
3248/6530 [=============>................] - ETA: 0s - loss: 0.0106
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0107
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0107
3872/6530 [================>.............] - ETA: 0s - loss: 0.0107
4096/6530 [=================>............] - ETA: 0s - loss: 0.0106
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0105
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0105
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0105
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0103
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0103
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0103
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0102
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0102
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0102
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0102
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0103
6384/6530 [============================>.] - ETA: 0s - loss: 0.0102
6528/6530 [============================>.] - ETA: 0s - loss: 0.0104
6530/6530 [==============================] - 2s 290us/step - loss: 0.0104 - val_loss: 0.0091
Epoch 38/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0066
 176/6530 [..............................] - ETA: 1s - loss: 0.0108
 320/6530 [>.............................] - ETA: 2s - loss: 0.0103
 480/6530 [=>............................] - ETA: 2s - loss: 0.0111
 640/6530 [=>............................] - ETA: 1s - loss: 0.0107
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0107
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0104
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0108
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0109
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0107
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0106
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0104
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0102
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0103
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0104
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0105
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0105
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0104
2960/6530 [============>.................] - ETA: 1s - loss: 0.0104
3104/6530 [=============>................] - ETA: 1s - loss: 0.0104
3264/6530 [=============>................] - ETA: 1s - loss: 0.0104
3392/6530 [==============>...............] - ETA: 1s - loss: 0.0105
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0106
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0105
3824/6530 [================>.............] - ETA: 0s - loss: 0.0105
4016/6530 [=================>............] - ETA: 0s - loss: 0.0105
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0104
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0103
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0103
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0103
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0103
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0102
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0101
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0101
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0101
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0100
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0101
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0100
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0100
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0101
6416/6530 [============================>.] - ETA: 0s - loss: 0.0101
6530/6530 [==============================] - 2s 334us/step - loss: 0.0102 - val_loss: 0.0091
Epoch 39/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0066
 176/6530 [..............................] - ETA: 2s - loss: 0.0105
 320/6530 [>.............................] - ETA: 2s - loss: 0.0101
 480/6530 [=>............................] - ETA: 1s - loss: 0.0108
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0105
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0104
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0102
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0106
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0104
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0103
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0101
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0102
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0102
2544/6530 [==========>...................] - ETA: 1s - loss: 0.0103
2704/6530 [===========>..................] - ETA: 1s - loss: 0.0102
2880/6530 [============>.................] - ETA: 1s - loss: 0.0102
3040/6530 [============>.................] - ETA: 0s - loss: 0.0102
3200/6530 [=============>................] - ETA: 0s - loss: 0.0102
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0103
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0103
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0103
3872/6530 [================>.............] - ETA: 0s - loss: 0.0103
4064/6530 [=================>............] - ETA: 0s - loss: 0.0103
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0102
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0101
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0101
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0101
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0100
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0099
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0099
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0098
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0098
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0099
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0098
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0099
6496/6530 [============================>.] - ETA: 0s - loss: 0.0099
6530/6530 [==============================] - 2s 292us/step - loss: 0.0100 - val_loss: 0.0091
Epoch 40/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0066
 176/6530 [..............................] - ETA: 2s - loss: 0.0103
 320/6530 [>.............................] - ETA: 2s - loss: 0.0099
 464/6530 [=>............................] - ETA: 2s - loss: 0.0106
 624/6530 [=>............................] - ETA: 2s - loss: 0.0102
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0103
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0099
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0103
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0104
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0101
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0101
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0100
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0099
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0100
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0101
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0101
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0101
2784/6530 [===========>..................] - ETA: 1s - loss: 0.0100
2944/6530 [============>.................] - ETA: 1s - loss: 0.0100
3152/6530 [=============>................] - ETA: 1s - loss: 0.0100
3344/6530 [==============>...............] - ETA: 1s - loss: 0.0101
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0102
3712/6530 [================>.............] - ETA: 0s - loss: 0.0101
3936/6530 [=================>............] - ETA: 0s - loss: 0.0101
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0100
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0100
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0099
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0098
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0098
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0098
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0096
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0096
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0097
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0097
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0097
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0097
6464/6530 [============================>.] - ETA: 0s - loss: 0.0097
6530/6530 [==============================] - 2s 302us/step - loss: 0.0098 - val_loss: 0.0092

# training | RMSE: 0.0852, MAE: 0.0665
worker 1  xfile  [1, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 31, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.08523612271722787, 'rmse': 0.08523612271722787, 'mae': 0.06648195032502532, 'early_stop': True}
vggnet done  1
all of workers have been done
calculation finished
#0 epoch=81.0 loss={'loss': 0.12588820408791238, 'rmse': 0.12588820408791238, 'mae': 0.0955917510952807, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22878813023045885}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4932102540157758}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1274633279440146}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#1 epoch=81.0 loss={'loss': 0.08523612271722787, 'rmse': 0.08523612271722787, 'mae': 0.06648195032502532, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 31, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
get a list [results] of length 201
get a list [loss] of length 2
get a list [val_loss] of length 2
length of indices is [1 0]
length of indices is 2
length of T is 2
s=0
T is of size 5
T=[{'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3103224703264372}, 'layer_3_size': 60, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 5, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28132301116256553}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2681854440647853}, 'layer_1_size': 11, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2601875463062653}, 'layer_4_size': 23, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23850018209295085}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2799184668293387}, 'layer_3_size': 35, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1107019734063146}, 'layer_2_size': 46, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23700177217888424}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.25674344188971665}, 'layer_2_size': 96, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]
newly formed T structure is:[[0, 81, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3103224703264372}, 'layer_3_size': 60, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 5, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28132301116256553}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [1, 81, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2681854440647853}, 'layer_1_size': 11, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2601875463062653}, 'layer_4_size': 23, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [2, 81, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23850018209295085}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2799184668293387}, 'layer_3_size': 35, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [3, 81, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1107019734063146}, 'layer_2_size': 46, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23700177217888424}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [4, 81, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.25674344188971665}, 'layer_2_size': 96, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]] 

*** 5 configurations x 81.0 iterations each

2 | Thu Sep 27 18:44:48 2018 | lowest loss so far: 0.0762 (run 6)

{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  11 | activation: relu    | extras: dropout - rate: 26.8% 
layer 2 | size:   9 | activation: relu    | extras: None 
layer 3 | size:  85 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  64/6530 [..............................] - ETA: 1:17 - loss: 0.7470
1152/6530 [====>.........................] - ETA: 3s - loss: 0.4327  
2432/6530 [==========>...................] - ETA: 1s - loss: 0.3610
4096/6530 [=================>............] - ETA: 0s - loss: 0.3194
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2940
6530/6530 [==============================] - 1s 155us/step - loss: 0.2859 - val_loss: 0.2203
Epoch 2/81

  64/6530 [..............................] - ETA: 0s - loss: 0.2469{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  54 | activation: tanh    | extras: None 
layer 2 | size:  78 | activation: sigmoid | extras: dropout - rate: 23.9% 
layer 3 | size:  35 | activation: tanh    | extras: dropout - rate: 28.0% 
layer 4 | size:  28 | activation: relu    | extras: None 
layer 5 | size:  57 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  32/6530 [..............................] - ETA: 2:57 - loss: 0.4838
1664/6530 [======>.......................] - ETA: 0s - loss: 0.2299
 704/6530 [==>...........................] - ETA: 7s - loss: 0.3804  
3520/6530 [===============>..............] - ETA: 0s - loss: 0.2231
1504/6530 [=====>........................] - ETA: 3s - loss: 0.3174
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2190
2304/6530 [=========>....................] - ETA: 1s - loss: 0.2896
6530/6530 [==============================] - 0s 28us/step - loss: 0.2174 - val_loss: 0.2020
Epoch 3/81

  64/6530 [..............................] - ETA: 0s - loss: 0.2512
3072/6530 [=============>................] - ETA: 1s - loss: 0.2724
1856/6530 [=======>......................] - ETA: 0s - loss: 0.2173
3904/6530 [================>.............] - ETA: 0s - loss: 0.2623
3712/6530 [================>.............] - ETA: 0s - loss: 0.2119
4704/6530 [====================>.........] - ETA: 0s - loss: 0.2528
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2095
5440/6530 [=======================>......] - ETA: 0s - loss: 0.2443
6530/6530 [==============================] - 0s 29us/step - loss: 0.2082 - val_loss: 0.1921
Epoch 4/81

  64/6530 [..............................] - ETA: 0s - loss: 0.2289
6144/6530 [===========================>..] - ETA: 0s - loss: 0.2398
2176/6530 [========>.....................] - ETA: 0s - loss: 0.2106
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2037
6530/6530 [==============================] - 1s 206us/step - loss: 0.2372 - val_loss: 0.2095
Epoch 2/81

  32/6530 [..............................] - ETA: 0s - loss: 0.2655
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2022
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1975
6530/6530 [==============================] - 0s 27us/step - loss: 0.2013 - val_loss: 0.1845
Epoch 5/81

  64/6530 [..............................] - ETA: 0s - loss: 0.2057
1632/6530 [======>.......................] - ETA: 0s - loss: 0.1971
2112/6530 [========>.....................] - ETA: 0s - loss: 0.2015
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1954
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1988
3200/6530 [=============>................] - ETA: 0s - loss: 0.1924
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1972
6530/6530 [==============================] - 0s 27us/step - loss: 0.1972 - val_loss: 0.1828
Epoch 6/81

  64/6530 [..............................] - ETA: 0s - loss: 0.2142
4000/6530 [=================>............] - ETA: 0s - loss: 0.1920{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  78 | activation: relu    | extras: batchnorm 
layer 2 | size:  19 | activation: tanh    | extras: batchnorm 
layer 3 | size:  60 | activation: sigmoid | extras: dropout - rate: 31.0% 
layer 4 | size:   5 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f5f065bd240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  32/6530 [..............................] - ETA: 3:42 - loss: 0.0816
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1986
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1900
 448/6530 [=>............................] - ETA: 15s - loss: 0.0769 
3968/6530 [=================>............] - ETA: 0s - loss: 0.1959
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1892
 864/6530 [==>...........................] - ETA: 7s - loss: 0.0734 
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1932
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1877
6530/6530 [==============================] - 0s 27us/step - loss: 0.1926 - val_loss: 0.1775
Epoch 7/81

  64/6530 [..............................] - ETA: 0s - loss: 0.2068
1280/6530 [====>.........................] - ETA: 5s - loss: 0.0673
6530/6530 [==============================] - 0s 68us/step - loss: 0.1878 - val_loss: 0.1568
Epoch 3/81

  32/6530 [..............................] - ETA: 0s - loss: 0.2177
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1953
1664/6530 [======>.......................] - ETA: 3s - loss: 0.0619
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1782
3968/6530 [=================>............] - ETA: 0s - loss: 0.1921
2080/6530 [========>.....................] - ETA: 2s - loss: 0.0591
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1760
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1893
2496/6530 [==========>...................] - ETA: 2s - loss: 0.0575
6530/6530 [==============================] - 0s 28us/step - loss: 0.1890 - val_loss: 0.1756

2400/6530 [==========>...................] - ETA: 0s - loss: 0.1744Epoch 8/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1941
2912/6530 [============>.................] - ETA: 1s - loss: 0.0556
3200/6530 [=============>................] - ETA: 0s - loss: 0.1751
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1911
3328/6530 [==============>...............] - ETA: 1s - loss: 0.0541
3904/6530 [================>.............] - ETA: 0s - loss: 0.1875
3808/6530 [================>.............] - ETA: 0s - loss: 0.1746
3744/6530 [================>.............] - ETA: 1s - loss: 0.0528
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1875
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1739
6530/6530 [==============================] - 0s 27us/step - loss: 0.1873 - val_loss: 0.1775
Epoch 9/81

  64/6530 [..............................] - ETA: 0s - loss: 0.2080
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0516
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1735
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1898
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0507
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1732
3904/6530 [================>.............] - ETA: 0s - loss: 0.1857
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0497
6530/6530 [==============================] - 0s 70us/step - loss: 0.1729 - val_loss: 0.2526
Epoch 4/81

  32/6530 [..............................] - ETA: 0s - loss: 0.2634
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1855
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0484
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1714
6530/6530 [==============================] - 0s 28us/step - loss: 0.1842 - val_loss: 0.1698
Epoch 10/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1968
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0478
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1698
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1820
6400/6530 [============================>.] - ETA: 0s - loss: 0.0469
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1676
4032/6530 [=================>............] - ETA: 0s - loss: 0.1813
3168/6530 [=============>................] - ETA: 0s - loss: 0.1687
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1817
6530/6530 [==============================] - 0s 26us/step - loss: 0.1813 - val_loss: 0.1684
Epoch 11/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1766
6530/6530 [==============================] - 2s 300us/step - loss: 0.0466 - val_loss: 0.0320
Epoch 2/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0295
3936/6530 [=================>............] - ETA: 0s - loss: 0.1662
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1805
 480/6530 [=>............................] - ETA: 0s - loss: 0.0317
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1665
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1794
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0321
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1656
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1790
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0313
6530/6530 [==============================] - 0s 26us/step - loss: 0.1788 - val_loss: 0.1638
Epoch 12/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1823
6336/6530 [============================>.] - ETA: 0s - loss: 0.1653
6530/6530 [==============================] - 0s 67us/step - loss: 0.1645 - val_loss: 0.2048
Epoch 5/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1733
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0313
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1822
 864/6530 [==>...........................] - ETA: 0s - loss: 0.1624
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0312
4032/6530 [=================>............] - ETA: 0s - loss: 0.1786
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1602
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0323
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1771
6530/6530 [==============================] - 0s 28us/step - loss: 0.1766 - val_loss: 0.1623
Epoch 13/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1858
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1582
3072/6530 [=============>................] - ETA: 0s - loss: 0.0314
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1795
3136/6530 [=============>................] - ETA: 0s - loss: 0.1574
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0316
3712/6530 [================>.............] - ETA: 0s - loss: 0.1776
3968/6530 [=================>............] - ETA: 0s - loss: 0.1574
3904/6530 [================>.............] - ETA: 0s - loss: 0.0315
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1764
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1581
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0313
6530/6530 [==============================] - 0s 27us/step - loss: 0.1761 - val_loss: 0.1640
Epoch 14/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1913
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1584
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0313
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1763
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1578
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0310
4032/6530 [=================>............] - ETA: 0s - loss: 0.1760
6530/6530 [==============================] - 0s 69us/step - loss: 0.1571 - val_loss: 0.2150
Epoch 6/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1993
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0308
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1739
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1523
6530/6530 [==============================] - 0s 27us/step - loss: 0.1737 - val_loss: 0.1599
Epoch 15/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1724
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0307
1632/6530 [======>.......................] - ETA: 0s - loss: 0.1540
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1744
6528/6530 [============================>.] - ETA: 0s - loss: 0.0305
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1543
4032/6530 [=================>............] - ETA: 0s - loss: 0.1732
6530/6530 [==============================] - 1s 124us/step - loss: 0.0304 - val_loss: 0.0222
Epoch 3/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0310
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1730
3200/6530 [=============>................] - ETA: 0s - loss: 0.1518
 448/6530 [=>............................] - ETA: 0s - loss: 0.0262
6530/6530 [==============================] - 0s 26us/step - loss: 0.1724 - val_loss: 0.1576
Epoch 16/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1872
4000/6530 [=================>............] - ETA: 0s - loss: 0.1521
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0252
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1711
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1512
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0252
4096/6530 [=================>............] - ETA: 0s - loss: 0.1707
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1489
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0249
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1694
6530/6530 [==============================] - 0s 27us/step - loss: 0.1691 - val_loss: 0.1523
Epoch 17/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1819
6336/6530 [============================>.] - ETA: 0s - loss: 0.1492
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0249
6530/6530 [==============================] - 0s 67us/step - loss: 0.1488 - val_loss: 0.1345
Epoch 7/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1562
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1682
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0260
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1474
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1673
3040/6530 [============>.................] - ETA: 0s - loss: 0.0254
1632/6530 [======>.......................] - ETA: 0s - loss: 0.1479
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1661
6530/6530 [==============================] - 0s 26us/step - loss: 0.1658 - val_loss: 0.1476
Epoch 18/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1702
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0256
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1479
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1653
3936/6530 [=================>............] - ETA: 0s - loss: 0.0255
3232/6530 [=============>................] - ETA: 0s - loss: 0.1462
4096/6530 [=================>............] - ETA: 0s - loss: 0.1639
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0253
4032/6530 [=================>............] - ETA: 0s - loss: 0.1458
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1642
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0255
6530/6530 [==============================] - 0s 26us/step - loss: 0.1638 - val_loss: 0.1500

4832/6530 [=====================>........] - ETA: 0s - loss: 0.1470Epoch 19/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1591
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0253
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1638
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1468
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0252
3968/6530 [=================>............] - ETA: 0s - loss: 0.1618
6464/6530 [============================>.] - ETA: 0s - loss: 0.1464
6530/6530 [==============================] - 0s 65us/step - loss: 0.1467 - val_loss: 0.1468
Epoch 8/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1849
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0253
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1617
 800/6530 [==>...........................] - ETA: 0s - loss: 0.1434
6530/6530 [==============================] - 0s 27us/step - loss: 0.1612 - val_loss: 0.1441
Epoch 20/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1466
6432/6530 [============================>.] - ETA: 0s - loss: 0.0252
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1484
6530/6530 [==============================] - 1s 125us/step - loss: 0.0252 - val_loss: 0.0184

1856/6530 [=======>......................] - ETA: 0s - loss: 0.1606Epoch 4/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0212
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1470
3840/6530 [================>.............] - ETA: 0s - loss: 0.1599
 448/6530 [=>............................] - ETA: 0s - loss: 0.0222
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1606
3168/6530 [=============>................] - ETA: 0s - loss: 0.1468
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0209
6530/6530 [==============================] - 0s 29us/step - loss: 0.1604 - val_loss: 0.1430
Epoch 21/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1650
3872/6530 [================>.............] - ETA: 0s - loss: 0.1451
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0218
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1568
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1444
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0212
3840/6530 [================>.............] - ETA: 0s - loss: 0.1578
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1434
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0219
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1584
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1431
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0225
6530/6530 [==============================] - 0s 28us/step - loss: 0.1581 - val_loss: 0.1421
Epoch 22/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1669
6530/6530 [==============================] - 0s 70us/step - loss: 0.1423 - val_loss: 0.2115
Epoch 9/81

  32/6530 [..............................] - ETA: 0s - loss: 0.2128
2912/6530 [============>.................] - ETA: 0s - loss: 0.0222
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1604
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1449
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0220
4096/6530 [=================>............] - ETA: 0s - loss: 0.1574
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1454
3776/6530 [================>.............] - ETA: 0s - loss: 0.0220
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1572
6530/6530 [==============================] - 0s 26us/step - loss: 0.1568 - val_loss: 0.1480
Epoch 23/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1721
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1409
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0220
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1589
3200/6530 [=============>................] - ETA: 0s - loss: 0.1408
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0220
3904/6530 [================>.............] - ETA: 0s - loss: 0.1564
3968/6530 [=================>............] - ETA: 0s - loss: 0.1389
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0220
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1560
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1406
6530/6530 [==============================] - 0s 28us/step - loss: 0.1560 - val_loss: 0.1422
Epoch 24/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1382
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0218
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1395
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1490
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0218
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1390
3968/6530 [=================>............] - ETA: 0s - loss: 0.1502
6336/6530 [============================>.] - ETA: 0s - loss: 0.0219
6530/6530 [==============================] - 0s 68us/step - loss: 0.1387 - val_loss: 0.2009
Epoch 10/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1699
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1505
6530/6530 [==============================] - 1s 128us/step - loss: 0.0218 - val_loss: 0.0174
Epoch 5/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0181
6530/6530 [==============================] - 0s 27us/step - loss: 0.1508 - val_loss: 0.1357

 832/6530 [==>...........................] - ETA: 0s - loss: 0.1396Epoch 25/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1532
 416/6530 [>.............................] - ETA: 0s - loss: 0.0192
1632/6530 [======>.......................] - ETA: 0s - loss: 0.1396
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1543
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0191
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1382
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1525
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0193
3232/6530 [=============>................] - ETA: 0s - loss: 0.1375
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1529
6530/6530 [==============================] - 0s 26us/step - loss: 0.1526 - val_loss: 0.1432
Epoch 26/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1439
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0189
4032/6530 [=================>............] - ETA: 0s - loss: 0.1362
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1504
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0192
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1356
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1507
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0193
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1353
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1502
6530/6530 [==============================] - 0s 26us/step - loss: 0.1498 - val_loss: 0.1374
Epoch 27/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1224
6336/6530 [============================>.] - ETA: 0s - loss: 0.1358
2976/6530 [============>.................] - ETA: 0s - loss: 0.0193
6530/6530 [==============================] - 0s 67us/step - loss: 0.1356 - val_loss: 0.1478
Epoch 11/81

  32/6530 [..............................] - ETA: 1s - loss: 0.1705
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1524
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0192
4096/6530 [=================>............] - ETA: 0s - loss: 0.1500
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1295
3872/6530 [================>.............] - ETA: 0s - loss: 0.0193
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1495
1632/6530 [======>.......................] - ETA: 0s - loss: 0.1324
6530/6530 [==============================] - 0s 26us/step - loss: 0.1489 - val_loss: 0.1304

4320/6530 [==================>...........] - ETA: 0s - loss: 0.0192Epoch 28/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1427
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1317
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0192
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1491
3232/6530 [=============>................] - ETA: 0s - loss: 0.1315
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1474
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0192
4032/6530 [=================>............] - ETA: 0s - loss: 0.1315
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1474
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0192
6530/6530 [==============================] - 0s 27us/step - loss: 0.1466 - val_loss: 0.1366
Epoch 29/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1250
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1321
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0193
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1421
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1329
6464/6530 [============================>.] - ETA: 0s - loss: 0.0192
3904/6530 [================>.............] - ETA: 0s - loss: 0.1428
6530/6530 [==============================] - 1s 125us/step - loss: 0.0192 - val_loss: 0.0152

6400/6530 [============================>.] - ETA: 0s - loss: 0.1335Epoch 6/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0206
6530/6530 [==============================] - 0s 68us/step - loss: 0.1331 - val_loss: 0.1757

5952/6530 [==========================>...] - ETA: 0s - loss: 0.1433
 448/6530 [=>............................] - ETA: 0s - loss: 0.0192
6530/6530 [==============================] - 0s 27us/step - loss: 0.1433 - val_loss: 0.1611
Epoch 30/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1451
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0187
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1426
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0179
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1437
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0176
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1438
6530/6530 [==============================] - 0s 26us/step - loss: 0.1434 - val_loss: 0.1285
Epoch 31/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1432
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0175
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1411
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0178
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1424
3008/6530 [============>.................] - ETA: 0s - loss: 0.0177
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1434
6530/6530 [==============================] - 0s 26us/step - loss: 0.1431 - val_loss: 0.1288
Epoch 32/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1608
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0176
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1424
3904/6530 [================>.............] - ETA: 0s - loss: 0.0177
3968/6530 [=================>............] - ETA: 0s - loss: 0.1413
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0176
# training | RMSE: 0.2083, MAE: 0.1729
worker 2  xfile  [2, 81, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23850018209295085}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2799184668293387}, 'layer_3_size': 35, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.20834604937231727, 'rmse': 0.20834604937231727, 'mae': 0.17294974934356583, 'early_stop': True}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  10 | activation: sigmoid | extras: None 
layer 2 | size:  46 | activation: relu    | extras: dropout - rate: 11.1% 
layer 3 | size:  74 | activation: tanh    | extras: dropout - rate: 23.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e913508d0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  32/6530 [..............................] - ETA: 35s - loss: 1.3386
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1416
6530/6530 [==============================] - 0s 26us/step - loss: 0.1411 - val_loss: 0.1256
Epoch 33/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1373
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0178
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1238 
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1408
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0177
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0898
3968/6530 [=================>............] - ETA: 0s - loss: 0.1424
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0178
3104/6530 [=============>................] - ETA: 0s - loss: 0.0776
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1418
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0179
6530/6530 [==============================] - 0s 27us/step - loss: 0.1417 - val_loss: 0.1266
Epoch 34/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1445
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0706
6464/6530 [============================>.] - ETA: 0s - loss: 0.0179
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1408
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0662
6530/6530 [==============================] - 1s 125us/step - loss: 0.0178 - val_loss: 0.0143
Epoch 7/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0197
4096/6530 [=================>............] - ETA: 0s - loss: 0.1395
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0628
 480/6530 [=>............................] - ETA: 0s - loss: 0.0184
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1400
6530/6530 [==============================] - 1s 82us/step - loss: 0.0617 - val_loss: 0.0705
Epoch 2/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0783
6530/6530 [==============================] - 0s 27us/step - loss: 0.1397 - val_loss: 0.1289
Epoch 35/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1283
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0175
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0488
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1353
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0176
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0454
4096/6530 [=================>............] - ETA: 0s - loss: 0.1373
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0170
3232/6530 [=============>................] - ETA: 0s - loss: 0.0449
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1384
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0170
6530/6530 [==============================] - 0s 26us/step - loss: 0.1383 - val_loss: 0.1469
Epoch 36/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1194
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0450
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0172
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1357
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0450
2976/6530 [============>.................] - ETA: 0s - loss: 0.0171
3776/6530 [================>.............] - ETA: 0s - loss: 0.1377
6336/6530 [============================>.] - ETA: 0s - loss: 0.0450
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0169
6530/6530 [==============================] - 0s 51us/step - loss: 0.0448 - val_loss: 0.0597
Epoch 3/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0696
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1380
6530/6530 [==============================] - 0s 27us/step - loss: 0.1378 - val_loss: 0.1281
Epoch 37/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1170
3840/6530 [================>.............] - ETA: 0s - loss: 0.0170
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0452
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1329
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0170
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0424
4032/6530 [=================>............] - ETA: 0s - loss: 0.1360
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0171
3136/6530 [=============>................] - ETA: 0s - loss: 0.0415
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1372
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0169
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0417
6530/6530 [==============================] - 0s 27us/step - loss: 0.1370 - val_loss: 0.1406

5600/6530 [========================>.....] - ETA: 0s - loss: 0.0169
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0417
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0170
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0413
6530/6530 [==============================] - 0s 52us/step - loss: 0.0409 - val_loss: 0.0473
Epoch 4/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0596
6432/6530 [============================>.] - ETA: 0s - loss: 0.0170
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0411
6530/6530 [==============================] - 1s 126us/step - loss: 0.0170 - val_loss: 0.0124
Epoch 8/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0178
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0397
 480/6530 [=>............................] - ETA: 0s - loss: 0.0168
3264/6530 [=============>................] - ETA: 0s - loss: 0.0394
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0163
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0395
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0163
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0394
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0159
6464/6530 [============================>.] - ETA: 0s - loss: 0.0393
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0160
6530/6530 [==============================] - 0s 50us/step - loss: 0.0392 - val_loss: 0.0442
Epoch 5/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0461
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0162
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0397
3104/6530 [=============>................] - ETA: 0s - loss: 0.0161
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0373
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0160
3136/6530 [=============>................] - ETA: 0s - loss: 0.0367
4000/6530 [=================>............] - ETA: 0s - loss: 0.0162
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0371
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0160
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0372
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0160
6336/6530 [============================>.] - ETA: 0s - loss: 0.0370
6530/6530 [==============================] - 0s 51us/step - loss: 0.0367 - val_loss: 0.0462
Epoch 6/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0477
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0159
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0373
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0160
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0351
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0160
3136/6530 [=============>................] - ETA: 0s - loss: 0.0355
6530/6530 [==============================] - 1s 121us/step - loss: 0.0161 - val_loss: 0.0119
Epoch 9/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0218
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0359
 480/6530 [=>............................] - ETA: 0s - loss: 0.0154
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0360
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0154
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0356
6530/6530 [==============================] - 0s 51us/step - loss: 0.0355 - val_loss: 0.0439
Epoch 7/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0474
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0155
# training | RMSE: 0.1781, MAE: 0.1402
worker 1  xfile  [1, 81, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2681854440647853}, 'layer_1_size': 11, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2601875463062653}, 'layer_4_size': 23, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.1780745508225921, 'rmse': 0.1780745508225921, 'mae': 0.14024201602411634, 'early_stop': True}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  90 | activation: relu    | extras: batchnorm 
layer 2 | size:  96 | activation: tanh    | extras: dropout - rate: 25.7% 
layer 3 | size:  44 | activation: relu    | extras: None 
layer 4 | size:  77 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f5e91350860>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  32/6530 [..............................] - ETA: 1:31 - loss: 0.5827
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0346
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0151
 576/6530 [=>............................] - ETA: 5s - loss: 0.4470  
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0338
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0151
1120/6530 [====>.........................] - ETA: 2s - loss: 0.4043
3232/6530 [=============>................] - ETA: 0s - loss: 0.0337
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0154
1664/6530 [======>.......................] - ETA: 1s - loss: 0.3716
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0341
3008/6530 [============>.................] - ETA: 0s - loss: 0.0155
2144/6530 [========>.....................] - ETA: 1s - loss: 0.3551
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0344
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0155
2688/6530 [===========>..................] - ETA: 1s - loss: 0.3397
6464/6530 [============================>.] - ETA: 0s - loss: 0.0344
6530/6530 [==============================] - 0s 49us/step - loss: 0.0344 - val_loss: 0.0372
Epoch 8/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0336
3840/6530 [================>.............] - ETA: 0s - loss: 0.0155
3200/6530 [=============>................] - ETA: 0s - loss: 0.3282
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0341
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0154
3744/6530 [================>.............] - ETA: 0s - loss: 0.3181
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0325
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0154
4224/6530 [==================>...........] - ETA: 0s - loss: 0.3088
3040/6530 [============>.................] - ETA: 0s - loss: 0.0327
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0154
4736/6530 [====================>.........] - ETA: 0s - loss: 0.3010
4064/6530 [=================>............] - ETA: 0s - loss: 0.0330
5280/6530 [=======================>......] - ETA: 0s - loss: 0.2941
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0154
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0332
5824/6530 [=========================>....] - ETA: 0s - loss: 0.2870
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0154
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0330
6368/6530 [============================>.] - ETA: 0s - loss: 0.2814
6400/6530 [============================>.] - ETA: 0s - loss: 0.0156
6530/6530 [==============================] - 0s 51us/step - loss: 0.0329 - val_loss: 0.0370
Epoch 9/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0361
6530/6530 [==============================] - 1s 127us/step - loss: 0.0156 - val_loss: 0.0128
Epoch 10/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0192
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0338
6530/6530 [==============================] - 1s 174us/step - loss: 0.2799 - val_loss: 0.1709
Epoch 2/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1912
 416/6530 [>.............................] - ETA: 0s - loss: 0.0150
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0329
 576/6530 [=>............................] - ETA: 0s - loss: 0.2172
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0151
3168/6530 [=============>................] - ETA: 0s - loss: 0.0327
1120/6530 [====>.........................] - ETA: 0s - loss: 0.2131
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0151
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0326
1632/6530 [======>.......................] - ETA: 0s - loss: 0.2078
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0149
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0327
2144/6530 [========>.....................] - ETA: 0s - loss: 0.2054
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0148
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0329
2688/6530 [===========>..................] - ETA: 0s - loss: 0.2033
6530/6530 [==============================] - 0s 51us/step - loss: 0.0327 - val_loss: 0.0369
Epoch 10/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0385
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0152
3232/6530 [=============>................] - ETA: 0s - loss: 0.2005
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0309
2880/6530 [============>.................] - ETA: 0s - loss: 0.0148
3776/6530 [================>.............] - ETA: 0s - loss: 0.1982
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0305
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0147
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1985
3136/6530 [=============>................] - ETA: 0s - loss: 0.0305
3712/6530 [================>.............] - ETA: 0s - loss: 0.0149
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1976
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0311
4128/6530 [=================>............] - ETA: 0s - loss: 0.0150
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1954
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0314
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0150
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1943
6336/6530 [============================>.] - ETA: 0s - loss: 0.0316
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0150
6530/6530 [==============================] - 0s 51us/step - loss: 0.0315 - val_loss: 0.0344
Epoch 11/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0308
6464/6530 [============================>.] - ETA: 0s - loss: 0.1928
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0150
6530/6530 [==============================] - 1s 100us/step - loss: 0.1926 - val_loss: 0.1777
Epoch 3/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1402
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0314
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0150
 576/6530 [=>............................] - ETA: 0s - loss: 0.1741
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0308
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0151
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1738
3168/6530 [=============>................] - ETA: 0s - loss: 0.0309
6530/6530 [==============================] - 1s 127us/step - loss: 0.0150 - val_loss: 0.0113
Epoch 11/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0156
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1684
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0311
 448/6530 [=>............................] - ETA: 0s - loss: 0.0147
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1697
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0314
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0144
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1697
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0315
6530/6530 [==============================] - 0s 51us/step - loss: 0.0313 - val_loss: 0.0327
Epoch 12/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0331
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0150
3168/6530 [=============>................] - ETA: 0s - loss: 0.1681
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0313
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0145
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1685
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0303
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0143
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1683
3072/6530 [=============>................] - ETA: 0s - loss: 0.0303
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0143
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1684
4128/6530 [=================>............] - ETA: 0s - loss: 0.0305
3008/6530 [============>.................] - ETA: 0s - loss: 0.0142
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1672
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0304
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0141
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1662
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0307
3904/6530 [================>.............] - ETA: 0s - loss: 0.0142
6368/6530 [============================>.] - ETA: 0s - loss: 0.1655
6530/6530 [==============================] - 0s 51us/step - loss: 0.0306 - val_loss: 0.0312
Epoch 13/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0341
6530/6530 [==============================] - 1s 100us/step - loss: 0.1651 - val_loss: 0.1440
Epoch 4/81

  32/6530 [..............................] - ETA: 0s - loss: 0.2014
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0142
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0294
 544/6530 [=>............................] - ETA: 0s - loss: 0.1583
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0142
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0295
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1564
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0142
3264/6530 [=============>................] - ETA: 0s - loss: 0.0296
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1545
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0143
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0297
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1539
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0142
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0300
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1540
6496/6530 [============================>.] - ETA: 0s - loss: 0.0143
6400/6530 [============================>.] - ETA: 0s - loss: 0.0300
6530/6530 [==============================] - 1s 124us/step - loss: 0.0143 - val_loss: 0.0120
Epoch 12/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0141
6530/6530 [==============================] - 0s 50us/step - loss: 0.0298 - val_loss: 0.0332
Epoch 14/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0276
3200/6530 [=============>................] - ETA: 0s - loss: 0.1536
 480/6530 [=>............................] - ETA: 0s - loss: 0.0147
3744/6530 [================>.............] - ETA: 0s - loss: 0.1526
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0292
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0142
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1526
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0287
3168/6530 [=============>................] - ETA: 0s - loss: 0.0284
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0140
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1531
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0138
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0287
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1523
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0288
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0140
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1517
6336/6530 [============================>.] - ETA: 0s - loss: 0.0292
6368/6530 [============================>.] - ETA: 0s - loss: 0.1517
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0141
6530/6530 [==============================] - 0s 51us/step - loss: 0.0290 - val_loss: 0.0308
Epoch 15/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0243
6530/6530 [==============================] - 1s 100us/step - loss: 0.1514 - val_loss: 0.1263
Epoch 5/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1719
3072/6530 [=============>................] - ETA: 0s - loss: 0.0140
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0282
 544/6530 [=>............................] - ETA: 0s - loss: 0.1394
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0141
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0276
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1389
3936/6530 [=================>............] - ETA: 0s - loss: 0.0140
3200/6530 [=============>................] - ETA: 0s - loss: 0.0281
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1377
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0140
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0285
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1373
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0140
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0284
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1379
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0140
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0285
3136/6530 [=============>................] - ETA: 0s - loss: 0.1373
6530/6530 [==============================] - 0s 51us/step - loss: 0.0283 - val_loss: 0.0294
Epoch 16/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0224
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0140
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1374
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0269
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0140
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1376
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0273
6528/6530 [============================>.] - ETA: 0s - loss: 0.0140
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1371
6530/6530 [==============================] - 1s 125us/step - loss: 0.0140 - val_loss: 0.0109
Epoch 13/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0158
3136/6530 [=============>................] - ETA: 0s - loss: 0.0279
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1378
 448/6530 [=>............................] - ETA: 0s - loss: 0.0142
4128/6530 [=================>............] - ETA: 0s - loss: 0.0283
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1380
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0133
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0281
6336/6530 [============================>.] - ETA: 0s - loss: 0.1381
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0141
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0283
6530/6530 [==============================] - 1s 100us/step - loss: 0.1376 - val_loss: 0.1304
Epoch 6/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1648
6530/6530 [==============================] - 0s 52us/step - loss: 0.0281 - val_loss: 0.0287
Epoch 17/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0319
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0137
 576/6530 [=>............................] - ETA: 0s - loss: 0.1289
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0271
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0136
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1316
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0269
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0138
1632/6530 [======>.......................] - ETA: 0s - loss: 0.1305
3072/6530 [=============>................] - ETA: 0s - loss: 0.0273
2976/6530 [============>.................] - ETA: 0s - loss: 0.0136
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1296
4128/6530 [=================>............] - ETA: 0s - loss: 0.0272
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0136
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1277
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0272
3872/6530 [================>.............] - ETA: 0s - loss: 0.0136
3264/6530 [=============>................] - ETA: 0s - loss: 0.1281
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0274
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0136
6530/6530 [==============================] - 0s 52us/step - loss: 0.0273 - val_loss: 0.0291
Epoch 18/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0295
3744/6530 [================>.............] - ETA: 0s - loss: 0.1285
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0135
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0264
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1276
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0135
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0270
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1277
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0135
3008/6530 [============>.................] - ETA: 0s - loss: 0.0271
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1279
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0136
4064/6530 [=================>............] - ETA: 0s - loss: 0.0274
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1275
6400/6530 [============================>.] - ETA: 0s - loss: 0.0137
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0277
6464/6530 [============================>.] - ETA: 0s - loss: 0.1278
6530/6530 [==============================] - 1s 126us/step - loss: 0.0136 - val_loss: 0.0112
Epoch 14/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0128
6530/6530 [==============================] - 1s 100us/step - loss: 0.1276 - val_loss: 0.1746

6176/6530 [===========================>..] - ETA: 0s - loss: 0.0275Epoch 7/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1819
6530/6530 [==============================] - 0s 52us/step - loss: 0.0274 - val_loss: 0.0265
Epoch 19/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0199
 480/6530 [=>............................] - ETA: 0s - loss: 0.0132
 576/6530 [=>............................] - ETA: 0s - loss: 0.1268
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0259
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0127
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1286
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0255
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0130
1632/6530 [======>.......................] - ETA: 0s - loss: 0.1253
3232/6530 [=============>................] - ETA: 0s - loss: 0.0262
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0128
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1245
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0262
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0128
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1246
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0263
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0130
3136/6530 [=============>................] - ETA: 0s - loss: 0.1241
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0264
2976/6530 [============>.................] - ETA: 0s - loss: 0.0129
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1246
6530/6530 [==============================] - 0s 51us/step - loss: 0.0262 - val_loss: 0.0264
Epoch 20/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0268
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0127
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1241
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0247
3808/6530 [================>.............] - ETA: 0s - loss: 0.0129
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1240
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0251
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0130
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1237
3072/6530 [=============>................] - ETA: 0s - loss: 0.0259
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0130
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1226
3968/6530 [=================>............] - ETA: 0s - loss: 0.0260
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0131
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1224
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0264
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0131
6530/6530 [==============================] - 1s 102us/step - loss: 0.1220 - val_loss: 0.0963
Epoch 8/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1523
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0264
6530/6530 [==============================] - 0s 52us/step - loss: 0.0264 - val_loss: 0.0260
Epoch 21/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0216
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0131
 544/6530 [=>............................] - ETA: 0s - loss: 0.1228
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0265
6336/6530 [============================>.] - ETA: 0s - loss: 0.0132
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1225
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0259
6530/6530 [==============================] - 1s 128us/step - loss: 0.0132 - val_loss: 0.0101
Epoch 15/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0137
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1210
3168/6530 [=============>................] - ETA: 0s - loss: 0.0259
 448/6530 [=>............................] - ETA: 0s - loss: 0.0126
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1200
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0260
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0127
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1192
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0258
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0127
3232/6530 [=============>................] - ETA: 0s - loss: 0.1182
6336/6530 [============================>.] - ETA: 0s - loss: 0.0261
3744/6530 [================>.............] - ETA: 0s - loss: 0.1178
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0126
6530/6530 [==============================] - 0s 50us/step - loss: 0.0260 - val_loss: 0.0263
Epoch 22/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0226
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1172
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0127
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0254
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0127
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1177
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0247
3008/6530 [============>.................] - ETA: 0s - loss: 0.0126
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1174
3200/6530 [=============>................] - ETA: 0s - loss: 0.0244
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0126
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1166
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0245
3872/6530 [================>.............] - ETA: 0s - loss: 0.0127
6368/6530 [============================>.] - ETA: 0s - loss: 0.1165
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0248
6530/6530 [==============================] - 1s 100us/step - loss: 0.1164 - val_loss: 0.1135
Epoch 9/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1121
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0127
6368/6530 [============================>.] - ETA: 0s - loss: 0.0249
6530/6530 [==============================] - 0s 50us/step - loss: 0.0249 - val_loss: 0.0240
Epoch 23/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0267
 576/6530 [=>............................] - ETA: 0s - loss: 0.1129
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0128
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0238
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1126
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0127
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0237
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1123
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0127
3232/6530 [=============>................] - ETA: 0s - loss: 0.0240
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1115
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0127
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0239
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1120
6432/6530 [============================>.] - ETA: 0s - loss: 0.0128
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0241
6530/6530 [==============================] - 1s 124us/step - loss: 0.0127 - val_loss: 0.0100

3168/6530 [=============>................] - ETA: 0s - loss: 0.1117Epoch 16/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0118
6464/6530 [============================>.] - ETA: 0s - loss: 0.0242
3712/6530 [================>.............] - ETA: 0s - loss: 0.1116
 448/6530 [=>............................] - ETA: 0s - loss: 0.0125
6530/6530 [==============================] - 0s 50us/step - loss: 0.0241 - val_loss: 0.0229
Epoch 24/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0199
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0121
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0242
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1123
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0241
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1128
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0124
3168/6530 [=============>................] - ETA: 0s - loss: 0.0242
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1130
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0123
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0243
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1127
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0124
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0245
6432/6530 [============================>.] - ETA: 0s - loss: 0.1135
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0127
6530/6530 [==============================] - 1s 100us/step - loss: 0.1135 - val_loss: 0.1523
Epoch 10/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1453
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0249
3040/6530 [============>.................] - ETA: 0s - loss: 0.0125
6530/6530 [==============================] - 0s 51us/step - loss: 0.0247 - val_loss: 0.0228
Epoch 25/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0233
 576/6530 [=>............................] - ETA: 0s - loss: 0.1143
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0125
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0247
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1110
3872/6530 [================>.............] - ETA: 0s - loss: 0.0127
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0235
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1109
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0126
3104/6530 [=============>................] - ETA: 0s - loss: 0.0238
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1083
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0126
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0241
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1088
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0126
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0245
3104/6530 [=============>................] - ETA: 0s - loss: 0.1079
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0126
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0244
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1081
6530/6530 [==============================] - 0s 51us/step - loss: 0.0243 - val_loss: 0.0243
Epoch 26/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0229
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0126
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1074
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0246
6464/6530 [============================>.] - ETA: 0s - loss: 0.0127
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1076
6530/6530 [==============================] - 1s 125us/step - loss: 0.0127 - val_loss: 0.0100
Epoch 17/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0111
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0236
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1071
 448/6530 [=>............................] - ETA: 0s - loss: 0.0125
3104/6530 [=============>................] - ETA: 0s - loss: 0.0243
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1068
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0127
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0238
6336/6530 [============================>.] - ETA: 0s - loss: 0.1073
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0126
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0240
6530/6530 [==============================] - 1s 101us/step - loss: 0.1072 - val_loss: 0.1310
Epoch 11/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1042
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0123
6336/6530 [============================>.] - ETA: 0s - loss: 0.0240
6530/6530 [==============================] - 0s 51us/step - loss: 0.0239 - val_loss: 0.0231
Epoch 27/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0201
 576/6530 [=>............................] - ETA: 0s - loss: 0.1096
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0123
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0242
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1069
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0124
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0243
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1059
3040/6530 [============>.................] - ETA: 0s - loss: 0.0122
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0245
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1058
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0124
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0246
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1058
3936/6530 [=================>............] - ETA: 0s - loss: 0.0124
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0246
3264/6530 [=============>................] - ETA: 0s - loss: 0.1058
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0124
6432/6530 [============================>.] - ETA: 0s - loss: 0.0244
3776/6530 [================>.............] - ETA: 0s - loss: 0.1050
6530/6530 [==============================] - 0s 49us/step - loss: 0.0243 - val_loss: 0.0215
Epoch 28/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0181
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0124
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1048
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0217
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0125
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1053
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0223
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0125
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1051
3232/6530 [=============>................] - ETA: 0s - loss: 0.0223
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0124
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1045
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0228
6530/6530 [==============================] - 1s 124us/step - loss: 0.0125 - val_loss: 0.0097

6496/6530 [============================>.] - ETA: 0s - loss: 0.1049
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0231Epoch 18/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0140
6530/6530 [==============================] - 1s 99us/step - loss: 0.1049 - val_loss: 0.0916
Epoch 12/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1253
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0234
 448/6530 [=>............................] - ETA: 0s - loss: 0.0133
6530/6530 [==============================] - 0s 51us/step - loss: 0.0232 - val_loss: 0.0230
Epoch 29/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0207
 544/6530 [=>............................] - ETA: 0s - loss: 0.0997
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0124
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0228
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0999
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0127
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0221
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1013
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0121
3072/6530 [=============>................] - ETA: 0s - loss: 0.0228
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1005
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0121
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0230
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1006
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0122
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0234
3200/6530 [=============>................] - ETA: 0s - loss: 0.1000
2976/6530 [============>.................] - ETA: 0s - loss: 0.0122
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0236
3744/6530 [================>.............] - ETA: 0s - loss: 0.1005
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0121
6530/6530 [==============================] - 0s 53us/step - loss: 0.0233 - val_loss: 0.0214
Epoch 30/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0247
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1002
3776/6530 [================>.............] - ETA: 0s - loss: 0.0122
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0229
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1005
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0122
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0229
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1000
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0122
3136/6530 [=============>................] - ETA: 0s - loss: 0.0229
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0999
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0122
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0231
6336/6530 [============================>.] - ETA: 0s - loss: 0.1007
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0122
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0231
6530/6530 [==============================] - 1s 101us/step - loss: 0.1005 - val_loss: 0.0936
Epoch 13/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1089
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0232
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0121
 576/6530 [=>............................] - ETA: 0s - loss: 0.0985
6530/6530 [==============================] - 0s 51us/step - loss: 0.0230 - val_loss: 0.0223
Epoch 31/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0212
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0122
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0969
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0250
6530/6530 [==============================] - 1s 128us/step - loss: 0.0121 - val_loss: 0.0094
Epoch 19/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0102
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0986
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0238
 448/6530 [=>............................] - ETA: 0s - loss: 0.0121
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0990
3264/6530 [=============>................] - ETA: 0s - loss: 0.0234
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0117
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0997
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0234
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0120
3200/6530 [=============>................] - ETA: 0s - loss: 0.0996
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0237
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0118
3712/6530 [================>.............] - ETA: 0s - loss: 0.0997
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0235
6530/6530 [==============================] - 0s 51us/step - loss: 0.0234 - val_loss: 0.0226
Epoch 32/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0203
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0120
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0993
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0245
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0999
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0120
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0240
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0994
3040/6530 [============>.................] - ETA: 0s - loss: 0.0118
3232/6530 [=============>................] - ETA: 0s - loss: 0.0238
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0993
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0118
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0235
6368/6530 [============================>.] - ETA: 0s - loss: 0.0995
3872/6530 [================>.............] - ETA: 0s - loss: 0.0119
6530/6530 [==============================] - 1s 100us/step - loss: 0.0994 - val_loss: 0.0994
Epoch 14/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1336
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0237
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0119
 576/6530 [=>............................] - ETA: 0s - loss: 0.1013
6464/6530 [============================>.] - ETA: 0s - loss: 0.0239
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0119
6530/6530 [==============================] - 0s 49us/step - loss: 0.0238 - val_loss: 0.0211
Epoch 33/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0182
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0986
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0227
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0119
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0978
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0232
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0118
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0981
3200/6530 [=============>................] - ETA: 0s - loss: 0.0230
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0118
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0975
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0231
6432/6530 [============================>.] - ETA: 0s - loss: 0.0118
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0969
6530/6530 [==============================] - 1s 126us/step - loss: 0.0118 - val_loss: 0.0097
Epoch 20/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0113
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0234
3840/6530 [================>.............] - ETA: 0s - loss: 0.0971
 448/6530 [=>............................] - ETA: 0s - loss: 0.0119
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0236
6530/6530 [==============================] - 0s 50us/step - loss: 0.0234 - val_loss: 0.0209
Epoch 34/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0199
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0967
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0116
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0233
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0970
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0118
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0233
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0967
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0115
3072/6530 [=============>................] - ETA: 0s - loss: 0.0230
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0965
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0113
4096/6530 [=================>............] - ETA: 0s - loss: 0.0232
6464/6530 [============================>.] - ETA: 0s - loss: 0.0969
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0113
6530/6530 [==============================] - 1s 98us/step - loss: 0.0969 - val_loss: 0.1189
Epoch 15/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1316
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0234
3040/6530 [============>.................] - ETA: 0s - loss: 0.0112
 576/6530 [=>............................] - ETA: 0s - loss: 0.0983
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0233
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0113
6530/6530 [==============================] - 0s 52us/step - loss: 0.0231 - val_loss: 0.0209
Epoch 35/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0208
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0970
3872/6530 [================>.............] - ETA: 0s - loss: 0.0114
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0216
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0955
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0114
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0218
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0948
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0114
3136/6530 [=============>................] - ETA: 0s - loss: 0.0223
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0953
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0225
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0114
3232/6530 [=============>................] - ETA: 0s - loss: 0.0956
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0114
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0226
3744/6530 [================>.............] - ETA: 0s - loss: 0.0958
6432/6530 [============================>.] - ETA: 0s - loss: 0.0228
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0114
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0961
6530/6530 [==============================] - 0s 50us/step - loss: 0.0227 - val_loss: 0.0212
Epoch 36/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0156
6432/6530 [============================>.] - ETA: 0s - loss: 0.0115
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0963
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0232
6530/6530 [==============================] - 1s 125us/step - loss: 0.0115 - val_loss: 0.0099
Epoch 21/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0089
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0961
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0224
 480/6530 [=>............................] - ETA: 0s - loss: 0.0125
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0958
3072/6530 [=============>................] - ETA: 0s - loss: 0.0228
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0118
6336/6530 [============================>.] - ETA: 0s - loss: 0.0961
4096/6530 [=================>............] - ETA: 0s - loss: 0.0229
6530/6530 [==============================] - 1s 100us/step - loss: 0.0960 - val_loss: 0.0979

1312/6530 [=====>........................] - ETA: 0s - loss: 0.0120Epoch 16/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1229
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0231
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0116
 576/6530 [=>............................] - ETA: 0s - loss: 0.1015
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0230
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0115
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0984
6530/6530 [==============================] - 0s 52us/step - loss: 0.0230 - val_loss: 0.0198
Epoch 37/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0189
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0973
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0115
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0212
3040/6530 [============>.................] - ETA: 0s - loss: 0.0113
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0968
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0211
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0114
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0954
3168/6530 [=============>................] - ETA: 0s - loss: 0.0217
3872/6530 [================>.............] - ETA: 0s - loss: 0.0115
3232/6530 [=============>................] - ETA: 0s - loss: 0.0949
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0223
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0114
3776/6530 [================>.............] - ETA: 0s - loss: 0.0952
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0227
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0950
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0114
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0228
6530/6530 [==============================] - 0s 52us/step - loss: 0.0227 - val_loss: 0.0206
Epoch 38/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0187
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0114
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0950
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0225
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0946
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0114
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0226
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0942
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0115
3264/6530 [=============>................] - ETA: 0s - loss: 0.0223
6432/6530 [============================>.] - ETA: 0s - loss: 0.0945
6496/6530 [============================>.] - ETA: 0s - loss: 0.0115
6530/6530 [==============================] - 1s 125us/step - loss: 0.0115 - val_loss: 0.0092

6530/6530 [==============================] - 1s 100us/step - loss: 0.0945 - val_loss: 0.0848

4320/6530 [==================>...........] - ETA: 0s - loss: 0.0221Epoch 17/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1272Epoch 22/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0106
 448/6530 [=>............................] - ETA: 0s - loss: 0.0117
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0226
 576/6530 [=>............................] - ETA: 0s - loss: 0.0973
6336/6530 [============================>.] - ETA: 0s - loss: 0.0226
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0933
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0113
6530/6530 [==============================] - 0s 50us/step - loss: 0.0225 - val_loss: 0.0211
Epoch 39/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0177
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0923
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0112
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0204
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0922
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0111
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0217
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0916
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0109
3072/6530 [=============>................] - ETA: 0s - loss: 0.0223
3200/6530 [=============>................] - ETA: 0s - loss: 0.0918
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0110
4128/6530 [=================>............] - ETA: 0s - loss: 0.0223
3744/6530 [================>.............] - ETA: 0s - loss: 0.0924
3072/6530 [=============>................] - ETA: 0s - loss: 0.0109
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0223
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0921
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0110
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0224
6530/6530 [==============================] - 0s 51us/step - loss: 0.0222 - val_loss: 0.0200
Epoch 40/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0181
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0924
3936/6530 [=================>............] - ETA: 0s - loss: 0.0110
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0213
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0922
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0110
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0212
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0920
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0110
3104/6530 [=============>................] - ETA: 0s - loss: 0.0213
6432/6530 [============================>.] - ETA: 0s - loss: 0.0925
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0110
6530/6530 [==============================] - 1s 100us/step - loss: 0.0925 - val_loss: 0.0800
Epoch 18/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0914
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0213
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0110
 576/6530 [=>............................] - ETA: 0s - loss: 0.0871
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0214
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0110
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0888
6336/6530 [============================>.] - ETA: 0s - loss: 0.0216
6530/6530 [==============================] - 0s 50us/step - loss: 0.0214 - val_loss: 0.0196

6528/6530 [============================>.] - ETA: 0s - loss: 0.0110Epoch 41/81

  32/6530 [..............................] - ETA: 1s - loss: 0.0227
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0891
6530/6530 [==============================] - 1s 125us/step - loss: 0.0110 - val_loss: 0.0096
Epoch 23/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0087
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0228
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0906
 448/6530 [=>............................] - ETA: 0s - loss: 0.0117
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0217
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0897
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0113
3104/6530 [=============>................] - ETA: 0s - loss: 0.0223
3264/6530 [=============>................] - ETA: 0s - loss: 0.0899
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0116
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0221
3808/6530 [================>.............] - ETA: 0s - loss: 0.0904
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0113
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0223
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0901
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0111
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0222
6530/6530 [==============================] - 0s 52us/step - loss: 0.0222 - val_loss: 0.0200

4864/6530 [=====================>........] - ETA: 0s - loss: 0.0904Epoch 42/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0200
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0112
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0234
3008/6530 [============>.................] - ETA: 0s - loss: 0.0110
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0899
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0226
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0109
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0898
3200/6530 [=============>................] - ETA: 0s - loss: 0.0219
3840/6530 [================>.............] - ETA: 0s - loss: 0.0110
6496/6530 [============================>.] - ETA: 0s - loss: 0.0899
6530/6530 [==============================] - 1s 100us/step - loss: 0.0899 - val_loss: 0.0779
Epoch 19/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1021
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0222
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0111
 576/6530 [=>............................] - ETA: 0s - loss: 0.0954
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0221
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0111
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0922
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0221
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0111
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0904
6530/6530 [==============================] - 0s 52us/step - loss: 0.0221 - val_loss: 0.0205
Epoch 43/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0201
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0111
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0905
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0215
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0111
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0895
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0213
6400/6530 [============================>.] - ETA: 0s - loss: 0.0111
3232/6530 [=============>................] - ETA: 0s - loss: 0.0215
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0890
6530/6530 [==============================] - 1s 125us/step - loss: 0.0111 - val_loss: 0.0092
Epoch 24/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0105
3840/6530 [================>.............] - ETA: 0s - loss: 0.0889
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0216
 448/6530 [=>............................] - ETA: 0s - loss: 0.0117
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0885
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0218
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0111
6432/6530 [============================>.] - ETA: 0s - loss: 0.0217
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0886
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0111
6530/6530 [==============================] - 0s 50us/step - loss: 0.0217 - val_loss: 0.0198
Epoch 44/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0246
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0883
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0219
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0108
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0885
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0219
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0107
6528/6530 [============================>.] - ETA: 0s - loss: 0.0888
3232/6530 [=============>................] - ETA: 0s - loss: 0.0218
6530/6530 [==============================] - 1s 98us/step - loss: 0.0888 - val_loss: 0.1137
Epoch 20/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1279
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0108
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0216
3072/6530 [=============>................] - ETA: 0s - loss: 0.0108
 576/6530 [=>............................] - ETA: 0s - loss: 0.0959
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0219
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0904
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0109
6400/6530 [============================>.] - ETA: 0s - loss: 0.0218
3936/6530 [=================>............] - ETA: 0s - loss: 0.0108
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0888
6530/6530 [==============================] - 0s 50us/step - loss: 0.0217 - val_loss: 0.0198
Epoch 45/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0153
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0877
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0108
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0225
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0109
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0886
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0216
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0108
3264/6530 [=============>................] - ETA: 0s - loss: 0.0877
3168/6530 [=============>................] - ETA: 0s - loss: 0.0212
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0109
3808/6530 [================>.............] - ETA: 0s - loss: 0.0879
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0218
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0109
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0876
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0218
6464/6530 [============================>.] - ETA: 0s - loss: 0.0109
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0880
6336/6530 [============================>.] - ETA: 0s - loss: 0.0217
6530/6530 [==============================] - 0s 50us/step - loss: 0.0216 - val_loss: 0.0195

6530/6530 [==============================] - 1s 125us/step - loss: 0.0109 - val_loss: 0.0088
Epoch 46/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0207Epoch 25/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0093
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0879
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0212
 448/6530 [=>............................] - ETA: 0s - loss: 0.0124
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0880
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0209
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0113
6464/6530 [============================>.] - ETA: 0s - loss: 0.0885
6530/6530 [==============================] - 1s 99us/step - loss: 0.0885 - val_loss: 0.0972
Epoch 21/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1080
3072/6530 [=============>................] - ETA: 0s - loss: 0.0210
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0113
 480/6530 [=>............................] - ETA: 0s - loss: 0.0898
4096/6530 [=================>............] - ETA: 0s - loss: 0.0214
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0109
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0878
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0212
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0108
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0215
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0891
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0109
6530/6530 [==============================] - 0s 51us/step - loss: 0.0213 - val_loss: 0.0190
Epoch 47/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0265
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0880
2976/6530 [============>.................] - ETA: 0s - loss: 0.0108
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0217
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0886
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0107
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0217
3168/6530 [=============>................] - ETA: 0s - loss: 0.0880
3872/6530 [================>.............] - ETA: 0s - loss: 0.0108
3200/6530 [=============>................] - ETA: 0s - loss: 0.0216
3712/6530 [================>.............] - ETA: 0s - loss: 0.0877
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0108
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0216
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0874
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0108
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0220
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0880
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0108
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0218
6530/6530 [==============================] - 0s 51us/step - loss: 0.0217 - val_loss: 0.0187

5312/6530 [=======================>......] - ETA: 0s - loss: 0.0879Epoch 48/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0215
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0108
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0878
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0205
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0108
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0881
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0207
6464/6530 [============================>.] - ETA: 0s - loss: 0.0109
6530/6530 [==============================] - 1s 101us/step - loss: 0.0878 - val_loss: 0.0795
Epoch 22/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1096
6530/6530 [==============================] - 1s 125us/step - loss: 0.0108 - val_loss: 0.0093

3104/6530 [=============>................] - ETA: 0s - loss: 0.0210Epoch 26/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0094
 544/6530 [=>............................] - ETA: 0s - loss: 0.0894
4064/6530 [=================>............] - ETA: 0s - loss: 0.0210
 448/6530 [=>............................] - ETA: 0s - loss: 0.0116
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0875
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0211
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0109
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0869
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0211
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0109
6530/6530 [==============================] - 0s 52us/step - loss: 0.0211 - val_loss: 0.0188
Epoch 49/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0288
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0867
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0105
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0210
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0863
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0103
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0211
3168/6530 [=============>................] - ETA: 0s - loss: 0.0858
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0105
3200/6530 [=============>................] - ETA: 0s - loss: 0.0213
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0861
2976/6530 [============>.................] - ETA: 0s - loss: 0.0104
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0220
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0862
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0103
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0218
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0866
3840/6530 [================>.............] - ETA: 0s - loss: 0.0105
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0221
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0867
6530/6530 [==============================] - 0s 50us/step - loss: 0.0220 - val_loss: 0.0186
Epoch 50/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0168
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0105
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0867
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0208
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0105
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0871
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0208
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0106
6530/6530 [==============================] - 1s 104us/step - loss: 0.0875 - val_loss: 0.1218

3136/6530 [=============>................] - ETA: 0s - loss: 0.0204Epoch 23/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1289
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0105
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0206
 576/6530 [=>............................] - ETA: 0s - loss: 0.0901
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0106
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0209
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0873
6432/6530 [============================>.] - ETA: 0s - loss: 0.0107
6530/6530 [==============================] - 1s 125us/step - loss: 0.0106 - val_loss: 0.0090
Epoch 27/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0091
6336/6530 [============================>.] - ETA: 0s - loss: 0.0209
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0866
6530/6530 [==============================] - 0s 51us/step - loss: 0.0208 - val_loss: 0.0189
Epoch 51/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0157
 448/6530 [=>............................] - ETA: 0s - loss: 0.0115
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0861
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0201
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0106
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0854
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0196
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0107
3264/6530 [=============>................] - ETA: 0s - loss: 0.0855
3232/6530 [=============>................] - ETA: 0s - loss: 0.0203
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0104
3808/6530 [================>.............] - ETA: 0s - loss: 0.0856
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0204
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0103
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0852
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0210
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0104
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0853
6400/6530 [============================>.] - ETA: 0s - loss: 0.0208
6530/6530 [==============================] - 0s 50us/step - loss: 0.0207 - val_loss: 0.0181
Epoch 52/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0151
3040/6530 [============>.................] - ETA: 0s - loss: 0.0102
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0853
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0190
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0103
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0849
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0198
3904/6530 [================>.............] - ETA: 0s - loss: 0.0103
6400/6530 [============================>.] - ETA: 0s - loss: 0.0853
6530/6530 [==============================] - 1s 100us/step - loss: 0.0854 - val_loss: 0.0975

3200/6530 [=============>................] - ETA: 0s - loss: 0.0203
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0104
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0209
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0105
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0213
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0105
# training | RMSE: 0.1144, MAE: 0.0918
worker 1  xfile  [4, 81, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.25674344188971665}, 'layer_2_size': 96, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.11441984683097572, 'rmse': 0.11441984683097572, 'mae': 0.09183651013431873, 'early_stop': True}
vggnet done  1

6272/6530 [===========================>..] - ETA: 0s - loss: 0.0215
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0105
6530/6530 [==============================] - 0s 51us/step - loss: 0.0214 - val_loss: 0.0185
Epoch 53/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0199
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0105
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0226
6432/6530 [============================>.] - ETA: 0s - loss: 0.0105
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0226
6530/6530 [==============================] - 1s 125us/step - loss: 0.0105 - val_loss: 0.0084
Epoch 28/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0094
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0225
 480/6530 [=>............................] - ETA: 0s - loss: 0.0106
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0223
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0104
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0220
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0105
6530/6530 [==============================] - 0s 47us/step - loss: 0.0220 - val_loss: 0.0188
Epoch 54/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0188
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0103
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0209
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0101
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0212
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0103
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0210
3232/6530 [=============>................] - ETA: 0s - loss: 0.0101
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0212
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0102
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0214
4096/6530 [=================>............] - ETA: 0s - loss: 0.0103
6530/6530 [==============================] - 0s 47us/step - loss: 0.0215 - val_loss: 0.0189
Epoch 55/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0170
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0103
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0208
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0103
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0204
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0104
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0208
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0104
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0211
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0105
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0212
6530/6530 [==============================] - 1s 119us/step - loss: 0.0104 - val_loss: 0.0096
Epoch 29/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0090
6530/6530 [==============================] - 0s 47us/step - loss: 0.0212 - val_loss: 0.0187
Epoch 56/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0152
 512/6530 [=>............................] - ETA: 0s - loss: 0.0111
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0204
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0104
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0209
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0103
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0205
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0101
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0208
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0102
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0212
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0102
6530/6530 [==============================] - 0s 47us/step - loss: 0.0211 - val_loss: 0.0183

3200/6530 [=============>................] - ETA: 0s - loss: 0.0100
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0102
# training | RMSE: 0.1297, MAE: 0.1020
worker 2  xfile  [3, 81, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1107019734063146}, 'layer_2_size': 46, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23700177217888424}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.12969573834268394, 'rmse': 0.12969573834268394, 'mae': 0.1020288774435534, 'early_stop': True}
vggnet done  2

4128/6530 [=================>............] - ETA: 0s - loss: 0.0101
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0102
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0102
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0102
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0102
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0103
6530/6530 [==============================] - 1s 121us/step - loss: 0.0103 - val_loss: 0.0084
Epoch 30/81

  32/6530 [..............................] - ETA: 1s - loss: 0.0114
 448/6530 [=>............................] - ETA: 0s - loss: 0.0114
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0106
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0102
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0104
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0101
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0101
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0102
2944/6530 [============>.................] - ETA: 0s - loss: 0.0101
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0099
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0101
3936/6530 [=================>............] - ETA: 0s - loss: 0.0101
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0102
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0102
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0102
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0103
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0102
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0102
6528/6530 [============================>.] - ETA: 0s - loss: 0.0102
6530/6530 [==============================] - 1s 149us/step - loss: 0.0102 - val_loss: 0.0088
Epoch 31/81

  32/6530 [..............................] - ETA: 1s - loss: 0.0100
 288/6530 [>.............................] - ETA: 1s - loss: 0.0116
 576/6530 [=>............................] - ETA: 1s - loss: 0.0109
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0104
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0102
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0103
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0100
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0099
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0099
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0101
2944/6530 [============>.................] - ETA: 0s - loss: 0.0099
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0098
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0100
3904/6530 [================>.............] - ETA: 0s - loss: 0.0100
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0100
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0100
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0100
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0100
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0100
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0100
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0100
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0100
6528/6530 [============================>.] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 1s 187us/step - loss: 0.0100 - val_loss: 0.0086
Epoch 32/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0073
 352/6530 [>.............................] - ETA: 1s - loss: 0.0111
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0108
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0102
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0101
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0100
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0099
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0099
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0099
3040/6530 [============>.................] - ETA: 0s - loss: 0.0097
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0096
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0098
3904/6530 [================>.............] - ETA: 0s - loss: 0.0098
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0098
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0098
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0097
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0098
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0098
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0098
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0098
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0099
6530/6530 [==============================] - 1s 173us/step - loss: 0.0099 - val_loss: 0.0086

# training | RMSE: 0.0873, MAE: 0.0680
worker 0  xfile  [0, 81, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3103224703264372}, 'layer_3_size': 60, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 5, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28132301116256553}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.08731062482081998, 'rmse': 0.08731062482081998, 'mae': 0.06796233426351422, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#2 epoch=81 loss={'loss': 0.20834604937231727, 'rmse': 0.20834604937231727, 'mae': 0.17294974934356583, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23850018209295085}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2799184668293387}, 'layer_3_size': 35, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#1 epoch=81 loss={'loss': 0.1780745508225921, 'rmse': 0.1780745508225921, 'mae': 0.14024201602411634, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2681854440647853}, 'layer_1_size': 11, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2601875463062653}, 'layer_4_size': 23, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#4 epoch=81 loss={'loss': 0.11441984683097572, 'rmse': 0.11441984683097572, 'mae': 0.09183651013431873, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.25674344188971665}, 'layer_2_size': 96, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#3 epoch=81 loss={'loss': 0.12969573834268394, 'rmse': 0.12969573834268394, 'mae': 0.1020288774435534, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1107019734063146}, 'layer_2_size': 46, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23700177217888424}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#0 epoch=81 loss={'loss': 0.08731062482081998, 'rmse': 0.08731062482081998, 'mae': 0.06796233426351422, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3103224703264372}, 'layer_3_size': 60, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 5, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28132301116256553}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
get a list [results] of length 206
get a list [loss] of length 5
get a list [val_loss] of length 5
length of indices is [4 2 3 1 0]
length of indices is 5
length of T is 5
206 total, best:

loss: 7.62%  | #27.0th iterations | run 6 
("{'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', "
 "'layer_1_extras': {'name': 'dropout', 'rate': 0.13763780045412988}, "
 "'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': "
 "'dropout', 'rate': 0.38028865807259427}, 'layer_2_size': 45, "
 "'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, "
 "'layer_3_size': 30, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': "
 "None}, 'layer_4_size': 2, 'layer_5_activation': 'relu', 'layer_5_extras': "
 "{'name': 'batchnorm'}, 'layer_5_size': 72, 'loss': 'mean_squared_error', "
 "'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': "
 'False}')

loss: 8.52%  | #81.0th iterations | run 1 
("{'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', "
 "'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, "
 "'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, "
 "'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': "
 "None}, 'layer_3_size': 31, 'layer_4_activation': 'sigmoid', "
 "'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': "
 "'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': "
 "'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': "
 "'StandardScaler', 'shuffle': False}")

loss: 8.73%  | #81th iterations | run 0 
("{'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', "
 "'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 78, "
 "'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, "
 "'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': "
 "{'name': 'dropout', 'rate': 0.3103224703264372}, 'layer_3_size': 60, "
 "'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, "
 "'layer_4_size': 5, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': "
 "'dropout', 'rate': 0.28132301116256553}, 'layer_5_size': 57, 'loss': "
 "'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': "
 "'MinMaxScaler', 'shuffle': False}")

loss: 8.73%  | #27.0th iterations | run 3 
("{'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', "
 "'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': "
 "'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 82, "
 "'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, "
 "'layer_3_size': 34, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': "
 "'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'tanh', "
 "'layer_5_extras': {'name': 'dropout', 'rate': 0.11389578672392409}, "
 "'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 4, "
 "'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}")

loss: 8.74%  | #27.0th iterations | run 7 
("{'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', "
 "'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, "
 "'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, "
 "'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': "
 "None}, 'layer_3_size': 31, 'layer_4_activation': 'sigmoid', "
 "'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': "
 "'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': "
 "'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': "
 "'StandardScaler', 'shuffle': False}")

saving...

446 seconds.
