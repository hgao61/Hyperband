loading data...
None
[0, 1, 2]
s=4
T is of size 81
T=[{'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 10, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 31, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15150219925735608}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4248286730916818}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 59, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12856911536933047}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2940647648620235}, 'layer_4_size': 94, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.28590221771346425}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 21, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 22, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2000848348489106}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2820274393310773}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.41324829818739683}, 'layer_1_size': 61, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13975984822005746}, 'layer_2_size': 3, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 20, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39132251248214844}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37621340906553147}, 'layer_2_size': 97, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3580614653399373}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46040379328519976}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 41, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3727395901974413}, 'layer_4_size': 44, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 28, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35320267780836245}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26723916442710643}, 'layer_4_size': 6, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 91, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 90, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47570319050681575}, 'layer_1_size': 14, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4787974697856523}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1315792131888107}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4554561189849352}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.379198274209536}, 'layer_4_size': 21, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45512557545093135}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 35, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22993113229229667}, 'layer_1_size': 54, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2918635922408792}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22807816925739277}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 38, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.343391534079115}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3383178210354845}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2956271235310313}, 'layer_5_size': 4, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.40750158041712903}, 'layer_3_size': 69, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4048369905403586}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2738264200831}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 11, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4205563349101873}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3741232536469342}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12932097718679955}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4763850999934498}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 44, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3360410430399626}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23380908750718243}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2423132848388729}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21048884801807022}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.18924415336054554}, 'layer_4_size': 53, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14684617353368484}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3859620787869812}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15776419479082585}, 'layer_3_size': 23, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 89, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 39, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17463050599410126}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4086446132962229}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35029228823647773}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4226684261892001}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38909506041759967}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11883962019293581}, 'layer_5_size': 81, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21704675266779963}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18201157345841038}, 'layer_3_size': 8, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4479261069663404}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4979476961384771}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 75, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32191812317889457}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18336331704955233}, 'layer_5_size': 9, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.31082407783245625}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26877401178424}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15758638976998862}, 'layer_2_size': 92, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22134854478231542}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4731865489395358}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35223028731922856}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4611808223007189}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 30, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 2, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 51, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1776477202562827}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14640075203083014}, 'layer_1_size': 57, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16892410402799152}, 'layer_4_size': 13, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35579117525473236}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4220893331897563}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4195269456256038}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4196428123159085}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 76, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 98, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.11422937510548073}, 'layer_1_size': 73, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 5, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 43, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4649735147365355}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 5, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16911047813414518}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4631191913250753}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3162994555792672}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3801891065249704}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 36, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11923992646601543}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1741077956753633}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2169358952905691}, 'layer_4_size': 19, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4017496563510651}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15068232159870046}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36942877284804576}, 'layer_4_size': 52, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39962949672182224}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10015621671395541}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11581437543817096}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 80, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41716159434200817}, 'layer_2_size': 22, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2771543675500986}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11232213779671203}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.36313792725811167}, 'layer_5_size': 49, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2007607438272935}, 'layer_4_size': 91, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1050661373343273}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36950674751954005}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16745485505931956}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31409164825353697}, 'layer_5_size': 91, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39805956850419555}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 61, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15198369221316727}, 'layer_4_size': 25, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3875675418941298}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 57, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 80, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 8, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.31887286985950053}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45841837674693586}, 'layer_3_size': 62, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 11, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.43181898460277857}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36120883956828265}, 'layer_1_size': 8, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35109665238578447}, 'layer_1_size': 80, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 25, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 66, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19810962561359832}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 2, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1579993964245491}, 'layer_5_size': 35, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43877911310812734}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 21, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31194649777488603}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15353575675845962}, 'layer_3_size': 88, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 49, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 12, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 91, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.46574055977979867}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17922789040016737}, 'layer_3_size': 84, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.262831125587177}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 28, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.44774437757480967}, 'layer_2_size': 85, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4624925505301145}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2545763365188426}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3379367451056181}, 'layer_4_size': 56, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 46, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14491643896200868}, 'layer_5_size': 98, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15132983247231468}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13553206179667554}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37013647248944015}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2717217268544919}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 18, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 91, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.39924731376297173}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.453485542524365}, 'layer_1_size': 59, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3822643582013816}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4743773597797818}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12857141911581393}, 'layer_4_size': 3, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2253611744405719}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18391069289201956}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 56, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20412479331356678}, 'layer_3_size': 97, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]
newly formed T structure is:[[0, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 10, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 31, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15150219925735608}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [1, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4248286730916818}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [2, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 59, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12856911536933047}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2940647648620235}, 'layer_4_size': 94, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [3, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.28590221771346425}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 21, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [4, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 22, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2000848348489106}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [5, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2820274393310773}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [6, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.41324829818739683}, 'layer_1_size': 61, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13975984822005746}, 'layer_2_size': 3, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 20, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [7, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39132251248214844}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37621340906553147}, 'layer_2_size': 97, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3580614653399373}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [8, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46040379328519976}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 41, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3727395901974413}, 'layer_4_size': 44, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [9, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 28, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35320267780836245}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26723916442710643}, 'layer_4_size': 6, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [10, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 91, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 90, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [11, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47570319050681575}, 'layer_1_size': 14, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4787974697856523}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1315792131888107}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [12, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4554561189849352}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.379198274209536}, 'layer_4_size': 21, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [13, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45512557545093135}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 35, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [14, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22993113229229667}, 'layer_1_size': 54, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2918635922408792}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [15, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22807816925739277}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [16, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 38, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.343391534079115}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3383178210354845}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2956271235310313}, 'layer_5_size': 4, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [17, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.40750158041712903}, 'layer_3_size': 69, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4048369905403586}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [18, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2738264200831}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 11, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4205563349101873}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3741232536469342}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [19, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12932097718679955}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4763850999934498}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [20, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 44, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3360410430399626}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23380908750718243}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [21, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2423132848388729}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [22, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21048884801807022}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [23, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.18924415336054554}, 'layer_4_size': 53, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [24, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14684617353368484}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3859620787869812}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15776419479082585}, 'layer_3_size': 23, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 89, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [25, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 39, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17463050599410126}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [26, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4086446132962229}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35029228823647773}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [27, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4226684261892001}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38909506041759967}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11883962019293581}, 'layer_5_size': 81, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [28, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21704675266779963}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [29, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18201157345841038}, 'layer_3_size': 8, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [30, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4479261069663404}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [31, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4979476961384771}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [32, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 75, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32191812317889457}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18336331704955233}, 'layer_5_size': 9, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [33, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.31082407783245625}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26877401178424}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [34, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [35, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15758638976998862}, 'layer_2_size': 92, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22134854478231542}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4731865489395358}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [36, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35223028731922856}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4611808223007189}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 30, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [37, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 2, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 51, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1776477202562827}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [38, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14640075203083014}, 'layer_1_size': 57, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16892410402799152}, 'layer_4_size': 13, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [39, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [40, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35579117525473236}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4220893331897563}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [41, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4195269456256038}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [42, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4196428123159085}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 76, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [43, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 98, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [44, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.11422937510548073}, 'layer_1_size': 73, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 5, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [45, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 43, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4649735147365355}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [46, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 5, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16911047813414518}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [47, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4631191913250753}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [48, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3162994555792672}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [49, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3801891065249704}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 36, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11923992646601543}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [50, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1741077956753633}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2169358952905691}, 'layer_4_size': 19, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4017496563510651}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [51, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15068232159870046}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36942877284804576}, 'layer_4_size': 52, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [52, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39962949672182224}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10015621671395541}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11581437543817096}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [53, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 80, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41716159434200817}, 'layer_2_size': 22, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2771543675500986}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11232213779671203}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.36313792725811167}, 'layer_5_size': 49, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [54, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2007607438272935}, 'layer_4_size': 91, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [55, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1050661373343273}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36950674751954005}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [56, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16745485505931956}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [57, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31409164825353697}, 'layer_5_size': 91, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [58, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39805956850419555}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 61, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15198369221316727}, 'layer_4_size': 25, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [59, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3875675418941298}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 57, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 80, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [60, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 8, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [61, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.31887286985950053}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45841837674693586}, 'layer_3_size': 62, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [62, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 11, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.43181898460277857}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [63, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36120883956828265}, 'layer_1_size': 8, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [64, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35109665238578447}, 'layer_1_size': 80, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 25, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [65, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [66, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 66, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19810962561359832}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 2, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [67, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1579993964245491}, 'layer_5_size': 35, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [68, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43877911310812734}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 21, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31194649777488603}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [69, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15353575675845962}, 'layer_3_size': 88, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 49, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [70, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 12, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 91, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [71, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.46574055977979867}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [72, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17922789040016737}, 'layer_3_size': 84, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.262831125587177}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [73, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [74, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 28, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.44774437757480967}, 'layer_2_size': 85, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4624925505301145}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [75, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2545763365188426}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3379367451056181}, 'layer_4_size': 56, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [76, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 46, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14491643896200868}, 'layer_5_size': 98, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [77, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15132983247231468}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13553206179667554}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [78, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37013647248944015}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2717217268544919}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 18, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 91, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.39924731376297173}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [79, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.453485542524365}, 'layer_1_size': 59, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3822643582013816}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4743773597797818}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12857141911581393}, 'layer_4_size': 3, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2253611744405719}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [80, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18391069289201956}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 56, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20412479331356678}, 'layer_3_size': 97, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]] 

*** 81 configurations x 1.0 iterations each

1 | Fri Sep 28 01:18:47 2018 | lowest loss so far: inf (run -1)

{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  60 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 34s - loss: 1.0329
4224/6530 [==================>...........] - ETA: 0s - loss: 0.3834 
6530/6530 [==============================] - 1s 121us/step - loss: 0.2787 - val_loss: 0.0818
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  59 | activation: tanh    | extras: batchnorm 
layer 2 | size:  18 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 21s - loss: 0.7358
5376/6530 [=======================>......] - ETA: 0s - loss: 0.6332 
6530/6530 [==============================] - 1s 147us/step - loss: 0.6062 - val_loss: 0.4202

# training | RMSE: 0.4818, MAE: 0.4151
worker 2  xfile  [2, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 59, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12856911536933047}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2940647648620235}, 'layer_4_size': 94, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.4818073798484275, 'rmse': 0.4818073798484275, 'mae': 0.41512152035146405, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  22 | activation: sigmoid | extras: None 
layer 2 | size:  30 | activation: relu    | extras: dropout - rate: 20.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f041fa28be0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 8s - loss: 0.7359
3712/6530 [================>.............] - ETA: 0s - loss: 0.4306
6530/6530 [==============================] - 0s 45us/step - loss: 0.3337 - val_loss: 0.2237

# training | RMSE: 0.2855, MAE: 0.2321
worker 1  xfile  [1, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4248286730916818}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2854713958259964, 'rmse': 0.2854713958259964, 'mae': 0.2321143678494358, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  55 | activation: tanh    | extras: None 
layer 2 | size:  28 | activation: relu    | extras: None 
layer 3 | size:  87 | activation: tanh    | extras: dropout - rate: 28.6% 
layer 4 | size:  21 | activation: sigmoid | extras: batchnorm 
layer 5 | size:  21 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f041fa28b38>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 22s - loss: 0.5566
1792/6530 [=======>......................] - ETA: 1s - loss: 0.4904 {'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  10 | activation: tanh    | extras: batchnorm 
layer 2 | size:  31 | activation: tanh    | extras: batchnorm 
layer 3 | size:  32 | activation: tanh    | extras: batchnorm 
layer 4 | size:  10 | activation: tanh    | extras: None 
layer 5 | size:  57 | activation: sigmoid | extras: dropout - rate: 15.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 7:56 - loss: 0.5886
3584/6530 [===============>..............] - ETA: 0s - loss: 0.4113
 192/6530 [..............................] - ETA: 40s - loss: 0.3785 
5376/6530 [=======================>......] - ETA: 0s - loss: 0.3351
 368/6530 [>.............................] - ETA: 21s - loss: 0.2972
# training | RMSE: 0.2733, MAE: 0.2214
worker 2  xfile  [4, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 22, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2000848348489106}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.27334764907378634, 'rmse': 0.27334764907378634, 'mae': 0.22143080124281378, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  37 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c0d4ef0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 56s - loss: 0.5134
6530/6530 [==============================] - 1s 108us/step - loss: 0.2949 - val_loss: 0.1241

 544/6530 [=>............................] - ETA: 14s - loss: 0.2658
 704/6530 [==>...........................] - ETA: 1s - loss: 0.4599 
 704/6530 [==>...........................] - ETA: 11s - loss: 0.2478
1360/6530 [=====>........................] - ETA: 0s - loss: 0.3375
 880/6530 [===>..........................] - ETA: 9s - loss: 0.2364 
2080/6530 [========>.....................] - ETA: 0s - loss: 0.2430
1056/6530 [===>..........................] - ETA: 7s - loss: 0.2282
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1914
1232/6530 [====>.........................] - ETA: 6s - loss: 0.2191
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1609
1408/6530 [=====>........................] - ETA: 5s - loss: 0.2130
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1405
1584/6530 [======>.......................] - ETA: 5s - loss: 0.2084
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1267
1760/6530 [=======>......................] - ETA: 4s - loss: 0.2044
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1157
1952/6530 [=======>......................] - ETA: 4s - loss: 0.2022
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1078
2128/6530 [========>.....................] - ETA: 3s - loss: 0.2000
6530/6530 [==============================] - 1s 99us/step - loss: 0.1051 - val_loss: 0.0412

2304/6530 [=========>....................] - ETA: 3s - loss: 0.1975
2480/6530 [==========>...................] - ETA: 3s - loss: 0.1956
2656/6530 [===========>..................] - ETA: 2s - loss: 0.1942
2832/6530 [============>.................] - ETA: 2s - loss: 0.1921
3024/6530 [============>.................] - ETA: 2s - loss: 0.1910
3216/6530 [=============>................] - ETA: 2s - loss: 0.1896
3408/6530 [==============>...............] - ETA: 1s - loss: 0.1883
3584/6530 [===============>..............] - ETA: 1s - loss: 0.1878
3760/6530 [================>.............] - ETA: 1s - loss: 0.1871
# training | RMSE: 0.3516, MAE: 0.3087
worker 1  xfile  [3, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.28590221771346425}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 21, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.3516271411053419, 'rmse': 0.3516271411053419, 'mae': 0.30870277854837624, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  61 | activation: tanh    | extras: dropout - rate: 41.3% 
layer 2 | size:   3 | activation: relu    | extras: dropout - rate: 14.0% 
layer 3 | size:  20 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  87 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f03f06fafd0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 19s - loss: 0.7678
3936/6530 [=================>............] - ETA: 1s - loss: 0.1857
2560/6530 [==========>...................] - ETA: 0s - loss: 0.2011 
4112/6530 [=================>............] - ETA: 1s - loss: 0.1852
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1483
4288/6530 [==================>...........] - ETA: 1s - loss: 0.1846
4464/6530 [===================>..........] - ETA: 1s - loss: 0.1844
6530/6530 [==============================] - 1s 89us/step - loss: 0.1311 - val_loss: 0.0730

4640/6530 [====================>.........] - ETA: 1s - loss: 0.1838
# training | RMSE: 0.1998, MAE: 0.1630
worker 2  xfile  [5, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2820274393310773}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.19978911117306108, 'rmse': 0.19978911117306108, 'mae': 0.16301657340483808, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  63 | activation: sigmoid | extras: dropout - rate: 39.1% 
layer 2 | size:  97 | activation: relu    | extras: dropout - rate: 37.6% 
layer 3 | size:  29 | activation: relu    | extras: dropout - rate: 35.8% 
layer 4 | size:  82 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c09ab70>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 12s - loss: 0.4335
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1832
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1066 
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1830
6528/6530 [============================>.] - ETA: 0s - loss: 0.0911
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1825
6530/6530 [==============================] - 0s 61us/step - loss: 0.0911 - val_loss: 0.0691

5296/6530 [=======================>......] - ETA: 0s - loss: 0.1819
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1815
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1814
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1804
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1800
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1790
# training | RMSE: 0.2744, MAE: 0.2198
worker 1  xfile  [6, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.41324829818739683}, 'layer_1_size': 61, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13975984822005746}, 'layer_2_size': 3, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 20, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.27436726312961646, 'rmse': 0.27436726312961646, 'mae': 0.21976571403899378, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  42 | activation: relu    | extras: dropout - rate: 46.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f03f06faeb8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 4s - loss: 1.7598
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1792
6530/6530 [==============================] - 0s 38us/step - loss: 1.1351 - val_loss: 0.5608

6464/6530 [============================>.] - ETA: 0s - loss: 0.1792
6530/6530 [==============================] - 3s 493us/step - loss: 0.1790 - val_loss: 0.2010

# training | RMSE: 0.7643, MAE: 0.6954
worker 1  xfile  [8, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46040379328519976}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 41, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3727395901974413}, 'layer_4_size': 44, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.7642582081345706, 'rmse': 0.7642582081345706, 'mae': 0.695399545263153, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  91 | activation: tanh    | extras: None 
layer 2 | size:  90 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f03f073f710>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 10s - loss: 1.0160
4352/6530 [==================>...........] - ETA: 0s - loss: 0.4043 
6530/6530 [==============================] - 0s 48us/step - loss: 0.3415 - val_loss: 0.3862

# training | RMSE: 0.2640, MAE: 0.2152
worker 2  xfile  [7, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39132251248214844}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37621340906553147}, 'layer_2_size': 97, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3580614653399373}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.263971315913358, 'rmse': 0.263971315913358, 'mae': 0.21518096852158028, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  15 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  28 | activation: relu    | extras: batchnorm 
layer 3 | size:  86 | activation: tanh    | extras: dropout - rate: 35.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f03d84a5ba8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:55 - loss: 0.9634
 416/6530 [>.............................] - ETA: 9s - loss: 0.5056  
 832/6530 [==>...........................] - ETA: 4s - loss: 0.4070
# training | RMSE: 0.2393, MAE: 0.1988
worker 0  xfile  [0, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 10, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 31, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15150219925735608}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.2393276975245625, 'rmse': 0.2393276975245625, 'mae': 0.19878275285056046, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  14 | activation: sigmoid | extras: dropout - rate: 47.6% 
layer 2 | size:  94 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041fa28a90>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 11s - loss: 0.7133
1248/6530 [====>.........................] - ETA: 3s - loss: 0.3650
3456/6530 [==============>...............] - ETA: 0s - loss: 0.4172 
1760/6530 [=======>......................] - ETA: 2s - loss: 0.3297
2368/6530 [=========>....................] - ETA: 1s - loss: 0.3084
6530/6530 [==============================] - 0s 54us/step - loss: 0.3387 - val_loss: 0.2160

2944/6530 [============>.................] - ETA: 1s - loss: 0.2950
3552/6530 [===============>..............] - ETA: 0s - loss: 0.2831
4192/6530 [==================>...........] - ETA: 0s - loss: 0.2716
4800/6530 [=====================>........] - ETA: 0s - loss: 0.2623
5344/6530 [=======================>......] - ETA: 0s - loss: 0.2556
5920/6530 [==========================>...] - ETA: 0s - loss: 0.2481
6528/6530 [============================>.] - ETA: 0s - loss: 0.2434
6530/6530 [==============================] - 1s 193us/step - loss: 0.2435 - val_loss: 0.2056

# training | RMSE: 0.4767, MAE: 0.3864
worker 1  xfile  [10, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 91, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 90, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.4766602841883529, 'rmse': 0.4766602841883529, 'mae': 0.3864265577571806, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  58 | activation: tanh    | extras: batchnorm 
layer 2 | size:  17 | activation: tanh    | extras: dropout - rate: 45.5% 
layer 3 | size:  38 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03d4328c88>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 27s - loss: 0.8777
2048/6530 [========>.....................] - ETA: 1s - loss: 0.6306 
3968/6530 [=================>............] - ETA: 0s - loss: 0.4807
6016/6530 [==========================>...] - ETA: 0s - loss: 0.3682
6530/6530 [==============================] - 1s 119us/step - loss: 0.3466 - val_loss: 0.0495

# training | RMSE: 0.2650, MAE: 0.2156
worker 0  xfile  [11, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47570319050681575}, 'layer_1_size': 14, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4787974697856523}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1315792131888107}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2650326614287187, 'rmse': 0.2650326614287187, 'mae': 0.2155820858417705, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  74 | activation: tanh    | extras: batchnorm 
layer 2 | size:  29 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  62 | activation: sigmoid | extras: dropout - rate: 45.5% 
layer 4 | size:  35 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03f4184668>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 26s - loss: 0.7478
1792/6530 [=======>......................] - ETA: 1s - loss: 0.4322 
3712/6530 [================>.............] - ETA: 0s - loss: 0.3112
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2613
6530/6530 [==============================] - 1s 115us/step - loss: 0.2497 - val_loss: 0.1657

# training | RMSE: 0.2530, MAE: 0.2011
worker 2  xfile  [9, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 28, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35320267780836245}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26723916442710643}, 'layer_4_size': 6, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.25295909320921367, 'rmse': 0.25295909320921367, 'mae': 0.20109288510138051, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  54 | activation: sigmoid | extras: dropout - rate: 23.0% 
layer 2 | size:  44 | activation: relu    | extras: batchnorm 
layer 3 | size:  21 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03d84a5a90>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 24s - loss: 0.6473
1664/6530 [======>.......................] - ETA: 1s - loss: 0.5466 
3328/6530 [==============>...............] - ETA: 0s - loss: 0.4689
4864/6530 [=====================>........] - ETA: 0s - loss: 0.4260
6272/6530 [===========================>..] - ETA: 0s - loss: 0.3995
# training | RMSE: 0.2182, MAE: 0.1727
worker 1  xfile  [12, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4554561189849352}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.379198274209536}, 'layer_4_size': 21, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.2181865332211612, 'rmse': 0.2181865332211612, 'mae': 0.17270334155146588, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  63 | activation: relu    | extras: batchnorm 
layer 2 | size:  18 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f03f805ae48>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 10s - loss: 1.0207
6530/6530 [==============================] - 1s 115us/step - loss: 0.3937 - val_loss: 0.2130

4608/6530 [====================>.........] - ETA: 0s - loss: 0.5910 
6530/6530 [==============================] - 1s 85us/step - loss: 0.5021 - val_loss: 0.2689

# training | RMSE: 0.2119, MAE: 0.1655
worker 0  xfile  [13, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45512557545093135}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 35, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.21186969401658748, 'rmse': 0.21186969401658748, 'mae': 0.16550892503617995, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  38 | activation: relu    | extras: batchnorm 
layer 2 | size:  13 | activation: sigmoid | extras: dropout - rate: 34.3% 
layer 3 | size:  28 | activation: sigmoid | extras: None 
layer 4 | size:  67 | activation: relu    | extras: dropout - rate: 33.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f03f41844a8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 42s - loss: 0.3541
1152/6530 [====>.........................] - ETA: 2s - loss: 0.2909 
2304/6530 [=========>....................] - ETA: 0s - loss: 0.2539
3648/6530 [===============>..............] - ETA: 0s - loss: 0.2310
# training | RMSE: 0.2625, MAE: 0.2154
worker 2  xfile  [14, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22993113229229667}, 'layer_1_size': 54, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2918635922408792}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.262513166064599, 'rmse': 0.262513166064599, 'mae': 0.2153860233147212, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  78 | activation: tanh    | extras: None 
layer 2 | size:  22 | activation: tanh    | extras: None 
layer 3 | size:  69 | activation: tanh    | extras: dropout - rate: 40.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f03d84a5978>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:17 - loss: 0.4624
4928/6530 [=====================>........] - ETA: 0s - loss: 0.2192
# training | RMSE: 0.3449, MAE: 0.2716
worker 1  xfile  [15, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22807816925739277}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.34494542731654787, 'rmse': 0.34494542731654787, 'mae': 0.27157603525218044, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  83 | activation: relu    | extras: dropout - rate: 27.4% 
layer 2 | size:  40 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f03f805ae80>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:18 - loss: 2.9243
 448/6530 [=>............................] - ETA: 5s - loss: 0.1200  
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2106
 928/6530 [===>..........................] - ETA: 2s - loss: 1.2566  
 848/6530 [==>...........................] - ETA: 2s - loss: 0.0951
6530/6530 [==============================] - 1s 116us/step - loss: 0.2089 - val_loss: 0.2530

1792/6530 [=======>......................] - ETA: 1s - loss: 0.9811
1264/6530 [====>.........................] - ETA: 2s - loss: 0.0857
2688/6530 [===========>..................] - ETA: 0s - loss: 0.8456
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0803
3552/6530 [===============>..............] - ETA: 0s - loss: 0.7566
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0736
4384/6530 [===================>..........] - ETA: 0s - loss: 0.6980
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0704
5152/6530 [======================>.......] - ETA: 0s - loss: 0.6540
2832/6530 [============>.................] - ETA: 0s - loss: 0.0686
5888/6530 [==========================>...] - ETA: 0s - loss: 0.6195
3232/6530 [=============>................] - ETA: 0s - loss: 0.0666
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0649
6530/6530 [==============================] - 1s 128us/step - loss: 0.5926 - val_loss: 0.2535

4080/6530 [=================>............] - ETA: 0s - loss: 0.0636
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0621
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0609
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0596
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0582
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0573
6530/6530 [==============================] - 1s 184us/step - loss: 0.0571 - val_loss: 0.0578

# training | RMSE: 0.3184, MAE: 0.2559
worker 1  xfile  [18, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2738264200831}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 11, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4205563349101873}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3741232536469342}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.31836518244079876, 'rmse': 0.31836518244079876, 'mae': 0.2559161899417563, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  44 | activation: relu    | extras: None 
layer 2 | size:  11 | activation: relu    | extras: None 
layer 3 | size:  37 | activation: relu    | extras: dropout - rate: 33.6% 
layer 4 | size:  33 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d8696dd8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 35s - loss: 0.8012
1408/6530 [=====>........................] - ETA: 1s - loss: 0.3928 
2624/6530 [===========>..................] - ETA: 0s - loss: 0.3474
3840/6530 [================>.............] - ETA: 0s - loss: 0.3158
# training | RMSE: 0.3094, MAE: 0.2555
worker 0  xfile  [16, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 38, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.343391534079115}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3383178210354845}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2956271235310313}, 'layer_5_size': 4, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.3093624745609728, 'rmse': 0.3093624745609728, 'mae': 0.2554810509969359, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  70 | activation: tanh    | extras: dropout - rate: 12.9% 
layer 2 | size:  19 | activation: tanh    | extras: batchnorm 
layer 3 | size:  41 | activation: relu    | extras: None 
layer 4 | size:  57 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03c06ee470>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 56s - loss: 0.7094
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2980
 832/6530 [==>...........................] - ETA: 4s - loss: 0.5445 
1664/6530 [======>.......................] - ETA: 1s - loss: 0.3782
6530/6530 [==============================] - 1s 102us/step - loss: 0.2821 - val_loss: 0.1986

2560/6530 [==========>...................] - ETA: 1s - loss: 0.3131
3520/6530 [===============>..............] - ETA: 0s - loss: 0.2762
4544/6530 [===================>..........] - ETA: 0s - loss: 0.2550
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2389
6530/6530 [==============================] - 1s 151us/step - loss: 0.2296 - val_loss: 0.2887

# training | RMSE: 0.2394, MAE: 0.2002
worker 2  xfile  [17, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.40750158041712903}, 'layer_3_size': 69, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4048369905403586}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2394052565106703, 'rmse': 0.2394052565106703, 'mae': 0.20020817975949054, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  56 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  42 | activation: relu    | extras: dropout - rate: 24.2% 
layer 3 | size:  15 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  37 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00dca70f98>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:02 - loss: 1.3469
 896/6530 [===>..........................] - ETA: 4s - loss: 0.7750  
1728/6530 [======>.......................] - ETA: 1s - loss: 0.5500
2560/6530 [==========>...................] - ETA: 1s - loss: 0.4165
3584/6530 [===============>..............] - ETA: 0s - loss: 0.3199
4544/6530 [===================>..........] - ETA: 0s - loss: 0.2638
# training | RMSE: 0.3475, MAE: 0.2856
worker 0  xfile  [19, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12932097718679955}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4763850999934498}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.34750579715072694, 'rmse': 0.34750579715072694, 'mae': 0.28557814235826606, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  46 | activation: sigmoid | extras: None 
layer 2 | size:  40 | activation: relu    | extras: None 
layer 3 | size:  16 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00e0166668>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:22 - loss: 1.1228
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2239
 544/6530 [=>............................] - ETA: 4s - loss: 0.1110  
1040/6530 [===>..........................] - ETA: 2s - loss: 0.0865
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0727
6530/6530 [==============================] - 1s 163us/step - loss: 0.1975 - val_loss: 0.0460

1984/6530 [========>.....................] - ETA: 1s - loss: 0.0674
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0639
2896/6530 [============>.................] - ETA: 0s - loss: 0.0609
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0583
3936/6530 [=================>............] - ETA: 0s - loss: 0.0565
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0551
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0537
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0524
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0517
6400/6530 [============================>.] - ETA: 0s - loss: 0.0508
6530/6530 [==============================] - 1s 168us/step - loss: 0.0504 - val_loss: 0.0669

# training | RMSE: 0.2364, MAE: 0.1932
worker 1  xfile  [20, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 44, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3360410430399626}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23380908750718243}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2363685915954125, 'rmse': 0.2363685915954125, 'mae': 0.1931584648541806, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  84 | activation: tanh    | extras: batchnorm 
layer 2 | size:  21 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  37 | activation: relu    | extras: batchnorm 
layer 4 | size:  60 | activation: sigmoid | extras: dropout - rate: 21.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d8696cc0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 40s - loss: 0.5745
1536/6530 [======>.......................] - ETA: 2s - loss: 0.2760 
3072/6530 [=============>................] - ETA: 1s - loss: 0.1616
# training | RMSE: 0.2060, MAE: 0.1615
worker 2  xfile  [21, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2423132848388729}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2060397182014802, 'rmse': 0.2060397182014802, 'mae': 0.16152860325564278, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  66 | activation: tanh    | extras: dropout - rate: 14.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00dc84f828>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 16s - loss: 0.7272
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1218
4352/6530 [==================>...........] - ETA: 0s - loss: 0.4744 
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1000
6530/6530 [==============================] - 0s 71us/step - loss: 0.3711 - val_loss: 0.1951

6530/6530 [==============================] - 1s 176us/step - loss: 0.0957 - val_loss: 0.0379

# training | RMSE: 0.2566, MAE: 0.2149
worker 0  xfile  [23, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.18924415336054554}, 'layer_4_size': 53, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.25662650521635705, 'rmse': 0.25662650521635705, 'mae': 0.21493768225380444, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  39 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00e0166470>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 12s - loss: 0.5294
6530/6530 [==============================] - 1s 92us/step - loss: 0.5238 - val_loss: 0.4548

# training | RMSE: 0.1853, MAE: 0.1406
worker 1  xfile  [22, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21048884801807022}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.18533863422514246, 'rmse': 0.18533863422514246, 'mae': 0.14058317879619167, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  52 | activation: tanh    | extras: dropout - rate: 42.3% 
layer 2 | size:  24 | activation: relu    | extras: None 
layer 3 | size: 100 | activation: tanh    | extras: dropout - rate: 38.9% 
layer 4 | size:  80 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d8099550>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 13s - loss: 0.5663
3328/6530 [==============>...............] - ETA: 0s - loss: 0.3657 
# training | RMSE: 0.2392, MAE: 0.1940
worker 2  xfile  [24, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14684617353368484}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3859620787869812}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15776419479082585}, 'layer_3_size': 23, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 89, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.2392001790667556, 'rmse': 0.2392001790667556, 'mae': 0.19404018503385126, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  16 | activation: sigmoid | extras: None 
layer 2 | size:  54 | activation: tanh    | extras: dropout - rate: 40.9% 
layer 3 | size:  67 | activation: tanh    | extras: batchnorm 
layer 4 | size:  88 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00dc0f60b8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 32s - loss: 1.4956
1920/6530 [=======>......................] - ETA: 1s - loss: 0.9435 
3840/6530 [================>.............] - ETA: 0s - loss: 0.7616
6530/6530 [==============================] - 1s 114us/step - loss: 0.3166 - val_loss: 0.1953

5760/6530 [=========================>....] - ETA: 0s - loss: 0.6581
6530/6530 [==============================] - 1s 141us/step - loss: 0.6316 - val_loss: 0.2297

# training | RMSE: 0.6718, MAE: 0.6314
worker 0  xfile  [25, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 39, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17463050599410126}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.6717972452706601, 'rmse': 0.6717972452706601, 'mae': 0.6314325958784691, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  49 | activation: sigmoid | extras: None 
layer 2 | size:  66 | activation: tanh    | extras: dropout - rate: 21.7% 
layer 3 | size:  10 | activation: tanh    | extras: batchnorm 
layer 4 | size:  25 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d36c4160>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:24 - loss: 1.5748
 176/6530 [..............................] - ETA: 25s - loss: 1.3396 
 352/6530 [>.............................] - ETA: 13s - loss: 1.0978
 560/6530 [=>............................] - ETA: 8s - loss: 0.9470 
 752/6530 [==>...........................] - ETA: 6s - loss: 0.8489
 944/6530 [===>..........................] - ETA: 5s - loss: 0.7714
1168/6530 [====>.........................] - ETA: 4s - loss: 0.6906
1376/6530 [=====>........................] - ETA: 3s - loss: 0.6232
1600/6530 [======>.......................] - ETA: 3s - loss: 0.5690
1808/6530 [=======>......................] - ETA: 2s - loss: 0.5301
2032/6530 [========>.....................] - ETA: 2s - loss: 0.4947
# training | RMSE: 0.2921, MAE: 0.2343
worker 2  xfile  [26, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4086446132962229}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35029228823647773}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2920726154462038, 'rmse': 0.2920726154462038, 'mae': 0.23432203706141852, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  33 | activation: relu    | extras: None 
layer 2 | size:  29 | activation: tanh    | extras: dropout - rate: 44.8% 
layer 3 | size:  67 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00bf9edd30>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 53s - loss: 0.6090
2288/6530 [=========>....................] - ETA: 2s - loss: 0.4585
2544/6530 [==========>...................] - ETA: 2s - loss: 0.4281
1728/6530 [======>.......................] - ETA: 1s - loss: 0.4318 
2800/6530 [===========>..................] - ETA: 1s - loss: 0.4022
3392/6530 [==============>...............] - ETA: 0s - loss: 0.3734
5120/6530 [======================>.......] - ETA: 0s - loss: 0.3392
3056/6530 [=============>................] - ETA: 1s - loss: 0.3804
3312/6530 [==============>...............] - ETA: 1s - loss: 0.3613
# training | RMSE: 0.2323, MAE: 0.1890
worker 1  xfile  [27, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4226684261892001}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38909506041759967}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11883962019293581}, 'layer_5_size': 81, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.23232000444381448, 'rmse': 0.23232000444381448, 'mae': 0.1890413233487523, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  17 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  17 | activation: sigmoid | extras: None 
layer 3 | size:   8 | activation: tanh    | extras: dropout - rate: 18.2% 
layer 4 | size:   8 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00ab8f4208>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 17s - loss: 0.5592
3552/6530 [===============>..............] - ETA: 1s - loss: 0.3449
6530/6530 [==============================] - 1s 122us/step - loss: 0.3219 - val_loss: 0.1958

4352/6530 [==================>...........] - ETA: 0s - loss: 0.4530 
3808/6530 [================>.............] - ETA: 1s - loss: 0.3297
4048/6530 [=================>............] - ETA: 0s - loss: 0.3159
6530/6530 [==============================] - 1s 135us/step - loss: 0.4043 - val_loss: 0.2748

4272/6530 [==================>...........] - ETA: 0s - loss: 0.3047
4544/6530 [===================>..........] - ETA: 0s - loss: 0.2926
4784/6530 [====================>.........] - ETA: 0s - loss: 0.2827
5040/6530 [======================>.......] - ETA: 0s - loss: 0.2730
5296/6530 [=======================>......] - ETA: 0s - loss: 0.2638
5552/6530 [========================>.....] - ETA: 0s - loss: 0.2556
5808/6530 [=========================>....] - ETA: 0s - loss: 0.2473
6048/6530 [==========================>...] - ETA: 0s - loss: 0.2403
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2343
6528/6530 [============================>.] - ETA: 0s - loss: 0.2281
6530/6530 [==============================] - 2s 341us/step - loss: 0.2281 - val_loss: 0.0471

# training | RMSE: 0.2511, MAE: 0.1978
worker 2  xfile  [30, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4479261069663404}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2510627303719418, 'rmse': 0.2510627303719418, 'mae': 0.19776570743644897, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  42 | activation: tanh    | extras: None 
layer 2 | size:  66 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00bf9edb70>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 45s - loss: 0.9023
2048/6530 [========>.....................] - ETA: 1s - loss: 0.3848 
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2141
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1570
# training | RMSE: 0.3298, MAE: 0.2657
worker 1  xfile  [29, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18201157345841038}, 'layer_3_size': 8, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.3298481515243315, 'rmse': 0.3298481515243315, 'mae': 0.2656673118393694, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  70 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00ab509240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 52s - loss: 0.3832
6530/6530 [==============================] - 1s 103us/step - loss: 0.1513 - val_loss: 0.0411

2304/6530 [=========>....................] - ETA: 1s - loss: 0.2067 
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1637
6530/6530 [==============================] - 1s 113us/step - loss: 0.1374 - val_loss: 0.0784

# training | RMSE: 0.2189, MAE: 0.1754
worker 0  xfile  [28, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21704675266779963}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.21892003173365218, 'rmse': 0.21892003173365218, 'mae': 0.17535006974133316, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  52 | activation: sigmoid | extras: dropout - rate: 31.1% 
layer 2 | size:  18 | activation: sigmoid | extras: None 
layer 3 | size:  98 | activation: relu    | extras: dropout - rate: 26.9% 
layer 4 | size:  55 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d2f477b8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:02 - loss: 1.6154
 672/6530 [==>...........................] - ETA: 5s - loss: 0.7670  
1344/6530 [=====>........................] - ETA: 2s - loss: 0.5454
# training | RMSE: 0.2758, MAE: 0.2217
worker 1  xfile  [32, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 75, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32191812317889457}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18336331704955233}, 'layer_5_size': 9, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.275775791924289, 'rmse': 0.275775791924289, 'mae': 0.221711197173964, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  36 | activation: sigmoid | extras: None 
layer 2 | size:  92 | activation: relu    | extras: dropout - rate: 15.8% 
layer 3 | size:  80 | activation: sigmoid | extras: dropout - rate: 22.1% 
layer 4 | size:  11 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00aaf4d630>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 14s - loss: 0.5799
2048/6530 [========>.....................] - ETA: 1s - loss: 0.4440
# training | RMSE: 0.2012, MAE: 0.1607
worker 2  xfile  [31, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4979476961384771}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20115904583170496, 'rmse': 0.20115904583170496, 'mae': 0.1607017169106847, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  15 | activation: tanh    | extras: batchnorm 
layer 2 | size:  45 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00bf9edb38>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:58 - loss: 1.1742
6400/6530 [============================>.] - ETA: 0s - loss: 0.1258 
2720/6530 [===========>..................] - ETA: 1s - loss: 0.3938
 800/6530 [==>...........................] - ETA: 4s - loss: 0.4620  
3392/6530 [==============>...............] - ETA: 0s - loss: 0.3602
1600/6530 [======>.......................] - ETA: 2s - loss: 0.3594
6530/6530 [==============================] - 1s 108us/step - loss: 0.1245 - val_loss: 0.0686

4128/6530 [=================>............] - ETA: 0s - loss: 0.3357
2400/6530 [==========>...................] - ETA: 1s - loss: 0.3170
4832/6530 [=====================>........] - ETA: 0s - loss: 0.3176
3200/6530 [=============>................] - ETA: 0s - loss: 0.2922
5504/6530 [========================>.....] - ETA: 0s - loss: 0.3057
4000/6530 [=================>............] - ETA: 0s - loss: 0.2782
6112/6530 [===========================>..] - ETA: 0s - loss: 0.2967
4704/6530 [====================>.........] - ETA: 0s - loss: 0.2670
5472/6530 [========================>.....] - ETA: 0s - loss: 0.2575
6240/6530 [===========================>..] - ETA: 0s - loss: 0.2491
6530/6530 [==============================] - 1s 181us/step - loss: 0.2910 - val_loss: 0.1863

6530/6530 [==============================] - 1s 168us/step - loss: 0.2462 - val_loss: 0.1713

# training | RMSE: 0.2636, MAE: 0.2159
worker 1  xfile  [35, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15758638976998862}, 'layer_2_size': 92, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22134854478231542}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4731865489395358}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.263565081085954, 'rmse': 0.263565081085954, 'mae': 0.21585005364450388, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  46 | activation: sigmoid | extras: dropout - rate: 35.2% 
layer 2 | size:  17 | activation: tanh    | extras: dropout - rate: 46.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00aad770f0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:46 - loss: 0.4765
 928/6530 [===>..........................] - ETA: 3s - loss: 0.2120  
1888/6530 [=======>......................] - ETA: 1s - loss: 0.1391
2624/6530 [===========>..................] - ETA: 1s - loss: 0.1191
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1055
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0971
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0904
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0852
# training | RMSE: 0.2301, MAE: 0.1880
worker 0  xfile  [33, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.31082407783245625}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26877401178424}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.23007905321937544, 'rmse': 0.23007905321937544, 'mae': 0.18798611802398194, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:   2 | activation: sigmoid | extras: None 
layer 2 | size:  22 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d2f47588>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 55s - loss: 0.4852
6530/6530 [==============================] - 1s 153us/step - loss: 0.0824 - val_loss: 0.0562

1984/6530 [========>.....................] - ETA: 1s - loss: 0.3095 
3904/6530 [================>.............] - ETA: 0s - loss: 0.2077
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1657
6530/6530 [==============================] - 1s 123us/step - loss: 0.1512 - val_loss: 0.0684

# training | RMSE: 0.2170, MAE: 0.1717
worker 2  xfile  [34, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.2169890926074878, 'rmse': 0.2169890926074878, 'mae': 0.17172221550808475, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  57 | activation: relu    | extras: dropout - rate: 14.6% 
layer 2 | size:  62 | activation: relu    | extras: batchnorm 
layer 3 | size:  70 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00bf375400>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 17s - loss: 1.2573
4352/6530 [==================>...........] - ETA: 0s - loss: 0.5668 
6530/6530 [==============================] - 1s 135us/step - loss: 0.4399 - val_loss: 0.0886

# training | RMSE: 0.2380, MAE: 0.1926
worker 1  xfile  [36, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35223028731922856}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4611808223007189}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 30, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.23799221779334728, 'rmse': 0.23799221779334728, 'mae': 0.19263193995157463, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  69 | activation: tanh    | extras: None 
layer 2 | size:  60 | activation: tanh    | extras: None 
layer 3 | size:  52 | activation: relu    | extras: None 
layer 4 | size:  60 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00aaa7f630>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 35s - loss: 1.6511
2560/6530 [==========>...................] - ETA: 1s - loss: 0.4179 
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2565
6530/6530 [==============================] - 1s 143us/step - loss: 0.2024 - val_loss: 0.0447

# training | RMSE: 0.2635, MAE: 0.2160
worker 0  xfile  [37, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 2, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 51, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1776477202562827}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2634752635232708, 'rmse': 0.2634752635232708, 'mae': 0.21599291178465846, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  28 | activation: tanh    | extras: dropout - rate: 35.6% 
layer 2 | size:  30 | activation: sigmoid | extras: dropout - rate: 42.2% 
layer 3 | size:  41 | activation: relu    | extras: batchnorm 
layer 4 | size:  81 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d2a6ac18>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 16s - loss: 1.1805
3584/6530 [===============>..............] - ETA: 0s - loss: 0.4322 
6530/6530 [==============================] - 1s 133us/step - loss: 0.3144 - val_loss: 0.0609

# training | RMSE: 0.3080, MAE: 0.2434
worker 2  xfile  [38, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14640075203083014}, 'layer_1_size': 57, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16892410402799152}, 'layer_4_size': 13, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.30796111422989403, 'rmse': 0.30796111422989403, 'mae': 0.24344387417252955, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  36 | activation: sigmoid | extras: dropout - rate: 42.0% 
layer 2 | size:  80 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  72 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00bfb0a278>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:10 - loss: 1.3109
 240/6530 [>.............................] - ETA: 21s - loss: 0.5244 
 496/6530 [=>............................] - ETA: 10s - loss: 0.3079
 784/6530 [==>...........................] - ETA: 6s - loss: 0.2233 
1056/6530 [===>..........................] - ETA: 5s - loss: 0.1865
1296/6530 [====>.........................] - ETA: 4s - loss: 0.1670
1584/6530 [======>.......................] - ETA: 3s - loss: 0.1527
1888/6530 [=======>......................] - ETA: 2s - loss: 0.1411
2176/6530 [========>.....................] - ETA: 2s - loss: 0.1344
2480/6530 [==========>...................] - ETA: 1s - loss: 0.1279
2816/6530 [===========>..................] - ETA: 1s - loss: 0.1220
3088/6530 [=============>................] - ETA: 1s - loss: 0.1171
3376/6530 [==============>...............] - ETA: 1s - loss: 0.1134
3712/6530 [================>.............] - ETA: 1s - loss: 0.1108
# training | RMSE: 0.2079, MAE: 0.1688
worker 1  xfile  [39, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2078858107008423, 'rmse': 0.2078858107008423, 'mae': 0.16875587435440598, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  68 | activation: tanh    | extras: dropout - rate: 42.0% 
layer 2 | size:  72 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00aadaa8d0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:34 - loss: 0.8586
4016/6530 [=================>............] - ETA: 0s - loss: 0.1076
 960/6530 [===>..........................] - ETA: 4s - loss: 0.1788  
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1056
1920/6530 [=======>......................] - ETA: 2s - loss: 0.1224
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1037
2784/6530 [===========>..................] - ETA: 1s - loss: 0.1029
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1017
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0910
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1005
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0833
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0994
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0784
# training | RMSE: 0.2487, MAE: 0.2023
worker 0  xfile  [40, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35579117525473236}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4220893331897563}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.24871212475003002, 'rmse': 0.24871212475003002, 'mae': 0.2022561387470193, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  60 | activation: tanh    | extras: batchnorm 
layer 2 | size:  97 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d2fd6898>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:03 - loss: 0.5301
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0982
6464/6530 [============================>.] - ETA: 0s - loss: 0.0749
1472/6530 [=====>........................] - ETA: 2s - loss: 0.1910  
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0972
2880/6530 [============>.................] - ETA: 0s - loss: 0.1790
6512/6530 [============================>.] - ETA: 0s - loss: 0.0960
6530/6530 [==============================] - 1s 187us/step - loss: 0.0745 - val_loss: 0.0438

4352/6530 [==================>...........] - ETA: 0s - loss: 0.1746
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1712
6530/6530 [==============================] - 2s 312us/step - loss: 0.0960 - val_loss: 0.0697

6530/6530 [==============================] - 1s 149us/step - loss: 0.1707 - val_loss: 0.2556

# training | RMSE: 0.2090, MAE: 0.1709
worker 1  xfile  [42, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4196428123159085}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 76, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.20901232995542493, 'rmse': 0.20901232995542493, 'mae': 0.1709407161386968, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  73 | activation: tanh    | extras: dropout - rate: 11.4% 
layer 2 | size:  82 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00ab02b5c0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:04 - loss: 2.5845
 576/6530 [=>............................] - ETA: 6s - loss: 1.2892  
1120/6530 [====>.........................] - ETA: 3s - loss: 0.7841
1568/6530 [======>.......................] - ETA: 2s - loss: 0.5800
1984/6530 [========>.....................] - ETA: 1s - loss: 0.4726
2528/6530 [==========>...................] - ETA: 1s - loss: 0.3836
3168/6530 [=============>................] - ETA: 0s - loss: 0.3165
3840/6530 [================>.............] - ETA: 0s - loss: 0.2703
4608/6530 [====================>.........] - ETA: 0s - loss: 0.2337
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2107
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1887
6530/6530 [==============================] - 1s 193us/step - loss: 0.1784 - val_loss: 0.0386

# training | RMSE: 0.3110, MAE: 0.2501
worker 0  xfile  [43, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 98, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.3110441502850494, 'rmse': 0.3110441502850494, 'mae': 0.2500884117232057, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  36 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  92 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  43 | activation: relu    | extras: None 
layer 4 | size:  83 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d2426a58>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 25s - loss: 2.9760
2816/6530 [===========>..................] - ETA: 1s - loss: 0.7537 
5632/6530 [========================>.....] - ETA: 0s - loss: 0.4491
6530/6530 [==============================] - 1s 194us/step - loss: 0.4032 - val_loss: 0.1015

# training | RMSE: 0.2681, MAE: 0.2198
worker 2  xfile  [41, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4195269456256038}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.26813155366771574, 'rmse': 0.26813155366771574, 'mae': 0.21983225402992002, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:   5 | activation: tanh    | extras: batchnorm 
layer 2 | size:  53 | activation: sigmoid | extras: dropout - rate: 16.9% 
layer 3 | size:   4 | activation: tanh    | extras: batchnorm 
layer 4 | size:  36 | activation: sigmoid | extras: None 
layer 5 | size:  34 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00bed02518>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:31 - loss: 0.2355
 320/6530 [>.............................] - ETA: 21s - loss: 0.2139 
 608/6530 [=>............................] - ETA: 11s - loss: 0.2216
 960/6530 [===>..........................] - ETA: 6s - loss: 0.2209 
1312/6530 [=====>........................] - ETA: 4s - loss: 0.2180
1664/6530 [======>.......................] - ETA: 3s - loss: 0.2184
2048/6530 [========>.....................] - ETA: 2s - loss: 0.2181
2432/6530 [==========>...................] - ETA: 2s - loss: 0.2139
2816/6530 [===========>..................] - ETA: 1s - loss: 0.2133
3200/6530 [=============>................] - ETA: 1s - loss: 0.2120
3552/6530 [===============>..............] - ETA: 1s - loss: 0.2106
3904/6530 [================>.............] - ETA: 1s - loss: 0.2094
4256/6530 [==================>...........] - ETA: 0s - loss: 0.2073
4576/6530 [====================>.........] - ETA: 0s - loss: 0.2044
4960/6530 [=====================>........] - ETA: 0s - loss: 0.2022
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2011
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2004
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1985
6496/6530 [============================>.] - ETA: 0s - loss: 0.1970
# training | RMSE: 0.1956, MAE: 0.1585
worker 1  xfile  [44, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.11422937510548073}, 'layer_1_size': 73, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 5, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.19561830897827898, 'rmse': 0.19561830897827898, 'mae': 0.15845823812151485, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  55 | activation: relu    | extras: None 
layer 2 | size:  84 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  95 | activation: sigmoid | extras: None 
layer 4 | size:  57 | activation: sigmoid | extras: dropout - rate: 46.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00aa430978>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:15 - loss: 0.4012
 224/6530 [>.............................] - ETA: 27s - loss: 0.2336 
# training | RMSE: 0.3154, MAE: 0.2524
worker 0  xfile  [45, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 43, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4649735147365355}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.3154231477038285, 'rmse': 0.3154231477038285, 'mae': 0.2524122401757359, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:   9 | activation: tanh    | extras: dropout - rate: 31.6% 
layer 2 | size:  35 | activation: tanh    | extras: None 
layer 3 | size:   3 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d305f860>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:12 - loss: 0.2467
 464/6530 [=>............................] - ETA: 13s - loss: 0.1509
6530/6530 [==============================] - 2s 327us/step - loss: 0.1968 - val_loss: 0.1633

 800/6530 [==>...........................] - ETA: 5s - loss: 0.2280  
 672/6530 [==>...........................] - ETA: 9s - loss: 0.1250 
1632/6530 [======>.......................] - ETA: 2s - loss: 0.1582
 896/6530 [===>..........................] - ETA: 7s - loss: 0.1102
2496/6530 [==========>...................] - ETA: 1s - loss: 0.1239
1120/6530 [====>.........................] - ETA: 5s - loss: 0.1024
3232/6530 [=============>................] - ETA: 0s - loss: 0.1077
1344/6530 [=====>........................] - ETA: 4s - loss: 0.0950
4032/6530 [=================>............] - ETA: 0s - loss: 0.0973
1568/6530 [======>.......................] - ETA: 4s - loss: 0.0910
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0889
1808/6530 [=======>......................] - ETA: 3s - loss: 0.0884
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0837
2048/6530 [========>.....................] - ETA: 3s - loss: 0.0861
6496/6530 [============================>.] - ETA: 0s - loss: 0.0798
2304/6530 [=========>....................] - ETA: 2s - loss: 0.0845
2544/6530 [==========>...................] - ETA: 2s - loss: 0.0829
6530/6530 [==============================] - 1s 178us/step - loss: 0.0796 - val_loss: 0.0424

2800/6530 [===========>..................] - ETA: 2s - loss: 0.0830
3056/6530 [=============>................] - ETA: 1s - loss: 0.0821
3280/6530 [==============>...............] - ETA: 1s - loss: 0.0812
3488/6530 [===============>..............] - ETA: 1s - loss: 0.0805
3728/6530 [================>.............] - ETA: 1s - loss: 0.0792
3936/6530 [=================>............] - ETA: 1s - loss: 0.0784
4144/6530 [==================>...........] - ETA: 1s - loss: 0.0769
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0750
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0732
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0721
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0710
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0696
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0687
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0676
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0668
# training | RMSE: 0.2029, MAE: 0.1639
worker 2  xfile  [46, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 5, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16911047813414518}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.20291635214217324, 'rmse': 0.20291635214217324, 'mae': 0.16387246613267648, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  36 | activation: sigmoid | extras: dropout - rate: 38.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00be842320>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:25 - loss: 0.9725
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0660
 576/6530 [=>............................] - ETA: 7s - loss: 0.5069  
6432/6530 [============================>.] - ETA: 0s - loss: 0.0653
1168/6530 [====>.........................] - ETA: 3s - loss: 0.3624
1760/6530 [=======>......................] - ETA: 2s - loss: 0.3003
2304/6530 [=========>....................] - ETA: 1s - loss: 0.2767
6530/6530 [==============================] - 3s 390us/step - loss: 0.0654 - val_loss: 0.0412

2928/6530 [============>.................] - ETA: 1s - loss: 0.2578
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2419
4176/6530 [==================>...........] - ETA: 0s - loss: 0.2319
4768/6530 [====================>.........] - ETA: 0s - loss: 0.2233
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2161
5984/6530 [==========================>...] - ETA: 0s - loss: 0.2110
6530/6530 [==============================] - 1s 202us/step - loss: 0.2069 - val_loss: 0.1756

# training | RMSE: 0.2069, MAE: 0.1668
worker 0  xfile  [48, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3162994555792672}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.20694596857317538, 'rmse': 0.20694596857317538, 'mae': 0.16684940687205502, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  24 | activation: tanh    | extras: dropout - rate: 17.4% 
layer 2 | size:  80 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  60 | activation: relu    | extras: None 
layer 4 | size:  19 | activation: tanh    | extras: dropout - rate: 21.7% 
layer 5 | size:  36 | activation: relu    | extras: dropout - rate: 40.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d16a6fd0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 21s - loss: 0.7111
3072/6530 [=============>................] - ETA: 1s - loss: 0.2947 
6144/6530 [===========================>..] - ETA: 0s - loss: 0.2083
6530/6530 [==============================] - 1s 173us/step - loss: 0.2010 - val_loss: 0.0492

# training | RMSE: 0.2004, MAE: 0.1608
worker 1  xfile  [47, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4631191913250753}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.20035904820306083, 'rmse': 0.20035904820306083, 'mae': 0.1607951185695707, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:   9 | activation: tanh    | extras: dropout - rate: 15.1% 
layer 2 | size:  19 | activation: sigmoid | extras: None 
layer 3 | size:  63 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  52 | activation: sigmoid | extras: dropout - rate: 36.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00aa12b7f0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:35 - loss: 0.6308
# training | RMSE: 0.2287, MAE: 0.1748
worker 2  xfile  [49, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3801891065249704}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 36, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11923992646601543}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.22871833187330065, 'rmse': 0.22871833187330065, 'mae': 0.17480438431391246, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  29 | activation: tanh    | extras: dropout - rate: 40.0% 
layer 2 | size:  42 | activation: tanh    | extras: None 
layer 3 | size:  53 | activation: tanh    | extras: dropout - rate: 10.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00bde993c8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:45 - loss: 0.7073
 208/6530 [..............................] - ETA: 31s - loss: 0.5373 
 576/6530 [=>............................] - ETA: 8s - loss: 0.5785  
 432/6530 [>.............................] - ETA: 15s - loss: 0.4056
# training | RMSE: 0.2182, MAE: 0.1746
worker 0  xfile  [50, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1741077956753633}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2169358952905691}, 'layer_4_size': 19, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4017496563510651}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.21822375434709196, 'rmse': 0.21822375434709196, 'mae': 0.17461347589514792, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  80 | activation: relu    | extras: None 
layer 2 | size:  22 | activation: relu    | extras: dropout - rate: 41.7% 
layer 3 | size:   3 | activation: sigmoid | extras: dropout - rate: 27.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d16a6eb8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:13 - loss: 0.7187
1184/6530 [====>.........................] - ETA: 4s - loss: 0.4586
 656/6530 [==>...........................] - ETA: 10s - loss: 0.3499
1600/6530 [======>.......................] - ETA: 2s - loss: 0.7186  
1824/6530 [=======>......................] - ETA: 2s - loss: 0.3796
3200/6530 [=============>................] - ETA: 0s - loss: 0.6630
 912/6530 [===>..........................] - ETA: 7s - loss: 0.3159 
2496/6530 [==========>...................] - ETA: 1s - loss: 0.3297
4672/6530 [====================>.........] - ETA: 0s - loss: 0.6115
1152/6530 [====>.........................] - ETA: 5s - loss: 0.2925
3136/6530 [=============>................] - ETA: 1s - loss: 0.2981
6208/6530 [===========================>..] - ETA: 0s - loss: 0.5528
1392/6530 [=====>........................] - ETA: 4s - loss: 0.2710
3840/6530 [================>.............] - ETA: 0s - loss: 0.2773
1632/6530 [======>.......................] - ETA: 4s - loss: 0.2556
4512/6530 [===================>..........] - ETA: 0s - loss: 0.2618
1872/6530 [=======>......................] - ETA: 3s - loss: 0.2453
5216/6530 [======================>.......] - ETA: 0s - loss: 0.2506
6530/6530 [==============================] - 1s 161us/step - loss: 0.5403 - val_loss: 0.2928

2112/6530 [========>.....................] - ETA: 3s - loss: 0.2360
6016/6530 [==========================>...] - ETA: 0s - loss: 0.2412
2336/6530 [=========>....................] - ETA: 2s - loss: 0.2296
2576/6530 [==========>...................] - ETA: 2s - loss: 0.2254
2800/6530 [===========>..................] - ETA: 2s - loss: 0.2199
6530/6530 [==============================] - 1s 220us/step - loss: 0.2356 - val_loss: 0.1653

3040/6530 [============>.................] - ETA: 1s - loss: 0.2145
3248/6530 [=============>................] - ETA: 1s - loss: 0.2121
3504/6530 [===============>..............] - ETA: 1s - loss: 0.2101
3744/6530 [================>.............] - ETA: 1s - loss: 0.2076
3968/6530 [=================>............] - ETA: 1s - loss: 0.2058
4160/6530 [==================>...........] - ETA: 1s - loss: 0.2045
4384/6530 [===================>..........] - ETA: 0s - loss: 0.2028
4608/6530 [====================>.........] - ETA: 0s - loss: 0.2018
4848/6530 [=====================>........] - ETA: 0s - loss: 0.2007
5072/6530 [======================>.......] - ETA: 0s - loss: 0.2000
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1989
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1973
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1964
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1948
6320/6530 [============================>.] - ETA: 0s - loss: 0.1939
6530/6530 [==============================] - 3s 398us/step - loss: 0.1929 - val_loss: 0.1630

# training | RMSE: 0.3529, MAE: 0.2869
worker 0  xfile  [53, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 80, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41716159434200817}, 'layer_2_size': 22, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2771543675500986}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11232213779671203}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.36313792725811167}, 'layer_5_size': 49, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.35289998834332276, 'rmse': 0.35289998834332276, 'mae': 0.286873253135401, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  38 | activation: relu    | extras: None 
layer 2 | size:  94 | activation: tanh    | extras: batchnorm 
layer 3 | size:  49 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d0de89e8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 46s - loss: 0.6562
2304/6530 [=========>....................] - ETA: 1s - loss: 0.3798 
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2657
6530/6530 [==============================] - 1s 181us/step - loss: 0.2225 - val_loss: 0.1464

# training | RMSE: 0.2032, MAE: 0.1595
worker 2  xfile  [52, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39962949672182224}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10015621671395541}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11581437543817096}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2032332944191214, 'rmse': 0.2032332944191214, 'mae': 0.15954064648352462, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  69 | activation: relu    | extras: batchnorm 
layer 2 | size:   4 | activation: relu    | extras: dropout - rate: 10.5% 
layer 3 | size:  10 | activation: sigmoid | extras: dropout - rate: 37.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00bde992b0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:16 - loss: 5.6530
 448/6530 [=>............................] - ETA: 13s - loss: 5.0448 
 896/6530 [===>..........................] - ETA: 6s - loss: 4.7194 
1376/6530 [=====>........................] - ETA: 4s - loss: 4.4026
1984/6530 [========>.....................] - ETA: 2s - loss: 4.0341
2496/6530 [==========>...................] - ETA: 1s - loss: 3.7514
3040/6530 [============>.................] - ETA: 1s - loss: 3.4851
3648/6530 [===============>..............] - ETA: 1s - loss: 3.2104
4256/6530 [==================>...........] - ETA: 0s - loss: 2.9566
4768/6530 [====================>.........] - ETA: 0s - loss: 2.7581
5248/6530 [=======================>......] - ETA: 0s - loss: 2.5808
5728/6530 [=========================>....] - ETA: 0s - loss: 2.4184
6240/6530 [===========================>..] - ETA: 0s - loss: 2.2601
6530/6530 [==============================] - 2s 271us/step - loss: 2.1791 - val_loss: 0.2467

# training | RMSE: 0.2034, MAE: 0.1630
worker 1  xfile  [51, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15068232159870046}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36942877284804576}, 'layer_4_size': 52, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.20341630902144853, 'rmse': 0.20341630902144853, 'mae': 0.1629527736649623, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  43 | activation: tanh    | extras: None 
layer 2 | size:  36 | activation: relu    | extras: batchnorm 
layer 3 | size:  38 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00a94c6320>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:07 - loss: 0.6958
 384/6530 [>.............................] - ETA: 15s - loss: 0.7048 
 800/6530 [==>...........................] - ETA: 7s - loss: 0.6450 
1280/6530 [====>.........................] - ETA: 4s - loss: 0.5489
1824/6530 [=======>......................] - ETA: 2s - loss: 0.4463
2336/6530 [=========>....................] - ETA: 2s - loss: 0.3853
2912/6530 [============>.................] - ETA: 1s - loss: 0.3397
3520/6530 [===============>..............] - ETA: 1s - loss: 0.3053
4096/6530 [=================>............] - ETA: 0s - loss: 0.2822
4640/6530 [====================>.........] - ETA: 0s - loss: 0.2643
5152/6530 [======================>.......] - ETA: 0s - loss: 0.2512
5728/6530 [=========================>....] - ETA: 0s - loss: 0.2379
6304/6530 [===========================>..] - ETA: 0s - loss: 0.2274
6530/6530 [==============================] - 2s 262us/step - loss: 0.2231 - val_loss: 0.1361

# training | RMSE: 0.1835, MAE: 0.1471
worker 0  xfile  [54, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2007607438272935}, 'layer_4_size': 91, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.18345784341608357, 'rmse': 0.18345784341608357, 'mae': 0.14709248787787313, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  90 | activation: relu    | extras: None 
layer 2 | size:  53 | activation: tanh    | extras: batchnorm 
layer 3 | size:  33 | activation: relu    | extras: batchnorm 
layer 4 | size:  26 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d0de8860>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 4:14 - loss: 1.6375
 320/6530 [>.............................] - ETA: 25s - loss: 1.0810 
 640/6530 [=>............................] - ETA: 12s - loss: 0.8690
1024/6530 [===>..........................] - ETA: 7s - loss: 0.7103 
1408/6530 [=====>........................] - ETA: 5s - loss: 0.5952
1824/6530 [=======>......................] - ETA: 3s - loss: 0.4993
2208/6530 [=========>....................] - ETA: 3s - loss: 0.4466
2656/6530 [===========>..................] - ETA: 2s - loss: 0.3973
3104/6530 [=============>................] - ETA: 1s - loss: 0.3607
3552/6530 [===============>..............] - ETA: 1s - loss: 0.3317
# training | RMSE: 0.4908, MAE: 0.4151
worker 2  xfile  [55, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1050661373343273}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36950674751954005}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.4908353202620713, 'rmse': 0.4908353202620713, 'mae': 0.41507802308895375, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  50 | activation: tanh    | extras: None 
layer 2 | size:  52 | activation: sigmoid | extras: dropout - rate: 39.8% 
layer 3 | size:  61 | activation: relu    | extras: None 
layer 4 | size:  25 | activation: tanh    | extras: dropout - rate: 15.2% 
layer 5 | size:  29 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00bd747278>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:26 - loss: 1.1610
4000/6530 [=================>............] - ETA: 1s - loss: 0.3077
1280/6530 [====>.........................] - ETA: 3s - loss: 0.3037  
4448/6530 [===================>..........] - ETA: 0s - loss: 0.2883
2624/6530 [===========>..................] - ETA: 1s - loss: 0.2025
4896/6530 [=====================>........] - ETA: 0s - loss: 0.2716
3904/6530 [================>.............] - ETA: 0s - loss: 0.1674
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1475
5312/6530 [=======================>......] - ETA: 0s - loss: 0.2595
6336/6530 [============================>.] - ETA: 0s - loss: 0.1358
5728/6530 [=========================>....] - ETA: 0s - loss: 0.2489
6144/6530 [===========================>..] - ETA: 0s - loss: 0.2393
6530/6530 [==============================] - 1s 191us/step - loss: 0.1339 - val_loss: 0.0617

6530/6530 [==============================] - 2s 347us/step - loss: 0.2313 - val_loss: 0.0820

# training | RMSE: 0.1708, MAE: 0.1310
worker 1  xfile  [56, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16745485505931956}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.17076888544174484, 'rmse': 0.17076888544174484, 'mae': 0.13103172870955235, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  27 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00a94c6128>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 38s - loss: 0.2474
3072/6530 [=============>................] - ETA: 0s - loss: 0.2005 
6400/6530 [============================>.] - ETA: 0s - loss: 0.1747
6530/6530 [==============================] - 1s 155us/step - loss: 0.1737 - val_loss: 0.1279

# training | RMSE: 0.2476, MAE: 0.2018
worker 2  xfile  [58, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39805956850419555}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 61, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15198369221316727}, 'layer_4_size': 25, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.24761279291624347, 'rmse': 0.24761279291624347, 'mae': 0.20184938545455272, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  87 | activation: tanh    | extras: batchnorm 
layer 2 | size:  78 | activation: relu    | extras: None 
layer 3 | size:   8 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00bd747048>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:52 - loss: 0.4929
 208/6530 [..............................] - ETA: 32s - loss: 0.2890 
 416/6530 [>.............................] - ETA: 16s - loss: 0.2012
# training | RMSE: 0.2786, MAE: 0.2204
worker 0  xfile  [57, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31409164825353697}, 'layer_5_size': 91, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.27856696108710005, 'rmse': 0.27856696108710005, 'mae': 0.22042897365333666, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  98 | activation: tanh    | extras: None 
layer 2 | size:  34 | activation: tanh    | extras: dropout - rate: 31.9% 
layer 3 | size:  62 | activation: tanh    | extras: dropout - rate: 45.8% 
layer 4 | size:  75 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d0860b38>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 22s - loss: 0.5236
 560/6530 [=>............................] - ETA: 12s - loss: 0.1778
4864/6530 [=====================>........] - ETA: 0s - loss: 0.4526 
 768/6530 [==>...........................] - ETA: 9s - loss: 0.1501 
1008/6530 [===>..........................] - ETA: 6s - loss: 0.1350
1248/6530 [====>.........................] - ETA: 5s - loss: 0.1210
6530/6530 [==============================] - 1s 176us/step - loss: 0.3646 - val_loss: 0.0727

1488/6530 [=====>........................] - ETA: 4s - loss: 0.1144
1728/6530 [======>.......................] - ETA: 3s - loss: 0.1070
1952/6530 [=======>......................] - ETA: 3s - loss: 0.1017
2176/6530 [========>.....................] - ETA: 3s - loss: 0.0975
2400/6530 [==========>...................] - ETA: 2s - loss: 0.0951
2656/6530 [===========>..................] - ETA: 2s - loss: 0.0922
2864/6530 [============>.................] - ETA: 2s - loss: 0.0892
3104/6530 [=============>................] - ETA: 1s - loss: 0.0864
3376/6530 [==============>...............] - ETA: 1s - loss: 0.0834
3648/6530 [===============>..............] - ETA: 1s - loss: 0.0817
3888/6530 [================>.............] - ETA: 1s - loss: 0.0796
4112/6530 [=================>............] - ETA: 1s - loss: 0.0783
# training | RMSE: 0.3549, MAE: 0.2853
worker 1  xfile  [59, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3875675418941298}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 57, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 80, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.3548989919965623, 'rmse': 0.3548989919965623, 'mae': 0.2852911567647061, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  90 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  20 | activation: tanh    | extras: None 
layer 3 | size:  11 | activation: relu    | extras: None 
layer 4 | size:  13 | activation: relu    | extras: batchnorm 
layer 5 | size:  13 | activation: tanh    | extras: dropout - rate: 43.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00a92559b0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 2:03 - loss: 0.7910
4352/6530 [==================>...........] - ETA: 1s - loss: 0.0768
 896/6530 [===>..........................] - ETA: 7s - loss: 0.5473  
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0756
1728/6530 [======>.......................] - ETA: 3s - loss: 0.4014
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0737
2560/6530 [==========>...................] - ETA: 2s - loss: 0.3227
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0721
3392/6530 [==============>...............] - ETA: 1s - loss: 0.2686
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0711
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2315
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0697
5056/6530 [======================>.......] - ETA: 0s - loss: 0.2068
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0687
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1879
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0676
6336/6530 [============================>.] - ETA: 0s - loss: 0.0667
6530/6530 [==============================] - 2s 275us/step - loss: 0.1744 - val_loss: 0.0579

6530/6530 [==============================] - 3s 404us/step - loss: 0.0662 - val_loss: 0.0534

# training | RMSE: 0.2614, MAE: 0.1979
worker 0  xfile  [61, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.31887286985950053}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45841837674693586}, 'layer_3_size': 62, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.26144584513798463, 'rmse': 0.26144584513798463, 'mae': 0.19785108063745688, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:   8 | activation: relu    | extras: dropout - rate: 36.1% 
layer 2 | size:  46 | activation: sigmoid | extras: None 
layer 3 | size:  25 | activation: sigmoid | extras: None 
layer 4 | size:  23 | activation: tanh    | extras: batchnorm 
layer 5 | size:  57 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d053b278>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 7:40 - loss: 0.5186
 192/6530 [..............................] - ETA: 38s - loss: 0.5628 
 352/6530 [>.............................] - ETA: 21s - loss: 0.5483
 512/6530 [=>............................] - ETA: 15s - loss: 0.4948
 704/6530 [==>...........................] - ETA: 11s - loss: 0.4384
 928/6530 [===>..........................] - ETA: 8s - loss: 0.3648 
1152/6530 [====>.........................] - ETA: 6s - loss: 0.3053
1408/6530 [=====>........................] - ETA: 5s - loss: 0.2577
1616/6530 [======>.......................] - ETA: 4s - loss: 0.2301
1824/6530 [=======>......................] - ETA: 4s - loss: 0.2096
2032/6530 [========>.....................] - ETA: 3s - loss: 0.1929
2240/6530 [=========>....................] - ETA: 3s - loss: 0.1789
2432/6530 [==========>...................] - ETA: 2s - loss: 0.1681
2656/6530 [===========>..................] - ETA: 2s - loss: 0.1584
2864/6530 [============>.................] - ETA: 2s - loss: 0.1496
# training | RMSE: 0.2332, MAE: 0.1860
worker 1  xfile  [62, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 11, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.43181898460277857}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.23324025726228878, 'rmse': 0.23324025726228878, 'mae': 0.18597987397969482, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  80 | activation: relu    | extras: dropout - rate: 35.1% 
layer 2 | size:  45 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00a90b6518>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:27 - loss: 0.5873
1920/6530 [=======>......................] - ETA: 2s - loss: 0.1116  
3104/6530 [=============>................] - ETA: 2s - loss: 0.1409
3840/6530 [================>.............] - ETA: 0s - loss: 0.0805
3360/6530 [==============>...............] - ETA: 1s - loss: 0.1339
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0705
3600/6530 [===============>..............] - ETA: 1s - loss: 0.1282
3824/6530 [================>.............] - ETA: 1s - loss: 0.1233
4048/6530 [=================>............] - ETA: 1s - loss: 0.1192
6530/6530 [==============================] - 1s 181us/step - loss: 0.0675 - val_loss: 0.0483

4256/6530 [==================>...........] - ETA: 1s - loss: 0.1153
4464/6530 [===================>..........] - ETA: 1s - loss: 0.1121
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1088
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1058
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1035
# training | RMSE: 0.2307, MAE: 0.1811
worker 2  xfile  [60, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 8, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.23069662816535952, 'rmse': 0.23069662816535952, 'mae': 0.1811118300977576, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  29 | activation: tanh    | extras: batchnorm 
layer 2 | size:  37 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00bd696f28>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 22s - loss: 0.6155
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1013
4096/6530 [=================>............] - ETA: 0s - loss: 0.2033 
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0994
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0974
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0954
6530/6530 [==============================] - 1s 173us/step - loss: 0.1530 - val_loss: 0.0965

6128/6530 [===========================>..] - ETA: 0s - loss: 0.0935
6384/6530 [============================>.] - ETA: 0s - loss: 0.0913
6530/6530 [==============================] - 3s 447us/step - loss: 0.0901 - val_loss: 0.0922

# training | RMSE: 0.2965, MAE: 0.2298
worker 2  xfile  [65, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.29651205001924613, 'rmse': 0.29651205001924613, 'mae': 0.22978963575487285, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  79 | activation: sigmoid | extras: None 
layer 2 | size:  86 | activation: sigmoid | extras: None 
layer 3 | size:  93 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00bcf01a20>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:55 - loss: 0.4015
 608/6530 [=>............................] - ETA: 8s - loss: 0.1359  
# training | RMSE: 0.2152, MAE: 0.1767
worker 1  xfile  [64, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35109665238578447}, 'layer_1_size': 80, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 25, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.21519131380210832, 'rmse': 0.21519131380210832, 'mae': 0.17670357682512552, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  66 | activation: tanh    | extras: batchnorm 
layer 2 | size:  11 | activation: relu    | extras: dropout - rate: 19.8% 
layer 3 | size:   2 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00a8921ef0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:47 - loss: 0.6337
1216/6530 [====>.........................] - ETA: 4s - loss: 0.1002
1024/6530 [===>..........................] - ETA: 6s - loss: 0.5286  
1856/6530 [=======>......................] - ETA: 2s - loss: 0.0821
2048/6530 [========>.....................] - ETA: 2s - loss: 0.4558
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0723
3072/6530 [=============>................] - ETA: 1s - loss: 0.3901
3392/6530 [==============>...............] - ETA: 1s - loss: 0.0653
4288/6530 [==================>...........] - ETA: 0s - loss: 0.3300
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0609
5440/6530 [=======================>......] - ETA: 0s - loss: 0.2909
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0581
6528/6530 [============================>.] - ETA: 0s - loss: 0.2641
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0558
6496/6530 [============================>.] - ETA: 0s - loss: 0.0544
6530/6530 [==============================] - 2s 234us/step - loss: 0.2640 - val_loss: 0.0947

6530/6530 [==============================] - 1s 224us/step - loss: 0.0543 - val_loss: 0.0645

# training | RMSE: 0.3019, MAE: 0.2548
worker 0  xfile  [63, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36120883956828265}, 'layer_1_size': 8, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.30193985022824577, 'rmse': 0.30193985022824577, 'mae': 0.2547594577063263, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  68 | activation: relu    | extras: dropout - rate: 43.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d0104470>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 21s - loss: 0.7381
4864/6530 [=====================>........] - ETA: 0s - loss: 0.5197 
6530/6530 [==============================] - 1s 170us/step - loss: 0.4493 - val_loss: 0.2480

# training | RMSE: 0.2533, MAE: 0.2137
worker 2  xfile  [67, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1579993964245491}, 'layer_5_size': 35, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.2532890779610247, 'rmse': 0.2532890779610247, 'mae': 0.21370346756397915, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  12 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00bcbcc2e8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:03 - loss: 2.6692
1056/6530 [===>..........................] - ETA: 4s - loss: 1.9330  
# training | RMSE: 0.2958, MAE: 0.2349
worker 1  xfile  [66, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 66, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19810962561359832}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 2, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2957982921356629, 'rmse': 0.2957982921356629, 'mae': 0.23485490255156136, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  33 | activation: tanh    | extras: None 
layer 2 | size:  14 | activation: sigmoid | extras: None 
layer 3 | size:  88 | activation: tanh    | extras: dropout - rate: 15.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00a8720748>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:06 - loss: 1.2875
2112/6530 [========>.....................] - ETA: 2s - loss: 1.3573
 832/6530 [==>...........................] - ETA: 6s - loss: 0.3161  
3200/6530 [=============>................] - ETA: 1s - loss: 1.0051
1664/6530 [======>.......................] - ETA: 2s - loss: 0.2590
# training | RMSE: 0.3011, MAE: 0.2453
worker 0  xfile  [68, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43877911310812734}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 21, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31194649777488603}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.30107574362076966, 'rmse': 0.30107574362076966, 'mae': 0.24533808838878848, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  25 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00abbb74a8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:57 - loss: 1.0037
4160/6530 [==================>...........] - ETA: 0s - loss: 0.8074
2496/6530 [==========>...................] - ETA: 1s - loss: 0.2348
 480/6530 [=>............................] - ETA: 11s - loss: 0.7720 
5056/6530 [======================>.......] - ETA: 0s - loss: 0.6890
3328/6530 [==============>...............] - ETA: 1s - loss: 0.2195
 976/6530 [===>..........................] - ETA: 5s - loss: 0.5594 
6112/6530 [===========================>..] - ETA: 0s - loss: 0.5890
4096/6530 [=================>............] - ETA: 0s - loss: 0.2113
1440/6530 [=====>........................] - ETA: 3s - loss: 0.4394
4928/6530 [=====================>........] - ETA: 0s - loss: 0.2060
1952/6530 [=======>......................] - ETA: 2s - loss: 0.3541
5696/6530 [=========================>....] - ETA: 0s - loss: 0.2013
2432/6530 [==========>...................] - ETA: 1s - loss: 0.2976
6530/6530 [==============================] - 1s 207us/step - loss: 0.5586 - val_loss: 0.1080

6528/6530 [============================>.] - ETA: 0s - loss: 0.1970
2912/6530 [============>.................] - ETA: 1s - loss: 0.2568
3424/6530 [==============>...............] - ETA: 1s - loss: 0.2252
3920/6530 [=================>............] - ETA: 0s - loss: 0.2029
6530/6530 [==============================] - 1s 225us/step - loss: 0.1970 - val_loss: 0.2537

4416/6530 [===================>..........] - ETA: 0s - loss: 0.1853
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1722
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1616
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1526
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1447
6530/6530 [==============================] - 2s 263us/step - loss: 0.1403 - val_loss: 0.0458

# training | RMSE: 0.2963, MAE: 0.2537
worker 1  xfile  [69, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15353575675845962}, 'layer_3_size': 88, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 49, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.29625253965509796, 'rmse': 0.29625253965509796, 'mae': 0.25365886478912153, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  55 | activation: sigmoid | extras: None 
layer 2 | size:  15 | activation: tanh    | extras: None 
layer 3 | size:  92 | activation: sigmoid | extras: None 
layer 4 | size:  92 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00a82a8940>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 50s - loss: 0.5073
2560/6530 [==========>...................] - ETA: 1s - loss: 0.1249 
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0908
# training | RMSE: 0.2099, MAE: 0.1690
worker 0  xfile  [71, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.46574055977979867}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2098753125049903, 'rmse': 0.2098753125049903, 'mae': 0.1689615537358505, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  28 | activation: sigmoid | extras: None 
layer 2 | size:  85 | activation: tanh    | extras: dropout - rate: 44.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00aba2b278>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:35 - loss: 0.4691
6530/6530 [==============================] - 1s 191us/step - loss: 0.0839 - val_loss: 0.0509

1728/6530 [======>.......................] - ETA: 2s - loss: 0.2330  
# training | RMSE: 0.3302, MAE: 0.2617
worker 2  xfile  [70, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 12, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 91, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.3301967411304659, 'rmse': 0.3301967411304659, 'mae': 0.26170383742333986, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  86 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  82 | activation: sigmoid | extras: None 
layer 3 | size:  84 | activation: tanh    | extras: dropout - rate: 17.9% 
layer 4 | size:  95 | activation: sigmoid | extras: dropout - rate: 26.3% 
layer 5 | size:  52 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00bcbcc2b0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 9:05 - loss: 0.5734
 224/6530 [>.............................] - ETA: 39s - loss: 0.2838 
3392/6530 [==============>...............] - ETA: 0s - loss: 0.2197
 464/6530 [=>............................] - ETA: 18s - loss: 0.1847
5056/6530 [======================>.......] - ETA: 0s - loss: 0.2119
 720/6530 [==>...........................] - ETA: 12s - loss: 0.1490
 912/6530 [===>..........................] - ETA: 9s - loss: 0.1353 
1088/6530 [===>..........................] - ETA: 7s - loss: 0.1265
6530/6530 [==============================] - 1s 198us/step - loss: 0.2048 - val_loss: 0.4617

1248/6530 [====>.........................] - ETA: 6s - loss: 0.1179
1408/6530 [=====>........................] - ETA: 6s - loss: 0.1118
1616/6530 [======>.......................] - ETA: 5s - loss: 0.1056
1840/6530 [=======>......................] - ETA: 4s - loss: 0.1007
2080/6530 [========>.....................] - ETA: 3s - loss: 0.0961
2320/6530 [=========>....................] - ETA: 3s - loss: 0.0924
2544/6530 [==========>...................] - ETA: 3s - loss: 0.0891
2736/6530 [===========>..................] - ETA: 2s - loss: 0.0868
2944/6530 [============>.................] - ETA: 2s - loss: 0.0842
3152/6530 [=============>................] - ETA: 2s - loss: 0.0816
3360/6530 [==============>...............] - ETA: 2s - loss: 0.0801
3504/6530 [===============>..............] - ETA: 1s - loss: 0.0789
3664/6530 [===============>..............] - ETA: 1s - loss: 0.0776
3824/6530 [================>.............] - ETA: 1s - loss: 0.0763
4016/6530 [=================>............] - ETA: 1s - loss: 0.0749
4176/6530 [==================>...........] - ETA: 1s - loss: 0.0739
4368/6530 [===================>..........] - ETA: 1s - loss: 0.0726
4576/6530 [====================>.........] - ETA: 1s - loss: 0.0717
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0704
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0694
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0687
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0681
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0673
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0669
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0659
# training | RMSE: 0.2281, MAE: 0.1858
worker 1  xfile  [73, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.22812632673613428, 'rmse': 0.22812632673613428, 'mae': 0.18579254033422657, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  61 | activation: sigmoid | extras: dropout - rate: 25.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00a807c898>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:40 - loss: 0.4830
6320/6530 [============================>.] - ETA: 0s - loss: 0.0653
 496/6530 [=>............................] - ETA: 12s - loss: 0.1647 
1008/6530 [===>..........................] - ETA: 5s - loss: 0.1191 
1488/6530 [=====>........................] - ETA: 3s - loss: 0.0995
1984/6530 [========>.....................] - ETA: 2s - loss: 0.0887
2432/6530 [==========>...................] - ETA: 2s - loss: 0.0820
6530/6530 [==============================] - 3s 486us/step - loss: 0.0644 - val_loss: 0.0469

2912/6530 [============>.................] - ETA: 1s - loss: 0.0766
3440/6530 [==============>...............] - ETA: 1s - loss: 0.0724
3968/6530 [=================>............] - ETA: 0s - loss: 0.0690
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0662
# training | RMSE: 0.5084, MAE: 0.4627
worker 0  xfile  [74, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 28, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.44774437757480967}, 'layer_2_size': 85, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4624925505301145}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.5083514375478057, 'rmse': 0.5083514375478057, 'mae': 0.4627151392582194, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  31 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00ab51ccc0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 55s - loss: 0.6731
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0639
3840/6530 [================>.............] - ETA: 0s - loss: 0.4565 
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0622
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0608
6496/6530 [============================>.] - ETA: 0s - loss: 0.0592
6530/6530 [==============================] - 1s 206us/step - loss: 0.3689 - val_loss: 0.2228

6530/6530 [==============================] - 2s 278us/step - loss: 0.0590 - val_loss: 0.0425

# training | RMSE: 0.2789, MAE: 0.2266
worker 0  xfile  [76, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 46, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14491643896200868}, 'layer_5_size': 98, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.27894092045060603, 'rmse': 0.27894092045060603, 'mae': 0.22663490360119334, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  84 | activation: sigmoid | extras: dropout - rate: 37.0% 
layer 2 | size:  29 | activation: relu    | extras: dropout - rate: 27.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00ab51cb38>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:34 - loss: 0.7682
1344/6530 [=====>........................] - ETA: 3s - loss: 0.3418  
3008/6530 [============>.................] - ETA: 1s - loss: 0.2741
4672/6530 [====================>.........] - ETA: 0s - loss: 0.2490
6464/6530 [============================>.] - ETA: 0s - loss: 0.2359
6530/6530 [==============================] - 1s 196us/step - loss: 0.2355 - val_loss: 0.1928

# training | RMSE: 0.2140, MAE: 0.1729
worker 2  xfile  [72, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17922789040016737}, 'layer_3_size': 84, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.262831125587177}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.21395102169042934, 'rmse': 0.21395102169042934, 'mae': 0.17288400924430788, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  28 | activation: tanh    | extras: dropout - rate: 15.1% 
layer 2 | size:  34 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  60 | activation: tanh    | extras: None 
layer 4 | size:  28 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00bc72cfd0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:53 - loss: 1.2349
 512/6530 [=>............................] - ETA: 14s - loss: 0.4301 
 992/6530 [===>..........................] - ETA: 6s - loss: 0.3212 
1472/6530 [=====>........................] - ETA: 4s - loss: 0.2672
1952/6530 [=======>......................] - ETA: 3s - loss: 0.2388
2528/6530 [==========>...................] - ETA: 2s - loss: 0.2117
3008/6530 [============>.................] - ETA: 1s - loss: 0.1954
# training | RMSE: 0.2054, MAE: 0.1667
worker 1  xfile  [75, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2545763365188426}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3379367451056181}, 'layer_4_size': 56, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.20542399816217674, 'rmse': 0.20542399816217674, 'mae': 0.16673669476167643, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  59 | activation: sigmoid | extras: dropout - rate: 45.3% 
layer 2 | size:  71 | activation: sigmoid | extras: dropout - rate: 38.2% 
layer 3 | size:  37 | activation: relu    | extras: dropout - rate: 47.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00a92b16d8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 28s - loss: 0.7146
3552/6530 [===============>..............] - ETA: 1s - loss: 0.1800
5376/6530 [=======================>......] - ETA: 0s - loss: 0.3441 
4064/6530 [=================>............] - ETA: 0s - loss: 0.1702
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1611
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1532
6530/6530 [==============================] - 1s 212us/step - loss: 0.3226 - val_loss: 0.2153

5664/6530 [=========================>....] - ETA: 0s - loss: 0.1468
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1405
6530/6530 [==============================] - 2s 300us/step - loss: 0.1372 - val_loss: 0.0479

# training | RMSE: 0.2651, MAE: 0.2169
worker 1  xfile  [79, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.453485542524365}, 'layer_1_size': 59, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3822643582013816}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4743773597797818}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12857141911581393}, 'layer_4_size': 3, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2253611744405719}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.26510108535780397, 'rmse': 0.26510108535780397, 'mae': 0.21685727932498117, 'early_stop': False}
vggnet done  1

# training | RMSE: 0.2219, MAE: 0.1775
worker 2  xfile  [77, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15132983247231468}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13553206179667554}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.22187939727917494, 'rmse': 0.22187939727917494, 'mae': 0.17752163027805207, 'early_stop': False}
vggnet done  2

# training | RMSE: 0.2402, MAE: 0.1958
worker 0  xfile  [78, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37013647248944015}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2717217268544919}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 18, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 91, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.39924731376297173}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.24024457967443438, 'rmse': 0.24024457967443438, 'mae': 0.19575440398680463, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  34 | activation: sigmoid | extras: dropout - rate: 18.4% 
layer 2 | size:  56 | activation: tanh    | extras: batchnorm 
layer 3 | size:  97 | activation: tanh    | extras: dropout - rate: 20.4% 
layer 4 | size:  12 | activation: relu    | extras: batchnorm 
layer 5 | size:  66 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00d0157cf8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 8:27 - loss: 0.8877
 128/6530 [..............................] - ETA: 1:04 - loss: 0.6909
 240/6530 [>.............................] - ETA: 35s - loss: 0.5243 
 368/6530 [>.............................] - ETA: 23s - loss: 0.4266
 496/6530 [=>............................] - ETA: 17s - loss: 0.3791
 624/6530 [=>............................] - ETA: 14s - loss: 0.3518
 736/6530 [==>...........................] - ETA: 12s - loss: 0.3345
 848/6530 [==>...........................] - ETA: 10s - loss: 0.3197
 960/6530 [===>..........................] - ETA: 9s - loss: 0.3092 
1072/6530 [===>..........................] - ETA: 8s - loss: 0.3019
1184/6530 [====>.........................] - ETA: 8s - loss: 0.2923
1296/6530 [====>.........................] - ETA: 7s - loss: 0.2844
1408/6530 [=====>........................] - ETA: 6s - loss: 0.2786
1536/6530 [======>.......................] - ETA: 6s - loss: 0.2721
1664/6530 [======>.......................] - ETA: 5s - loss: 0.2676
1776/6530 [=======>......................] - ETA: 5s - loss: 0.2636
1888/6530 [=======>......................] - ETA: 5s - loss: 0.2605
2016/6530 [========>.....................] - ETA: 4s - loss: 0.2577
2144/6530 [========>.....................] - ETA: 4s - loss: 0.2553
2256/6530 [=========>....................] - ETA: 4s - loss: 0.2531
2384/6530 [=========>....................] - ETA: 4s - loss: 0.2503
2512/6530 [==========>...................] - ETA: 3s - loss: 0.2482
2640/6530 [===========>..................] - ETA: 3s - loss: 0.2467
2784/6530 [===========>..................] - ETA: 3s - loss: 0.2439
2944/6530 [============>.................] - ETA: 3s - loss: 0.2408
3104/6530 [=============>................] - ETA: 2s - loss: 0.2377
3248/6530 [=============>................] - ETA: 2s - loss: 0.2362
3408/6530 [==============>...............] - ETA: 2s - loss: 0.2349
3536/6530 [===============>..............] - ETA: 2s - loss: 0.2335
3664/6530 [===============>..............] - ETA: 2s - loss: 0.2326
3808/6530 [================>.............] - ETA: 2s - loss: 0.2311
3936/6530 [=================>............] - ETA: 1s - loss: 0.2305
4064/6530 [=================>............] - ETA: 1s - loss: 0.2294
4224/6530 [==================>...........] - ETA: 1s - loss: 0.2277
4400/6530 [===================>..........] - ETA: 1s - loss: 0.2264
4608/6530 [====================>.........] - ETA: 1s - loss: 0.2253
4832/6530 [=====================>........] - ETA: 1s - loss: 0.2236
5040/6530 [======================>.......] - ETA: 0s - loss: 0.2220
5232/6530 [=======================>......] - ETA: 0s - loss: 0.2207
5408/6530 [=======================>......] - ETA: 0s - loss: 0.2197
5600/6530 [========================>.....] - ETA: 0s - loss: 0.2182
5792/6530 [=========================>....] - ETA: 0s - loss: 0.2174
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2166
6096/6530 [===========================>..] - ETA: 0s - loss: 0.2158
6240/6530 [===========================>..] - ETA: 0s - loss: 0.2149
6384/6530 [============================>.] - ETA: 0s - loss: 0.2141
6496/6530 [============================>.] - ETA: 0s - loss: 0.2136
6530/6530 [==============================] - 4s 609us/step - loss: 0.2134 - val_loss: 0.1879

# training | RMSE: 0.2259, MAE: 0.1866
worker 0  xfile  [80, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18391069289201956}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 56, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20412479331356678}, 'layer_3_size': 97, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.22589691670355388, 'rmse': 0.22589691670355388, 'mae': 0.18663698468941292, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#1 epoch=1.0 loss={'loss': 0.2854713958259964, 'rmse': 0.2854713958259964, 'mae': 0.2321143678494358, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4248286730916818}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#2 epoch=1.0 loss={'loss': 0.4818073798484275, 'rmse': 0.4818073798484275, 'mae': 0.41512152035146405, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 59, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12856911536933047}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2940647648620235}, 'layer_4_size': 94, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#4 epoch=1.0 loss={'loss': 0.27334764907378634, 'rmse': 0.27334764907378634, 'mae': 0.22143080124281378, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 22, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2000848348489106}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#3 epoch=1.0 loss={'loss': 0.3516271411053419, 'rmse': 0.3516271411053419, 'mae': 0.30870277854837624, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.28590221771346425}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 21, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#5 epoch=1.0 loss={'loss': 0.19978911117306108, 'rmse': 0.19978911117306108, 'mae': 0.16301657340483808, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2820274393310773}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#6 epoch=1.0 loss={'loss': 0.27436726312961646, 'rmse': 0.27436726312961646, 'mae': 0.21976571403899378, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.41324829818739683}, 'layer_1_size': 61, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13975984822005746}, 'layer_2_size': 3, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 20, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#7 epoch=1.0 loss={'loss': 0.263971315913358, 'rmse': 0.263971315913358, 'mae': 0.21518096852158028, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39132251248214844}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37621340906553147}, 'layer_2_size': 97, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3580614653399373}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#8 epoch=1.0 loss={'loss': 0.7642582081345706, 'rmse': 0.7642582081345706, 'mae': 0.695399545263153, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46040379328519976}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 41, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3727395901974413}, 'layer_4_size': 44, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
#0 epoch=1.0 loss={'loss': 0.2393276975245625, 'rmse': 0.2393276975245625, 'mae': 0.19878275285056046, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 10, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 31, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15150219925735608}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
#10 epoch=1.0 loss={'loss': 0.4766602841883529, 'rmse': 0.4766602841883529, 'mae': 0.3864265577571806, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 91, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 90, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#11 epoch=1.0 loss={'loss': 0.2650326614287187, 'rmse': 0.2650326614287187, 'mae': 0.2155820858417705, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47570319050681575}, 'layer_1_size': 14, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4787974697856523}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1315792131888107}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#9 epoch=1.0 loss={'loss': 0.25295909320921367, 'rmse': 0.25295909320921367, 'mae': 0.20109288510138051, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 28, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35320267780836245}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26723916442710643}, 'layer_4_size': 6, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#12 epoch=1.0 loss={'loss': 0.2181865332211612, 'rmse': 0.2181865332211612, 'mae': 0.17270334155146588, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4554561189849352}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.379198274209536}, 'layer_4_size': 21, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#13 epoch=1.0 loss={'loss': 0.21186969401658748, 'rmse': 0.21186969401658748, 'mae': 0.16550892503617995, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45512557545093135}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 35, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#14 epoch=1.0 loss={'loss': 0.262513166064599, 'rmse': 0.262513166064599, 'mae': 0.2153860233147212, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22993113229229667}, 'layer_1_size': 54, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2918635922408792}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#15 epoch=1.0 loss={'loss': 0.34494542731654787, 'rmse': 0.34494542731654787, 'mae': 0.27157603525218044, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22807816925739277}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#16 epoch=1.0 loss={'loss': 0.3093624745609728, 'rmse': 0.3093624745609728, 'mae': 0.2554810509969359, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 38, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.343391534079115}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3383178210354845}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2956271235310313}, 'layer_5_size': 4, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#18 epoch=1.0 loss={'loss': 0.31836518244079876, 'rmse': 0.31836518244079876, 'mae': 0.2559161899417563, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2738264200831}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 11, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4205563349101873}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3741232536469342}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
#17 epoch=1.0 loss={'loss': 0.2394052565106703, 'rmse': 0.2394052565106703, 'mae': 0.20020817975949054, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.40750158041712903}, 'layer_3_size': 69, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4048369905403586}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#20 epoch=1.0 loss={'loss': 0.2363685915954125, 'rmse': 0.2363685915954125, 'mae': 0.1931584648541806, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 44, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3360410430399626}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23380908750718243}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#19 epoch=1.0 loss={'loss': 0.34750579715072694, 'rmse': 0.34750579715072694, 'mae': 0.28557814235826606, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12932097718679955}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4763850999934498}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#21 epoch=1.0 loss={'loss': 0.2060397182014802, 'rmse': 0.2060397182014802, 'mae': 0.16152860325564278, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2423132848388729}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#23 epoch=1.0 loss={'loss': 0.25662650521635705, 'rmse': 0.25662650521635705, 'mae': 0.21493768225380444, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.18924415336054554}, 'layer_4_size': 53, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#24 epoch=1.0 loss={'loss': 0.2392001790667556, 'rmse': 0.2392001790667556, 'mae': 0.19404018503385126, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14684617353368484}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3859620787869812}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15776419479082585}, 'layer_3_size': 23, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 89, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#22 epoch=1.0 loss={'loss': 0.18533863422514246, 'rmse': 0.18533863422514246, 'mae': 0.14058317879619167, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21048884801807022}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#25 epoch=1.0 loss={'loss': 0.6717972452706601, 'rmse': 0.6717972452706601, 'mae': 0.6314325958784691, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 39, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17463050599410126}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#27 epoch=1.0 loss={'loss': 0.23232000444381448, 'rmse': 0.23232000444381448, 'mae': 0.1890413233487523, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4226684261892001}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38909506041759967}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11883962019293581}, 'layer_5_size': 81, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#26 epoch=1.0 loss={'loss': 0.2920726154462038, 'rmse': 0.2920726154462038, 'mae': 0.23432203706141852, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4086446132962229}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35029228823647773}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#30 epoch=1.0 loss={'loss': 0.2510627303719418, 'rmse': 0.2510627303719418, 'mae': 0.19776570743644897, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4479261069663404}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#29 epoch=1.0 loss={'loss': 0.3298481515243315, 'rmse': 0.3298481515243315, 'mae': 0.2656673118393694, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18201157345841038}, 'layer_3_size': 8, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#28 epoch=1.0 loss={'loss': 0.21892003173365218, 'rmse': 0.21892003173365218, 'mae': 0.17535006974133316, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21704675266779963}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#31 epoch=1.0 loss={'loss': 0.20115904583170496, 'rmse': 0.20115904583170496, 'mae': 0.1607017169106847, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4979476961384771}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#32 epoch=1.0 loss={'loss': 0.275775791924289, 'rmse': 0.275775791924289, 'mae': 0.221711197173964, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 75, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32191812317889457}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18336331704955233}, 'layer_5_size': 9, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#35 epoch=1.0 loss={'loss': 0.263565081085954, 'rmse': 0.263565081085954, 'mae': 0.21585005364450388, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15758638976998862}, 'layer_2_size': 92, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22134854478231542}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4731865489395358}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#33 epoch=1.0 loss={'loss': 0.23007905321937544, 'rmse': 0.23007905321937544, 'mae': 0.18798611802398194, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.31082407783245625}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 18, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26877401178424}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#34 epoch=1.0 loss={'loss': 0.2169890926074878, 'rmse': 0.2169890926074878, 'mae': 0.17172221550808475, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#36 epoch=1.0 loss={'loss': 0.23799221779334728, 'rmse': 0.23799221779334728, 'mae': 0.19263193995157463, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35223028731922856}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4611808223007189}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 30, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#37 epoch=1.0 loss={'loss': 0.2634752635232708, 'rmse': 0.2634752635232708, 'mae': 0.21599291178465846, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 2, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 51, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1776477202562827}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#38 epoch=1.0 loss={'loss': 0.30796111422989403, 'rmse': 0.30796111422989403, 'mae': 0.24344387417252955, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14640075203083014}, 'layer_1_size': 57, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16892410402799152}, 'layer_4_size': 13, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#39 epoch=1.0 loss={'loss': 0.2078858107008423, 'rmse': 0.2078858107008423, 'mae': 0.16875587435440598, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#40 epoch=1.0 loss={'loss': 0.24871212475003002, 'rmse': 0.24871212475003002, 'mae': 0.2022561387470193, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35579117525473236}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4220893331897563}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#42 epoch=1.0 loss={'loss': 0.20901232995542493, 'rmse': 0.20901232995542493, 'mae': 0.1709407161386968, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4196428123159085}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 76, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#43 epoch=1.0 loss={'loss': 0.3110441502850494, 'rmse': 0.3110441502850494, 'mae': 0.2500884117232057, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 98, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#41 epoch=1.0 loss={'loss': 0.26813155366771574, 'rmse': 0.26813155366771574, 'mae': 0.21983225402992002, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4195269456256038}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#44 epoch=1.0 loss={'loss': 0.19561830897827898, 'rmse': 0.19561830897827898, 'mae': 0.15845823812151485, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.11422937510548073}, 'layer_1_size': 73, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 5, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
#45 epoch=1.0 loss={'loss': 0.3154231477038285, 'rmse': 0.3154231477038285, 'mae': 0.2524122401757359, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 43, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4649735147365355}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#46 epoch=1.0 loss={'loss': 0.20291635214217324, 'rmse': 0.20291635214217324, 'mae': 0.16387246613267648, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 5, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16911047813414518}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#48 epoch=1.0 loss={'loss': 0.20694596857317538, 'rmse': 0.20694596857317538, 'mae': 0.16684940687205502, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3162994555792672}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#47 epoch=1.0 loss={'loss': 0.20035904820306083, 'rmse': 0.20035904820306083, 'mae': 0.1607951185695707, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4631191913250753}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#49 epoch=1.0 loss={'loss': 0.22871833187330065, 'rmse': 0.22871833187330065, 'mae': 0.17480438431391246, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3801891065249704}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 36, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11923992646601543}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#50 epoch=1.0 loss={'loss': 0.21822375434709196, 'rmse': 0.21822375434709196, 'mae': 0.17461347589514792, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1741077956753633}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2169358952905691}, 'layer_4_size': 19, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4017496563510651}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#53 epoch=1.0 loss={'loss': 0.35289998834332276, 'rmse': 0.35289998834332276, 'mae': 0.286873253135401, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 80, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41716159434200817}, 'layer_2_size': 22, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2771543675500986}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11232213779671203}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.36313792725811167}, 'layer_5_size': 49, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#52 epoch=1.0 loss={'loss': 0.2032332944191214, 'rmse': 0.2032332944191214, 'mae': 0.15954064648352462, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39962949672182224}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10015621671395541}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11581437543817096}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#51 epoch=1.0 loss={'loss': 0.20341630902144853, 'rmse': 0.20341630902144853, 'mae': 0.1629527736649623, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15068232159870046}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36942877284804576}, 'layer_4_size': 52, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#54 epoch=1.0 loss={'loss': 0.18345784341608357, 'rmse': 0.18345784341608357, 'mae': 0.14709248787787313, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2007607438272935}, 'layer_4_size': 91, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#55 epoch=1.0 loss={'loss': 0.4908353202620713, 'rmse': 0.4908353202620713, 'mae': 0.41507802308895375, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1050661373343273}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36950674751954005}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#56 epoch=1.0 loss={'loss': 0.17076888544174484, 'rmse': 0.17076888544174484, 'mae': 0.13103172870955235, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16745485505931956}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#58 epoch=1.0 loss={'loss': 0.24761279291624347, 'rmse': 0.24761279291624347, 'mae': 0.20184938545455272, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39805956850419555}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 61, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15198369221316727}, 'layer_4_size': 25, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#57 epoch=1.0 loss={'loss': 0.27856696108710005, 'rmse': 0.27856696108710005, 'mae': 0.22042897365333666, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31409164825353697}, 'layer_5_size': 91, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#59 epoch=1.0 loss={'loss': 0.3548989919965623, 'rmse': 0.3548989919965623, 'mae': 0.2852911567647061, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3875675418941298}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 57, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 80, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#61 epoch=1.0 loss={'loss': 0.26144584513798463, 'rmse': 0.26144584513798463, 'mae': 0.19785108063745688, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.31887286985950053}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45841837674693586}, 'layer_3_size': 62, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#62 epoch=1.0 loss={'loss': 0.23324025726228878, 'rmse': 0.23324025726228878, 'mae': 0.18597987397969482, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 11, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.43181898460277857}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#60 epoch=1.0 loss={'loss': 0.23069662816535952, 'rmse': 0.23069662816535952, 'mae': 0.1811118300977576, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 8, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#64 epoch=1.0 loss={'loss': 0.21519131380210832, 'rmse': 0.21519131380210832, 'mae': 0.17670357682512552, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35109665238578447}, 'layer_1_size': 80, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 25, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#65 epoch=1.0 loss={'loss': 0.29651205001924613, 'rmse': 0.29651205001924613, 'mae': 0.22978963575487285, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#63 epoch=1.0 loss={'loss': 0.30193985022824577, 'rmse': 0.30193985022824577, 'mae': 0.2547594577063263, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36120883956828265}, 'layer_1_size': 8, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#66 epoch=1.0 loss={'loss': 0.2957982921356629, 'rmse': 0.2957982921356629, 'mae': 0.23485490255156136, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 66, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19810962561359832}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 2, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#67 epoch=1.0 loss={'loss': 0.2532890779610247, 'rmse': 0.2532890779610247, 'mae': 0.21370346756397915, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1579993964245491}, 'layer_5_size': 35, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#68 epoch=1.0 loss={'loss': 0.30107574362076966, 'rmse': 0.30107574362076966, 'mae': 0.24533808838878848, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43877911310812734}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 21, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31194649777488603}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#70 epoch=1.0 loss={'loss': 0.3301967411304659, 'rmse': 0.3301967411304659, 'mae': 0.26170383742333986, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 12, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 91, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#69 epoch=1.0 loss={'loss': 0.29625253965509796, 'rmse': 0.29625253965509796, 'mae': 0.25365886478912153, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15353575675845962}, 'layer_3_size': 88, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 49, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#71 epoch=1.0 loss={'loss': 0.2098753125049903, 'rmse': 0.2098753125049903, 'mae': 0.1689615537358505, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.46574055977979867}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#73 epoch=1.0 loss={'loss': 0.22812632673613428, 'rmse': 0.22812632673613428, 'mae': 0.18579254033422657, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#74 epoch=1.0 loss={'loss': 0.5083514375478057, 'rmse': 0.5083514375478057, 'mae': 0.4627151392582194, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 28, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.44774437757480967}, 'layer_2_size': 85, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4624925505301145}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#72 epoch=1.0 loss={'loss': 0.21395102169042934, 'rmse': 0.21395102169042934, 'mae': 0.17288400924430788, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17922789040016737}, 'layer_3_size': 84, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.262831125587177}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#76 epoch=1.0 loss={'loss': 0.27894092045060603, 'rmse': 0.27894092045060603, 'mae': 0.22663490360119334, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 46, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14491643896200868}, 'layer_5_size': 98, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#75 epoch=1.0 loss={'loss': 0.20542399816217674, 'rmse': 0.20542399816217674, 'mae': 0.16673669476167643, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2545763365188426}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3379367451056181}, 'layer_4_size': 56, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#78 epoch=1.0 loss={'loss': 0.24024457967443438, 'rmse': 0.24024457967443438, 'mae': 0.19575440398680463, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37013647248944015}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2717217268544919}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 18, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 91, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.39924731376297173}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#79 epoch=1.0 loss={'loss': 0.26510108535780397, 'rmse': 0.26510108535780397, 'mae': 0.21685727932498117, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.453485542524365}, 'layer_1_size': 59, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3822643582013816}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4743773597797818}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12857141911581393}, 'layer_4_size': 3, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2253611744405719}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
#77 epoch=1.0 loss={'loss': 0.22187939727917494, 'rmse': 0.22187939727917494, 'mae': 0.17752163027805207, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15132983247231468}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13553206179667554}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#80 epoch=1.0 loss={'loss': 0.22589691670355388, 'rmse': 0.22589691670355388, 'mae': 0.18663698468941292, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18391069289201956}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 56, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20412479331356678}, 'layer_3_size': 97, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
get a list [results] of length 81
get a list [loss] of length 81
get a list [val_loss] of length 81
length of indices is (56, 54, 22, 44, 5, 47, 31, 46, 52, 51, 75, 21, 48, 39, 42, 71, 13, 72, 64, 34, 12, 50, 28, 77, 80, 73, 49, 33, 60, 27, 62, 20, 36, 24, 0, 17, 78, 58, 40, 30, 9, 67, 23, 61, 14, 37, 35, 7, 11, 79, 41, 4, 6, 32, 57, 76, 1, 26, 66, 69, 65, 68, 63, 38, 16, 43, 45, 18, 29, 70, 15, 19, 3, 53, 59, 10, 2, 55, 74, 25, 8)
length of indices is 81
length of T is 81
newly formed T structure is:[[0, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16745485505931956}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [1, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2007607438272935}, 'layer_4_size': 91, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [2, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21048884801807022}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [3, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.11422937510548073}, 'layer_1_size': 73, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 5, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [4, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2820274393310773}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [5, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4631191913250753}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [6, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4979476961384771}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [7, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 5, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16911047813414518}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [8, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39962949672182224}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10015621671395541}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11581437543817096}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [9, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15068232159870046}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36942877284804576}, 'layer_4_size': 52, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [10, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2545763365188426}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3379367451056181}, 'layer_4_size': 56, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [11, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2423132848388729}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [12, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3162994555792672}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [13, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [14, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4196428123159085}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 76, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [15, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.46574055977979867}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [16, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45512557545093135}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 35, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [17, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17922789040016737}, 'layer_3_size': 84, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.262831125587177}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [18, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35109665238578447}, 'layer_1_size': 80, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 25, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [19, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [20, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4554561189849352}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.379198274209536}, 'layer_4_size': 21, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [21, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1741077956753633}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2169358952905691}, 'layer_4_size': 19, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4017496563510651}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [22, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21704675266779963}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [23, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15132983247231468}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13553206179667554}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [24, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18391069289201956}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 56, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20412479331356678}, 'layer_3_size': 97, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [25, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [26, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3801891065249704}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 36, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11923992646601543}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]] 

*** 27.0 configurations x 3.0 iterations each

81 | Fri Sep 28 01:19:46 2018 | lowest loss so far: 0.1708 (run 56)

{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  43 | activation: tanh    | extras: None 
layer 2 | size:  36 | activation: relu    | extras: batchnorm 
layer 3 | size:  38 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 3:18 - loss: 0.6958
 544/6530 [=>............................] - ETA: 11s - loss: 0.6809 
1056/6530 [===>..........................] - ETA: 5s - loss: 0.6036 
1696/6530 [======>.......................] - ETA: 3s - loss: 0.4691
2272/6530 [=========>....................] - ETA: 2s - loss: 0.3924
2880/6530 [============>.................] - ETA: 1s - loss: 0.3414{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  38 | activation: relu    | extras: None 
layer 2 | size:  94 | activation: tanh    | extras: batchnorm 
layer 3 | size:  49 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 59s - loss: 0.8687
3520/6530 [===============>..............] - ETA: 1s - loss: 0.3060
2304/6530 [=========>....................] - ETA: 2s - loss: 0.5510 
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2797
4608/6530 [====================>.........] - ETA: 0s - loss: 0.3649
4832/6530 [=====================>........] - ETA: 0s - loss: 0.2594
6530/6530 [==============================] - 1s 211us/step - loss: 0.2974 - val_loss: 0.1762
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1437
5184/6530 [======================>.......] - ETA: 0s - loss: 0.2509
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1241
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2358
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1216{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  84 | activation: tanh    | extras: batchnorm 
layer 2 | size:  21 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  37 | activation: relu    | extras: batchnorm 
layer 4 | size:  60 | activation: sigmoid | extras: dropout - rate: 21.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 57s - loss: 0.4874
6528/6530 [============================>.] - ETA: 0s - loss: 0.2239
6530/6530 [==============================] - 0s 25us/step - loss: 0.1202 - val_loss: 0.1463
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1312
1792/6530 [=======>......................] - ETA: 3s - loss: 0.2009 
6530/6530 [==============================] - 2s 252us/step - loss: 0.2238 - val_loss: 0.1116
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1040
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1127
3328/6530 [==============>...............] - ETA: 1s - loss: 0.1259
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1157
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1103
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0964
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1109
6530/6530 [==============================] - 0s 25us/step - loss: 0.1093 - val_loss: 0.1202

2080/6530 [========>.....................] - ETA: 0s - loss: 0.1098
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1099
6530/6530 [==============================] - 1s 218us/step - loss: 0.0807 - val_loss: 0.0458
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0265
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1087
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0252
4096/6530 [=================>............] - ETA: 0s - loss: 0.1078
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0262
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1068
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0255
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1071
6530/6530 [==============================] - 0s 33us/step - loss: 0.0251 - val_loss: 0.0378
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0213
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1058
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0210
6530/6530 [==============================] - 1s 78us/step - loss: 0.1052 - val_loss: 0.0983
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0874
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0219
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0948
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0213
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0953
6530/6530 [==============================] - 0s 34us/step - loss: 0.0208 - val_loss: 0.0294

2144/6530 [========>.....................] - ETA: 0s - loss: 0.0954
# training | RMSE: 0.1508, MAE: 0.1157
worker 1  xfile  [1, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2007607438272935}, 'layer_4_size': 91, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.15084795371751028, 'rmse': 0.15084795371751028, 'mae': 0.115721588286351, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  73 | activation: tanh    | extras: dropout - rate: 11.4% 
layer 2 | size:  82 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041fa244a8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 44s - loss: 4.2648
2848/6530 [============>.................] - ETA: 0s - loss: 0.0956
1024/6530 [===>..........................] - ETA: 1s - loss: 1.9415 
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0957
1984/6530 [========>.....................] - ETA: 0s - loss: 1.0901
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0946
3040/6530 [============>.................] - ETA: 0s - loss: 0.7322
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0943
4032/6530 [=================>............] - ETA: 0s - loss: 0.5665
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0945
5088/6530 [======================>.......] - ETA: 0s - loss: 0.4602
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0943
5760/6530 [=========================>....] - ETA: 0s - loss: 0.4122
6530/6530 [==============================] - 1s 79us/step - loss: 0.0942 - val_loss: 0.0867

6530/6530 [==============================] - 1s 94us/step - loss: 0.3697 - val_loss: 0.0426
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0618
# training | RMSE: 0.1622, MAE: 0.1250
worker 2  xfile  [2, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21048884801807022}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.16219986493316654, 'rmse': 0.16219986493316654, 'mae': 0.12499960020300763, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  37 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c044470>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:00 - loss: 0.5139
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0487
 640/6530 [=>............................] - ETA: 1s - loss: 0.4656  
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0476
1312/6530 [=====>........................] - ETA: 0s - loss: 0.3418
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0458
2016/6530 [========>.....................] - ETA: 0s - loss: 0.2471
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0454
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1940
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0451
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1643
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0440
4064/6530 [=================>............] - ETA: 0s - loss: 0.1431
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1281
6530/6530 [==============================] - 0s 57us/step - loss: 0.0435 - val_loss: 0.0376
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0532
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1176
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0408
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1097
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0394
2944/6530 [============>.................] - ETA: 0s - loss: 0.0401
6530/6530 [==============================] - 1s 104us/step - loss: 0.1040 - val_loss: 0.0400
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0430
3936/6530 [=================>............] - ETA: 0s - loss: 0.0394
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0392
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0388
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0368
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0386
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0365
6530/6530 [==============================] - 0s 53us/step - loss: 0.0387 - val_loss: 0.0360

2752/6530 [===========>..................] - ETA: 0s - loss: 0.0366
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0360
4096/6530 [=================>............] - ETA: 0s - loss: 0.0363
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0362
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0358
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0359
6530/6530 [==============================] - 0s 77us/step - loss: 0.0356 - val_loss: 0.0350
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0360
# training | RMSE: 0.1059, MAE: 0.0803
worker 0  xfile  [0, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16745485505931956}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.10594900784371053, 'rmse': 0.10594900784371053, 'mae': 0.08028068016487888, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  55 | activation: relu    | extras: None 
layer 2 | size:  84 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  95 | activation: sigmoid | extras: None 
layer 4 | size:  57 | activation: sigmoid | extras: dropout - rate: 46.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f041fa24438>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:51 - loss: 0.9421
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0330
 272/6530 [>.............................] - ETA: 10s - loss: 0.3623 
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0316
 544/6530 [=>............................] - ETA: 5s - loss: 0.2188 
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0310
# training | RMSE: 0.1831, MAE: 0.1473
worker 1  xfile  [3, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.11422937510548073}, 'layer_1_size': 73, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 5, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.1830710435834075, 'rmse': 0.1830710435834075, 'mae': 0.14729340548151645, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  42 | activation: tanh    | extras: None 
layer 2 | size:  66 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c1990b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 18s - loss: 0.7535
 832/6530 [==>...........................] - ETA: 3s - loss: 0.1695
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0314
2304/6530 [=========>....................] - ETA: 0s - loss: 0.2521 
1104/6530 [====>.........................] - ETA: 3s - loss: 0.1462
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0309
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1489
1360/6530 [=====>........................] - ETA: 2s - loss: 0.1298
4048/6530 [=================>............] - ETA: 0s - loss: 0.0310
1632/6530 [======>.......................] - ETA: 2s - loss: 0.1188
6530/6530 [==============================] - 0s 55us/step - loss: 0.1178 - val_loss: 0.0428
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0384
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0308
1904/6530 [=======>......................] - ETA: 1s - loss: 0.1112
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0430
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0308
2160/6530 [========>.....................] - ETA: 1s - loss: 0.1054
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0417
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0306
2432/6530 [==========>...................] - ETA: 1s - loss: 0.1004
6530/6530 [==============================] - 0s 24us/step - loss: 0.0412 - val_loss: 0.0399
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0378
6530/6530 [==============================] - 1s 79us/step - loss: 0.0305 - val_loss: 0.0305

2688/6530 [===========>..................] - ETA: 1s - loss: 0.0958
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0389
2960/6530 [============>.................] - ETA: 1s - loss: 0.0919
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0393
3216/6530 [=============>................] - ETA: 1s - loss: 0.0879
6400/6530 [============================>.] - ETA: 0s - loss: 0.0389
6530/6530 [==============================] - 0s 25us/step - loss: 0.0389 - val_loss: 0.0382

3488/6530 [===============>..............] - ETA: 0s - loss: 0.0847
3776/6530 [================>.............] - ETA: 0s - loss: 0.0820
4080/6530 [=================>............] - ETA: 0s - loss: 0.0795
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0774
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0750
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0733
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0717
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0700
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0685
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0672
6336/6530 [============================>.] - ETA: 0s - loss: 0.0667
# training | RMSE: 0.1937, MAE: 0.1533
worker 1  xfile  [6, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4979476961384771}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.19365438569526677, 'rmse': 0.19365438569526677, 'mae': 0.15331101441494416, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  29 | activation: tanh    | extras: dropout - rate: 40.0% 
layer 2 | size:  42 | activation: tanh    | extras: None 
layer 3 | size:  53 | activation: tanh    | extras: dropout - rate: 10.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f03ec1c6f98>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 54s - loss: 0.6796
6530/6530 [==============================] - 2s 263us/step - loss: 0.0662 - val_loss: 0.0474
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0402
 832/6530 [==>...........................] - ETA: 2s - loss: 0.5296 
 240/6530 [>.............................] - ETA: 1s - loss: 0.0440
1696/6530 [======>.......................] - ETA: 1s - loss: 0.3887
 496/6530 [=>............................] - ETA: 1s - loss: 0.0404
2496/6530 [==========>...................] - ETA: 0s - loss: 0.3267
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0402
3328/6530 [==============>...............] - ETA: 0s - loss: 0.2887
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0426
4096/6530 [=================>............] - ETA: 0s - loss: 0.2674
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0419
4896/6530 [=====================>........] - ETA: 0s - loss: 0.2524
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0425
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2400
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0428
6496/6530 [============================>.] - ETA: 0s - loss: 0.2330
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0428
6530/6530 [==============================] - 1s 111us/step - loss: 0.2326 - val_loss: 0.1616
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1684
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0431
 864/6530 [==>...........................] - ETA: 0s - loss: 0.1737
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0432
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1687
2912/6530 [============>.................] - ETA: 0s - loss: 0.0431
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1694
3184/6530 [=============>................] - ETA: 0s - loss: 0.0432
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1676
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0435
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1656
3728/6530 [================>.............] - ETA: 0s - loss: 0.0439
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1648
4000/6530 [=================>............] - ETA: 0s - loss: 0.0439
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1643
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0436
6530/6530 [==============================] - 0s 62us/step - loss: 0.1633 - val_loss: 0.1336
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1573
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0435
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1533
# training | RMSE: 0.1695, MAE: 0.1365
worker 2  xfile  [4, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2820274393310773}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.16946652469011184, 'rmse': 0.16946652469011184, 'mae': 0.1364937187789613, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:   5 | activation: tanh    | extras: batchnorm 
layer 2 | size:  53 | activation: sigmoid | extras: dropout - rate: 16.9% 
layer 3 | size:   4 | activation: tanh    | extras: batchnorm 
layer 4 | size:  36 | activation: sigmoid | extras: None 
layer 5 | size:  34 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c044358>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 2:19 - loss: 0.2760
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0437
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1489
 448/6530 [=>............................] - ETA: 10s - loss: 0.2201 
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0437
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1505
 864/6530 [==>...........................] - ETA: 5s - loss: 0.2240 
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0435
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1488
1280/6530 [====>.........................] - ETA: 3s - loss: 0.2165
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0430
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1475
1696/6530 [======>.......................] - ETA: 2s - loss: 0.2148
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0428
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1474
2112/6530 [========>.....................] - ETA: 1s - loss: 0.2110
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0425
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1471
2496/6530 [==========>...................] - ETA: 1s - loss: 0.2092
6432/6530 [============================>.] - ETA: 0s - loss: 0.0422
6530/6530 [==============================] - 0s 63us/step - loss: 0.1464 - val_loss: 0.1244

2912/6530 [============>.................] - ETA: 1s - loss: 0.2086
6530/6530 [==============================] - 1s 199us/step - loss: 0.0422 - val_loss: 0.0364
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0141
3296/6530 [==============>...............] - ETA: 1s - loss: 0.2065
 272/6530 [>.............................] - ETA: 1s - loss: 0.0339
3712/6530 [================>.............] - ETA: 0s - loss: 0.2053
 544/6530 [=>............................] - ETA: 1s - loss: 0.0364
4128/6530 [=================>............] - ETA: 0s - loss: 0.2030
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0358
4512/6530 [===================>..........] - ETA: 0s - loss: 0.2011
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0352
4928/6530 [=====================>........] - ETA: 0s - loss: 0.2001
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0358
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1978
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0371
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1965
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0368
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1951
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0359
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0355
6530/6530 [==============================] - 2s 243us/step - loss: 0.1943 - val_loss: 0.1720
Epoch 2/3

  32/6530 [..............................] - ETA: 1s - loss: 0.1813
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0353
 448/6530 [=>............................] - ETA: 0s - loss: 0.1748
3040/6530 [============>.................] - ETA: 0s - loss: 0.0354
 800/6530 [==>...........................] - ETA: 0s - loss: 0.1757
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0352
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1741
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0351
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1729
3824/6530 [================>.............] - ETA: 0s - loss: 0.0350
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1722
4112/6530 [=================>............] - ETA: 0s - loss: 0.0347
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1711
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0347
3040/6530 [============>.................] - ETA: 0s - loss: 0.1712
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0346
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1721
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0347
3840/6530 [================>.............] - ETA: 0s - loss: 0.1720
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0345
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0346
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1724
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0344
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1719
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0345
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1724
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0345
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1717
6496/6530 [============================>.] - ETA: 0s - loss: 0.0346
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1717
# training | RMSE: 0.1537, MAE: 0.1202
worker 1  xfile  [8, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39962949672182224}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10015621671395541}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11581437543817096}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.15366325389898502, 'rmse': 0.15366325389898502, 'mae': 0.12024241122883954, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:   9 | activation: tanh    | extras: dropout - rate: 15.1% 
layer 2 | size:  19 | activation: sigmoid | extras: None 
layer 3 | size:  63 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  52 | activation: sigmoid | extras: dropout - rate: 36.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f03ec1c72b0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 3:36 - loss: 1.1371
6530/6530 [==============================] - 1s 196us/step - loss: 0.0346 - val_loss: 0.0305

6272/6530 [===========================>..] - ETA: 0s - loss: 0.1710
 288/6530 [>.............................] - ETA: 12s - loss: 0.9906 
6530/6530 [==============================] - 1s 130us/step - loss: 0.1711 - val_loss: 0.1593
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1396
 576/6530 [=>............................] - ETA: 6s - loss: 0.7247 
 448/6530 [=>............................] - ETA: 0s - loss: 0.1693
 848/6530 [==>...........................] - ETA: 4s - loss: 0.5712
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1719
1120/6530 [====>.........................] - ETA: 3s - loss: 0.4910
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1649
1392/6530 [=====>........................] - ETA: 2s - loss: 0.4322
1632/6530 [======>.......................] - ETA: 0s - loss: 0.1641
1648/6530 [======>.......................] - ETA: 2s - loss: 0.3930
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1650
1920/6530 [=======>......................] - ETA: 2s - loss: 0.3633
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1685
2192/6530 [=========>....................] - ETA: 1s - loss: 0.3397
2880/6530 [============>.................] - ETA: 0s - loss: 0.1666
2464/6530 [==========>...................] - ETA: 1s - loss: 0.3219
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1663
2720/6530 [===========>..................] - ETA: 1s - loss: 0.3081
3712/6530 [================>.............] - ETA: 0s - loss: 0.1664
3008/6530 [============>.................] - ETA: 1s - loss: 0.2937
# training | RMSE: 0.1707, MAE: 0.1338
worker 0  xfile  [5, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4631191913250753}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.1706558065828728, 'rmse': 0.1706558065828728, 'mae': 0.13383464679139845, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  61 | activation: sigmoid | extras: dropout - rate: 25.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c3727b8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:08 - loss: 0.6252
4096/6530 [=================>............] - ETA: 0s - loss: 0.1668
3296/6530 [==============>...............] - ETA: 1s - loss: 0.2831
 688/6530 [==>...........................] - ETA: 1s - loss: 0.1722  
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1673
3568/6530 [===============>..............] - ETA: 0s - loss: 0.2750
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1206
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1673
3856/6530 [================>.............] - ETA: 0s - loss: 0.2673
1936/6530 [=======>......................] - ETA: 0s - loss: 0.1005
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1674
4112/6530 [=================>............] - ETA: 0s - loss: 0.2616
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0898
4384/6530 [===================>..........] - ETA: 0s - loss: 0.2559
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1667
3136/6530 [=============>................] - ETA: 0s - loss: 0.0810
4672/6530 [====================>.........] - ETA: 0s - loss: 0.2510
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1664
3728/6530 [================>.............] - ETA: 0s - loss: 0.0763
4944/6530 [=====================>........] - ETA: 0s - loss: 0.2468
6530/6530 [==============================] - 1s 132us/step - loss: 0.1662 - val_loss: 0.1552

4384/6530 [===================>..........] - ETA: 0s - loss: 0.0715
5216/6530 [======================>.......] - ETA: 0s - loss: 0.2432
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0685
5488/6530 [========================>.....] - ETA: 0s - loss: 0.2394
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0657
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2357
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0635
6000/6530 [==========================>...] - ETA: 0s - loss: 0.2336
6530/6530 [==============================] - 1s 113us/step - loss: 0.0624 - val_loss: 0.0426
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0540
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2303
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0449
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0419
6530/6530 [==============================] - 2s 282us/step - loss: 0.2277 - val_loss: 0.1932
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1900
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0418
 272/6530 [>.............................] - ETA: 1s - loss: 0.1714
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0415
 560/6530 [=>............................] - ETA: 1s - loss: 0.1761
3088/6530 [=============>................] - ETA: 0s - loss: 0.0409
 848/6530 [==>...........................] - ETA: 1s - loss: 0.1759
3760/6530 [================>.............] - ETA: 0s - loss: 0.0411
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0414
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1753
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0416
1424/6530 [=====>........................] - ETA: 0s - loss: 0.1729
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0416
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1710
6464/6530 [============================>.] - ETA: 0s - loss: 0.0415
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1710
6530/6530 [==============================] - 1s 82us/step - loss: 0.0414 - val_loss: 0.0415
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0491
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1703
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0429
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1717
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0405
2832/6530 [============>.................] - ETA: 0s - loss: 0.1704
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0404
3104/6530 [=============>................] - ETA: 0s - loss: 0.1687
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0408
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1691
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0406
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1698
4064/6530 [=================>............] - ETA: 0s - loss: 0.0408
3936/6530 [=================>............] - ETA: 0s - loss: 0.1699
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0412
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1698
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0413
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1705
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0413
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1701
6530/6530 [==============================] - 1s 78us/step - loss: 0.0411 - val_loss: 0.0413

5040/6530 [======================>.......] - ETA: 0s - loss: 0.1703
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1705
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1702
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1701
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1701
# training | RMSE: 0.1953, MAE: 0.1532
worker 2  xfile  [7, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 5, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16911047813414518}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.1952990706212506, 'rmse': 0.1952990706212506, 'mae': 0.1532137729123815, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  56 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  42 | activation: relu    | extras: dropout - rate: 24.2% 
layer 3 | size:  15 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  37 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f04163e66d8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 1:07 - loss: 1.4242
6512/6530 [============================>.] - ETA: 0s - loss: 0.1695
1024/6530 [===>..........................] - ETA: 3s - loss: 0.8545  
6530/6530 [==============================] - 1s 187us/step - loss: 0.1695 - val_loss: 0.1713
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1828
1984/6530 [========>.....................] - ETA: 1s - loss: 0.6027
 304/6530 [>.............................] - ETA: 1s - loss: 0.1649
3008/6530 [============>.................] - ETA: 0s - loss: 0.4384
 576/6530 [=>............................] - ETA: 1s - loss: 0.1745
3968/6530 [=================>............] - ETA: 0s - loss: 0.3475
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1747
4800/6530 [=====================>........] - ETA: 0s - loss: 0.2956
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1743
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2533
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1703
1632/6530 [======>.......................] - ETA: 0s - loss: 0.1686
6530/6530 [==============================] - 1s 167us/step - loss: 0.2282 - val_loss: 0.0345
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0365
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1693
# training | RMSE: 0.2023, MAE: 0.1641
worker 0  xfile  [10, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2545763365188426}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3379367451056181}, 'layer_4_size': 56, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.20227682307026, 'rmse': 0.20227682307026, 'mae': 0.16408324235303956, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:   9 | activation: tanh    | extras: dropout - rate: 31.6% 
layer 2 | size:  35 | activation: tanh    | extras: None 
layer 3 | size:   3 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03ea391eb8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:05 - loss: 1.3838
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0415
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1697
 960/6530 [===>..........................] - ETA: 2s - loss: 1.0250  
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0389
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1692
1920/6530 [=======>......................] - ETA: 1s - loss: 0.7931
3008/6530 [============>.................] - ETA: 0s - loss: 0.0377
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1690
2784/6530 [===========>..................] - ETA: 0s - loss: 0.6397
3840/6530 [================>.............] - ETA: 0s - loss: 0.0368
2960/6530 [============>.................] - ETA: 0s - loss: 0.1682
3680/6530 [===============>..............] - ETA: 0s - loss: 0.5229
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0366
3248/6530 [=============>................] - ETA: 0s - loss: 0.1672
4608/6530 [====================>.........] - ETA: 0s - loss: 0.4377
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0355
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1682
5568/6530 [========================>.....] - ETA: 0s - loss: 0.3741
6530/6530 [==============================] - 0s 56us/step - loss: 0.0351 - val_loss: 0.0265

3808/6530 [================>.............] - ETA: 0s - loss: 0.1683Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0353
6528/6530 [============================>.] - ETA: 0s - loss: 0.3292
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0302
4080/6530 [=================>............] - ETA: 0s - loss: 0.1689
6530/6530 [==============================] - 1s 111us/step - loss: 0.3291 - val_loss: 0.0522
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0471
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1687
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0300
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0621
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1695
2944/6530 [============>.................] - ETA: 0s - loss: 0.0291
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0629
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1695
3904/6530 [================>.............] - ETA: 0s - loss: 0.0291
2848/6530 [============>.................] - ETA: 0s - loss: 0.0610
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0288
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1701
3808/6530 [================>.............] - ETA: 0s - loss: 0.0593
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0287
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1702
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0583
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1698
6530/6530 [==============================] - 0s 57us/step - loss: 0.0283 - val_loss: 0.0257

5664/6530 [=========================>....] - ETA: 0s - loss: 0.0581
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1700
6530/6530 [==============================] - 0s 57us/step - loss: 0.0570 - val_loss: 0.0462
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0511
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1696
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0501
6530/6530 [==============================] - 1s 195us/step - loss: 0.1692 - val_loss: 0.1655

1888/6530 [=======>......................] - ETA: 0s - loss: 0.0529
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0539
3744/6530 [================>.............] - ETA: 0s - loss: 0.0541
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0532
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0527
6464/6530 [============================>.] - ETA: 0s - loss: 0.0524
6530/6530 [==============================] - 0s 57us/step - loss: 0.0522 - val_loss: 0.0438

# training | RMSE: 0.1595, MAE: 0.1246
worker 2  xfile  [11, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2423132848388729}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.15946359563698403, 'rmse': 0.15946359563698403, 'mae': 0.12456863576156432, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  69 | activation: tanh    | extras: None 
layer 2 | size:  60 | activation: tanh    | extras: None 
layer 3 | size:  52 | activation: relu    | extras: None 
layer 4 | size:  60 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f04163e65c0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 16s - loss: 1.2557
3328/6530 [==============>...............] - ETA: 0s - loss: 0.2446 
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1582
6530/6530 [==============================] - 0s 73us/step - loss: 0.1490 - val_loss: 0.0429
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0381
# training | RMSE: 0.2092, MAE: 0.1678
worker 0  xfile  [12, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3162994555792672}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.20924728178706348, 'rmse': 0.20924728178706348, 'mae': 0.16782229764518206, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  25 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f03e8070390>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:24 - loss: 0.7725
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0389
# training | RMSE: 0.2142, MAE: 0.1647
worker 1  xfile  [9, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15068232159870046}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36942877284804576}, 'layer_4_size': 52, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2141942366987666, 'rmse': 0.2141942366987666, 'mae': 0.16466899052472514, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  68 | activation: tanh    | extras: dropout - rate: 42.0% 
layer 2 | size:  72 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f03c07b3198>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 55s - loss: 0.0915
 576/6530 [=>............................] - ETA: 2s - loss: 0.6273  
6530/6530 [==============================] - 0s 17us/step - loss: 0.0374 - val_loss: 0.0358
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0312
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0714 
1104/6530 [====>.........................] - ETA: 1s - loss: 0.4744
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0325
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0628
1680/6530 [======>.......................] - ETA: 1s - loss: 0.3734
3072/6530 [=============>................] - ETA: 0s - loss: 0.0576
6530/6530 [==============================] - 0s 16us/step - loss: 0.0318 - val_loss: 0.0320

2288/6530 [=========>....................] - ETA: 0s - loss: 0.3004
4000/6530 [=================>............] - ETA: 0s - loss: 0.0560
2864/6530 [============>.................] - ETA: 0s - loss: 0.2528
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0543
3440/6530 [==============>...............] - ETA: 0s - loss: 0.2200
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0532
4032/6530 [=================>............] - ETA: 0s - loss: 0.1953
6530/6530 [==============================] - 1s 101us/step - loss: 0.0525 - val_loss: 0.0422

4624/6530 [====================>.........] - ETA: 0s - loss: 0.1771Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0514
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1631
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0493
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1521
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0463
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1433
3168/6530 [=============>................] - ETA: 0s - loss: 0.0458
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0462
6530/6530 [==============================] - 1s 128us/step - loss: 0.1398 - val_loss: 0.0462
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0582
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0465
 576/6530 [=>............................] - ETA: 0s - loss: 0.0497
6336/6530 [============================>.] - ETA: 0s - loss: 0.0463
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0480
6530/6530 [==============================] - 0s 51us/step - loss: 0.0461 - val_loss: 0.0441
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0553
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0468
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0478
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0461
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0442
2896/6530 [============>.................] - ETA: 0s - loss: 0.0454
3008/6530 [============>.................] - ETA: 0s - loss: 0.0446
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0449
4064/6530 [=================>............] - ETA: 0s - loss: 0.0455
4128/6530 [=================>............] - ETA: 0s - loss: 0.0445
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0455
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0441
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0458
6530/6530 [==============================] - 0s 53us/step - loss: 0.0455 - val_loss: 0.0435

5216/6530 [======================>.......] - ETA: 0s - loss: 0.0436
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0431
6352/6530 [============================>.] - ETA: 0s - loss: 0.0430
6530/6530 [==============================] - 1s 92us/step - loss: 0.0427 - val_loss: 0.0370
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0494
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0380
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0374
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0367
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0366
3072/6530 [=============>................] - ETA: 0s - loss: 0.0358
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0356
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0352
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0349
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0346
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0345
# training | RMSE: 0.1711, MAE: 0.1340
worker 2  xfile  [13, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.17108039254895374, 'rmse': 0.17108039254895374, 'mae': 0.13403858758651174, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  74 | activation: tanh    | extras: batchnorm 
layer 2 | size:  29 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  62 | activation: sigmoid | extras: dropout - rate: 45.5% 
layer 4 | size:  35 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03fae93b00>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 39s - loss: 1.0171
2176/6530 [========>.....................] - ETA: 1s - loss: 0.6169 
6530/6530 [==============================] - 1s 89us/step - loss: 0.0341 - val_loss: 0.0301

4224/6530 [==================>...........] - ETA: 0s - loss: 0.4368
6144/6530 [===========================>..] - ETA: 0s - loss: 0.3550
6530/6530 [==============================] - 1s 158us/step - loss: 0.3437 - val_loss: 0.1641
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1685
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1654
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1650
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1646
6530/6530 [==============================] - 0s 27us/step - loss: 0.1641 - val_loss: 0.1625
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1663
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1655
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1653
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1655
6530/6530 [==============================] - 0s 31us/step - loss: 0.1645 - val_loss: 0.1649

# training | RMSE: 0.1731, MAE: 0.1376
worker 0  xfile  [15, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.46574055977979867}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.17312046451131938, 'rmse': 0.17312046451131938, 'mae': 0.13756871817135075, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  80 | activation: relu    | extras: dropout - rate: 35.1% 
layer 2 | size:  45 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03ea2cbf98>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 22s - loss: 0.6080
# training | RMSE: 0.2082, MAE: 0.1712
worker 1  xfile  [14, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4196428123159085}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 76, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.20817453363917193, 'rmse': 0.20817453363917193, 'mae': 0.17121906890139185, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  86 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  82 | activation: sigmoid | extras: None 
layer 3 | size:  84 | activation: tanh    | extras: dropout - rate: 17.9% 
layer 4 | size:  95 | activation: sigmoid | extras: dropout - rate: 26.3% 
layer 5 | size:  52 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00dce58ac8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 3:56 - loss: 0.4130
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1135 
 272/6530 [>.............................] - ETA: 14s - loss: 0.1658 
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0821
 544/6530 [=>............................] - ETA: 7s - loss: 0.1266 
 816/6530 [==>...........................] - ETA: 5s - loss: 0.1124
6530/6530 [==============================] - 0s 63us/step - loss: 0.0716 - val_loss: 0.0537
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0653
1104/6530 [====>.........................] - ETA: 3s - loss: 0.1049
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0457
1392/6530 [=====>........................] - ETA: 3s - loss: 0.0964
3968/6530 [=================>............] - ETA: 0s - loss: 0.0460
1664/6530 [======>.......................] - ETA: 2s - loss: 0.0922
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0456
6530/6530 [==============================] - 0s 27us/step - loss: 0.0453 - val_loss: 0.0469
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0544
1936/6530 [=======>......................] - ETA: 2s - loss: 0.0893
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0419
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0861
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0431
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0830
6530/6530 [==============================] - 0s 25us/step - loss: 0.0430 - val_loss: 0.0423

2768/6530 [===========>..................] - ETA: 1s - loss: 0.0803
3024/6530 [============>.................] - ETA: 1s - loss: 0.0770
3280/6530 [==============>...............] - ETA: 1s - loss: 0.0748
3568/6530 [===============>..............] - ETA: 1s - loss: 0.0728
3856/6530 [================>.............] - ETA: 0s - loss: 0.0709
4096/6530 [=================>............] - ETA: 0s - loss: 0.0697
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0679
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0667
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0654
# training | RMSE: 0.2083, MAE: 0.1616
worker 2  xfile  [16, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45512557545093135}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 35, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20830080620422795, 'rmse': 0.20830080620422795, 'mae': 0.16162873575574827, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  15 | activation: tanh    | extras: batchnorm 
layer 2 | size:  45 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c2201d0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:34 - loss: 0.8852
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0647
 832/6530 [==>...........................] - ETA: 3s - loss: 0.5044  
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0639
1632/6530 [======>.......................] - ETA: 1s - loss: 0.4001
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0632
2400/6530 [==========>...................] - ETA: 1s - loss: 0.3552
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0625
3168/6530 [=============>................] - ETA: 0s - loss: 0.3265
6320/6530 [============================>.] - ETA: 0s - loss: 0.0618
3936/6530 [=================>............] - ETA: 0s - loss: 0.3101
4704/6530 [====================>.........] - ETA: 0s - loss: 0.2953
6530/6530 [==============================] - 2s 294us/step - loss: 0.0610 - val_loss: 0.0438

5408/6530 [=======================>......] - ETA: 0s - loss: 0.2845Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0564
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2744
 288/6530 [>.............................] - ETA: 1s - loss: 0.0450
 560/6530 [=>............................] - ETA: 1s - loss: 0.0485
6530/6530 [==============================] - 1s 149us/step - loss: 0.2708 - val_loss: 0.2068
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2107
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0493
 800/6530 [==>...........................] - ETA: 0s - loss: 0.1974
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0489
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1953
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0468
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1932
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0454
3232/6530 [=============>................] - ETA: 0s - loss: 0.1900
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0457
# training | RMSE: 0.2004, MAE: 0.1640
worker 0  xfile  [18, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35109665238578447}, 'layer_1_size': 80, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 25, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.20040383736599218, 'rmse': 0.20040383736599218, 'mae': 0.16398443827298112, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  58 | activation: tanh    | extras: batchnorm 
layer 2 | size:  17 | activation: tanh    | extras: dropout - rate: 45.5% 
layer 3 | size:  38 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03c4197860>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 30s - loss: 1.1434
4032/6530 [=================>............] - ETA: 0s - loss: 0.1903
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0453
2304/6530 [=========>....................] - ETA: 1s - loss: 0.6913 
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1891
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0458
4480/6530 [===================>..........] - ETA: 0s - loss: 0.5183
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1875
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0458
6336/6530 [============================>.] - ETA: 0s - loss: 0.1861
3024/6530 [============>.................] - ETA: 0s - loss: 0.0451
6530/6530 [==============================] - 0s 67us/step - loss: 0.1860 - val_loss: 0.1760
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1856
6530/6530 [==============================] - 1s 125us/step - loss: 0.4110 - val_loss: 0.0719
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1473
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0452
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1694
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1136
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0454
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1695
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1039
3808/6530 [================>.............] - ETA: 0s - loss: 0.0454
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1689
6528/6530 [============================>.] - ETA: 0s - loss: 0.0953
6530/6530 [==============================] - 0s 26us/step - loss: 0.0953 - val_loss: 0.0424
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0651
4080/6530 [=================>............] - ETA: 0s - loss: 0.0456
3072/6530 [=============>................] - ETA: 0s - loss: 0.1680
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0697
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0454
3840/6530 [================>.............] - ETA: 0s - loss: 0.1680
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0662
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0457
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1676
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0639
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0456
6530/6530 [==============================] - 0s 26us/step - loss: 0.0633 - val_loss: 0.0393

5344/6530 [=======================>......] - ETA: 0s - loss: 0.1666
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0457
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1660
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0460
6530/6530 [==============================] - 0s 71us/step - loss: 0.1659 - val_loss: 0.1608

5600/6530 [========================>.....] - ETA: 0s - loss: 0.0457
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0458
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0456
6464/6530 [============================>.] - ETA: 0s - loss: 0.0454
6530/6530 [==============================] - 1s 199us/step - loss: 0.0453 - val_loss: 0.0424
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0511
 320/6530 [>.............................] - ETA: 1s - loss: 0.0428
 608/6530 [=>............................] - ETA: 1s - loss: 0.0476
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0471
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0461
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0453
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0439
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0444
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0442
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0449
2880/6530 [============>.................] - ETA: 0s - loss: 0.0444
3136/6530 [=============>................] - ETA: 0s - loss: 0.0438
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0443
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0445
3952/6530 [=================>............] - ETA: 0s - loss: 0.0448
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0446
# training | RMSE: 0.1972, MAE: 0.1569
worker 0  xfile  [20, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4554561189849352}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.379198274209536}, 'layer_4_size': 21, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.19717545781635487, 'rmse': 0.19717545781635487, 'mae': 0.1569289764553579, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  24 | activation: tanh    | extras: dropout - rate: 17.4% 
layer 2 | size:  80 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  60 | activation: relu    | extras: None 
layer 4 | size:  19 | activation: tanh    | extras: dropout - rate: 21.7% 
layer 5 | size:  36 | activation: relu    | extras: dropout - rate: 40.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f03ea2cbb38>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 12s - loss: 0.7544
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0450
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2939 
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0449
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0448
6530/6530 [==============================] - 1s 101us/step - loss: 0.2547 - val_loss: 0.0987
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1238
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0450
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1130
6530/6530 [==============================] - 0s 12us/step - loss: 0.1105 - val_loss: 0.0651

5552/6530 [========================>.....] - ETA: 0s - loss: 0.0449Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0892
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0451
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0916
6530/6530 [==============================] - 0s 12us/step - loss: 0.0907 - val_loss: 0.0571

6112/6530 [===========================>..] - ETA: 0s - loss: 0.0449
6368/6530 [============================>.] - ETA: 0s - loss: 0.0449
# training | RMSE: 0.1909, MAE: 0.1501
worker 2  xfile  [19, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.1908611577616503, 'rmse': 0.1908611577616503, 'mae': 0.15009028274271458, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  49 | activation: sigmoid | extras: None 
layer 2 | size:  66 | activation: tanh    | extras: dropout - rate: 21.7% 
layer 3 | size:  10 | activation: tanh    | extras: batchnorm 
layer 4 | size:  25 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0400101320>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 4:54 - loss: 1.2356
6530/6530 [==============================] - 1s 193us/step - loss: 0.0448 - val_loss: 0.0420

 288/6530 [>.............................] - ETA: 16s - loss: 0.9827 
 544/6530 [=>............................] - ETA: 9s - loss: 0.7928 
 784/6530 [==>...........................] - ETA: 6s - loss: 0.6773
1056/6530 [===>..........................] - ETA: 4s - loss: 0.5845
1312/6530 [=====>........................] - ETA: 3s - loss: 0.5317
1584/6530 [======>.......................] - ETA: 3s - loss: 0.4752
1840/6530 [=======>......................] - ETA: 2s - loss: 0.4430
2112/6530 [========>.....................] - ETA: 2s - loss: 0.4093
2368/6530 [=========>....................] - ETA: 2s - loss: 0.3798
2592/6530 [==========>...................] - ETA: 1s - loss: 0.3603
2800/6530 [===========>..................] - ETA: 1s - loss: 0.3439
3024/6530 [============>.................] - ETA: 1s - loss: 0.3267
3296/6530 [==============>...............] - ETA: 1s - loss: 0.3099
3552/6530 [===============>..............] - ETA: 1s - loss: 0.2974
3808/6530 [================>.............] - ETA: 1s - loss: 0.2843
4064/6530 [=================>............] - ETA: 0s - loss: 0.2730
4336/6530 [==================>...........] - ETA: 0s - loss: 0.2612
4608/6530 [====================>.........] - ETA: 0s - loss: 0.2510
4848/6530 [=====================>........] - ETA: 0s - loss: 0.2435
# training | RMSE: 0.2329, MAE: 0.1898
worker 0  xfile  [21, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1741077956753633}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2169358952905691}, 'layer_4_size': 19, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4017496563510651}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.23285364499309666, 'rmse': 0.23285364499309666, 'mae': 0.18979732980117883, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  28 | activation: tanh    | extras: dropout - rate: 15.1% 
layer 2 | size:  34 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  60 | activation: tanh    | extras: None 
layer 4 | size:  28 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f03ea391eb8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:45 - loss: 1.1774
5088/6530 [======================>.......] - ETA: 0s - loss: 0.2364
 672/6530 [==>...........................] - ETA: 4s - loss: 0.4276  
5312/6530 [=======================>......] - ETA: 0s - loss: 0.2298
1312/6530 [=====>........................] - ETA: 2s - loss: 0.3128
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2240
1856/6530 [=======>......................] - ETA: 1s - loss: 0.2676
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2184
2464/6530 [==========>...................] - ETA: 1s - loss: 0.2376
6000/6530 [==========================>...] - ETA: 0s - loss: 0.2134
3136/6530 [=============>................] - ETA: 0s - loss: 0.2101
6240/6530 [===========================>..] - ETA: 0s - loss: 0.2079
3776/6530 [================>.............] - ETA: 0s - loss: 0.1913
6480/6530 [============================>.] - ETA: 0s - loss: 0.2030
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1773
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1659
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1573
6530/6530 [==============================] - 2s 337us/step - loss: 0.2019 - val_loss: 0.0565
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0901
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1489
 272/6530 [>.............................] - ETA: 1s - loss: 0.0790
 512/6530 [=>............................] - ETA: 1s - loss: 0.0733
6530/6530 [==============================] - 1s 177us/step - loss: 0.1436 - val_loss: 0.0449
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0885
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0692
 576/6530 [=>............................] - ETA: 0s - loss: 0.0719
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0699
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0656
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0713
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0640
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0696
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0638
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0710
3040/6530 [============>.................] - ETA: 0s - loss: 0.0614
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0697
# training | RMSE: 0.2033, MAE: 0.1624
worker 1  xfile  [17, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17922789040016737}, 'layer_3_size': 84, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.262831125587177}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.20330629319433782, 'rmse': 0.20330629319433782, 'mae': 0.1624197784626277, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  34 | activation: sigmoid | extras: dropout - rate: 18.4% 
layer 2 | size:  56 | activation: tanh    | extras: batchnorm 
layer 3 | size:  97 | activation: tanh    | extras: dropout - rate: 20.4% 
layer 4 | size:  12 | activation: relu    | extras: batchnorm 
layer 5 | size:  66 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00dca1f5f8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 5:02 - loss: 0.6800
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0608
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0685
 240/6530 [>.............................] - ETA: 20s - loss: 0.3745 
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0594
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0682
 464/6530 [=>............................] - ETA: 11s - loss: 0.3037
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0587
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0678
 704/6530 [==>...........................] - ETA: 7s - loss: 0.2795 
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0569
2944/6530 [============>.................] - ETA: 0s - loss: 0.0670
 928/6530 [===>..........................] - ETA: 5s - loss: 0.2670
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0565
3200/6530 [=============>................] - ETA: 0s - loss: 0.0661
1152/6530 [====>.........................] - ETA: 4s - loss: 0.2587
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0655
6530/6530 [==============================] - 1s 89us/step - loss: 0.0560 - val_loss: 0.0325
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0624
1376/6530 [=====>........................] - ETA: 3s - loss: 0.2483
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0647
 640/6530 [=>............................] - ETA: 0s - loss: 0.0512
1600/6530 [======>.......................] - ETA: 3s - loss: 0.2424
3936/6530 [=================>............] - ETA: 0s - loss: 0.0640
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0488
1840/6530 [=======>......................] - ETA: 2s - loss: 0.2388
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0627
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0474
2080/6530 [========>.....................] - ETA: 2s - loss: 0.2337
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0622
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0468
2304/6530 [=========>....................] - ETA: 2s - loss: 0.2305
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0617
2976/6530 [============>.................] - ETA: 0s - loss: 0.0461
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0613
2528/6530 [==========>...................] - ETA: 2s - loss: 0.2282
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0462
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0607
2768/6530 [===========>..................] - ETA: 1s - loss: 0.2257
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0451
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0600
2992/6530 [============>.................] - ETA: 1s - loss: 0.2219
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0449
3200/6530 [=============>................] - ETA: 1s - loss: 0.2198
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0597
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0439
3424/6530 [==============>...............] - ETA: 1s - loss: 0.2181
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0591
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0436
3648/6530 [===============>..............] - ETA: 1s - loss: 0.2167
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0584
6530/6530 [==============================] - 1s 87us/step - loss: 0.0435 - val_loss: 0.0300

3840/6530 [================>.............] - ETA: 1s - loss: 0.2154
6384/6530 [============================>.] - ETA: 0s - loss: 0.0579
4064/6530 [=================>............] - ETA: 1s - loss: 0.2146
6530/6530 [==============================] - 1s 217us/step - loss: 0.0577 - val_loss: 0.0455
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0473
4288/6530 [==================>...........] - ETA: 0s - loss: 0.2128
 272/6530 [>.............................] - ETA: 1s - loss: 0.0443
4512/6530 [===================>..........] - ETA: 0s - loss: 0.2126
 496/6530 [=>............................] - ETA: 1s - loss: 0.0418
4736/6530 [====================>.........] - ETA: 0s - loss: 0.2108
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0403
4960/6530 [=====================>........] - ETA: 0s - loss: 0.2091
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0387
5200/6530 [======================>.......] - ETA: 0s - loss: 0.2075
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0392
5440/6530 [=======================>......] - ETA: 0s - loss: 0.2065
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0389
5664/6530 [=========================>....] - ETA: 0s - loss: 0.2057
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0397
5904/6530 [==========================>...] - ETA: 0s - loss: 0.2049
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0395
6144/6530 [===========================>..] - ETA: 0s - loss: 0.2042
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0391
6384/6530 [============================>.] - ETA: 0s - loss: 0.2033
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0389
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0388
2928/6530 [============>.................] - ETA: 0s - loss: 0.0390
6530/6530 [==============================] - 2s 360us/step - loss: 0.2028 - val_loss: 0.1630
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.2072
3152/6530 [=============>................] - ETA: 0s - loss: 0.0388
 256/6530 [>.............................] - ETA: 1s - loss: 0.1779
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0390
 480/6530 [=>............................] - ETA: 1s - loss: 0.1880
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0390
 704/6530 [==>...........................] - ETA: 1s - loss: 0.1880
3888/6530 [================>.............] - ETA: 0s - loss: 0.0388
 944/6530 [===>..........................] - ETA: 1s - loss: 0.1887
4128/6530 [=================>............] - ETA: 0s - loss: 0.0389
1184/6530 [====>.........................] - ETA: 1s - loss: 0.1868
# training | RMSE: 0.1673, MAE: 0.1324
worker 0  xfile  [23, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15132983247231468}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13553206179667554}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.16734912849644126, 'rmse': 0.16734912849644126, 'mae': 0.13244032601551262, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  55 | activation: sigmoid | extras: None 
layer 2 | size:  15 | activation: tanh    | extras: None 
layer 3 | size:  92 | activation: sigmoid | extras: None 
layer 4 | size:  92 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03ea3910b8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 22s - loss: 0.6330
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0388
1408/6530 [=====>........................] - ETA: 1s - loss: 0.1861
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1369 
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0386
1632/6530 [======>.......................] - ETA: 1s - loss: 0.1834
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0390
1856/6530 [=======>......................] - ETA: 1s - loss: 0.1828
6530/6530 [==============================] - 1s 92us/step - loss: 0.1043 - val_loss: 0.0585
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0725
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0388
2080/6530 [========>.....................] - ETA: 1s - loss: 0.1822
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0570
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0389
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1829
6530/6530 [==============================] - 0s 17us/step - loss: 0.0523 - val_loss: 0.0455

5584/6530 [========================>.....] - ETA: 0s - loss: 0.0387Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0545
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1835
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0445
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0386
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1837
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0385
6530/6530 [==============================] - 0s 16us/step - loss: 0.0431 - val_loss: 0.0431

2976/6530 [============>.................] - ETA: 0s - loss: 0.1824
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0382
3200/6530 [=============>................] - ETA: 0s - loss: 0.1813
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1819
6530/6530 [==============================] - 1s 220us/step - loss: 0.0381 - val_loss: 0.0319

3664/6530 [===============>..............] - ETA: 0s - loss: 0.1823
3904/6530 [================>.............] - ETA: 0s - loss: 0.1825
4128/6530 [=================>............] - ETA: 0s - loss: 0.1829
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1821
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1824
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1819
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1817
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1812
# training | RMSE: 0.1778, MAE: 0.1384
worker 2  xfile  [22, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21704675266779963}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.1777855462340963, 'rmse': 0.1777855462340963, 'mae': 0.13842379637894567, 'early_stop': False}
vggnet done  2

5472/6530 [========================>.....] - ETA: 0s - loss: 0.1810
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1812
# training | RMSE: 0.2068, MAE: 0.1697
worker 0  xfile  [25, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.20682366361007487, 'rmse': 0.20682366361007487, 'mae': 0.16968905380087157, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  36 | activation: sigmoid | extras: dropout - rate: 38.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00af47f4e0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:31 - loss: 0.8121
5968/6530 [==========================>...] - ETA: 0s - loss: 0.1812
 704/6530 [==>...........................] - ETA: 3s - loss: 0.3481  
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1807
1392/6530 [=====>........................] - ETA: 1s - loss: 0.2781
6448/6530 [============================>.] - ETA: 0s - loss: 0.1801
2080/6530 [========>.....................] - ETA: 1s - loss: 0.2477
6530/6530 [==============================] - 2s 232us/step - loss: 0.1800 - val_loss: 0.1605
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1822
2736/6530 [===========>..................] - ETA: 0s - loss: 0.2312
3424/6530 [==============>...............] - ETA: 0s - loss: 0.2190
 256/6530 [>.............................] - ETA: 1s - loss: 0.1728
 480/6530 [=>............................] - ETA: 1s - loss: 0.1812
4064/6530 [=================>............] - ETA: 0s - loss: 0.2097
4736/6530 [====================>.........] - ETA: 0s - loss: 0.2030
 720/6530 [==>...........................] - ETA: 1s - loss: 0.1815
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1982
 976/6530 [===>..........................] - ETA: 1s - loss: 0.1824
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1943
1216/6530 [====>.........................] - ETA: 1s - loss: 0.1806
1456/6530 [=====>........................] - ETA: 1s - loss: 0.1816
6530/6530 [==============================] - 1s 143us/step - loss: 0.1921 - val_loss: 0.1729
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1178
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1797
1936/6530 [=======>......................] - ETA: 0s - loss: 0.1789
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1605
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1783
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1637
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1615
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1790
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1624
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1798
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1616
2912/6530 [============>.................] - ETA: 0s - loss: 0.1784
4016/6530 [=================>............] - ETA: 0s - loss: 0.1621
3152/6530 [=============>................] - ETA: 0s - loss: 0.1765
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1624
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1769
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1623
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1770
5968/6530 [==========================>...] - ETA: 0s - loss: 0.1615
3856/6530 [================>.............] - ETA: 0s - loss: 0.1772
4112/6530 [=================>............] - ETA: 0s - loss: 0.1772
6530/6530 [==============================] - 1s 80us/step - loss: 0.1615 - val_loss: 0.1626
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1378
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1769
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1628
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1774
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1623
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1771
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1630
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1769
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1619
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1763
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1621
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1763
3840/6530 [================>.............] - ETA: 0s - loss: 0.1608
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1760
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1618
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1761
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1608
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1755
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1608
6416/6530 [============================>.] - ETA: 0s - loss: 0.1751
6336/6530 [============================>.] - ETA: 0s - loss: 0.1614
6530/6530 [==============================] - 1s 84us/step - loss: 0.1617 - val_loss: 0.1625

6530/6530 [==============================] - 1s 224us/step - loss: 0.1751 - val_loss: 0.1658

# training | RMSE: 0.2029, MAE: 0.1614
worker 0  xfile  [26, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3801891065249704}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 36, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11923992646601543}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.20291695469614723, 'rmse': 0.20291695469614723, 'mae': 0.16138415375032944, 'early_stop': False}
vggnet done  0

# training | RMSE: 0.2087, MAE: 0.1654
worker 1  xfile  [24, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18391069289201956}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 56, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20412479331356678}, 'layer_3_size': 97, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.20870709918847552, 'rmse': 0.20870709918847552, 'mae': 0.16536833332437803, 'early_stop': False}
vggnet done  1
all of workers have been done
calculation finished
#1 epoch=3.0 loss={'loss': 0.15084795371751028, 'rmse': 0.15084795371751028, 'mae': 0.115721588286351, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2007607438272935}, 'layer_4_size': 91, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#2 epoch=3.0 loss={'loss': 0.16219986493316654, 'rmse': 0.16219986493316654, 'mae': 0.12499960020300763, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21048884801807022}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#0 epoch=3.0 loss={'loss': 0.10594900784371053, 'rmse': 0.10594900784371053, 'mae': 0.08028068016487888, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16745485505931956}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#3 epoch=3.0 loss={'loss': 0.1830710435834075, 'rmse': 0.1830710435834075, 'mae': 0.14729340548151645, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.11422937510548073}, 'layer_1_size': 73, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 5, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
#4 epoch=3.0 loss={'loss': 0.16946652469011184, 'rmse': 0.16946652469011184, 'mae': 0.1364937187789613, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2820274393310773}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#6 epoch=3.0 loss={'loss': 0.19365438569526677, 'rmse': 0.19365438569526677, 'mae': 0.15331101441494416, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4979476961384771}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#8 epoch=3.0 loss={'loss': 0.15366325389898502, 'rmse': 0.15366325389898502, 'mae': 0.12024241122883954, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39962949672182224}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10015621671395541}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11581437543817096}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#5 epoch=3.0 loss={'loss': 0.1706558065828728, 'rmse': 0.1706558065828728, 'mae': 0.13383464679139845, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4631191913250753}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#7 epoch=3.0 loss={'loss': 0.1952990706212506, 'rmse': 0.1952990706212506, 'mae': 0.1532137729123815, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 5, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16911047813414518}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#10 epoch=3.0 loss={'loss': 0.20227682307026, 'rmse': 0.20227682307026, 'mae': 0.16408324235303956, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2545763365188426}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3379367451056181}, 'layer_4_size': 56, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#11 epoch=3.0 loss={'loss': 0.15946359563698403, 'rmse': 0.15946359563698403, 'mae': 0.12456863576156432, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2423132848388729}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#9 epoch=3.0 loss={'loss': 0.2141942366987666, 'rmse': 0.2141942366987666, 'mae': 0.16466899052472514, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15068232159870046}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36942877284804576}, 'layer_4_size': 52, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#12 epoch=3.0 loss={'loss': 0.20924728178706348, 'rmse': 0.20924728178706348, 'mae': 0.16782229764518206, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3162994555792672}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#13 epoch=3.0 loss={'loss': 0.17108039254895374, 'rmse': 0.17108039254895374, 'mae': 0.13403858758651174, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#14 epoch=3.0 loss={'loss': 0.20817453363917193, 'rmse': 0.20817453363917193, 'mae': 0.17121906890139185, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4196428123159085}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 76, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#15 epoch=3.0 loss={'loss': 0.17312046451131938, 'rmse': 0.17312046451131938, 'mae': 0.13756871817135075, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.46574055977979867}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#16 epoch=3.0 loss={'loss': 0.20830080620422795, 'rmse': 0.20830080620422795, 'mae': 0.16162873575574827, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45512557545093135}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 35, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#18 epoch=3.0 loss={'loss': 0.20040383736599218, 'rmse': 0.20040383736599218, 'mae': 0.16398443827298112, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35109665238578447}, 'layer_1_size': 80, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 25, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#20 epoch=3.0 loss={'loss': 0.19717545781635487, 'rmse': 0.19717545781635487, 'mae': 0.1569289764553579, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4554561189849352}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.379198274209536}, 'layer_4_size': 21, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#19 epoch=3.0 loss={'loss': 0.1908611577616503, 'rmse': 0.1908611577616503, 'mae': 0.15009028274271458, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#21 epoch=3.0 loss={'loss': 0.23285364499309666, 'rmse': 0.23285364499309666, 'mae': 0.18979732980117883, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1741077956753633}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2169358952905691}, 'layer_4_size': 19, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4017496563510651}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#17 epoch=3.0 loss={'loss': 0.20330629319433782, 'rmse': 0.20330629319433782, 'mae': 0.1624197784626277, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17922789040016737}, 'layer_3_size': 84, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.262831125587177}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#23 epoch=3.0 loss={'loss': 0.16734912849644126, 'rmse': 0.16734912849644126, 'mae': 0.13244032601551262, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15132983247231468}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13553206179667554}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#25 epoch=3.0 loss={'loss': 0.20682366361007487, 'rmse': 0.20682366361007487, 'mae': 0.16968905380087157, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#22 epoch=3.0 loss={'loss': 0.1777855462340963, 'rmse': 0.1777855462340963, 'mae': 0.13842379637894567, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21704675266779963}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#26 epoch=3.0 loss={'loss': 0.20291695469614723, 'rmse': 0.20291695469614723, 'mae': 0.16138415375032944, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3801891065249704}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 36, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11923992646601543}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#24 epoch=3.0 loss={'loss': 0.20870709918847552, 'rmse': 0.20870709918847552, 'mae': 0.16536833332437803, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18391069289201956}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 56, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20412479331356678}, 'layer_3_size': 97, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
get a list [results] of length 108
get a list [loss] of length 27
get a list [val_loss] of length 27
length of indices is (0, 1, 8, 11, 2, 23, 4, 5, 13, 15, 22, 3, 19, 6, 7, 20, 18, 10, 26, 17, 25, 14, 16, 24, 12, 9, 21)
length of indices is 27
length of T is 27
newly formed T structure is:[[0, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16745485505931956}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [1, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2007607438272935}, 'layer_4_size': 91, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [2, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39962949672182224}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10015621671395541}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11581437543817096}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [3, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2423132848388729}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [4, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21048884801807022}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [5, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15132983247231468}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13553206179667554}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [6, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2820274393310773}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [7, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4631191913250753}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [8, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]] 

*** 9.0 configurations x 9.0 iterations each

25 | Fri Sep 28 01:20:12 2018 | lowest loss so far: 0.1059 (run 0)

{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  29 | activation: tanh    | extras: dropout - rate: 40.0% 
layer 2 | size:  42 | activation: tanh    | extras: None 
layer 3 | size:  53 | activation: tanh    | extras: dropout - rate: 10.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 2:54 - loss: 0.7998
 736/6530 [==>...........................] - ETA: 7s - loss: 0.5867  
1440/6530 [=====>........................] - ETA: 3s - loss: 0.4312{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  43 | activation: tanh    | extras: None 
layer 2 | size:  36 | activation: relu    | extras: batchnorm 
layer 3 | size:  38 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 2:56 - loss: 0.6958
2272/6530 [=========>....................] - ETA: 1s - loss: 0.3474
 704/6530 [==>...........................] - ETA: 7s - loss: 0.6647  
3104/6530 [=============>................] - ETA: 1s - loss: 0.3016
1408/6530 [=====>........................] - ETA: 3s - loss: 0.5270
4000/6530 [=================>............] - ETA: 0s - loss: 0.2747
2016/6530 [========>.....................] - ETA: 2s - loss: 0.4222
4736/6530 [====================>.........] - ETA: 0s - loss: 0.2593
2656/6530 [===========>..................] - ETA: 1s - loss: 0.3592
5600/6530 [========================>.....] - ETA: 0s - loss: 0.2458
3360/6530 [==============>...............] - ETA: 1s - loss: 0.3136
6496/6530 [============================>.] - ETA: 0s - loss: 0.2352
4032/6530 [=================>............] - ETA: 0s - loss: 0.2850
6530/6530 [==============================] - 1s 200us/step - loss: 0.2347 - val_loss: 0.1513
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1762
4672/6530 [====================>.........] - ETA: 0s - loss: 0.2644
 864/6530 [==>...........................] - ETA: 0s - loss: 0.1746{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  38 | activation: relu    | extras: None 
layer 2 | size:  94 | activation: tanh    | extras: batchnorm 
layer 3 | size:  49 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 55s - loss: 0.8687
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1698
2176/6530 [========>.....................] - ETA: 2s - loss: 0.5644 
5184/6530 [======================>.......] - ETA: 0s - loss: 0.2513
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1662
4224/6530 [==================>...........] - ETA: 0s - loss: 0.3841
5824/6530 [=========================>....] - ETA: 0s - loss: 0.2375
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1640
6400/6530 [============================>.] - ETA: 0s - loss: 0.3011
6528/6530 [============================>.] - ETA: 0s - loss: 0.2243
4096/6530 [=================>............] - ETA: 0s - loss: 0.1620
6530/6530 [==============================] - 1s 200us/step - loss: 0.2974 - val_loss: 0.1762
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1437
6530/6530 [==============================] - 1s 229us/step - loss: 0.2242 - val_loss: 0.1244
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1050
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1605
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1246
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1119
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1592
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1221
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1086
6530/6530 [==============================] - 0s 64us/step - loss: 0.1580 - val_loss: 0.1284

6530/6530 [==============================] - 0s 24us/step - loss: 0.1202 - val_loss: 0.1463
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1336Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1312
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1089
 800/6530 [==>...........................] - ETA: 0s - loss: 0.1581
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1127
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1090
1632/6530 [======>.......................] - ETA: 0s - loss: 0.1518
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1105
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1082
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1514
6530/6530 [==============================] - 0s 24us/step - loss: 0.1093 - val_loss: 0.1202
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1166
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1071
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1505
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1049
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1066
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1495
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1025
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1063
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1483
6530/6530 [==============================] - 0s 23us/step - loss: 0.1019 - val_loss: 0.1111
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1250
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1058
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1475
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0983
6530/6530 [==============================] - 1s 78us/step - loss: 0.1053 - val_loss: 0.0957
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0750
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0968
6530/6530 [==============================] - 0s 64us/step - loss: 0.1466 - val_loss: 0.1246
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1706
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0946
6530/6530 [==============================] - 0s 23us/step - loss: 0.0962 - val_loss: 0.1081
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1023
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1392
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0959
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0938
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1379
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0961
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0923
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1403
2848/6530 [============>.................] - ETA: 0s - loss: 0.0966
6530/6530 [==============================] - 0s 23us/step - loss: 0.0920 - val_loss: 0.0983
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0989
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1393
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0975
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0896
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1393
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0969
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0896
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1396
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0965
6530/6530 [==============================] - 0s 24us/step - loss: 0.0898 - val_loss: 0.1375
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1023
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1391
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0968
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0875
6530/6530 [==============================] - 0s 63us/step - loss: 0.1384 - val_loss: 0.1206
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1325
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0963
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0874
 864/6530 [==>...........................] - ETA: 0s - loss: 0.1315
6530/6530 [==============================] - 1s 79us/step - loss: 0.0963 - val_loss: 0.0902
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0752
6530/6530 [==============================] - 0s 23us/step - loss: 0.0875 - val_loss: 0.0975
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0952
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1335
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0909
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0864
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1349
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0897
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1350
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0854
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0900
6530/6530 [==============================] - 0s 24us/step - loss: 0.0856 - val_loss: 0.1307

4256/6530 [==================>...........] - ETA: 0s - loss: 0.1341
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0907
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1341
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0907
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1341
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0908
6530/6530 [==============================] - 0s 62us/step - loss: 0.1339 - val_loss: 0.1156
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1130
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0911
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1287
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0910
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1286
6400/6530 [============================>.] - ETA: 0s - loss: 0.0913
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1305
6530/6530 [==============================] - 1s 77us/step - loss: 0.0911 - val_loss: 0.0856
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0770
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1303
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0875
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1297
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0870
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1307
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0876
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1321
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0881
6530/6530 [==============================] - 0s 60us/step - loss: 0.1312 - val_loss: 0.1128
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1198
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0879
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1290
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0880
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1305
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0883
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1301
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0885
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1295
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0889
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1290
6530/6530 [==============================] - 0s 76us/step - loss: 0.0888 - val_loss: 0.0819
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0690
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1290
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0869
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1286
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0864
6530/6530 [==============================] - 0s 62us/step - loss: 0.1283 - val_loss: 0.1098
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1454
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0875
 864/6530 [==>...........................] - ETA: 0s - loss: 0.1208
2880/6530 [============>.................] - ETA: 0s - loss: 0.0878
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1230
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0877
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1266
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0872
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1251
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0872
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1236
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0876
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1242
# training | RMSE: 0.1481, MAE: 0.1222
worker 1  xfile  [1, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2007607438272935}, 'layer_4_size': 91, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.1481184715076548, 'rmse': 0.1481184715076548, 'mae': 0.12219525418981315, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  56 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  42 | activation: relu    | extras: dropout - rate: 24.2% 
layer 3 | size:  15 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  37 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9d72e8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 53s - loss: 2.7763
6336/6530 [============================>.] - ETA: 0s - loss: 0.0877
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1246
1088/6530 [===>..........................] - ETA: 2s - loss: 1.8247 
6530/6530 [==============================] - 1s 77us/step - loss: 0.0874 - val_loss: 0.0822
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0724
6530/6530 [==============================] - 0s 62us/step - loss: 0.1244 - val_loss: 0.1102
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1281
2048/6530 [========>.....................] - ETA: 1s - loss: 1.3762
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0839
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1257
3008/6530 [============>.................] - ETA: 0s - loss: 1.0777
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0820
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1242
4032/6530 [=================>............] - ETA: 0s - loss: 0.8506
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0837
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1252
5056/6530 [======================>.......] - ETA: 0s - loss: 0.6940
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0851
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1238
6080/6530 [==========================>...] - ETA: 0s - loss: 0.5855
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0846
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1225
4064/6530 [=================>............] - ETA: 0s - loss: 0.0855
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1231
6530/6530 [==============================] - 1s 143us/step - loss: 0.5488 - val_loss: 0.0378
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0499
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0854
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1221
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0437
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0856
6530/6530 [==============================] - 0s 63us/step - loss: 0.1219 - val_loss: 0.1089

2112/6530 [========>.....................] - ETA: 0s - loss: 0.0433
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0850
3072/6530 [=============>................] - ETA: 0s - loss: 0.0434
6530/6530 [==============================] - 1s 79us/step - loss: 0.0851 - val_loss: 0.0854
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0735
4032/6530 [=================>............] - ETA: 0s - loss: 0.0416
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0867
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0411
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0840
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0406
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0838
6530/6530 [==============================] - 0s 55us/step - loss: 0.0394 - val_loss: 0.0299
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0440
2848/6530 [============>.................] - ETA: 0s - loss: 0.0846
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0354
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0846
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0333
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0849
3072/6530 [=============>................] - ETA: 0s - loss: 0.0326
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0850
4096/6530 [=================>............] - ETA: 0s - loss: 0.0320
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0852
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0314
6496/6530 [============================>.] - ETA: 0s - loss: 0.0851
6530/6530 [==============================] - 0s 75us/step - loss: 0.0850 - val_loss: 0.0794
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0719
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0307
6530/6530 [==============================] - 0s 53us/step - loss: 0.0305 - val_loss: 0.0224
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0235
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0837
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0289
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0822
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0286
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0834
3072/6530 [=============>................] - ETA: 0s - loss: 0.0274
2912/6530 [============>.................] - ETA: 0s - loss: 0.0838
4032/6530 [=================>............] - ETA: 0s - loss: 0.0270
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0837
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0262
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0838
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0258
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0845
6530/6530 [==============================] - 0s 53us/step - loss: 0.0257 - val_loss: 0.0171
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0279
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0843
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0264
6432/6530 [============================>.] - ETA: 0s - loss: 0.0844
6530/6530 [==============================] - 0s 75us/step - loss: 0.0842 - val_loss: 0.0800

2048/6530 [========>.....................] - ETA: 0s - loss: 0.0245
3136/6530 [=============>................] - ETA: 0s - loss: 0.0242
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0240
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0239
6400/6530 [============================>.] - ETA: 0s - loss: 0.0235
6530/6530 [==============================] - 0s 51us/step - loss: 0.0235 - val_loss: 0.0159
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0243
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0242
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0238
3200/6530 [=============>................] - ETA: 0s - loss: 0.0228
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0224
# training | RMSE: 0.1329, MAE: 0.1015
worker 2  xfile  [2, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39962949672182224}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10015621671395541}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11581437543817096}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.13287444528735404, 'rmse': 0.13287444528735404, 'mae': 0.10151783828881777, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  84 | activation: tanh    | extras: batchnorm 
layer 2 | size:  21 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  37 | activation: relu    | extras: batchnorm 
layer 4 | size:  60 | activation: sigmoid | extras: dropout - rate: 21.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9d7320>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 30s - loss: 0.2615
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0220
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0946 
6464/6530 [============================>.] - ETA: 0s - loss: 0.0219
6530/6530 [==============================] - 0s 50us/step - loss: 0.0219 - val_loss: 0.0157
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0203
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0689
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0203
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0566
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0212
3200/6530 [=============>................] - ETA: 0s - loss: 0.0208
6530/6530 [==============================] - 1s 134us/step - loss: 0.0507 - val_loss: 0.0589
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0277
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0204
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0253
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0204
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0265
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0208
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0259
6530/6530 [==============================] - 0s 53us/step - loss: 0.0208 - val_loss: 0.0145
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0192
# training | RMSE: 0.0980, MAE: 0.0740
worker 0  xfile  [0, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16745485505931956}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.09803135808607771, 'rmse': 0.09803135808607771, 'mae': 0.07398292526369528, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  28 | activation: tanh    | extras: dropout - rate: 15.1% 
layer 2 | size:  34 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  60 | activation: tanh    | extras: None 
layer 4 | size:  28 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9d7240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 1:15 - loss: 2.6166
6530/6530 [==============================] - 0s 33us/step - loss: 0.0255 - val_loss: 0.0554
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0210
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0225
 672/6530 [==>...........................] - ETA: 3s - loss: 0.9382  
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0207
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0210
1312/6530 [=====>........................] - ETA: 1s - loss: 0.6051
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0221
2880/6530 [============>.................] - ETA: 0s - loss: 0.0206
1952/6530 [=======>......................] - ETA: 1s - loss: 0.4627
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0217
3840/6530 [================>.............] - ETA: 0s - loss: 0.0199
2560/6530 [==========>...................] - ETA: 0s - loss: 0.3855
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0201
6528/6530 [============================>.] - ETA: 0s - loss: 0.0213
6530/6530 [==============================] - 0s 34us/step - loss: 0.0213 - val_loss: 0.0550
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0169
3232/6530 [=============>................] - ETA: 0s - loss: 0.3304
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0199
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0173
3872/6530 [================>.............] - ETA: 0s - loss: 0.2945
6530/6530 [==============================] - 0s 55us/step - loss: 0.0198 - val_loss: 0.0166
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0170
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0185
4448/6530 [===================>..........] - ETA: 0s - loss: 0.2694
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0205
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0181
5088/6530 [======================>.......] - ETA: 0s - loss: 0.2465
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0197
6530/6530 [==============================] - 0s 34us/step - loss: 0.0178 - val_loss: 0.0419
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0150
5696/6530 [=========================>....] - ETA: 0s - loss: 0.2293
2944/6530 [============>.................] - ETA: 0s - loss: 0.0194
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0156
6304/6530 [===========================>..] - ETA: 0s - loss: 0.2163
3968/6530 [=================>............] - ETA: 0s - loss: 0.0194
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0162
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0192
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0158
6530/6530 [==============================] - 1s 151us/step - loss: 0.2115 - val_loss: 0.0620
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0821
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0190
6400/6530 [============================>.] - ETA: 0s - loss: 0.0156
 640/6530 [=>............................] - ETA: 0s - loss: 0.0841
6530/6530 [==============================] - 0s 36us/step - loss: 0.0155 - val_loss: 0.0254
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0135
6464/6530 [============================>.] - ETA: 0s - loss: 0.0192
6530/6530 [==============================] - 0s 59us/step - loss: 0.0192 - val_loss: 0.0158

1280/6530 [====>.........................] - ETA: 0s - loss: 0.0759
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0138
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0719
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0144
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0714
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0143
3168/6530 [=============>................] - ETA: 0s - loss: 0.0699
6528/6530 [============================>.] - ETA: 0s - loss: 0.0140
6530/6530 [==============================] - 0s 34us/step - loss: 0.0140 - val_loss: 0.0196
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0127
3840/6530 [================>.............] - ETA: 0s - loss: 0.0682
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0125
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0660
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0130
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0642
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0130
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0632
6400/6530 [============================>.] - ETA: 0s - loss: 0.0128
6530/6530 [==============================] - 0s 35us/step - loss: 0.0127 - val_loss: 0.0203

6336/6530 [============================>.] - ETA: 0s - loss: 0.0623Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0118
# training | RMSE: 0.1173, MAE: 0.0906
worker 1  xfile  [3, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2423132848388729}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.11730446249325358, 'rmse': 0.11730446249325358, 'mae': 0.090637013355121, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  37 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f03e0035a20>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 1:07 - loss: 0.5158
6530/6530 [==============================] - 1s 85us/step - loss: 0.0619 - val_loss: 0.0411
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0424
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0120
 688/6530 [==>...........................] - ETA: 1s - loss: 0.4618  
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0524
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0122
1360/6530 [=====>........................] - ETA: 1s - loss: 0.3448
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0507
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0121
1984/6530 [========>.....................] - ETA: 0s - loss: 0.2618
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0504
6530/6530 [==============================] - 0s 33us/step - loss: 0.0118 - val_loss: 0.0236
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0114
2624/6530 [===========>..................] - ETA: 0s - loss: 0.2086
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0501
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0111
3264/6530 [=============>................] - ETA: 0s - loss: 0.1748
3200/6530 [=============>................] - ETA: 0s - loss: 0.0486
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0113
3904/6530 [================>.............] - ETA: 0s - loss: 0.1530
3840/6530 [================>.............] - ETA: 0s - loss: 0.0488
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0112
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1366
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0482
6400/6530 [============================>.] - ETA: 0s - loss: 0.0111
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1248
6530/6530 [==============================] - 0s 35us/step - loss: 0.0110 - val_loss: 0.0224

5024/6530 [======================>.......] - ETA: 0s - loss: 0.0479
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1147
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0473
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0472
6530/6530 [==============================] - 1s 109us/step - loss: 0.1074 - val_loss: 0.0400
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0432
6530/6530 [==============================] - 1s 85us/step - loss: 0.0468 - val_loss: 0.0254
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0481
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0385
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0436
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0363
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0411
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0362
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0399
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0366
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0398
3264/6530 [=============>................] - ETA: 0s - loss: 0.0355
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0399
3904/6530 [================>.............] - ETA: 0s - loss: 0.0356
3968/6530 [=================>............] - ETA: 0s - loss: 0.0394
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0354
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0388
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0354
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0384
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0352
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0383
6528/6530 [============================>.] - ETA: 0s - loss: 0.0348
6530/6530 [==============================] - 1s 82us/step - loss: 0.0348 - val_loss: 0.0345
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0333
6530/6530 [==============================] - 1s 81us/step - loss: 0.0382 - val_loss: 0.0241
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0331
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0313
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0343
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0299
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0333
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0301
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0325
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0306
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0323
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0300
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0321
3984/6530 [=================>............] - ETA: 0s - loss: 0.0301
3904/6530 [================>.............] - ETA: 0s - loss: 0.0322
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0299
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0321
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0299
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0317
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0298
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0317
6530/6530 [==============================] - 1s 78us/step - loss: 0.0296 - val_loss: 0.0309
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0259
6464/6530 [============================>.] - ETA: 0s - loss: 0.0319
# training | RMSE: 0.1441, MAE: 0.1126
worker 2  xfile  [4, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21048884801807022}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.14407420275731414, 'rmse': 0.14407420275731414, 'mae': 0.1125966744464101, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  55 | activation: relu    | extras: None 
layer 2 | size:  84 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  95 | activation: sigmoid | extras: None 
layer 4 | size:  57 | activation: sigmoid | extras: dropout - rate: 46.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c0e6358>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 3:17 - loss: 1.0407
6530/6530 [==============================] - 1s 83us/step - loss: 0.0319 - val_loss: 0.0247
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0379
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0264
 272/6530 [>.............................] - ETA: 12s - loss: 0.4411 
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0330
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0262
 528/6530 [=>............................] - ETA: 6s - loss: 0.2713 
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0318
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0263
 768/6530 [==>...........................] - ETA: 4s - loss: 0.2089
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0311
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0271
1024/6530 [===>..........................] - ETA: 3s - loss: 0.1740
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0310
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0267
1280/6530 [====>.........................] - ETA: 3s - loss: 0.1535
3200/6530 [=============>................] - ETA: 0s - loss: 0.0301
4064/6530 [=================>............] - ETA: 0s - loss: 0.0266
1552/6530 [======>.......................] - ETA: 2s - loss: 0.1400
3872/6530 [================>.............] - ETA: 0s - loss: 0.0304
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0266
1808/6530 [=======>......................] - ETA: 2s - loss: 0.1299
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0302
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0267
2080/6530 [========>.....................] - ETA: 1s - loss: 0.1218
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0302
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0267
2336/6530 [=========>....................] - ETA: 1s - loss: 0.1156
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0298
6530/6530 [==============================] - 1s 80us/step - loss: 0.0266 - val_loss: 0.0286
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0227
2576/6530 [==========>...................] - ETA: 1s - loss: 0.1111
6400/6530 [============================>.] - ETA: 0s - loss: 0.0296
 576/6530 [=>............................] - ETA: 0s - loss: 0.0242
6530/6530 [==============================] - 1s 84us/step - loss: 0.0296 - val_loss: 0.0198
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0240
2816/6530 [===========>..................] - ETA: 1s - loss: 0.1072
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0236
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0301
3072/6530 [=============>................] - ETA: 1s - loss: 0.1034
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0240
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0280
3328/6530 [==============>...............] - ETA: 1s - loss: 0.0998
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0250
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0274
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0961
3248/6530 [=============>................] - ETA: 0s - loss: 0.0246
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0268
3856/6530 [================>.............] - ETA: 0s - loss: 0.0925
3888/6530 [================>.............] - ETA: 0s - loss: 0.0245
3200/6530 [=============>................] - ETA: 0s - loss: 0.0264
4128/6530 [=================>............] - ETA: 0s - loss: 0.0895
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0245
3840/6530 [================>.............] - ETA: 0s - loss: 0.0269
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0866
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0246
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0267
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0842
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0247
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0267
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0821
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0264
6530/6530 [==============================] - 1s 81us/step - loss: 0.0246 - val_loss: 0.0269
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0211
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0803
6368/6530 [============================>.] - ETA: 0s - loss: 0.0266
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0222
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0785
6530/6530 [==============================] - 1s 84us/step - loss: 0.0265 - val_loss: 0.0186
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0193
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0217
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0769
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0262
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0222
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0754
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0249
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0233
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0738
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0256
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0228
6432/6530 [============================>.] - ETA: 0s - loss: 0.0729
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0256
4048/6530 [=================>............] - ETA: 0s - loss: 0.0229
3232/6530 [=============>................] - ETA: 0s - loss: 0.0249
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0230
6530/6530 [==============================] - 2s 287us/step - loss: 0.0727 - val_loss: 0.0449
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0148
3840/6530 [================>.............] - ETA: 0s - loss: 0.0256
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0230
 272/6530 [>.............................] - ETA: 1s - loss: 0.0422
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0256
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0230
 544/6530 [=>............................] - ETA: 1s - loss: 0.0407
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0255
6530/6530 [==============================] - 1s 79us/step - loss: 0.0229 - val_loss: 0.0254
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0194
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0412
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0253
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0205
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0419
6432/6530 [============================>.] - ETA: 0s - loss: 0.0252
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0205
6530/6530 [==============================] - 1s 83us/step - loss: 0.0252 - val_loss: 0.0202

1328/6530 [=====>........................] - ETA: 1s - loss: 0.0436Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0185
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0208
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0438
 640/6530 [=>............................] - ETA: 0s - loss: 0.0240
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0219
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0438
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0233
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0216
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0447
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0232
3952/6530 [=================>............] - ETA: 0s - loss: 0.0215
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0447
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0236
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0215
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0448
3232/6530 [=============>................] - ETA: 0s - loss: 0.0235
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0216
2864/6530 [============>.................] - ETA: 0s - loss: 0.0441
3872/6530 [================>.............] - ETA: 0s - loss: 0.0234
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0216
3136/6530 [=============>................] - ETA: 0s - loss: 0.0435
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0234
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0430
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0234
6530/6530 [==============================] - 1s 81us/step - loss: 0.0215 - val_loss: 0.0241
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0170
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0430
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0231
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0194
6400/6530 [============================>.] - ETA: 0s - loss: 0.0233
3936/6530 [=================>............] - ETA: 0s - loss: 0.0425
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0192
6530/6530 [==============================] - 1s 83us/step - loss: 0.0233 - val_loss: 0.0138

4208/6530 [==================>...........] - ETA: 0s - loss: 0.0428
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0196
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0429
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0207
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0431
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0204
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0430
4048/6530 [=================>............] - ETA: 0s - loss: 0.0203
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0431
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0204
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0432
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0203
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0204
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0434
6530/6530 [==============================] - 1s 78us/step - loss: 0.0203 - val_loss: 0.0230
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0152
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0433
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0186
6384/6530 [============================>.] - ETA: 0s - loss: 0.0431
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0186
6530/6530 [==============================] - 1s 201us/step - loss: 0.0430 - val_loss: 0.0469
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0319
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0188
 288/6530 [>.............................] - ETA: 1s - loss: 0.0421
2864/6530 [============>.................] - ETA: 0s - loss: 0.0194
 560/6530 [=>............................] - ETA: 1s - loss: 0.0408
# training | RMSE: 0.1135, MAE: 0.0903
worker 0  xfile  [5, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15132983247231468}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13553206179667554}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.11351891816856072, 'rmse': 0.11351891816856072, 'mae': 0.0902882276772562, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  69 | activation: tanh    | extras: None 
layer 2 | size:  60 | activation: tanh    | extras: None 
layer 3 | size:  52 | activation: relu    | extras: None 
layer 4 | size:  60 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c34fcc0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 11s - loss: 0.3421
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0193
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0420
3712/6530 [================>.............] - ETA: 0s - loss: 0.0835 
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0192
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0430
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0193
6530/6530 [==============================] - 0s 54us/step - loss: 0.0631 - val_loss: 0.0330
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0352
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0416
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0192
3712/6530 [================>.............] - ETA: 0s - loss: 0.0291
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0409
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0193
6530/6530 [==============================] - 0s 16us/step - loss: 0.0283 - val_loss: 0.0274
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0284
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0409
6530/6530 [==============================] - 1s 78us/step - loss: 0.0193 - val_loss: 0.0220

3712/6530 [================>.............] - ETA: 0s - loss: 0.0243
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0404
6530/6530 [==============================] - 0s 15us/step - loss: 0.0239 - val_loss: 0.0239
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0240
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0408
3712/6530 [================>.............] - ETA: 0s - loss: 0.0213
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0410
# training | RMSE: 0.1389, MAE: 0.1089
worker 1  xfile  [6, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2820274393310773}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.138905328631744, 'rmse': 0.138905328631744, 'mae': 0.10885113245031137, 'early_stop': False}
vggnet done  1

6530/6530 [==============================] - 0s 15us/step - loss: 0.0212 - val_loss: 0.0214
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0211
2928/6530 [============>.................] - ETA: 0s - loss: 0.0413
3840/6530 [================>.............] - ETA: 0s - loss: 0.0193
3184/6530 [=============>................] - ETA: 0s - loss: 0.0407
6530/6530 [==============================] - 0s 14us/step - loss: 0.0192 - val_loss: 0.0196
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0187
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0401
3968/6530 [=================>............] - ETA: 0s - loss: 0.0177
3760/6530 [================>.............] - ETA: 0s - loss: 0.0397
6530/6530 [==============================] - 0s 14us/step - loss: 0.0177 - val_loss: 0.0181
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0169
4048/6530 [=================>............] - ETA: 0s - loss: 0.0399
3968/6530 [=================>............] - ETA: 0s - loss: 0.0164
6530/6530 [==============================] - 0s 14us/step - loss: 0.0164 - val_loss: 0.0168

4336/6530 [==================>...........] - ETA: 0s - loss: 0.0396Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0154
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0394
3712/6530 [================>.............] - ETA: 0s - loss: 0.0152
6530/6530 [==============================] - 0s 15us/step - loss: 0.0152 - val_loss: 0.0157
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0141
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0388
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0140
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0387
6530/6530 [==============================] - 0s 15us/step - loss: 0.0141 - val_loss: 0.0146

5392/6530 [=======================>......] - ETA: 0s - loss: 0.0385
# training | RMSE: 0.1153, MAE: 0.0893
worker 0  xfile  [8, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.11525713664326916, 'rmse': 0.11525713664326916, 'mae': 0.08930008129432626, 'early_stop': False}
vggnet done  0

5664/6530 [=========================>....] - ETA: 0s - loss: 0.0382
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0380
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0380
6528/6530 [============================>.] - ETA: 0s - loss: 0.0377
6530/6530 [==============================] - 1s 194us/step - loss: 0.0377 - val_loss: 0.0309
Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0140
 304/6530 [>.............................] - ETA: 1s - loss: 0.0303
 592/6530 [=>............................] - ETA: 1s - loss: 0.0321
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0313
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0311
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0306
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0306
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0312
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0320
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0320
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0316
2896/6530 [============>.................] - ETA: 0s - loss: 0.0323
3104/6530 [=============>................] - ETA: 0s - loss: 0.0319
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0321
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0321
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0319
3872/6530 [================>.............] - ETA: 0s - loss: 0.0317
4080/6530 [=================>............] - ETA: 0s - loss: 0.0314
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0313
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0309
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0307
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0309
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0310
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0308
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0308
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0309
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0305
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0304
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0306
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0303
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0303
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0303
6480/6530 [============================>.] - ETA: 0s - loss: 0.0300
6530/6530 [==============================] - 2s 270us/step - loss: 0.0301 - val_loss: 0.0331
Epoch 5/9

  16/6530 [..............................] - ETA: 2s - loss: 0.0222
 160/6530 [..............................] - ETA: 2s - loss: 0.0372
 320/6530 [>.............................] - ETA: 2s - loss: 0.0341
 480/6530 [=>............................] - ETA: 2s - loss: 0.0309
 640/6530 [=>............................] - ETA: 1s - loss: 0.0297
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0280
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0283
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0277
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0277
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0280
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0280
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0280
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0279
2288/6530 [=========>....................] - ETA: 1s - loss: 0.0279
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0278
2592/6530 [==========>...................] - ETA: 1s - loss: 0.0274
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0270
2880/6530 [============>.................] - ETA: 1s - loss: 0.0266
3040/6530 [============>.................] - ETA: 1s - loss: 0.0273
3200/6530 [=============>................] - ETA: 1s - loss: 0.0274
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0275
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0275
3712/6530 [================>.............] - ETA: 0s - loss: 0.0272
3920/6530 [=================>............] - ETA: 0s - loss: 0.0272
4112/6530 [=================>............] - ETA: 0s - loss: 0.0270
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0274
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0272
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0271
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0271
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0271
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0271
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0273
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0272
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0269
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0269
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0270
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0269
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0270
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0269
6384/6530 [============================>.] - ETA: 0s - loss: 0.0268
6530/6530 [==============================] - 2s 329us/step - loss: 0.0265 - val_loss: 0.0227
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0205
 208/6530 [..............................] - ETA: 1s - loss: 0.0223
 416/6530 [>.............................] - ETA: 1s - loss: 0.0198
 608/6530 [=>............................] - ETA: 1s - loss: 0.0219
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0213
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0209
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0222
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0224
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0227
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0230
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0240
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0241
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0234
2288/6530 [=========>....................] - ETA: 1s - loss: 0.0232
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0230
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0225
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0226
2976/6530 [============>.................] - ETA: 1s - loss: 0.0229
3168/6530 [=============>................] - ETA: 1s - loss: 0.0230
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0228
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0226
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0224
3808/6530 [================>.............] - ETA: 0s - loss: 0.0223
3952/6530 [=================>............] - ETA: 0s - loss: 0.0224
4096/6530 [=================>............] - ETA: 0s - loss: 0.0225
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0223
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0223
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0224
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0222
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0221
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0222
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0222
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0222
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0222
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0219
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0219
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0217
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0217
6352/6530 [============================>.] - ETA: 0s - loss: 0.0218
6512/6530 [============================>.] - ETA: 0s - loss: 0.0218
6530/6530 [==============================] - 2s 324us/step - loss: 0.0218 - val_loss: 0.0185
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0334
 208/6530 [..............................] - ETA: 1s - loss: 0.0212
 368/6530 [>.............................] - ETA: 1s - loss: 0.0227
 512/6530 [=>............................] - ETA: 1s - loss: 0.0208
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0201
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0197
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0191
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0193
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0195
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0195
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0197
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0196
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0194
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0190
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0190
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0192
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0195
2608/6530 [==========>...................] - ETA: 1s - loss: 0.0195
2816/6530 [===========>..................] - ETA: 1s - loss: 0.0198
3040/6530 [============>.................] - ETA: 1s - loss: 0.0195
3264/6530 [=============>................] - ETA: 1s - loss: 0.0192
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0188
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0190
3920/6530 [=================>............] - ETA: 0s - loss: 0.0190
4096/6530 [=================>............] - ETA: 0s - loss: 0.0189
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0189
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0188
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0189
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0189
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0189
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0190
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0191
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0191
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0191
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0191
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0192
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0193
6352/6530 [============================>.] - ETA: 0s - loss: 0.0193
6530/6530 [==============================] - 2s 313us/step - loss: 0.0195 - val_loss: 0.0342
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0152
 192/6530 [..............................] - ETA: 1s - loss: 0.0202
 384/6530 [>.............................] - ETA: 1s - loss: 0.0198
 560/6530 [=>............................] - ETA: 1s - loss: 0.0191
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0184
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0181
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0179
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0171
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0171
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0171
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0175
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0175
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0175
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0175
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0174
2560/6530 [==========>...................] - ETA: 1s - loss: 0.0172
2784/6530 [===========>..................] - ETA: 1s - loss: 0.0170
3008/6530 [============>.................] - ETA: 1s - loss: 0.0170
3248/6530 [=============>................] - ETA: 0s - loss: 0.0172
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0172
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0175
3856/6530 [================>.............] - ETA: 0s - loss: 0.0174
4016/6530 [=================>............] - ETA: 0s - loss: 0.0175
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0175
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0174
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0174
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0175
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0175
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0174
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0173
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0173
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0174
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0173
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0173
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0174
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0174
6400/6530 [============================>.] - ETA: 0s - loss: 0.0174
6530/6530 [==============================] - 2s 302us/step - loss: 0.0173 - val_loss: 0.0163
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0308
 208/6530 [..............................] - ETA: 1s - loss: 0.0191
 400/6530 [>.............................] - ETA: 1s - loss: 0.0176
 608/6530 [=>............................] - ETA: 1s - loss: 0.0192
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0185
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0178
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0174
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0176
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0182
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0182
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0182
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0177
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0177
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0177
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0176
2992/6530 [============>.................] - ETA: 0s - loss: 0.0175
3184/6530 [=============>................] - ETA: 0s - loss: 0.0179
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0179
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0179
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0177
3792/6530 [================>.............] - ETA: 0s - loss: 0.0174
3952/6530 [=================>............] - ETA: 0s - loss: 0.0174
4112/6530 [=================>............] - ETA: 0s - loss: 0.0174
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0174
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0173
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0174
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0176
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0177
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0178
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0179
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0178
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0176
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0178
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0177
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0178
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0179
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0178
6432/6530 [============================>.] - ETA: 0s - loss: 0.0178
6530/6530 [==============================] - 2s 316us/step - loss: 0.0179 - val_loss: 0.0362

# training | RMSE: 0.1823, MAE: 0.1572
worker 2  xfile  [7, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4631191913250753}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.18227462658487076, 'rmse': 0.18227462658487076, 'mae': 0.15718542045290085, 'early_stop': False}
vggnet done  2
all of workers have been done
calculation finished
#1 epoch=9.0 loss={'loss': 0.1481184715076548, 'rmse': 0.1481184715076548, 'mae': 0.12219525418981315, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2007607438272935}, 'layer_4_size': 91, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#2 epoch=9.0 loss={'loss': 0.13287444528735404, 'rmse': 0.13287444528735404, 'mae': 0.10151783828881777, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39962949672182224}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10015621671395541}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11581437543817096}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#0 epoch=9.0 loss={'loss': 0.09803135808607771, 'rmse': 0.09803135808607771, 'mae': 0.07398292526369528, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16745485505931956}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#3 epoch=9.0 loss={'loss': 0.11730446249325358, 'rmse': 0.11730446249325358, 'mae': 0.090637013355121, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2423132848388729}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#4 epoch=9.0 loss={'loss': 0.14407420275731414, 'rmse': 0.14407420275731414, 'mae': 0.1125966744464101, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21048884801807022}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#5 epoch=9.0 loss={'loss': 0.11351891816856072, 'rmse': 0.11351891816856072, 'mae': 0.0902882276772562, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15132983247231468}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13553206179667554}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#6 epoch=9.0 loss={'loss': 0.138905328631744, 'rmse': 0.138905328631744, 'mae': 0.10885113245031137, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2820274393310773}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#8 epoch=9.0 loss={'loss': 0.11525713664326916, 'rmse': 0.11525713664326916, 'mae': 0.08930008129432626, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#7 epoch=9.0 loss={'loss': 0.18227462658487076, 'rmse': 0.18227462658487076, 'mae': 0.15718542045290085, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 55, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4631191913250753}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
get a list [results] of length 117
get a list [loss] of length 9
get a list [val_loss] of length 9
length of indices is (0, 5, 8, 3, 2, 6, 4, 1, 7)
length of indices is 9
length of T is 9
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16745485505931956}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [1, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15132983247231468}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13553206179667554}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [2, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]] 

*** 3.0 configurations x 27.0 iterations each

8 | Fri Sep 28 01:20:38 2018 | lowest loss so far: 0.0980 (run 0)

{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  69 | activation: tanh    | extras: None 
layer 2 | size:  60 | activation: tanh    | extras: None 
layer 3 | size:  52 | activation: relu    | extras: None 
layer 4 | size:  60 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 128/6530 [..............................] - ETA: 41s - loss: 0.2704
3200/6530 [=============>................] - ETA: 0s - loss: 0.0807 
6530/6530 [==============================] - 1s 145us/step - loss: 0.0576 - val_loss: 0.0306
Epoch 2/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0255
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0281
6400/6530 [============================>.] - ETA: 0s - loss: 0.0270
6530/6530 [==============================] - 0s 17us/step - loss: 0.0269 - val_loss: 0.0257
Epoch 3/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0213{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  43 | activation: tanh    | extras: None 
layer 2 | size:  36 | activation: relu    | extras: batchnorm 
layer 3 | size:  38 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 3:04 - loss: 0.6958
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0233
 704/6530 [==>...........................] - ETA: 7s - loss: 0.6647  
1280/6530 [====>.........................] - ETA: 4s - loss: 0.5568
6530/6530 [==============================] - 0s 17us/step - loss: 0.0225 - val_loss: 0.0223
Epoch 4/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0179
1920/6530 [=======>......................] - ETA: 2s - loss: 0.4350
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0198
6530/6530 [==============================] - 0s 16us/step - loss: 0.0193 - val_loss: 0.0197

2592/6530 [==========>...................] - ETA: 1s - loss: 0.3644Epoch 5/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0155{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  28 | activation: tanh    | extras: dropout - rate: 15.1% 
layer 2 | size:  34 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  60 | activation: tanh    | extras: None 
layer 4 | size:  28 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 3:24 - loss: 1.9465
3232/6530 [=============>................] - ETA: 1s - loss: 0.3200
3712/6530 [================>.............] - ETA: 0s - loss: 0.0173
 640/6530 [=>............................] - ETA: 9s - loss: 0.6732  
6530/6530 [==============================] - 0s 15us/step - loss: 0.0169 - val_loss: 0.0178
Epoch 6/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0138
3904/6530 [================>.............] - ETA: 0s - loss: 0.2901
1280/6530 [====>.........................] - ETA: 4s - loss: 0.4728
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0154
4608/6530 [====================>.........] - ETA: 0s - loss: 0.2666
1888/6530 [=======>......................] - ETA: 2s - loss: 0.3957
6530/6530 [==============================] - 0s 16us/step - loss: 0.0152 - val_loss: 0.0165
Epoch 7/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0125
2496/6530 [==========>...................] - ETA: 1s - loss: 0.3385
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0141
5184/6530 [======================>.......] - ETA: 0s - loss: 0.2516
3104/6530 [=============>................] - ETA: 1s - loss: 0.2995
6530/6530 [==============================] - 0s 16us/step - loss: 0.0139 - val_loss: 0.0155
Epoch 8/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0116
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2362
3744/6530 [================>.............] - ETA: 0s - loss: 0.2695
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0132
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2476
6530/6530 [==============================] - 0s 16us/step - loss: 0.0130 - val_loss: 0.0148
Epoch 9/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0109
6530/6530 [==============================] - 2s 234us/step - loss: 0.2240 - val_loss: 0.1278
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1008
4960/6530 [=====================>........] - ETA: 0s - loss: 0.2295
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0124
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1118
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2151
6530/6530 [==============================] - 0s 16us/step - loss: 0.0123 - val_loss: 0.0142
Epoch 10/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0102
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1081
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2021
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0119
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1082
6530/6530 [==============================] - 0s 16us/step - loss: 0.0117 - val_loss: 0.0137
Epoch 11/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0097
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1082
6530/6530 [==============================] - 2s 245us/step - loss: 0.1960 - val_loss: 0.0667
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1130
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0113
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1082
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0831
6530/6530 [==============================] - 0s 15us/step - loss: 0.0112 - val_loss: 0.0133
Epoch 12/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0093
4128/6530 [=================>............] - ETA: 0s - loss: 0.1073
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0822
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0108
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1065
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0789
6530/6530 [==============================] - 0s 15us/step - loss: 0.0108 - val_loss: 0.0129
Epoch 13/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0089
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1063
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0778
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0105
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1053
3200/6530 [=============>................] - ETA: 0s - loss: 0.0748
6530/6530 [==============================] - 0s 16us/step - loss: 0.0104 - val_loss: 0.0126
Epoch 14/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0086
6530/6530 [==============================] - 1s 78us/step - loss: 0.1047 - val_loss: 0.0950
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0742
3840/6530 [================>.............] - ETA: 0s - loss: 0.0735
3840/6530 [================>.............] - ETA: 0s - loss: 0.0101
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0945
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0718
6530/6530 [==============================] - 0s 15us/step - loss: 0.0101 - val_loss: 0.0123
Epoch 15/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0083
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0950
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0705
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0098
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0948
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0695
6530/6530 [==============================] - 0s 15us/step - loss: 0.0098 - val_loss: 0.0121
Epoch 16/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0081
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0947
6432/6530 [============================>.] - ETA: 0s - loss: 0.0685
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0095
6530/6530 [==============================] - 1s 84us/step - loss: 0.0683 - val_loss: 0.0529
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0742
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0951
6530/6530 [==============================] - 0s 15us/step - loss: 0.0095 - val_loss: 0.0118
Epoch 17/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0079
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0539
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0946
3840/6530 [================>.............] - ETA: 0s - loss: 0.0092
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0531
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0942
6530/6530 [==============================] - 0s 15us/step - loss: 0.0092 - val_loss: 0.0116
Epoch 18/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0077
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0539
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0942
3712/6530 [================>.............] - ETA: 0s - loss: 0.0090
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0529
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0943
6530/6530 [==============================] - 0s 15us/step - loss: 0.0090 - val_loss: 0.0115
Epoch 19/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0076
3200/6530 [=============>................] - ETA: 0s - loss: 0.0524
6530/6530 [==============================] - 1s 77us/step - loss: 0.0939 - val_loss: 0.0895
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0608
3712/6530 [================>.............] - ETA: 0s - loss: 0.0087
3872/6530 [================>.............] - ETA: 0s - loss: 0.0528
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0899
6530/6530 [==============================] - 0s 15us/step - loss: 0.0087 - val_loss: 0.0113
Epoch 20/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0074
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0523
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0892
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0086
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0521
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0895
6530/6530 [==============================] - 0s 16us/step - loss: 0.0085 - val_loss: 0.0111
Epoch 21/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0073
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0515
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0906
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0084
6400/6530 [============================>.] - ETA: 0s - loss: 0.0512
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0907
6530/6530 [==============================] - 0s 16us/step - loss: 0.0083 - val_loss: 0.0110
Epoch 22/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0072
6530/6530 [==============================] - 1s 84us/step - loss: 0.0510 - val_loss: 0.0390
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0807
4064/6530 [=================>............] - ETA: 0s - loss: 0.0907
2944/6530 [============>.................] - ETA: 0s - loss: 0.0083
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0459
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0904
6528/6530 [============================>.] - ETA: 0s - loss: 0.0081
6530/6530 [==============================] - 0s 16us/step - loss: 0.0081 - val_loss: 0.0108
Epoch 23/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0070
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0444
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0915
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0080
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0442
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0911
6530/6530 [==============================] - 0s 15us/step - loss: 0.0080 - val_loss: 0.0107
Epoch 24/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0069
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0439
6530/6530 [==============================] - 1s 78us/step - loss: 0.0910 - val_loss: 0.0866
Epoch 5/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0652
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0078
3168/6530 [=============>................] - ETA: 0s - loss: 0.0433
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0903
6530/6530 [==============================] - 0s 16us/step - loss: 0.0078 - val_loss: 0.0106
Epoch 25/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0067
3840/6530 [================>.............] - ETA: 0s - loss: 0.0431
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0879
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0077
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0436
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0878
6530/6530 [==============================] - 0s 16us/step - loss: 0.0077 - val_loss: 0.0104
Epoch 26/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0066
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0433
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0891
3712/6530 [================>.............] - ETA: 0s - loss: 0.0075
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0430
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0883
6530/6530 [==============================] - 0s 15us/step - loss: 0.0075 - val_loss: 0.0103
Epoch 27/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0064
6368/6530 [============================>.] - ETA: 0s - loss: 0.0429
4064/6530 [=================>............] - ETA: 0s - loss: 0.0886
6530/6530 [==============================] - 1s 84us/step - loss: 0.0429 - val_loss: 0.0311
Epoch 5/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0502
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0074
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0886
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0403
6530/6530 [==============================] - 0s 16us/step - loss: 0.0074 - val_loss: 0.0102

5472/6530 [========================>.....] - ETA: 0s - loss: 0.0893
# training | RMSE: 0.0838, MAE: 0.0650
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.0838447089275304, 'rmse': 0.0838447089275304, 'mae': 0.06497174230422362, 'early_stop': False}
vggnet done  2

1280/6530 [====>.........................] - ETA: 0s - loss: 0.0404
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0888
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0388
6530/6530 [==============================] - 1s 78us/step - loss: 0.0890 - val_loss: 0.0791
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0713
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0388
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0916
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0380
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0901
4000/6530 [=================>............] - ETA: 0s - loss: 0.0381
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0892
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0378
2976/6530 [============>.................] - ETA: 0s - loss: 0.0883
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0372
3712/6530 [================>.............] - ETA: 0s - loss: 0.0880
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0368
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0876
6530/6530 [==============================] - 1s 80us/step - loss: 0.0365 - val_loss: 0.0295
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0370
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0883
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0377
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0883
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0338
6530/6530 [==============================] - 0s 72us/step - loss: 0.0880 - val_loss: 0.0804
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0687
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0331
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0884
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0330
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0862
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0329
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0866
4128/6530 [=================>............] - ETA: 0s - loss: 0.0326
3008/6530 [============>.................] - ETA: 0s - loss: 0.0860
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0331
3776/6530 [================>.............] - ETA: 0s - loss: 0.0852
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0328
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0860
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0326
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0865
6530/6530 [==============================] - 1s 78us/step - loss: 0.0327 - val_loss: 0.0265
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0378
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0861
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0327
6530/6530 [==============================] - 0s 73us/step - loss: 0.0863 - val_loss: 0.0831
Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0590
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0304
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0847
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0300
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0841
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0301
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0846
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0304
2944/6530 [============>.................] - ETA: 0s - loss: 0.0853
4032/6530 [=================>............] - ETA: 0s - loss: 0.0303
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0849
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0304
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0846
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0299
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0852
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0297
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0851
6530/6530 [==============================] - 1s 79us/step - loss: 0.0294 - val_loss: 0.0251
Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0587
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0327
6530/6530 [==============================] - 0s 73us/step - loss: 0.0848 - val_loss: 0.0807
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0661
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0305
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0846
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0292
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0841
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0294
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0846
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0290
2976/6530 [============>.................] - ETA: 0s - loss: 0.0842
4096/6530 [=================>............] - ETA: 0s - loss: 0.0287
3712/6530 [================>.............] - ETA: 0s - loss: 0.0839
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0285
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0837
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0282
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0840
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0282
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0842
6432/6530 [============================>.] - ETA: 0s - loss: 0.0841
6530/6530 [==============================] - 1s 80us/step - loss: 0.0282 - val_loss: 0.0291
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0387
6530/6530 [==============================] - 0s 74us/step - loss: 0.0839 - val_loss: 0.0816
Epoch 10/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0652
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0283
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0836
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0275
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0829
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0263
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0829
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0264
2944/6530 [============>.................] - ETA: 0s - loss: 0.0831
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0266
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0827
4096/6530 [=================>............] - ETA: 0s - loss: 0.0262
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0827
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0261
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0833
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0259
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0830
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0261
6528/6530 [============================>.] - ETA: 0s - loss: 0.0828
6530/6530 [==============================] - 0s 73us/step - loss: 0.0828 - val_loss: 0.0859

6530/6530 [==============================] - 1s 79us/step - loss: 0.0260 - val_loss: 0.0199
Epoch 10/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0474
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0285
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0262
# training | RMSE: 0.1009, MAE: 0.0776
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16745485505931956}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.10086304724776736, 'rmse': 0.10086304724776736, 'mae': 0.07756997667347139, 'early_stop': True}
vggnet done  0

2080/6530 [========>.....................] - ETA: 0s - loss: 0.0257
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0255
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0253
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0250
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0247
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0246
6368/6530 [============================>.] - ETA: 0s - loss: 0.0247
6530/6530 [==============================] - 0s 75us/step - loss: 0.0247 - val_loss: 0.0161
Epoch 11/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0258
 544/6530 [=>............................] - ETA: 0s - loss: 0.0260
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0244
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0239
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0236
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0236
4032/6530 [=================>............] - ETA: 0s - loss: 0.0235
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0237
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0235
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0237
6528/6530 [============================>.] - ETA: 0s - loss: 0.0238
6530/6530 [==============================] - 1s 82us/step - loss: 0.0238 - val_loss: 0.0203
Epoch 12/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0286
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0244
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0232
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0225
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0227
3200/6530 [=============>................] - ETA: 0s - loss: 0.0225
3712/6530 [================>.............] - ETA: 0s - loss: 0.0225
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0227
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0227
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0224
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0226
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0226
6530/6530 [==============================] - 1s 95us/step - loss: 0.0226 - val_loss: 0.0147
Epoch 13/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0311
 544/6530 [=>............................] - ETA: 0s - loss: 0.0229
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0211
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0207
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0208
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0210
3040/6530 [============>.................] - ETA: 0s - loss: 0.0211
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0212
3968/6530 [=================>............] - ETA: 0s - loss: 0.0213
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0213
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0213
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0214
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0216
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0216
6530/6530 [==============================] - 1s 115us/step - loss: 0.0217 - val_loss: 0.0145
Epoch 14/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0283
 384/6530 [>.............................] - ETA: 0s - loss: 0.0227
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0231
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0225
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0218
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0214
2976/6530 [============>.................] - ETA: 0s - loss: 0.0213
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0213
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0211
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0211
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0209
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0208
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0210
6530/6530 [==============================] - 1s 105us/step - loss: 0.0211 - val_loss: 0.0212
Epoch 15/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0267
 480/6530 [=>............................] - ETA: 0s - loss: 0.0227
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0208
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0207
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0202
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0203
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0203
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0203
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0204
3136/6530 [=============>................] - ETA: 0s - loss: 0.0200
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0201
3904/6530 [================>.............] - ETA: 0s - loss: 0.0201
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0199
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0199
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0201
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0201
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0202
6400/6530 [============================>.] - ETA: 0s - loss: 0.0204
6530/6530 [==============================] - 1s 143us/step - loss: 0.0204 - val_loss: 0.0193
Epoch 16/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0211
 480/6530 [=>............................] - ETA: 0s - loss: 0.0231
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0212
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0212
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0203
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0205
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0205
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0203
3104/6530 [=============>................] - ETA: 0s - loss: 0.0199
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0200
3904/6530 [================>.............] - ETA: 0s - loss: 0.0200
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0197
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0197
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0198
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0201
6530/6530 [==============================] - 1s 119us/step - loss: 0.0203 - val_loss: 0.0153
Epoch 17/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0291
 576/6530 [=>............................] - ETA: 0s - loss: 0.0199
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0193
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0184
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0186
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0186
2880/6530 [============>.................] - ETA: 0s - loss: 0.0185
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0186
3904/6530 [================>.............] - ETA: 0s - loss: 0.0185
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0186
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0187
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0186
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0185
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0187
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0187
6530/6530 [==============================] - 1s 121us/step - loss: 0.0188 - val_loss: 0.0189
Epoch 18/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0297
 480/6530 [=>............................] - ETA: 0s - loss: 0.0211
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0195
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0191
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0189
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0193
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0192
3200/6530 [=============>................] - ETA: 0s - loss: 0.0189
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0191
3872/6530 [================>.............] - ETA: 0s - loss: 0.0190
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0190
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0191
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0190
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0189
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0188
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0189
6400/6530 [============================>.] - ETA: 0s - loss: 0.0189
6530/6530 [==============================] - 1s 137us/step - loss: 0.0190 - val_loss: 0.0121
Epoch 19/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0210
 384/6530 [>.............................] - ETA: 0s - loss: 0.0188
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0181
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0185
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0182
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0185
3072/6530 [=============>................] - ETA: 0s - loss: 0.0185
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0184
3968/6530 [=================>............] - ETA: 0s - loss: 0.0185
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0184
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0185
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0184
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0183
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0184
6336/6530 [============================>.] - ETA: 0s - loss: 0.0186
6530/6530 [==============================] - 1s 123us/step - loss: 0.0187 - val_loss: 0.0117
Epoch 20/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0235
 352/6530 [>.............................] - ETA: 1s - loss: 0.0186
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0180
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0175
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0179
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0176
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0173
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0178
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0179
3104/6530 [=============>................] - ETA: 0s - loss: 0.0178
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0180
3744/6530 [================>.............] - ETA: 0s - loss: 0.0180
4128/6530 [=================>............] - ETA: 0s - loss: 0.0181
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0181
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0181
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0180
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0179
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0180
6432/6530 [============================>.] - ETA: 0s - loss: 0.0181
6530/6530 [==============================] - 1s 153us/step - loss: 0.0181 - val_loss: 0.0140
Epoch 21/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0322
 384/6530 [>.............................] - ETA: 0s - loss: 0.0197
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0189
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0180
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0174
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0178
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0180
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0181
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0182
3872/6530 [================>.............] - ETA: 0s - loss: 0.0182
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0179
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0178
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0178
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0178
6432/6530 [============================>.] - ETA: 0s - loss: 0.0180
6530/6530 [==============================] - 1s 118us/step - loss: 0.0180 - val_loss: 0.0111
Epoch 22/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0160
 512/6530 [=>............................] - ETA: 0s - loss: 0.0175
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0163
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0168
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0167
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0168
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0169
2944/6530 [============>.................] - ETA: 0s - loss: 0.0170
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0170
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0170
4000/6530 [=================>............] - ETA: 0s - loss: 0.0170
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0170
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0170
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0170
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0170
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0170
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0172
6530/6530 [==============================] - 1s 138us/step - loss: 0.0173 - val_loss: 0.0194
Epoch 23/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0319
 384/6530 [>.............................] - ETA: 0s - loss: 0.0201
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0191
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0181
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0174
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0173
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0175
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0174
3168/6530 [=============>................] - ETA: 0s - loss: 0.0170
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0172
3904/6530 [================>.............] - ETA: 0s - loss: 0.0173
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0174
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0173
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0172
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0172
6400/6530 [============================>.] - ETA: 0s - loss: 0.0173
6530/6530 [==============================] - 1s 126us/step - loss: 0.0174 - val_loss: 0.0114
Epoch 24/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0200
 608/6530 [=>............................] - ETA: 0s - loss: 0.0166
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0158
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0162
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0165
2912/6530 [============>.................] - ETA: 0s - loss: 0.0163
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0167
4064/6530 [=================>............] - ETA: 0s - loss: 0.0166
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0166
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0165
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0164
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0165
6336/6530 [============================>.] - ETA: 0s - loss: 0.0168
6530/6530 [==============================] - 1s 104us/step - loss: 0.0169 - val_loss: 0.0121
Epoch 25/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0211
 384/6530 [>.............................] - ETA: 0s - loss: 0.0176
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0167
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0164
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0162
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0159
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0157
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0164
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0163
3168/6530 [=============>................] - ETA: 0s - loss: 0.0159
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0161
4032/6530 [=================>............] - ETA: 0s - loss: 0.0161
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0160
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0161
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0160
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0162
6432/6530 [============================>.] - ETA: 0s - loss: 0.0164
6530/6530 [==============================] - 1s 134us/step - loss: 0.0164 - val_loss: 0.0182
Epoch 26/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0216
 544/6530 [=>............................] - ETA: 0s - loss: 0.0167
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0164
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0163
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0160
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0162
3168/6530 [=============>................] - ETA: 0s - loss: 0.0161
3712/6530 [================>.............] - ETA: 0s - loss: 0.0163
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0162
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0161
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0160
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0160
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0161
6496/6530 [============================>.] - ETA: 0s - loss: 0.0163
6530/6530 [==============================] - 1s 111us/step - loss: 0.0163 - val_loss: 0.0109
Epoch 27/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0180
 384/6530 [>.............................] - ETA: 0s - loss: 0.0154
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0160
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0158
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0159
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0160
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0160
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0162
2976/6530 [============>.................] - ETA: 0s - loss: 0.0160
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0158
3776/6530 [================>.............] - ETA: 0s - loss: 0.0158
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0161
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0161
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0160
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0160
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0160
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0160
6530/6530 [==============================] - 1s 142us/step - loss: 0.0163 - val_loss: 0.0175

# training | RMSE: 0.1267, MAE: 0.1021
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15132983247231468}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13553206179667554}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.1267404260539156, 'rmse': 0.1267404260539156, 'mae': 0.10211686568897753, 'early_stop': False}
vggnet done  1
all of workers have been done
calculation finished
#2 epoch=27.0 loss={'loss': 0.0838447089275304, 'rmse': 0.0838447089275304, 'mae': 0.06497174230422362, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#0 epoch=27.0 loss={'loss': 0.10086304724776736, 'rmse': 0.10086304724776736, 'mae': 0.07756997667347139, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16745485505931956}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#1 epoch=27.0 loss={'loss': 0.1267404260539156, 'rmse': 0.1267404260539156, 'mae': 0.10211686568897753, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15132983247231468}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13553206179667554}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
get a list [results] of length 120
get a list [loss] of length 3
get a list [val_loss] of length 3
length of indices is (2, 0, 1)
length of indices is 3
length of T is 3
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]] 

*** 1.0 configurations x 81.0 iterations each

2 | Fri Sep 28 01:20:59 2018 | lowest loss so far: 0.0838 (run 2)

vggnet done  1
vggnet done  2
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  69 | activation: tanh    | extras: None 
layer 2 | size:  60 | activation: tanh    | extras: None 
layer 3 | size:  52 | activation: relu    | extras: None 
layer 4 | size:  60 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

 128/6530 [..............................] - ETA: 35s - loss: 0.2704
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0914 
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0664
6530/6530 [==============================] - 1s 137us/step - loss: 0.0576 - val_loss: 0.0306
Epoch 2/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0255
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0287
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0276
6530/6530 [==============================] - 0s 23us/step - loss: 0.0269 - val_loss: 0.0257
Epoch 3/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0213
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0236
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0228
6530/6530 [==============================] - 0s 22us/step - loss: 0.0225 - val_loss: 0.0223
Epoch 4/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0179
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0203
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0195
6530/6530 [==============================] - 0s 22us/step - loss: 0.0193 - val_loss: 0.0197
Epoch 5/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0156
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0176
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0170
6530/6530 [==============================] - 0s 23us/step - loss: 0.0169 - val_loss: 0.0178
Epoch 6/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0138
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0156
4096/6530 [=================>............] - ETA: 0s - loss: 0.0153
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0152
6530/6530 [==============================] - 0s 29us/step - loss: 0.0152 - val_loss: 0.0165
Epoch 7/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0125
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0142
3712/6530 [================>.............] - ETA: 0s - loss: 0.0141
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0140
6530/6530 [==============================] - 0s 31us/step - loss: 0.0139 - val_loss: 0.0155
Epoch 8/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0116
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0134
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0130
6530/6530 [==============================] - 0s 24us/step - loss: 0.0130 - val_loss: 0.0148
Epoch 9/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0109
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0127
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0123
6530/6530 [==============================] - 0s 23us/step - loss: 0.0123 - val_loss: 0.0142
Epoch 10/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0102
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0121
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0116
6530/6530 [==============================] - 0s 24us/step - loss: 0.0117 - val_loss: 0.0137
Epoch 11/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0097
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0116
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0112
6530/6530 [==============================] - 0s 22us/step - loss: 0.0112 - val_loss: 0.0133
Epoch 12/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0093
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0111
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0107
6530/6530 [==============================] - 0s 25us/step - loss: 0.0108 - val_loss: 0.0129
Epoch 13/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0089
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0107
3968/6530 [=================>............] - ETA: 0s - loss: 0.0105
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0104
6530/6530 [==============================] - 0s 27us/step - loss: 0.0104 - val_loss: 0.0126
Epoch 14/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0086
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0105
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0101
6530/6530 [==============================] - 0s 22us/step - loss: 0.0101 - val_loss: 0.0123
Epoch 15/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0084
3072/6530 [=============>................] - ETA: 0s - loss: 0.0099
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0098
6530/6530 [==============================] - 0s 19us/step - loss: 0.0098 - val_loss: 0.0121
Epoch 16/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0081
3200/6530 [=============>................] - ETA: 0s - loss: 0.0095
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0095
6530/6530 [==============================] - 0s 22us/step - loss: 0.0095 - val_loss: 0.0118
Epoch 17/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0080
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0095
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0092
6530/6530 [==============================] - 0s 23us/step - loss: 0.0092 - val_loss: 0.0116
Epoch 18/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0078
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0093
4096/6530 [=================>............] - ETA: 0s - loss: 0.0090
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0090
6530/6530 [==============================] - 0s 27us/step - loss: 0.0090 - val_loss: 0.0115
Epoch 19/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0076
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0090
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0087
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0088
6530/6530 [==============================] - 0s 27us/step - loss: 0.0087 - val_loss: 0.0113
Epoch 20/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0075
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0088
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0085
6530/6530 [==============================] - 0s 23us/step - loss: 0.0085 - val_loss: 0.0111
Epoch 21/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0073
3072/6530 [=============>................] - ETA: 0s - loss: 0.0084
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 0s 18us/step - loss: 0.0083 - val_loss: 0.0110
Epoch 22/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0072
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0081
6530/6530 [==============================] - 0s 17us/step - loss: 0.0081 - val_loss: 0.0108
Epoch 23/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0070
2944/6530 [============>.................] - ETA: 0s - loss: 0.0082
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0079
6530/6530 [==============================] - 0s 22us/step - loss: 0.0080 - val_loss: 0.0107
Epoch 24/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0069
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0081
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0078
6528/6530 [============================>.] - ETA: 0s - loss: 0.0078
6530/6530 [==============================] - 0s 25us/step - loss: 0.0078 - val_loss: 0.0105
Epoch 25/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0067
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0080
3968/6530 [=================>............] - ETA: 0s - loss: 0.0077
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0076
6530/6530 [==============================] - 0s 27us/step - loss: 0.0077 - val_loss: 0.0104
Epoch 26/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0066
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0078
3968/6530 [=================>............] - ETA: 0s - loss: 0.0076
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0075
6530/6530 [==============================] - 0s 27us/step - loss: 0.0075 - val_loss: 0.0103
Epoch 27/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0065
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0077
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0074
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0074
6530/6530 [==============================] - 0s 27us/step - loss: 0.0074 - val_loss: 0.0102
Epoch 28/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0063
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0075
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0072
6530/6530 [==============================] - 0s 25us/step - loss: 0.0072 - val_loss: 0.0101
Epoch 29/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0062
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0074
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0071
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0071
6530/6530 [==============================] - 0s 31us/step - loss: 0.0071 - val_loss: 0.0100
Epoch 30/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0061
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0073
4096/6530 [=================>............] - ETA: 0s - loss: 0.0071
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0070
6530/6530 [==============================] - 0s 26us/step - loss: 0.0070 - val_loss: 0.0099
Epoch 31/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0059
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0072
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0070
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0068
6530/6530 [==============================] - 0s 31us/step - loss: 0.0069 - val_loss: 0.0098
Epoch 32/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0058
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0071
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0069
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0067
6530/6530 [==============================] - 0s 30us/step - loss: 0.0068 - val_loss: 0.0097
Epoch 33/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0057
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0070
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0066
6530/6530 [==============================] - 0s 22us/step - loss: 0.0066 - val_loss: 0.0096
Epoch 34/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0056
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0068
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0065
6530/6530 [==============================] - 0s 20us/step - loss: 0.0065 - val_loss: 0.0095
Epoch 35/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0055
2944/6530 [============>.................] - ETA: 0s - loss: 0.0067
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0064
6530/6530 [==============================] - 0s 19us/step - loss: 0.0064 - val_loss: 0.0094
Epoch 36/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0054
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0066
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0063
6530/6530 [==============================] - 0s 21us/step - loss: 0.0063 - val_loss: 0.0094
Epoch 37/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0053
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0065
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0062
6530/6530 [==============================] - 0s 20us/step - loss: 0.0062 - val_loss: 0.0094
Epoch 38/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0052
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0064
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0061
6530/6530 [==============================] - 0s 20us/step - loss: 0.0061 - val_loss: 0.0094
Epoch 39/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0052
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0063
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0060
6530/6530 [==============================] - 0s 21us/step - loss: 0.0061 - val_loss: 0.0094
Epoch 40/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0051
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0062
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0059
6530/6530 [==============================] - 0s 21us/step - loss: 0.0060 - val_loss: 0.0094
Epoch 41/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0052
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0062
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0059
6530/6530 [==============================] - 0s 25us/step - loss: 0.0059 - val_loss: 0.0093
Epoch 42/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0050
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0060
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0057
6530/6530 [==============================] - 0s 21us/step - loss: 0.0058 - val_loss: 0.0094
Epoch 43/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0052
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0061
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0057
6530/6530 [==============================] - 0s 22us/step - loss: 0.0058 - val_loss: 0.0092
Epoch 44/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0049
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0059
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0056
6530/6530 [==============================] - 0s 22us/step - loss: 0.0056 - val_loss: 0.0093
Epoch 45/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0049
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0059
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0056
6528/6530 [============================>.] - ETA: 0s - loss: 0.0056
6530/6530 [==============================] - 0s 25us/step - loss: 0.0056 - val_loss: 0.0092
Epoch 46/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0048
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0057
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0054
6530/6530 [==============================] - 0s 22us/step - loss: 0.0055 - val_loss: 0.0093
Epoch 47/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0049
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0057
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0054
6530/6530 [==============================] - 0s 21us/step - loss: 0.0055 - val_loss: 0.0091
Epoch 48/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0047
2944/6530 [============>.................] - ETA: 0s - loss: 0.0055
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0053
6530/6530 [==============================] - 0s 18us/step - loss: 0.0053 - val_loss: 0.0091
Epoch 49/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0048
3072/6530 [=============>................] - ETA: 0s - loss: 0.0055
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0053
6530/6530 [==============================] - 0s 19us/step - loss: 0.0053 - val_loss: 0.0091
Epoch 50/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0046
2944/6530 [============>.................] - ETA: 0s - loss: 0.0053
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0052
6530/6530 [==============================] - 0s 21us/step - loss: 0.0052 - val_loss: 0.0091
Epoch 51/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0047
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0056
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0052
6530/6530 [==============================] - 0s 22us/step - loss: 0.0052 - val_loss: 0.0090
Epoch 52/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0045
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0053
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0050
6530/6530 [==============================] - 0s 21us/step - loss: 0.0051 - val_loss: 0.0088
Epoch 53/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0046
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0052
6528/6530 [============================>.] - ETA: 0s - loss: 0.0051
6530/6530 [==============================] - 0s 17us/step - loss: 0.0051 - val_loss: 0.0090
Epoch 54/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0046
3072/6530 [=============>................] - ETA: 0s - loss: 0.0051
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0049
6530/6530 [==============================] - 0s 22us/step - loss: 0.0050 - val_loss: 0.0086
Epoch 55/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0044
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0053
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0050
6530/6530 [==============================] - 0s 23us/step - loss: 0.0050 - val_loss: 0.0090
Epoch 56/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0046
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0051
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0048
6530/6530 [==============================] - 0s 25us/step - loss: 0.0049 - val_loss: 0.0085
Epoch 57/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0043
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0053
3840/6530 [================>.............] - ETA: 0s - loss: 0.0050
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0049
6530/6530 [==============================] - 0s 29us/step - loss: 0.0049 - val_loss: 0.0090
Epoch 58/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0046
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0051
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0048
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0047
6530/6530 [==============================] - 0s 27us/step - loss: 0.0048 - val_loss: 0.0085
Epoch 59/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0042
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0051
3712/6530 [================>.............] - ETA: 0s - loss: 0.0048
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0048
6530/6530 [==============================] - 0s 31us/step - loss: 0.0048 - val_loss: 0.0091
Epoch 60/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0046
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0050
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0048
6530/6530 [==============================] - 0s 25us/step - loss: 0.0047 - val_loss: 0.0084
Epoch 61/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0042
2944/6530 [============>.................] - ETA: 0s - loss: 0.0049
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0047
6530/6530 [==============================] - 0s 19us/step - loss: 0.0047 - val_loss: 0.0091
Epoch 62/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0047
3200/6530 [=============>................] - ETA: 0s - loss: 0.0047
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0046
6530/6530 [==============================] - 0s 19us/step - loss: 0.0046 - val_loss: 0.0085
Epoch 63/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0042
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0049
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0046
6530/6530 [==============================] - 0s 21us/step - loss: 0.0046 - val_loss: 0.0091
Epoch 64/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0046
2944/6530 [============>.................] - ETA: 0s - loss: 0.0048
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0045
6530/6530 [==============================] - 0s 18us/step - loss: 0.0046 - val_loss: 0.0085
Epoch 65/81

 128/6530 [..............................] - ETA: 0s - loss: 0.0042
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0047
6400/6530 [============================>.] - ETA: 0s - loss: 0.0045
6530/6530 [==============================] - 0s 17us/step - loss: 0.0045 - val_loss: 0.0092

# training | RMSE: 0.0705, MAE: 0.0548
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.07048690800098392, 'rmse': 0.07048690800098392, 'mae': 0.05483086138486129, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=81.0 loss={'loss': 0.07048690800098392, 'rmse': 0.07048690800098392, 'mae': 0.05483086138486129, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
get a list [results] of length 121
get a list [loss] of length 1
get a list [val_loss] of length 1
length of indices is (0,)
length of indices is 1
length of T is 1
s=3
T is of size 34
T=[{'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.44784773985102067}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 26, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44033065163415996}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20166402653594076}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26587831508262944}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1965905869310106}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42824272704555977}, 'layer_2_size': 62, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 2, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4527782389368925}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23426686206342065}, 'layer_2_size': 50, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28944113514402675}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3686040864853135}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17200213178174123}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 85, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4913128604900715}, 'layer_4_size': 39, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18659663112190664}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 5, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3792106070229092}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 63, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 23, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4538953694584704}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1250234434350652}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4870497906088097}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11777333135437362}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 57, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3199343307861644}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 65, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.33651217972212344}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16610695119268348}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.493726340309126}, 'layer_2_size': 60, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.46088760222654934}, 'layer_3_size': 53, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2545311629703454}, 'layer_3_size': 19, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10875526387691031}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3486207689229829}, 'layer_5_size': 26, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3453854105957833}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3019121415717443}, 'layer_2_size': 33, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17041083125845424}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4623567061254169}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3828312175948112}, 'layer_5_size': 30, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15117667146973882}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.445130852344082}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.42344179496252854}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1904029235723187}, 'layer_4_size': 43, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.32981446545793247}, 'layer_1_size': 62, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4310542042589266}, 'layer_2_size': 21, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45502672490880824}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1486492480861062}, 'layer_4_size': 80, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20519868061145666}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 87, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23524451736866495}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38441826406735924}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3982749725810574}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.44222094159696235}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2289264129983852}, 'layer_3_size': 31, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30076382776869837}, 'layer_1_size': 8, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 5, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 86, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.29138166195004556}, 'layer_3_size': 74, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1874248444420219}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14069231273407207}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4970046503928497}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4171571693762234}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 49, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43102617271254307}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27301050218054207}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2650511587729206}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15264298881590707}, 'layer_1_size': 2, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 100, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17174719681706674}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4842611817250523}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12788149052012576}, 'layer_3_size': 51, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4298233179129576}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 35, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24867068309885548}, 'layer_1_size': 95, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4626676222228512}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19732678881540575}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20791950143380367}, 'layer_3_size': 85, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]
newly formed T structure is:[[0, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.44784773985102067}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 26, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44033065163415996}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [1, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20166402653594076}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26587831508262944}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1965905869310106}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [2, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42824272704555977}, 'layer_2_size': 62, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 2, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4527782389368925}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [3, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23426686206342065}, 'layer_2_size': 50, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [4, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28944113514402675}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3686040864853135}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17200213178174123}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [5, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 85, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4913128604900715}, 'layer_4_size': 39, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [6, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18659663112190664}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [7, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 5, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3792106070229092}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [8, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 63, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 23, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [9, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4538953694584704}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1250234434350652}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [10, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4870497906088097}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11777333135437362}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [11, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 57, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3199343307861644}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [12, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [13, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 65, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.33651217972212344}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [14, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16610695119268348}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.493726340309126}, 'layer_2_size': 60, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.46088760222654934}, 'layer_3_size': 53, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [15, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2545311629703454}, 'layer_3_size': 19, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10875526387691031}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3486207689229829}, 'layer_5_size': 26, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [16, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3453854105957833}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3019121415717443}, 'layer_2_size': 33, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17041083125845424}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4623567061254169}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [17, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3828312175948112}, 'layer_5_size': 30, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [18, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15117667146973882}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.445130852344082}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.42344179496252854}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1904029235723187}, 'layer_4_size': 43, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [19, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.32981446545793247}, 'layer_1_size': 62, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4310542042589266}, 'layer_2_size': 21, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [20, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45502672490880824}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1486492480861062}, 'layer_4_size': 80, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [21, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20519868061145666}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 87, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23524451736866495}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38441826406735924}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [22, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3982749725810574}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.44222094159696235}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2289264129983852}, 'layer_3_size': 31, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [23, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30076382776869837}, 'layer_1_size': 8, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 5, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [24, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 86, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.29138166195004556}, 'layer_3_size': 74, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1874248444420219}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14069231273407207}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [25, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4970046503928497}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4171571693762234}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 49, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [26, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [27, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [28, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43102617271254307}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27301050218054207}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2650511587729206}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [29, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15264298881590707}, 'layer_1_size': 2, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 100, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17174719681706674}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [30, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4842611817250523}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12788149052012576}, 'layer_3_size': 51, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [31, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4298233179129576}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 35, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [32, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24867068309885548}, 'layer_1_size': 95, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4626676222228512}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [33, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19732678881540575}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20791950143380367}, 'layer_3_size': 85, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]] 

*** 34 configurations x 3.0 iterations each

1 | Fri Sep 28 01:21:10 2018 | lowest loss so far: 0.0705 (run 0)

{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  19 | activation: tanh    | extras: None 
layer 2 | size:  62 | activation: sigmoid | extras: dropout - rate: 42.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 1:19 - loss: 0.5293
1792/6530 [=======>......................] - ETA: 2s - loss: 0.2316  
3648/6530 [===============>..............] - ETA: 0s - loss: 0.2073
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1952
6530/6530 [==============================] - 1s 149us/step - loss: 0.1923 - val_loss: 0.1699
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1849
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1673
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1673
6530/6530 [==============================] - 0s 23us/step - loss: 0.1661 - val_loss: 0.1634
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1771
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1626
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1629
6528/6530 [============================>.] - ETA: 0s - loss: 0.1632
6530/6530 [==============================] - 0s 25us/step - loss: 0.1631 - val_loss: 0.1638
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  96 | activation: relu    | extras: dropout - rate: 20.2% 
layer 2 | size:  55 | activation: relu    | extras: batchnorm 
layer 3 | size:  93 | activation: tanh    | extras: dropout - rate: 26.6% 
layer 4 | size:  86 | activation: sigmoid | extras: dropout - rate: 19.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 3:21 - loss: 0.7709
 640/6530 [=>............................] - ETA: 9s - loss: 0.2593  
1312/6530 [=====>........................] - ETA: 4s - loss: 0.2161
1952/6530 [=======>......................] - ETA: 2s - loss: 0.2004{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  86 | activation: sigmoid | extras: dropout - rate: 44.8% 
layer 2 | size:  19 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  26 | activation: relu    | extras: None 
layer 4 | size:  82 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 23s - loss: 1.3333
2592/6530 [==========>...................] - ETA: 1s - loss: 0.1904
4096/6530 [=================>............] - ETA: 0s - loss: 0.4684 
3264/6530 [=============>................] - ETA: 1s - loss: 0.1829
6530/6530 [==============================] - 1s 167us/step - loss: 0.3306 - val_loss: 0.2791
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1016
3840/6530 [================>.............] - ETA: 0s - loss: 0.1790
4096/6530 [=================>............] - ETA: 0s - loss: 0.0902
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1744
6530/6530 [==============================] - 0s 14us/step - loss: 0.0872 - val_loss: 0.1982
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0824
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1709
4096/6530 [=================>............] - ETA: 0s - loss: 0.0796
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1674
6530/6530 [==============================] - 0s 14us/step - loss: 0.0776 - val_loss: 0.1702

6272/6530 [===========================>..] - ETA: 0s - loss: 0.1647
6530/6530 [==============================] - 2s 242us/step - loss: 0.1634 - val_loss: 0.1328
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1339
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1336
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1312
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1316
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1315
# training | RMSE: 0.2041, MAE: 0.1629
worker 2  xfile  [2, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42824272704555977}, 'layer_2_size': 62, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 2, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4527782389368925}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.204143968901908, 'rmse': 0.204143968901908, 'mae': 0.16292964608194271, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  67 | activation: relu    | extras: batchnorm 
layer 2 | size:  50 | activation: relu    | extras: dropout - rate: 23.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9da128>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 30s - loss: 1.3623
3200/6530 [=============>................] - ETA: 0s - loss: 0.1319
1280/6530 [====>.........................] - ETA: 1s - loss: 0.8161 
3840/6530 [================>.............] - ETA: 0s - loss: 0.1317
2496/6530 [==========>...................] - ETA: 0s - loss: 0.6332
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1314
3712/6530 [================>.............] - ETA: 0s - loss: 0.5461
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1305
4992/6530 [=====================>........] - ETA: 0s - loss: 0.4853
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1302
6272/6530 [===========================>..] - ETA: 0s - loss: 0.4456
6530/6530 [==============================] - 1s 82us/step - loss: 0.1298 - val_loss: 0.1154
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1182
6530/6530 [==============================] - 1s 94us/step - loss: 0.4392 - val_loss: 0.3121
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.3320
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1224
1408/6530 [=====>........................] - ETA: 0s - loss: 0.2537
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1204
2688/6530 [===========>..................] - ETA: 0s - loss: 0.2423
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1202
3968/6530 [=================>............] - ETA: 0s - loss: 0.2398
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1217
5312/6530 [=======================>......] - ETA: 0s - loss: 0.2363
3168/6530 [=============>................] - ETA: 0s - loss: 0.1206
6530/6530 [==============================] - 0s 41us/step - loss: 0.2322 - val_loss: 0.2189
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2496
3808/6530 [================>.............] - ETA: 0s - loss: 0.1207
1344/6530 [=====>........................] - ETA: 0s - loss: 0.2043
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1209
2688/6530 [===========>..................] - ETA: 0s - loss: 0.2010
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1214
4032/6530 [=================>............] - ETA: 0s - loss: 0.1970
# training | RMSE: 0.4105, MAE: 0.3484
worker 0  xfile  [0, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.44784773985102067}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 26, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44033065163415996}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.41050688405457686, 'rmse': 0.41050688405457686, 'mae': 0.34840252525775867, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  96 | activation: relu    | extras: dropout - rate: 28.9% 
layer 2 | size:  85 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  48 | activation: tanh    | extras: None 
layer 4 | size:  59 | activation: relu    | extras: dropout - rate: 36.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c09d828>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 3:15 - loss: 0.7304
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1212
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1941
 288/6530 [>.............................] - ETA: 11s - loss: 0.4160 
6368/6530 [============================>.] - ETA: 0s - loss: 0.1215
6528/6530 [============================>.] - ETA: 0s - loss: 0.1923
6530/6530 [==============================] - 0s 41us/step - loss: 0.1923 - val_loss: 0.1944

6530/6530 [==============================] - 1s 84us/step - loss: 0.1215 - val_loss: 0.1010

 544/6530 [=>............................] - ETA: 6s - loss: 0.3431 
 800/6530 [==>...........................] - ETA: 4s - loss: 0.3161
1056/6530 [===>..........................] - ETA: 3s - loss: 0.2988
1312/6530 [=====>........................] - ETA: 2s - loss: 0.2798
1600/6530 [======>.......................] - ETA: 2s - loss: 0.2660
1888/6530 [=======>......................] - ETA: 2s - loss: 0.2537
2144/6530 [========>.....................] - ETA: 1s - loss: 0.2486
2416/6530 [==========>...................] - ETA: 1s - loss: 0.2417
2656/6530 [===========>..................] - ETA: 1s - loss: 0.2371
2864/6530 [============>.................] - ETA: 1s - loss: 0.2336
3088/6530 [=============>................] - ETA: 1s - loss: 0.2291
3328/6530 [==============>...............] - ETA: 1s - loss: 0.2253
3600/6530 [===============>..............] - ETA: 0s - loss: 0.2217
3872/6530 [================>.............] - ETA: 0s - loss: 0.2190
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2156
4432/6530 [===================>..........] - ETA: 0s - loss: 0.2135
# training | RMSE: 0.2420, MAE: 0.1900
worker 2  xfile  [3, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23426686206342065}, 'layer_2_size': 50, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.24197744404965188, 'rmse': 0.24197744404965188, 'mae': 0.18997983575514762, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  85 | activation: tanh    | extras: batchnorm 
layer 2 | size:  68 | activation: sigmoid | extras: None 
layer 3 | size:  76 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03e1750b38>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 15s - loss: 0.5916
2944/6530 [============>.................] - ETA: 0s - loss: 0.0783 
4672/6530 [====================>.........] - ETA: 0s - loss: 0.2124
# training | RMSE: 0.1247, MAE: 0.0943
worker 1  xfile  [1, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20166402653594076}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26587831508262944}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1965905869310106}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.1246583321505086, 'rmse': 0.1246583321505086, 'mae': 0.09428940857051811, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:   3 | activation: sigmoid | extras: None 
layer 2 | size:  67 | activation: tanh    | extras: batchnorm 
layer 3 | size:  91 | activation: relu    | extras: dropout - rate: 18.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9da0b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:11 - loss: 0.4430
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0667
4944/6530 [=====================>........] - ETA: 0s - loss: 0.2108
 368/6530 [>.............................] - ETA: 6s - loss: 0.1483  
5184/6530 [======================>.......] - ETA: 0s - loss: 0.2086
6530/6530 [==============================] - 0s 72us/step - loss: 0.0617 - val_loss: 0.1975
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1949
 736/6530 [==>...........................] - ETA: 3s - loss: 0.1404
5408/6530 [=======================>......] - ETA: 0s - loss: 0.2067
2944/6530 [============>.................] - ETA: 0s - loss: 0.0468
1088/6530 [===>..........................] - ETA: 2s - loss: 0.1391
5664/6530 [=========================>....] - ETA: 0s - loss: 0.2049
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0437
6530/6530 [==============================] - 0s 20us/step - loss: 0.0439 - val_loss: 0.1440
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1462
1440/6530 [=====>........................] - ETA: 1s - loss: 0.1305
5920/6530 [==========================>...] - ETA: 0s - loss: 0.2034
2944/6530 [============>.................] - ETA: 0s - loss: 0.0372
1808/6530 [=======>......................] - ETA: 1s - loss: 0.1235
6176/6530 [===========================>..] - ETA: 0s - loss: 0.2019
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0349
2160/6530 [========>.....................] - ETA: 1s - loss: 0.1207
6432/6530 [============================>.] - ETA: 0s - loss: 0.2005
6530/6530 [==============================] - 0s 20us/step - loss: 0.0343 - val_loss: 0.0950

2528/6530 [==========>...................] - ETA: 1s - loss: 0.1172
2896/6530 [============>.................] - ETA: 0s - loss: 0.1156
6530/6530 [==============================] - 2s 285us/step - loss: 0.2001 - val_loss: 0.1393
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1476
3264/6530 [=============>................] - ETA: 0s - loss: 0.1140
 288/6530 [>.............................] - ETA: 1s - loss: 0.1596
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1122
 512/6530 [=>............................] - ETA: 1s - loss: 0.1652
3984/6530 [=================>............] - ETA: 0s - loss: 0.1111
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1669
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1096
1008/6530 [===>..........................] - ETA: 1s - loss: 0.1628
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1080
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1633
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1067
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1600
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1063
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1587
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1057
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1571
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1044
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1577
6528/6530 [============================>.] - ETA: 0s - loss: 0.1037
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1569
6530/6530 [==============================] - 1s 200us/step - loss: 0.1036 - val_loss: 0.3907
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1604
2832/6530 [============>.................] - ETA: 0s - loss: 0.1552
 368/6530 [>.............................] - ETA: 0s - loss: 0.0797
3088/6530 [=============>................] - ETA: 0s - loss: 0.1541
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0879
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1543
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0919
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1543
# training | RMSE: 0.3006, MAE: 0.2566
worker 2  xfile  [5, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 85, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4913128604900715}, 'layer_4_size': 39, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.30062759288945645, 'rmse': 0.30062759288945645, 'mae': 0.25664241012793504, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  31 | activation: sigmoid | extras: None 
layer 2 | size:   5 | activation: sigmoid | extras: None 
layer 3 | size:  64 | activation: sigmoid | extras: None 
layer 4 | size:  87 | activation: sigmoid | extras: None 
layer 5 | size:  72 | activation: tanh    | extras: dropout - rate: 37.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f03c8236390>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 19s - loss: 1.8094
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0896
3888/6530 [================>.............] - ETA: 0s - loss: 0.1540
3072/6530 [=============>................] - ETA: 0s - loss: 0.3201 
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0858
4128/6530 [=================>............] - ETA: 0s - loss: 0.1534
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2081
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0862
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1520
6530/6530 [==============================] - 1s 82us/step - loss: 0.1946 - val_loss: 0.0716
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0837
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0862
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1524
2944/6530 [============>.................] - ETA: 0s - loss: 0.0724
2864/6530 [============>.................] - ETA: 0s - loss: 0.0852
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1523
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0704
3184/6530 [=============>................] - ETA: 0s - loss: 0.0838
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1517
6530/6530 [==============================] - 0s 19us/step - loss: 0.0699 - val_loss: 0.0683
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0798
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0829
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1514
2944/6530 [============>.................] - ETA: 0s - loss: 0.0714
3904/6530 [================>.............] - ETA: 0s - loss: 0.0827
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1503
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0696
6530/6530 [==============================] - 0s 19us/step - loss: 0.0693 - val_loss: 0.0683

4272/6530 [==================>...........] - ETA: 0s - loss: 0.0815
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1500
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0808
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1494
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0801
6432/6530 [============================>.] - ETA: 0s - loss: 0.1490
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0798
6530/6530 [==============================] - 1s 207us/step - loss: 0.1492 - val_loss: 0.1146
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1640
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0794
 288/6530 [>.............................] - ETA: 1s - loss: 0.1436
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0786
 528/6530 [=>............................] - ETA: 1s - loss: 0.1433
6480/6530 [============================>.] - ETA: 0s - loss: 0.0777
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1406
6530/6530 [==============================] - 1s 148us/step - loss: 0.0775 - val_loss: 0.1636
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0861
1040/6530 [===>..........................] - ETA: 1s - loss: 0.1405
 352/6530 [>.............................] - ETA: 0s - loss: 0.0532
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1411
# training | RMSE: 0.2626, MAE: 0.2148
worker 2  xfile  [7, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 5, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3792106070229092}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2626450274448594, 'rmse': 0.2626450274448594, 'mae': 0.21481343787837748, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  72 | activation: relu    | extras: None 
layer 2 | size:  29 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03c0135c88>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:23 - loss: 0.5432
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0622
1552/6530 [======>.......................] - ETA: 1s - loss: 0.1387
 624/6530 [=>............................] - ETA: 2s - loss: 0.0983  
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0643
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1384
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0838
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0615
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1369
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0763
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0595
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1370
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0705
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0592
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1372
3040/6530 [============>.................] - ETA: 0s - loss: 0.0670
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0601
2832/6530 [============>.................] - ETA: 0s - loss: 0.1362
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0640
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0592
3088/6530 [=============>................] - ETA: 0s - loss: 0.1355
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0611
3168/6530 [=============>................] - ETA: 0s - loss: 0.0583
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1349
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0593
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0584
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1346
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0579
3904/6530 [================>.............] - ETA: 0s - loss: 0.0588
3840/6530 [================>.............] - ETA: 0s - loss: 0.1349
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0565
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0585
4096/6530 [=================>............] - ETA: 0s - loss: 0.1345
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0586
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1341
6530/6530 [==============================] - 1s 126us/step - loss: 0.0555 - val_loss: 0.0421
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0613
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0583
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1342
 576/6530 [=>............................] - ETA: 0s - loss: 0.0438
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0587
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1340
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0406
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0584
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1338
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0396
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0583
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1336
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0394
6400/6530 [============================>.] - ETA: 0s - loss: 0.0581
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1330
2912/6530 [============>.................] - ETA: 0s - loss: 0.0402
6530/6530 [==============================] - 1s 151us/step - loss: 0.0579 - val_loss: 0.0941

5840/6530 [=========================>....] - ETA: 0s - loss: 0.1331
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0404
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1333
4048/6530 [=================>............] - ETA: 0s - loss: 0.0410
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1332
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0405
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0410
6530/6530 [==============================] - 1s 210us/step - loss: 0.1329 - val_loss: 0.1068

5776/6530 [=========================>....] - ETA: 0s - loss: 0.0410
6400/6530 [============================>.] - ETA: 0s - loss: 0.0410
6530/6530 [==============================] - 1s 91us/step - loss: 0.0410 - val_loss: 0.0454
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0576
 592/6530 [=>............................] - ETA: 0s - loss: 0.0374
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0373
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0385
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0393
3168/6530 [=============>................] - ETA: 0s - loss: 0.0395
3776/6530 [================>.............] - ETA: 0s - loss: 0.0402
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0405
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0401
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0399
# training | RMSE: 0.1353, MAE: 0.1071
worker 0  xfile  [4, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28944113514402675}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3686040864853135}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17200213178174123}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.13529093519101165, 'rmse': 0.13529093519101165, 'mae': 0.10714273773227759, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  11 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c09d630>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 32s - loss: 0.6019
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0397
1312/6530 [=====>........................] - ETA: 0s - loss: 0.3793 
6530/6530 [==============================] - 1s 85us/step - loss: 0.0397 - val_loss: 0.0398

2752/6530 [===========>..................] - ETA: 0s - loss: 0.2761
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2154
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1784
6530/6530 [==============================] - 0s 65us/step - loss: 0.1641 - val_loss: 0.0646
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0520
# training | RMSE: 0.3070, MAE: 0.2476
worker 1  xfile  [6, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18659663112190664}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.3070327174008272, 'rmse': 0.3070327174008272, 'mae': 0.24757249059226763, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  15 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  68 | activation: tanh    | extras: None 
layer 3 | size:  63 | activation: relu    | extras: dropout - rate: 45.4% 
layer 4 | size:   7 | activation: sigmoid | extras: dropout - rate: 12.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c18af98>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 38s - loss: 1.0282
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0593
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1933 
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0568
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1429
4032/6530 [=================>............] - ETA: 0s - loss: 0.0554
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1201
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0546
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1071
6530/6530 [==============================] - 0s 40us/step - loss: 0.0530 - val_loss: 0.0477
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0349
6530/6530 [==============================] - 1s 104us/step - loss: 0.1005 - val_loss: 0.0415
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0702
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0448
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0593
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0443
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0602
4032/6530 [=================>............] - ETA: 0s - loss: 0.0429
3904/6530 [================>.............] - ETA: 0s - loss: 0.0585
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0421
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0572
6530/6530 [==============================] - 0s 39us/step - loss: 0.0410 - val_loss: 0.0400

6530/6530 [==============================] - 0s 40us/step - loss: 0.0562 - val_loss: 0.0380
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0638
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0507
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0507
4032/6530 [=================>............] - ETA: 0s - loss: 0.0491
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0491
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0484
6530/6530 [==============================] - 0s 44us/step - loss: 0.0482 - val_loss: 0.0332

# training | RMSE: 0.1973, MAE: 0.1585
worker 2  xfile  [8, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 63, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 23, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.19733965335179443, 'rmse': 0.19733965335179443, 'mae': 0.1584925232294999, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  49 | activation: relu    | extras: batchnorm 
layer 2 | size:  57 | activation: sigmoid | extras: None 
layer 3 | size:  85 | activation: sigmoid | extras: batchnorm 
layer 4 | size:   2 | activation: relu    | extras: dropout - rate: 32.0% 
layer 5 | size:  59 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00dd20ecf8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 2:00 - loss: 6.7393
 512/6530 [=>............................] - ETA: 7s - loss: 2.3542  
# training | RMSE: 0.1768, MAE: 0.1392
worker 1  xfile  [9, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4538953694584704}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1250234434350652}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.17683298409967943, 'rmse': 0.17683298409967943, 'mae': 0.13921062325398847, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  65 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03fa2811d0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 18s - loss: 1.0267
 992/6530 [===>..........................] - ETA: 3s - loss: 1.4146
2688/6530 [===========>..................] - ETA: 0s - loss: 0.7914 
1440/6530 [=====>........................] - ETA: 2s - loss: 1.0167
5056/6530 [======================>.......] - ETA: 0s - loss: 0.6204
1952/6530 [=======>......................] - ETA: 1s - loss: 0.7715
6530/6530 [==============================] - 0s 54us/step - loss: 0.5576 - val_loss: 0.3285
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.4018
2464/6530 [==========>...................] - ETA: 1s - loss: 0.6281
2624/6530 [===========>..................] - ETA: 0s - loss: 0.3061
2976/6530 [============>.................] - ETA: 1s - loss: 0.5329
5184/6530 [======================>.......] - ETA: 0s - loss: 0.2888
3488/6530 [===============>..............] - ETA: 0s - loss: 0.4636
6530/6530 [==============================] - 0s 21us/step - loss: 0.2795 - val_loss: 0.2435
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2877
# training | RMSE: 0.1926, MAE: 0.1540
worker 0  xfile  [10, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4870497906088097}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11777333135437362}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.19263856303784027, 'rmse': 0.19263856303784027, 'mae': 0.15396825552494034, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:   7 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  55 | activation: sigmoid | extras: None 
layer 3 | size:  98 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03f832b4e0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 46s - loss: 0.5001
3968/6530 [=================>............] - ETA: 0s - loss: 0.4147
2624/6530 [===========>..................] - ETA: 0s - loss: 0.2356
1344/6530 [=====>........................] - ETA: 1s - loss: 0.2870 
4448/6530 [===================>..........] - ETA: 0s - loss: 0.3757
5056/6530 [======================>.......] - ETA: 0s - loss: 0.2301
2496/6530 [==========>...................] - ETA: 0s - loss: 0.2577
6530/6530 [==============================] - 0s 22us/step - loss: 0.2252 - val_loss: 0.2099

4928/6530 [=====================>........] - ETA: 0s - loss: 0.3452
3712/6530 [================>.............] - ETA: 0s - loss: 0.2413
5440/6530 [=======================>......] - ETA: 0s - loss: 0.3181
4928/6530 [=====================>........] - ETA: 0s - loss: 0.2310
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2955
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2224
6432/6530 [============================>.] - ETA: 0s - loss: 0.2775
6530/6530 [==============================] - 1s 120us/step - loss: 0.2197 - val_loss: 0.1685
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1883
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1663
6530/6530 [==============================] - 1s 208us/step - loss: 0.2741 - val_loss: 0.0536
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0527
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1662
 544/6530 [=>............................] - ETA: 0s - loss: 0.0581
3776/6530 [================>.............] - ETA: 0s - loss: 0.1639
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0562
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1646
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0572
6336/6530 [============================>.] - ETA: 0s - loss: 0.1636
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0563
6530/6530 [==============================] - 0s 42us/step - loss: 0.1633 - val_loss: 0.1648
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1719
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0551
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1630
3008/6530 [============>.................] - ETA: 0s - loss: 0.0547
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1626
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0552
3776/6530 [================>.............] - ETA: 0s - loss: 0.1617
4032/6530 [=================>............] - ETA: 0s - loss: 0.0554
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1624
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0549
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1625
6530/6530 [==============================] - 0s 44us/step - loss: 0.1619 - val_loss: 0.1651

5088/6530 [======================>.......] - ETA: 0s - loss: 0.0546
# training | RMSE: 0.2635, MAE: 0.2081
worker 1  xfile  [13, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 65, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.33651217972212344}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2634800953671358, 'rmse': 0.2634800953671358, 'mae': 0.20813111618510202, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  78 | activation: tanh    | extras: dropout - rate: 16.6% 
layer 2 | size:  60 | activation: tanh    | extras: dropout - rate: 49.4% 
layer 3 | size:  53 | activation: sigmoid | extras: dropout - rate: 46.1% 
layer 4 | size:   7 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f03f0153400>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 17s - loss: 0.8071
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0548
3328/6530 [==============>...............] - ETA: 0s - loss: 0.4698 
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0545
6400/6530 [============================>.] - ETA: 0s - loss: 0.3513
6530/6530 [==============================] - 0s 76us/step - loss: 0.3484 - val_loss: 0.1866
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2396
6530/6530 [==============================] - 1s 108us/step - loss: 0.0543 - val_loss: 0.0487
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0392
2944/6530 [============>.................] - ETA: 0s - loss: 0.2040
 480/6530 [=>............................] - ETA: 0s - loss: 0.0506
6144/6530 [===========================>..] - ETA: 0s - loss: 0.2003
6530/6530 [==============================] - 0s 18us/step - loss: 0.1996 - val_loss: 0.1632
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1972
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0544
3200/6530 [=============>................] - ETA: 0s - loss: 0.1887
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0526
6400/6530 [============================>.] - ETA: 0s - loss: 0.1886
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0528
6530/6530 [==============================] - 0s 18us/step - loss: 0.1885 - val_loss: 0.1642

2400/6530 [==========>...................] - ETA: 0s - loss: 0.0525
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0520
3232/6530 [=============>................] - ETA: 0s - loss: 0.0519
3712/6530 [================>.............] - ETA: 0s - loss: 0.0519
# training | RMSE: 0.2035, MAE: 0.1629
worker 0  xfile  [12, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.2035386365865549, 'rmse': 0.2035386365865549, 'mae': 0.16288916485428703, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  93 | activation: sigmoid | extras: None 
layer 2 | size:  80 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f03f81c1898>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:57 - loss: 0.7639
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0517
 448/6530 [=>............................] - ETA: 4s - loss: 0.0983  
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0509
 912/6530 [===>..........................] - ETA: 2s - loss: 0.0724
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0507
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0625
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0508
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0606
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0511
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0579
6530/6530 [==============================] - 1s 110us/step - loss: 0.0512 - val_loss: 0.0472

2832/6530 [============>.................] - ETA: 0s - loss: 0.0559
# training | RMSE: 0.2119, MAE: 0.1641
worker 1  xfile  [14, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16610695119268348}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.493726340309126}, 'layer_2_size': 60, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.46088760222654934}, 'layer_3_size': 53, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.21192832302510958, 'rmse': 0.21192832302510958, 'mae': 0.1640908843524241, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  75 | activation: tanh    | extras: dropout - rate: 34.5% 
layer 2 | size:  33 | activation: relu    | extras: dropout - rate: 30.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f03f0153208>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 51s - loss: 0.5656
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0569
1088/6530 [===>..........................] - ETA: 1s - loss: 0.3986 
3792/6530 [================>.............] - ETA: 0s - loss: 0.0574
2144/6530 [========>.....................] - ETA: 0s - loss: 0.2361
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0557
3232/6530 [=============>................] - ETA: 0s - loss: 0.1715
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0561
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1398
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0561
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1208
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0554
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0552
6530/6530 [==============================] - 1s 93us/step - loss: 0.1080 - val_loss: 0.0449
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0597
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0472
6530/6530 [==============================] - 1s 159us/step - loss: 0.0549 - val_loss: 0.1131
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0932
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0443
 528/6530 [=>............................] - ETA: 0s - loss: 0.0480
3264/6530 [=============>................] - ETA: 0s - loss: 0.0441
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0445
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0443
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0462
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0447
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0468
6432/6530 [============================>.] - ETA: 0s - loss: 0.0445
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0463
6530/6530 [==============================] - 0s 50us/step - loss: 0.0443 - val_loss: 0.0442
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0558
2944/6530 [============>.................] - ETA: 0s - loss: 0.0452
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0443
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0445
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0436
3840/6530 [================>.............] - ETA: 0s - loss: 0.0442
3072/6530 [=============>................] - ETA: 0s - loss: 0.0434
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0440
4096/6530 [=================>............] - ETA: 0s - loss: 0.0440
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0438
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0443
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0430
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0442
6530/6530 [==============================] - 0s 50us/step - loss: 0.0439 - val_loss: 0.0453

5728/6530 [=========================>....] - ETA: 0s - loss: 0.0426
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0420
# training | RMSE: 0.2167, MAE: 0.1729
worker 2  xfile  [11, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 57, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3199343307861644}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.21670707544286386, 'rmse': 0.21670707544286386, 'mae': 0.17289221541311903, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  70 | activation: tanh    | extras: batchnorm 
layer 2 | size:  93 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00dd0417f0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 11s - loss: 0.6200
6530/6530 [==============================] - 1s 111us/step - loss: 0.0423 - val_loss: 0.0526
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0439
4864/6530 [=====================>........] - ETA: 0s - loss: 0.4423 
 464/6530 [=>............................] - ETA: 0s - loss: 0.0369
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0367
6530/6530 [==============================] - 1s 92us/step - loss: 0.4022 - val_loss: 0.2784
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2956
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0349
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1761
6530/6530 [==============================] - 0s 12us/step - loss: 0.1603 - val_loss: 0.1041
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1046
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0373
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0366
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0639
6530/6530 [==============================] - 0s 11us/step - loss: 0.0591 - val_loss: 0.0425

2832/6530 [============>.................] - ETA: 0s - loss: 0.0360
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0357
3808/6530 [================>.............] - ETA: 0s - loss: 0.0353
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0354
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0348
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0346
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0339
# training | RMSE: 0.2115, MAE: 0.1734
worker 1  xfile  [16, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3453854105957833}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3019121415717443}, 'layer_2_size': 33, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17041083125845424}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4623567061254169}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.21153139541001023, 'rmse': 0.21153139541001023, 'mae': 0.17340308373832117, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  25 | activation: relu    | extras: dropout - rate: 15.1% 
layer 2 | size:  47 | activation: tanh    | extras: dropout - rate: 44.5% 
layer 3 | size:  82 | activation: relu    | extras: dropout - rate: 42.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f03e80db0b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 59s - loss: 0.7590
6320/6530 [============================>.] - ETA: 0s - loss: 0.0335
 992/6530 [===>..........................] - ETA: 1s - loss: 0.2971 
6530/6530 [==============================] - 1s 109us/step - loss: 0.0335 - val_loss: 0.1490

1952/6530 [=======>......................] - ETA: 0s - loss: 0.2587
2848/6530 [============>.................] - ETA: 0s - loss: 0.2401
3744/6530 [================>.............] - ETA: 0s - loss: 0.2293
# training | RMSE: 0.2109, MAE: 0.1654
worker 2  xfile  [17, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3828312175948112}, 'layer_5_size': 30, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.21092530600686765, 'rmse': 0.21092530600686765, 'mae': 0.16539107486495377, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  62 | activation: tanh    | extras: dropout - rate: 33.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f00dc69cf28>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 15s - loss: 1.5747
4608/6530 [====================>.........] - ETA: 0s - loss: 0.2206
4608/6530 [====================>.........] - ETA: 0s - loss: 1.0089 
5408/6530 [=======================>......] - ETA: 0s - loss: 0.2145
6530/6530 [==============================] - 0s 64us/step - loss: 0.8129 - val_loss: 0.2575
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2370
6240/6530 [===========================>..] - ETA: 0s - loss: 0.2092
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1563
6530/6530 [==============================] - 0s 12us/step - loss: 0.1356 - val_loss: 0.0800
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0729
6530/6530 [==============================] - 1s 111us/step - loss: 0.2078 - val_loss: 0.2501
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2836
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0739
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1795
6530/6530 [==============================] - 0s 12us/step - loss: 0.0724 - val_loss: 0.0696

1952/6530 [=======>......................] - ETA: 0s - loss: 0.1792
2944/6530 [============>.................] - ETA: 0s - loss: 0.1746
3936/6530 [=================>............] - ETA: 0s - loss: 0.1744
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1746
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1740
6530/6530 [==============================] - 0s 52us/step - loss: 0.1734 - val_loss: 0.1594
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1663
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1718
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1739
3008/6530 [============>.................] - ETA: 0s - loss: 0.1700
4000/6530 [=================>............] - ETA: 0s - loss: 0.1675
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1675
# training | RMSE: 0.2612, MAE: 0.2005
worker 2  xfile  [19, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.32981446545793247}, 'layer_1_size': 62, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4310542042589266}, 'layer_2_size': 21, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.261234941777578, 'rmse': 0.261234941777578, 'mae': 0.20049671080390383, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  18 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00dc69cd30>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 7s - loss: 0.5937
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1675
6530/6530 [==============================] - 0s 52us/step - loss: 0.1678 - val_loss: 0.1705

6530/6530 [==============================] - 0s 57us/step - loss: 0.5171 - val_loss: 0.4414
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.4522
6530/6530 [==============================] - 0s 6us/step - loss: 0.3590 - val_loss: 0.2652
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2740
6530/6530 [==============================] - 0s 5us/step - loss: 0.1867 - val_loss: 0.1093

# training | RMSE: 0.3944, MAE: 0.3530
worker 0  xfile  [15, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2545311629703454}, 'layer_3_size': 19, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10875526387691031}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3486207689229829}, 'layer_5_size': 26, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.3943639949389148, 'rmse': 0.3943639949389148, 'mae': 0.3529843331665071, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  73 | activation: tanh    | extras: dropout - rate: 45.5% 
layer 2 | size:  98 | activation: tanh    | extras: batchnorm 
layer 3 | size:  74 | activation: relu    | extras: None 
layer 4 | size:  80 | activation: sigmoid | extras: dropout - rate: 14.9% 
layer 5 | size:   3 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f03d4108c88>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 32s - loss: 0.7490
1920/6530 [=======>......................] - ETA: 1s - loss: 0.7262 
3840/6530 [================>.............] - ETA: 0s - loss: 0.7109
5888/6530 [==========================>...] - ETA: 0s - loss: 0.6969
# training | RMSE: 0.3296, MAE: 0.2871
worker 2  xfile  [21, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20519868061145666}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 87, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23524451736866495}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38441826406735924}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.3296375706945452, 'rmse': 0.3296375706945452, 'mae': 0.28711168289035893, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:   8 | activation: relu    | extras: dropout - rate: 30.1% 
layer 2 | size:  54 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00dc57e7f0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 17s - loss: 0.2064
6530/6530 [==============================] - 1s 135us/step - loss: 0.6917 - val_loss: 0.6687
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.6782
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0775 
2176/6530 [========>.....................] - ETA: 0s - loss: 0.6586
# training | RMSE: 0.2061, MAE: 0.1689
worker 1  xfile  [18, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15117667146973882}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.445130852344082}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.42344179496252854}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1904029235723187}, 'layer_4_size': 43, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.20612878842750368, 'rmse': 0.20612878842750368, 'mae': 0.16894724744243375, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  24 | activation: tanh    | extras: dropout - rate: 39.8% 
layer 2 | size:  21 | activation: tanh    | extras: dropout - rate: 44.2% 
layer 3 | size:  31 | activation: tanh    | extras: dropout - rate: 22.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f03d5673630>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:15 - loss: 0.6578
4096/6530 [=================>............] - ETA: 0s - loss: 0.6482
 448/6530 [=>............................] - ETA: 5s - loss: 0.5922  
5760/6530 [=========================>....] - ETA: 0s - loss: 0.6385
 912/6530 [===>..........................] - ETA: 2s - loss: 0.4312
6530/6530 [==============================] - 0s 29us/step - loss: 0.6365 - val_loss: 0.6167
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.6224
1376/6530 [=====>........................] - ETA: 1s - loss: 0.3533
6530/6530 [==============================] - 1s 86us/step - loss: 0.0719 - val_loss: 0.0542
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0642
2048/6530 [========>.....................] - ETA: 0s - loss: 0.5912
1792/6530 [=======>......................] - ETA: 1s - loss: 0.3173
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0582
3968/6530 [=================>............] - ETA: 0s - loss: 0.5948
6530/6530 [==============================] - 0s 13us/step - loss: 0.0573 - val_loss: 0.0532
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0720
2224/6530 [=========>....................] - ETA: 1s - loss: 0.2953
5888/6530 [==========================>...] - ETA: 0s - loss: 0.5870
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0546
2672/6530 [===========>..................] - ETA: 0s - loss: 0.2778
6530/6530 [==============================] - 0s 28us/step - loss: 0.5846 - val_loss: 0.5652

6530/6530 [==============================] - 0s 14us/step - loss: 0.0537 - val_loss: 0.0460

3088/6530 [=============>................] - ETA: 0s - loss: 0.2634
3536/6530 [===============>..............] - ETA: 0s - loss: 0.2535
3888/6530 [================>.............] - ETA: 0s - loss: 0.2467
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2393
4816/6530 [=====================>........] - ETA: 0s - loss: 0.2340
5264/6530 [=======================>......] - ETA: 0s - loss: 0.2291
5696/6530 [=========================>....] - ETA: 0s - loss: 0.2250
6160/6530 [===========================>..] - ETA: 0s - loss: 0.2212
6530/6530 [==============================] - 1s 179us/step - loss: 0.2183 - val_loss: 0.1650
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1532
 480/6530 [=>............................] - ETA: 0s - loss: 0.1787
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1780
# training | RMSE: 0.6169, MAE: 0.5579
worker 0  xfile  [20, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45502672490880824}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1486492480861062}, 'layer_4_size': 80, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.6169146064580298, 'rmse': 0.6169146064580298, 'mae': 0.5578802605377515, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  25 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00afa04b00>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 7s - loss: 0.5097
1328/6530 [=====>........................] - ETA: 0s - loss: 0.1725
6530/6530 [==============================] - 0s 59us/step - loss: 0.3033 - val_loss: 0.1525
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1511
1744/6530 [=======>......................] - ETA: 0s - loss: 0.1722
6530/6530 [==============================] - 0s 6us/step - loss: 0.1047 - val_loss: 0.0765
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0736
2160/6530 [========>.....................] - ETA: 0s - loss: 0.1717
# training | RMSE: 0.2134, MAE: 0.1719
worker 2  xfile  [23, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30076382776869837}, 'layer_1_size': 8, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 5, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.21338948295348614, 'rmse': 0.21338948295348614, 'mae': 0.17194850006813128, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  52 | activation: relu    | extras: dropout - rate: 49.7% 
layer 2 | size:  44 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00dc6f3978>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 40s - loss: 0.5959
6530/6530 [==============================] - 0s 7us/step - loss: 0.0686 - val_loss: 0.0629

2576/6530 [==========>...................] - ETA: 0s - loss: 0.1733
2048/6530 [========>.....................] - ETA: 0s - loss: 0.3333 
3008/6530 [============>.................] - ETA: 0s - loss: 0.1709
4096/6530 [=================>............] - ETA: 0s - loss: 0.2834
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1711
6080/6530 [==========================>...] - ETA: 0s - loss: 0.2590
3888/6530 [================>.............] - ETA: 0s - loss: 0.1708
6530/6530 [==============================] - 1s 96us/step - loss: 0.2545 - val_loss: 0.1696
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1835
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1707
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1894
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1712
4096/6530 [=================>............] - ETA: 0s - loss: 0.1866
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1716
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1852
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1715
6530/6530 [==============================] - 0s 27us/step - loss: 0.1845 - val_loss: 0.1635
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1657
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1715
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1781
6496/6530 [============================>.] - ETA: 0s - loss: 0.1709
4096/6530 [=================>............] - ETA: 0s - loss: 0.1758
6530/6530 [==============================] - 1s 122us/step - loss: 0.1708 - val_loss: 0.1784
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1835
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1766
 400/6530 [>.............................] - ETA: 0s - loss: 0.1718
6530/6530 [==============================] - 0s 28us/step - loss: 0.1751 - val_loss: 0.1593

 864/6530 [==>...........................] - ETA: 0s - loss: 0.1730
1328/6530 [=====>........................] - ETA: 0s - loss: 0.1682
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1676
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1678
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1693
3120/6530 [=============>................] - ETA: 0s - loss: 0.1675
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1693
3840/6530 [================>.............] - ETA: 0s - loss: 0.1685
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1688
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1692
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1695
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1694
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1688
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1687
6530/6530 [==============================] - 1s 128us/step - loss: 0.1684 - val_loss: 0.1763

# training | RMSE: 0.2017, MAE: 0.1579
worker 2  xfile  [25, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4970046503928497}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4171571693762234}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 49, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2017358120554465, 'rmse': 0.2017358120554465, 'mae': 0.15786389616300575, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  81 | activation: sigmoid | extras: None 
layer 2 | size:  12 | activation: tanh    | extras: batchnorm 
layer 3 | size:  58 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f00dc6f3438>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 50s - loss: 0.7041
1216/6530 [====>.........................] - ETA: 2s - loss: 0.3928 
2496/6530 [==========>...................] - ETA: 0s - loss: 0.2981
3968/6530 [=================>............] - ETA: 0s - loss: 0.2530
5440/6530 [=======================>......] - ETA: 0s - loss: 0.2304
6530/6530 [==============================] - 1s 124us/step - loss: 0.2186 - val_loss: 2.2289
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1625
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1590
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1603
# training | RMSE: 0.2520, MAE: 0.2060
worker 0  xfile  [24, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 86, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.29138166195004556}, 'layer_3_size': 74, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1874248444420219}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14069231273407207}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2520148998259068, 'rmse': 0.2520148998259068, 'mae': 0.2059621243723038, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:   3 | activation: tanh    | extras: batchnorm 
layer 2 | size:  45 | activation: relu    | extras: batchnorm 
layer 3 | size:  57 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  81 | activation: sigmoid | extras: None 
layer 5 | size:  14 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00afa04978>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 1:41 - loss: 0.6535
3968/6530 [=================>............] - ETA: 0s - loss: 0.1597
 768/6530 [==>...........................] - ETA: 7s - loss: 0.5462  
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1606
1472/6530 [=====>........................] - ETA: 3s - loss: 0.4274
6464/6530 [============================>.] - ETA: 0s - loss: 0.1597
6530/6530 [==============================] - 0s 42us/step - loss: 0.1596 - val_loss: 0.4182
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1726
2176/6530 [========>.....................] - ETA: 2s - loss: 0.3440
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1582
2944/6530 [============>.................] - ETA: 1s - loss: 0.2733
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1580
3648/6530 [===============>..............] - ETA: 1s - loss: 0.2304
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1583
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2002
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1584
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1794
6530/6530 [==============================] - 0s 39us/step - loss: 0.1581 - val_loss: 0.1830

5632/6530 [========================>.....] - ETA: 0s - loss: 0.1632
6336/6530 [============================>.] - ETA: 0s - loss: 0.1492
# training | RMSE: 0.2166, MAE: 0.1765
worker 1  xfile  [22, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3982749725810574}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.44222094159696235}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2289264129983852}, 'layer_3_size': 31, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.21663935605955045, 'rmse': 0.21663935605955045, 'mae': 0.17647010995138768, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  46 | activation: sigmoid | extras: dropout - rate: 43.1% 
layer 2 | size:  34 | activation: relu    | extras: None 
layer 3 | size:  25 | activation: relu    | extras: dropout - rate: 27.3% 
layer 4 | size:  50 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f03d51fc518>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 24s - loss: 0.8998
6530/6530 [==============================] - 2s 246us/step - loss: 0.1458 - val_loss: 0.0397
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0398
3072/6530 [=============>................] - ETA: 0s - loss: 0.3185 
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0364
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2230
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0348
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0348
6530/6530 [==============================] - 1s 101us/step - loss: 0.2113 - val_loss: 0.0641
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1083
2944/6530 [============>.................] - ETA: 0s - loss: 0.0352
2944/6530 [============>.................] - ETA: 0s - loss: 0.0983
3776/6530 [================>.............] - ETA: 0s - loss: 0.0355
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0951
6530/6530 [==============================] - 0s 20us/step - loss: 0.0940 - val_loss: 0.0652
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0985
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0357
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0881
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0362
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0837
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0361
6530/6530 [==============================] - 0s 19us/step - loss: 0.0831 - val_loss: 0.0653

6530/6530 [==============================] - 0s 71us/step - loss: 0.0358 - val_loss: 0.0374
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0435
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0351
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0353
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0344
2944/6530 [============>.................] - ETA: 0s - loss: 0.0349
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0342
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0336
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0334
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0336
# training | RMSE: 0.2372, MAE: 0.1794
worker 2  xfile  [27, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.23724761464551822, 'rmse': 0.23724761464551822, 'mae': 0.17936543218375842, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:   2 | activation: tanh    | extras: dropout - rate: 15.3% 
layer 2 | size: 100 | activation: relu    | extras: None 
layer 3 | size:  42 | activation: tanh    | extras: None 
layer 4 | size:  81 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00b7f09fd0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 53s - loss: 0.2528
6528/6530 [============================>.] - ETA: 0s - loss: 0.0336
6530/6530 [==============================] - 0s 76us/step - loss: 0.0337 - val_loss: 0.0359

1472/6530 [=====>........................] - ETA: 1s - loss: 0.1219 
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0990
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0897
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0834
6530/6530 [==============================] - 1s 129us/step - loss: 0.0800 - val_loss: 0.0470
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0516
# training | RMSE: 0.2576, MAE: 0.2112
worker 1  xfile  [28, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43102617271254307}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27301050218054207}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2650511587729206}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.25760568283909674, 'rmse': 0.25760568283909674, 'mae': 0.21123783191360906, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  69 | activation: relu    | extras: dropout - rate: 48.4% 
layer 2 | size:  32 | activation: tanh    | extras: None 
layer 3 | size:  51 | activation: relu    | extras: dropout - rate: 12.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f03d4f6fa90>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 37s - loss: 1.0018
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0441
1792/6530 [=======>......................] - ETA: 1s - loss: 0.3325 
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0440
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2443
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0435
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2009
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0435
6530/6530 [==============================] - 0s 38us/step - loss: 0.0434 - val_loss: 0.0416
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0351
6530/6530 [==============================] - 1s 94us/step - loss: 0.1812 - val_loss: 0.0518
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1012
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0410
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0859
2944/6530 [============>.................] - ETA: 0s - loss: 0.0431
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0776
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0424
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0735
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0425
6530/6530 [==============================] - 0s 30us/step - loss: 0.0714 - val_loss: 0.0483
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0751
6530/6530 [==============================] - 0s 38us/step - loss: 0.0426 - val_loss: 0.0427

1984/6530 [========>.....................] - ETA: 0s - loss: 0.0577
3968/6530 [=================>............] - ETA: 0s - loss: 0.0559
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0544
6530/6530 [==============================] - 0s 30us/step - loss: 0.0543 - val_loss: 0.0492

# training | RMSE: 0.1868, MAE: 0.1496
worker 0  xfile  [26, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.18681886169284156, 'rmse': 0.18681886169284156, 'mae': 0.14959481851775777, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  83 | activation: tanh    | extras: batchnorm 
layer 2 | size:  33 | activation: tanh    | extras: dropout - rate: 43.0% 
layer 3 | size:   4 | activation: tanh    | extras: batchnorm 
layer 4 | size:  33 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00af86fe10>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 19s - loss: 3.8720
3072/6530 [=============>................] - ETA: 0s - loss: 1.1750 
6144/6530 [===========================>..] - ETA: 0s - loss: 0.6784
6530/6530 [==============================] - 1s 151us/step - loss: 0.6452 - val_loss: 0.0792
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1037
# training | RMSE: 0.2042, MAE: 0.1663
worker 2  xfile  [29, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15264298881590707}, 'layer_1_size': 2, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 100, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17174719681706674}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20421430160193707, 'rmse': 0.20421430160193707, 'mae': 0.16634380667609455, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  95 | activation: relu    | extras: dropout - rate: 24.9% 
layer 2 | size:   9 | activation: tanh    | extras: None 
layer 3 | size:  92 | activation: tanh    | extras: None 
layer 4 | size:  87 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f00b7f71ef0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 3:50 - loss: 0.3369
3840/6530 [================>.............] - ETA: 0s - loss: 0.0989
 416/6530 [>.............................] - ETA: 9s - loss: 0.1763  
6530/6530 [==============================] - 0s 15us/step - loss: 0.0959 - val_loss: 0.0660
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0784
 784/6530 [==>...........................] - ETA: 4s - loss: 0.1474
3840/6530 [================>.............] - ETA: 0s - loss: 0.0798
1168/6530 [====>.........................] - ETA: 3s - loss: 0.1289
6530/6530 [==============================] - 0s 15us/step - loss: 0.0780 - val_loss: 0.0623

1568/6530 [======>.......................] - ETA: 2s - loss: 0.1148
1968/6530 [========>.....................] - ETA: 1s - loss: 0.1107
2352/6530 [=========>....................] - ETA: 1s - loss: 0.1067
2736/6530 [===========>..................] - ETA: 1s - loss: 0.1019
# training | RMSE: 0.2482, MAE: 0.2002
worker 0  xfile  [31, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4298233179129576}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 35, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.24824943935478713, 'rmse': 0.24824943935478713, 'mae': 0.2002297380366794, 'early_stop': False}
vggnet done  0

# training | RMSE: 0.2167, MAE: 0.1799
worker 1  xfile  [30, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4842611817250523}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12788149052012576}, 'layer_3_size': 51, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.2167236106799172, 'rmse': 0.2167236106799172, 'mae': 0.17990850529852317, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  89 | activation: relu    | extras: batchnorm 
layer 2 | size:   2 | activation: sigmoid | extras: dropout - rate: 19.7% 
layer 3 | size:  85 | activation: relu    | extras: dropout - rate: 20.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f03d4a084e0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 1:07 - loss: 0.6908
3136/6530 [=============>................] - ETA: 1s - loss: 0.0978
1216/6530 [====>.........................] - ETA: 3s - loss: 0.5581  
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0947
2432/6530 [==========>...................] - ETA: 1s - loss: 0.4371
3952/6530 [=================>............] - ETA: 0s - loss: 0.0922
3584/6530 [===============>..............] - ETA: 0s - loss: 0.3645
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0897
4736/6530 [====================>.........] - ETA: 0s - loss: 0.3240
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0879
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2965
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0856
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0842
6530/6530 [==============================] - 1s 158us/step - loss: 0.2844 - val_loss: 0.1532
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1646
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0830
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1762
6512/6530 [============================>.] - ETA: 0s - loss: 0.0809
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1687
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1658
6530/6530 [==============================] - 1s 227us/step - loss: 0.0809 - val_loss: 0.1317
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1304
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1637
 400/6530 [>.............................] - ETA: 0s - loss: 0.0523
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1615
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0523
6530/6530 [==============================] - 0s 45us/step - loss: 0.1602 - val_loss: 0.1455
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1597
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0519
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1459
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0526
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1440
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0526
3776/6530 [================>.............] - ETA: 0s - loss: 0.1428
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0525
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1424
2960/6530 [============>.................] - ETA: 0s - loss: 0.0522
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1405
6530/6530 [==============================] - 0s 43us/step - loss: 0.1396 - val_loss: 0.1339

3408/6530 [==============>...............] - ETA: 0s - loss: 0.0513
3840/6530 [================>.............] - ETA: 0s - loss: 0.0508
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0506
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0504
# training | RMSE: 0.1697, MAE: 0.1319
worker 1  xfile  [33, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19732678881540575}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20791950143380367}, 'layer_3_size': 85, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1696946370675339, 'rmse': 0.1696946370675339, 'mae': 0.13192183669362484, 'early_stop': False}
vggnet done  1

5024/6530 [======================>.......] - ETA: 0s - loss: 0.0501
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0494
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0492
6320/6530 [============================>.] - ETA: 0s - loss: 0.0486
6530/6530 [==============================] - 1s 127us/step - loss: 0.0481 - val_loss: 0.1293
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1136
 384/6530 [>.............................] - ETA: 0s - loss: 0.0412
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0412
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0412
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0411
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0397
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0412
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0407
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0410
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0408
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0418
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0410
3040/6530 [============>.................] - ETA: 0s - loss: 0.0407
3248/6530 [=============>................] - ETA: 0s - loss: 0.0408
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0406
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0398
3904/6530 [================>.............] - ETA: 0s - loss: 0.0399
4128/6530 [=================>............] - ETA: 0s - loss: 0.0401
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0398
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0403
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0399
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0395
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0394
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0393
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0389
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0388
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0386
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0385
6432/6530 [============================>.] - ETA: 0s - loss: 0.0384
6530/6530 [==============================] - 2s 237us/step - loss: 0.0383 - val_loss: 0.0864

# training | RMSE: 0.2917, MAE: 0.2556
worker 2  xfile  [32, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24867068309885548}, 'layer_1_size': 95, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4626676222228512}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.29167610030535734, 'rmse': 0.29167610030535734, 'mae': 0.2555683332888808, 'early_stop': False}
vggnet done  2
all of workers have been done
calculation finished
#2 epoch=3.0 loss={'loss': 0.204143968901908, 'rmse': 0.204143968901908, 'mae': 0.16292964608194271, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42824272704555977}, 'layer_2_size': 62, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 2, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4527782389368925}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#0 epoch=3.0 loss={'loss': 0.41050688405457686, 'rmse': 0.41050688405457686, 'mae': 0.34840252525775867, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.44784773985102067}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 26, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44033065163415996}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#3 epoch=3.0 loss={'loss': 0.24197744404965188, 'rmse': 0.24197744404965188, 'mae': 0.18997983575514762, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23426686206342065}, 'layer_2_size': 50, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
#1 epoch=3.0 loss={'loss': 0.1246583321505086, 'rmse': 0.1246583321505086, 'mae': 0.09428940857051811, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20166402653594076}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26587831508262944}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1965905869310106}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#5 epoch=3.0 loss={'loss': 0.30062759288945645, 'rmse': 0.30062759288945645, 'mae': 0.25664241012793504, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 85, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4913128604900715}, 'layer_4_size': 39, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#7 epoch=3.0 loss={'loss': 0.2626450274448594, 'rmse': 0.2626450274448594, 'mae': 0.21481343787837748, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 5, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3792106070229092}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#6 epoch=3.0 loss={'loss': 0.3070327174008272, 'rmse': 0.3070327174008272, 'mae': 0.24757249059226763, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18659663112190664}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#4 epoch=3.0 loss={'loss': 0.13529093519101165, 'rmse': 0.13529093519101165, 'mae': 0.10714273773227759, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28944113514402675}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3686040864853135}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17200213178174123}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#8 epoch=3.0 loss={'loss': 0.19733965335179443, 'rmse': 0.19733965335179443, 'mae': 0.1584925232294999, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 63, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 23, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#10 epoch=3.0 loss={'loss': 0.19263856303784027, 'rmse': 0.19263856303784027, 'mae': 0.15396825552494034, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4870497906088097}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11777333135437362}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#9 epoch=3.0 loss={'loss': 0.17683298409967943, 'rmse': 0.17683298409967943, 'mae': 0.13921062325398847, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4538953694584704}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1250234434350652}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#13 epoch=3.0 loss={'loss': 0.2634800953671358, 'rmse': 0.2634800953671358, 'mae': 0.20813111618510202, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 65, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.33651217972212344}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#12 epoch=3.0 loss={'loss': 0.2035386365865549, 'rmse': 0.2035386365865549, 'mae': 0.16288916485428703, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#14 epoch=3.0 loss={'loss': 0.21192832302510958, 'rmse': 0.21192832302510958, 'mae': 0.1640908843524241, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16610695119268348}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.493726340309126}, 'layer_2_size': 60, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.46088760222654934}, 'layer_3_size': 53, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#11 epoch=3.0 loss={'loss': 0.21670707544286386, 'rmse': 0.21670707544286386, 'mae': 0.17289221541311903, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 57, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3199343307861644}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#16 epoch=3.0 loss={'loss': 0.21153139541001023, 'rmse': 0.21153139541001023, 'mae': 0.17340308373832117, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3453854105957833}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3019121415717443}, 'layer_2_size': 33, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17041083125845424}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4623567061254169}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#17 epoch=3.0 loss={'loss': 0.21092530600686765, 'rmse': 0.21092530600686765, 'mae': 0.16539107486495377, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3828312175948112}, 'layer_5_size': 30, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#15 epoch=3.0 loss={'loss': 0.3943639949389148, 'rmse': 0.3943639949389148, 'mae': 0.3529843331665071, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2545311629703454}, 'layer_3_size': 19, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10875526387691031}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3486207689229829}, 'layer_5_size': 26, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
#19 epoch=3.0 loss={'loss': 0.261234941777578, 'rmse': 0.261234941777578, 'mae': 0.20049671080390383, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.32981446545793247}, 'layer_1_size': 62, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4310542042589266}, 'layer_2_size': 21, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#18 epoch=3.0 loss={'loss': 0.20612878842750368, 'rmse': 0.20612878842750368, 'mae': 0.16894724744243375, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15117667146973882}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.445130852344082}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.42344179496252854}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1904029235723187}, 'layer_4_size': 43, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#21 epoch=3.0 loss={'loss': 0.3296375706945452, 'rmse': 0.3296375706945452, 'mae': 0.28711168289035893, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20519868061145666}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 87, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23524451736866495}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38441826406735924}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#20 epoch=3.0 loss={'loss': 0.6169146064580298, 'rmse': 0.6169146064580298, 'mae': 0.5578802605377515, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45502672490880824}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1486492480861062}, 'layer_4_size': 80, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#23 epoch=3.0 loss={'loss': 0.21338948295348614, 'rmse': 0.21338948295348614, 'mae': 0.17194850006813128, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30076382776869837}, 'layer_1_size': 8, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 5, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#24 epoch=3.0 loss={'loss': 0.2520148998259068, 'rmse': 0.2520148998259068, 'mae': 0.2059621243723038, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 86, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.29138166195004556}, 'layer_3_size': 74, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1874248444420219}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14069231273407207}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#25 epoch=3.0 loss={'loss': 0.2017358120554465, 'rmse': 0.2017358120554465, 'mae': 0.15786389616300575, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4970046503928497}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4171571693762234}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 49, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#22 epoch=3.0 loss={'loss': 0.21663935605955045, 'rmse': 0.21663935605955045, 'mae': 0.17647010995138768, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3982749725810574}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.44222094159696235}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2289264129983852}, 'layer_3_size': 31, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#27 epoch=3.0 loss={'loss': 0.23724761464551822, 'rmse': 0.23724761464551822, 'mae': 0.17936543218375842, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#28 epoch=3.0 loss={'loss': 0.25760568283909674, 'rmse': 0.25760568283909674, 'mae': 0.21123783191360906, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43102617271254307}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27301050218054207}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2650511587729206}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#26 epoch=3.0 loss={'loss': 0.18681886169284156, 'rmse': 0.18681886169284156, 'mae': 0.14959481851775777, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#29 epoch=3.0 loss={'loss': 0.20421430160193707, 'rmse': 0.20421430160193707, 'mae': 0.16634380667609455, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15264298881590707}, 'layer_1_size': 2, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 100, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17174719681706674}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#30 epoch=3.0 loss={'loss': 0.2167236106799172, 'rmse': 0.2167236106799172, 'mae': 0.17990850529852317, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4842611817250523}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12788149052012576}, 'layer_3_size': 51, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#31 epoch=3.0 loss={'loss': 0.24824943935478713, 'rmse': 0.24824943935478713, 'mae': 0.2002297380366794, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4298233179129576}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 35, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#33 epoch=3.0 loss={'loss': 0.1696946370675339, 'rmse': 0.1696946370675339, 'mae': 0.13192183669362484, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19732678881540575}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20791950143380367}, 'layer_3_size': 85, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#32 epoch=3.0 loss={'loss': 0.29167610030535734, 'rmse': 0.29167610030535734, 'mae': 0.2555683332888808, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24867068309885548}, 'layer_1_size': 95, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4626676222228512}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}
get a list [results] of length 155
get a list [loss] of length 34
get a list [val_loss] of length 34
length of indices is (1, 4, 33, 9, 26, 10, 8, 25, 12, 2, 29, 18, 17, 16, 14, 23, 22, 11, 30, 27, 3, 31, 24, 28, 19, 7, 13, 32, 5, 6, 21, 15, 0, 20)
length of indices is 34
length of T is 34
newly formed T structure is:[[0, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20166402653594076}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26587831508262944}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1965905869310106}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [1, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28944113514402675}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3686040864853135}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17200213178174123}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [2, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19732678881540575}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20791950143380367}, 'layer_3_size': 85, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [3, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4538953694584704}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1250234434350652}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [4, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [5, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4870497906088097}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11777333135437362}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [6, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 63, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 23, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [7, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4970046503928497}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4171571693762234}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 49, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [8, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [9, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42824272704555977}, 'layer_2_size': 62, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 2, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4527782389368925}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [10, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15264298881590707}, 'layer_1_size': 2, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 100, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17174719681706674}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]] 

*** 11.333333333333332 configurations x 9.0 iterations each

33 | Fri Sep 28 01:21:36 2018 | lowest loss so far: 0.0705 (run 0)

{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  96 | activation: relu    | extras: dropout - rate: 20.2% 
layer 2 | size:  55 | activation: relu    | extras: batchnorm 
layer 3 | size:  93 | activation: tanh    | extras: dropout - rate: 26.6% 
layer 4 | size:  86 | activation: sigmoid | extras: dropout - rate: 19.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 3:36 - loss: 0.7709
 448/6530 [=>............................] - ETA: 15s - loss: 0.2857 
 992/6530 [===>..........................] - ETA: 6s - loss: 0.2294 {'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  89 | activation: relu    | extras: batchnorm 
layer 2 | size:   2 | activation: sigmoid | extras: dropout - rate: 19.7% 
layer 3 | size:  85 | activation: relu    | extras: dropout - rate: 20.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9f4c88>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 1:56 - loss: 0.6761
1568/6530 [======>.......................] - ETA: 3s - loss: 0.2063
1088/6530 [===>..........................] - ETA: 6s - loss: 0.5527  {'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  96 | activation: relu    | extras: dropout - rate: 28.9% 
layer 2 | size:  85 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  48 | activation: tanh    | extras: None 
layer 4 | size:  59 | activation: relu    | extras: dropout - rate: 36.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 7:27 - loss: 0.6945
2176/6530 [========>.....................] - ETA: 2s - loss: 0.1958
2240/6530 [=========>....................] - ETA: 2s - loss: 0.4344
 272/6530 [>.............................] - ETA: 26s - loss: 0.3721 
2784/6530 [===========>..................] - ETA: 1s - loss: 0.1877
3392/6530 [==============>...............] - ETA: 1s - loss: 0.3625
 480/6530 [=>............................] - ETA: 15s - loss: 0.3352
3392/6530 [==============>...............] - ETA: 1s - loss: 0.1819
4544/6530 [===================>..........] - ETA: 0s - loss: 0.3199
 736/6530 [==>...........................] - ETA: 9s - loss: 0.3083 
4000/6530 [=================>............] - ETA: 0s - loss: 0.1773
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2914
 960/6530 [===>..........................] - ETA: 7s - loss: 0.2904
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1730
6530/6530 [==============================] - 2s 230us/step - loss: 0.2775 - val_loss: 0.1566

1216/6530 [====>.........................] - ETA: 5s - loss: 0.2790
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1687Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1766
1472/6530 [=====>........................] - ETA: 4s - loss: 0.2744
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1641
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1658
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1605
1728/6530 [======>.......................] - ETA: 4s - loss: 0.2659
6464/6530 [============================>.] - ETA: 0s - loss: 0.1632
1984/6530 [========>.....................] - ETA: 3s - loss: 0.2606
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1576
6530/6530 [==============================] - 2s 260us/step - loss: 0.1631 - val_loss: 0.1327
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1451
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1541
2224/6530 [=========>....................] - ETA: 3s - loss: 0.2550
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1353
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1517
2480/6530 [==========>...................] - ETA: 2s - loss: 0.2521
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1312
6530/6530 [==============================] - 0s 47us/step - loss: 0.1505 - val_loss: 0.1454
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1282
2736/6530 [===========>..................] - ETA: 2s - loss: 0.2484
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1325
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1393
2992/6530 [============>.................] - ETA: 2s - loss: 0.2433
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1319
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1359
3248/6530 [=============>................] - ETA: 1s - loss: 0.2392
3136/6530 [=============>................] - ETA: 0s - loss: 0.1313
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1343
3488/6530 [===============>..............] - ETA: 1s - loss: 0.2365
3712/6530 [================>.............] - ETA: 0s - loss: 0.1314
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1337
3728/6530 [================>.............] - ETA: 1s - loss: 0.2328
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1305
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1327
3984/6530 [=================>............] - ETA: 1s - loss: 0.2297
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1311
6530/6530 [==============================] - 0s 44us/step - loss: 0.1323 - val_loss: 0.1372
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1037
4240/6530 [==================>...........] - ETA: 1s - loss: 0.2275
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1301
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1264
4496/6530 [===================>..........] - ETA: 0s - loss: 0.2256
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1299
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1247
4752/6530 [====================>.........] - ETA: 0s - loss: 0.2229
6530/6530 [==============================] - 1s 86us/step - loss: 0.1296 - val_loss: 0.1188
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1334
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1247
5008/6530 [======================>.......] - ETA: 0s - loss: 0.2205
 640/6530 [=>............................] - ETA: 0s - loss: 0.1257
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1244
5264/6530 [=======================>......] - ETA: 0s - loss: 0.2180
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1214
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1244
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2165
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1206
6530/6530 [==============================] - 0s 46us/step - loss: 0.1242 - val_loss: 0.1317
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1107
5792/6530 [=========================>....] - ETA: 0s - loss: 0.2154
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1217
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1186
6032/6530 [==========================>...] - ETA: 0s - loss: 0.2138
3168/6530 [=============>................] - ETA: 0s - loss: 0.1204
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1205
6256/6530 [===========================>..] - ETA: 0s - loss: 0.2125
3808/6530 [================>.............] - ETA: 0s - loss: 0.1204
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1205
6512/6530 [============================>.] - ETA: 0s - loss: 0.2106
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1206
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1203
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1206
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1199
6530/6530 [==============================] - 3s 389us/step - loss: 0.2105 - val_loss: 0.1735
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1672
6530/6530 [==============================] - 0s 46us/step - loss: 0.1197 - val_loss: 0.1332
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1014
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1207
 272/6530 [>.............................] - ETA: 1s - loss: 0.1802
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1175
6464/6530 [============================>.] - ETA: 0s - loss: 0.1210
6530/6530 [==============================] - 1s 83us/step - loss: 0.1209 - val_loss: 0.1043
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1371
 544/6530 [=>............................] - ETA: 1s - loss: 0.1667
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1166
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1128
 800/6530 [==>...........................] - ETA: 1s - loss: 0.1704
3712/6530 [================>.............] - ETA: 0s - loss: 0.1166
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1130
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1694
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1161
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1126
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1677
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1159
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1145
6530/6530 [==============================] - 0s 45us/step - loss: 0.1156 - val_loss: 0.1309
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0909
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1675
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1142
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1112
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1668
3936/6530 [=================>............] - ETA: 0s - loss: 0.1154
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1133
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1661
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1162
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1136
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1654
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1155
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1130
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1646
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1149
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1124
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1636
6530/6530 [==============================] - 0s 45us/step - loss: 0.1127 - val_loss: 0.1326
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0996
6528/6530 [============================>.] - ETA: 0s - loss: 0.1154
3024/6530 [============>.................] - ETA: 0s - loss: 0.1622
6530/6530 [==============================] - 1s 82us/step - loss: 0.1154 - val_loss: 0.1057
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1208
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1108
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1610
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1108
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1130
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1611
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1110
3712/6530 [================>.............] - ETA: 0s - loss: 0.1119
3776/6530 [================>.............] - ETA: 0s - loss: 0.1602
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1115
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1117
4032/6530 [=================>............] - ETA: 0s - loss: 0.1606
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1123
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1110
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1597
3232/6530 [=============>................] - ETA: 0s - loss: 0.1116
6530/6530 [==============================] - 0s 44us/step - loss: 0.1107 - val_loss: 0.1280
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0863
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1593
3904/6530 [================>.............] - ETA: 0s - loss: 0.1126
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1071
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1594
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1133
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1095
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1591
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1129
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1097
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1585
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1125
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1076
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1579
6464/6530 [============================>.] - ETA: 0s - loss: 0.1124
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1085
6530/6530 [==============================] - 1s 83us/step - loss: 0.1123 - val_loss: 0.0911
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0949
6530/6530 [==============================] - 0s 45us/step - loss: 0.1081 - val_loss: 0.1335

5872/6530 [=========================>....] - ETA: 0s - loss: 0.1577
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1060
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1567
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1071
6416/6530 [============================>.] - ETA: 0s - loss: 0.1561
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1076
6530/6530 [==============================] - 1s 206us/step - loss: 0.1564 - val_loss: 0.1231
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1362
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1079
 288/6530 [>.............................] - ETA: 1s - loss: 0.1466
3264/6530 [=============>................] - ETA: 0s - loss: 0.1070
 544/6530 [=>............................] - ETA: 1s - loss: 0.1425
3936/6530 [=================>............] - ETA: 0s - loss: 0.1074
 800/6530 [==>...........................] - ETA: 1s - loss: 0.1410
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1080
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1434
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1082
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1439
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1082
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1419
6528/6530 [============================>.] - ETA: 0s - loss: 0.1085
6530/6530 [==============================] - 1s 81us/step - loss: 0.1085 - val_loss: 0.0942
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1224
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1414
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1107
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1414
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1071
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1418
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1074
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1428
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1065
2896/6530 [============>.................] - ETA: 0s - loss: 0.1422
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1067
3168/6530 [=============>................] - ETA: 0s - loss: 0.1415
4096/6530 [=================>............] - ETA: 0s - loss: 0.1070
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1423
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1073
3712/6530 [================>.............] - ETA: 0s - loss: 0.1420
# training | RMSE: 0.1568, MAE: 0.1289
worker 2  xfile  [2, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19732678881540575}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20791950143380367}, 'layer_3_size': 85, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.15680852390421243, 'rmse': 0.15680852390421243, 'mae': 0.12888819905021745, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  15 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  68 | activation: tanh    | extras: None 
layer 3 | size:  63 | activation: relu    | extras: dropout - rate: 45.4% 
layer 4 | size:   7 | activation: sigmoid | extras: dropout - rate: 12.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9f4f28>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 35s - loss: 0.6517
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1074
3968/6530 [=================>............] - ETA: 0s - loss: 0.1425
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1242 
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1074
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1429
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0961
6530/6530 [==============================] - 1s 79us/step - loss: 0.1073 - val_loss: 0.0875
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1285
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1425
4096/6530 [=================>............] - ETA: 0s - loss: 0.0852
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1053
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1425
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0770
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1031
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1421
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1037
6530/6530 [==============================] - 1s 99us/step - loss: 0.0728 - val_loss: 0.0393

5264/6530 [=======================>......] - ETA: 0s - loss: 0.1418Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0367
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1043
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1416
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0466
3264/6530 [=============>................] - ETA: 0s - loss: 0.1035
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1415
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0458
3904/6530 [================>.............] - ETA: 0s - loss: 0.1041
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1414
3904/6530 [================>.............] - ETA: 0s - loss: 0.0445
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1043
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0433
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1414
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1044
6530/6530 [==============================] - 0s 41us/step - loss: 0.0428 - val_loss: 0.0318
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0341
6530/6530 [==============================] - 1s 203us/step - loss: 0.1413 - val_loss: 0.1071
Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1260
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1047
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0379
 272/6530 [>.............................] - ETA: 1s - loss: 0.1312
6432/6530 [============================>.] - ETA: 0s - loss: 0.1052
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0378
6530/6530 [==============================] - 1s 84us/step - loss: 0.1051 - val_loss: 0.0856
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1085
 528/6530 [=>............................] - ETA: 1s - loss: 0.1265
4032/6530 [=================>............] - ETA: 0s - loss: 0.0362
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1039
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1274
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0360
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1037
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1287
6530/6530 [==============================] - 0s 40us/step - loss: 0.0361 - val_loss: 0.0253
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0381
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1034
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1307
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0320
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1038
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1281
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0328
3168/6530 [=============>................] - ETA: 0s - loss: 0.1031
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1284
3904/6530 [================>.............] - ETA: 0s - loss: 0.0326
3808/6530 [================>.............] - ETA: 0s - loss: 0.1039
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1286
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0323
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1035
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1281
6464/6530 [============================>.] - ETA: 0s - loss: 0.0322
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1040
6530/6530 [==============================] - 0s 42us/step - loss: 0.0322 - val_loss: 0.0221
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0303
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1279
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1038
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0299
2864/6530 [============>.................] - ETA: 0s - loss: 0.1275
6336/6530 [============================>.] - ETA: 0s - loss: 0.1043
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0308
3120/6530 [=============>................] - ETA: 0s - loss: 0.1269
6530/6530 [==============================] - 1s 83us/step - loss: 0.1042 - val_loss: 0.0868

3904/6530 [================>.............] - ETA: 0s - loss: 0.0297
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1277
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0298
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1278
6530/6530 [==============================] - 0s 41us/step - loss: 0.0300 - val_loss: 0.0210

3904/6530 [================>.............] - ETA: 0s - loss: 0.1276Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0269
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1281
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0281
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1280
2944/6530 [============>.................] - ETA: 0s - loss: 0.0282
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1285
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0278
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1282
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0280
6530/6530 [==============================] - 0s 38us/step - loss: 0.0282 - val_loss: 0.0195
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0272
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1283
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0267
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1279
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0267
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1274
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1274
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0265
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0268
6336/6530 [============================>.] - ETA: 0s - loss: 0.1271
6530/6530 [==============================] - 0s 38us/step - loss: 0.0268 - val_loss: 0.0184
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0285
6530/6530 [==============================] - 1s 201us/step - loss: 0.1275 - val_loss: 0.0955
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0934
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0262
 288/6530 [>.............................] - ETA: 1s - loss: 0.1157
2880/6530 [============>.................] - ETA: 0s - loss: 0.0267
 544/6530 [=>............................] - ETA: 1s - loss: 0.1138
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0259
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1164
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0261
1040/6530 [===>..........................] - ETA: 1s - loss: 0.1183
6530/6530 [==============================] - 0s 38us/step - loss: 0.0262 - val_loss: 0.0185
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0262
1264/6530 [====>.........................] - ETA: 1s - loss: 0.1192
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0244
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1190
2944/6530 [============>.................] - ETA: 0s - loss: 0.0251
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1184
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0244
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1173
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0246
6530/6530 [==============================] - 0s 38us/step - loss: 0.0247 - val_loss: 0.0176

2304/6530 [=========>....................] - ETA: 0s - loss: 0.1174
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1189
2848/6530 [============>.................] - ETA: 0s - loss: 0.1187
3136/6530 [=============>................] - ETA: 0s - loss: 0.1178
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1188
3712/6530 [================>.............] - ETA: 0s - loss: 0.1182
3984/6530 [=================>............] - ETA: 0s - loss: 0.1181
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1184
# training | RMSE: 0.1255, MAE: 0.0986
worker 2  xfile  [3, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4538953694584704}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1250234434350652}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.1255249415905743, 'rmse': 0.1255249415905743, 'mae': 0.0985766344446653, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  11 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f03f812f518>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 30s - loss: 0.7140
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1189
1280/6530 [====>.........................] - ETA: 0s - loss: 0.3996 
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1192
2656/6530 [===========>..................] - ETA: 0s - loss: 0.3219
# training | RMSE: 0.1075, MAE: 0.0804
worker 0  xfile  [0, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20166402653594076}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26587831508262944}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1965905869310106}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.10752888348474429, 'rmse': 0.10752888348474429, 'mae': 0.08040341390386829, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:   3 | activation: tanh    | extras: batchnorm 
layer 2 | size:  45 | activation: relu    | extras: batchnorm 
layer 3 | size:  57 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  81 | activation: sigmoid | extras: None 
layer 5 | size:  14 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9f4eb8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 1:07 - loss: 0.8278
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1193
3904/6530 [================>.............] - ETA: 0s - loss: 0.2706
 832/6530 [==>...........................] - ETA: 4s - loss: 0.6234  
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1189
5280/6530 [=======================>......] - ETA: 0s - loss: 0.2289
1600/6530 [======>.......................] - ETA: 2s - loss: 0.4777
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1182
2368/6530 [=========>....................] - ETA: 1s - loss: 0.3793
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1183
6530/6530 [==============================] - 0s 67us/step - loss: 0.2003 - val_loss: 0.0774
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0809
3072/6530 [=============>................] - ETA: 0s - loss: 0.3157
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1184
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0618
3840/6530 [================>.............] - ETA: 0s - loss: 0.2650
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1182
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0603
4608/6530 [====================>.........] - ETA: 0s - loss: 0.2296
4064/6530 [=================>............] - ETA: 0s - loss: 0.0568
6530/6530 [==============================] - 1s 203us/step - loss: 0.1182 - val_loss: 0.1030
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0807
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2035
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0552
 272/6530 [>.............................] - ETA: 1s - loss: 0.1132
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1833
6530/6530 [==============================] - 0s 40us/step - loss: 0.0540 - val_loss: 0.0495
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0361
 512/6530 [=>............................] - ETA: 1s - loss: 0.1100
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0440
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1104
6530/6530 [==============================] - 1s 184us/step - loss: 0.1753 - val_loss: 0.0455
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0451
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0466
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1123
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0472
3904/6530 [================>.............] - ETA: 0s - loss: 0.0454
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1134
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0449
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0450
1552/6530 [======>.......................] - ETA: 0s - loss: 0.1118
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0444
6464/6530 [============================>.] - ETA: 0s - loss: 0.0444
6530/6530 [==============================] - 0s 41us/step - loss: 0.0443 - val_loss: 0.0442
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0428
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1122
3136/6530 [=============>................] - ETA: 0s - loss: 0.0437
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0410
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1119
3904/6530 [================>.............] - ETA: 0s - loss: 0.0431
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0402
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1127
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0428
4000/6530 [=================>............] - ETA: 0s - loss: 0.0404
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1124
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0431
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0401
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1118
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0428
3056/6530 [=============>................] - ETA: 0s - loss: 0.1114
6530/6530 [==============================] - 0s 40us/step - loss: 0.0398 - val_loss: 0.0400
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0541
6530/6530 [==============================] - 0s 70us/step - loss: 0.0429 - val_loss: 0.0429
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0408
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1113
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0397
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0430
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1116
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0383
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0404
3824/6530 [================>.............] - ETA: 0s - loss: 0.1115
4064/6530 [=================>............] - ETA: 0s - loss: 0.0377
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0401
4080/6530 [=================>............] - ETA: 0s - loss: 0.1116
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0360
3136/6530 [=============>................] - ETA: 0s - loss: 0.0402
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1115
6530/6530 [==============================] - 0s 40us/step - loss: 0.0359 - val_loss: 0.0360
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0417
3904/6530 [================>.............] - ETA: 0s - loss: 0.0402
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1122
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0330
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0405
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1125
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0347
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0401
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1129
3968/6530 [=================>............] - ETA: 0s - loss: 0.0333
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0397
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1127
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0330
6530/6530 [==============================] - 0s 69us/step - loss: 0.0395 - val_loss: 0.0413
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0270
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1124
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0342
6530/6530 [==============================] - 0s 40us/step - loss: 0.0326 - val_loss: 0.0337
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0451
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1120
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0372
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0306
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1118
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0364
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0302
6464/6530 [============================>.] - ETA: 0s - loss: 0.1117
4128/6530 [=================>............] - ETA: 0s - loss: 0.0297
3200/6530 [=============>................] - ETA: 0s - loss: 0.0363
6530/6530 [==============================] - 1s 205us/step - loss: 0.1119 - val_loss: 0.0948
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1301
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0298
3968/6530 [=================>............] - ETA: 0s - loss: 0.0361
 288/6530 [>.............................] - ETA: 1s - loss: 0.1128
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0358
6530/6530 [==============================] - 0s 39us/step - loss: 0.0299 - val_loss: 0.0318
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0264
 560/6530 [=>............................] - ETA: 1s - loss: 0.1119
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0355
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0283
 816/6530 [==>...........................] - ETA: 1s - loss: 0.1105
6336/6530 [============================>.] - ETA: 0s - loss: 0.0355
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0294
6530/6530 [==============================] - 0s 68us/step - loss: 0.0355 - val_loss: 0.0380
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0404
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1140
4096/6530 [=================>............] - ETA: 0s - loss: 0.0282
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0393
1328/6530 [=====>........................] - ETA: 1s - loss: 0.1138
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0279
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0367
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1129
6530/6530 [==============================] - 0s 41us/step - loss: 0.0278 - val_loss: 0.0301
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0130
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0353
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1133
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0284
3008/6530 [============>.................] - ETA: 0s - loss: 0.0355
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1122
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0276
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1113
3776/6530 [================>.............] - ETA: 0s - loss: 0.0355
3968/6530 [=================>............] - ETA: 0s - loss: 0.0266
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1113
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0352
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0262
2848/6530 [============>.................] - ETA: 0s - loss: 0.1104
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0342
6530/6530 [==============================] - 0s 40us/step - loss: 0.0262 - val_loss: 0.0283

3104/6530 [=============>................] - ETA: 0s - loss: 0.1097
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0339
6530/6530 [==============================] - 0s 72us/step - loss: 0.0339 - val_loss: 0.0348

3360/6530 [==============>...............] - ETA: 0s - loss: 0.1100Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0289
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1102
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0332
3888/6530 [================>.............] - ETA: 0s - loss: 0.1102
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0325
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1101
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0330
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1095
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0328
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1105
4096/6530 [=================>............] - ETA: 0s - loss: 0.0326
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1104
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0325
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1102
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0322
# training | RMSE: 0.1586, MAE: 0.1241
worker 2  xfile  [5, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4870497906088097}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11777333135437362}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.15862556303552547, 'rmse': 0.15862556303552547, 'mae': 0.12410039825840798, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  72 | activation: relu    | extras: None 
layer 2 | size:  29 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03f80c4b70>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 1:18 - loss: 0.6609
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1099
6336/6530 [============================>.] - ETA: 0s - loss: 0.0325
6530/6530 [==============================] - 0s 69us/step - loss: 0.0324 - val_loss: 0.0377

 544/6530 [=>............................] - ETA: 2s - loss: 0.1209  Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0296
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1095
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0949
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0336
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1095
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0833
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0326
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1094
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0789
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0321
6400/6530 [============================>.] - ETA: 0s - loss: 0.1091
2928/6530 [============>.................] - ETA: 0s - loss: 0.0756
3136/6530 [=============>................] - ETA: 0s - loss: 0.0320
6530/6530 [==============================] - 1s 209us/step - loss: 0.1091 - val_loss: 0.0842
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0879
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0723
3840/6530 [================>.............] - ETA: 0s - loss: 0.0310
 256/6530 [>.............................] - ETA: 1s - loss: 0.1015
4096/6530 [=================>............] - ETA: 0s - loss: 0.0693
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0308
 512/6530 [=>............................] - ETA: 1s - loss: 0.1000
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0666
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0309
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1027
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0646
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0310
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1050
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0630
6530/6530 [==============================] - 0s 72us/step - loss: 0.0312 - val_loss: 0.0363
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0341
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1063
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0294
6530/6530 [==============================] - 1s 122us/step - loss: 0.0617 - val_loss: 0.0468
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0594
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1059
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0303
 640/6530 [=>............................] - ETA: 0s - loss: 0.0479
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1056
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0306
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0455
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1052
3072/6530 [=============>................] - ETA: 0s - loss: 0.0305
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0467
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1053
3840/6530 [================>.............] - ETA: 0s - loss: 0.0309
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0464
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1050
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0308
2896/6530 [============>.................] - ETA: 0s - loss: 0.0460
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1044
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0307
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0459
3072/6530 [=============>................] - ETA: 0s - loss: 0.1035
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0305
4096/6530 [=================>............] - ETA: 0s - loss: 0.0460
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1034
6530/6530 [==============================] - 0s 70us/step - loss: 0.0306 - val_loss: 0.0343
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0203
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0457
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1038
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0296
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0457
3856/6530 [================>.............] - ETA: 0s - loss: 0.1041
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0289
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0451
4112/6530 [=================>............] - ETA: 0s - loss: 0.1041
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0298
6480/6530 [============================>.] - ETA: 0s - loss: 0.0449
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1043
3200/6530 [=============>................] - ETA: 0s - loss: 0.0292
6530/6530 [==============================] - 1s 90us/step - loss: 0.0450 - val_loss: 0.0437
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0445
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1050
3968/6530 [=================>............] - ETA: 0s - loss: 0.0293
 560/6530 [=>............................] - ETA: 0s - loss: 0.0432
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1050
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0294
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0421
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1049
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0291
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0431
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1047
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0293
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0436
6530/6530 [==============================] - 0s 69us/step - loss: 0.0293 - val_loss: 0.0335

5680/6530 [=========================>....] - ETA: 0s - loss: 0.1045
2912/6530 [============>.................] - ETA: 0s - loss: 0.0435
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1046
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0437
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1048
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0435
6464/6530 [============================>.] - ETA: 0s - loss: 0.1046
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0436
6530/6530 [==============================] - 1s 205us/step - loss: 0.1046 - val_loss: 0.0864
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0769
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0433
 272/6530 [>.............................] - ETA: 1s - loss: 0.1037
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0430
 528/6530 [=>............................] - ETA: 1s - loss: 0.1012
6530/6530 [==============================] - 1s 89us/step - loss: 0.0430 - val_loss: 0.0425
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0268
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1015
 576/6530 [=>............................] - ETA: 0s - loss: 0.0411
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1031
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0424
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1038
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0414
1488/6530 [=====>........................] - ETA: 1s - loss: 0.1031
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0410
1744/6530 [=======>......................] - ETA: 1s - loss: 0.1035
3040/6530 [============>.................] - ETA: 0s - loss: 0.0418
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1028
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0421
2256/6530 [=========>....................] - ETA: 0s - loss: 0.1024
# training | RMSE: 0.1706, MAE: 0.1359
worker 0  xfile  [4, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.17062629871549945, 'rmse': 0.17062629871549945, 'mae': 0.13594530985060033, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  52 | activation: relu    | extras: dropout - rate: 49.7% 
layer 2 | size:  44 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f03c469ed30>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 23s - loss: 0.7187
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0421
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1025
2048/6530 [========>.....................] - ETA: 0s - loss: 0.3631 
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0421
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1020
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2988
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0419
3040/6530 [============>.................] - ETA: 0s - loss: 0.1015
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2682
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0415
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1017
6530/6530 [==============================] - 0s 66us/step - loss: 0.2660 - val_loss: 0.1728
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1622
6530/6530 [==============================] - 1s 88us/step - loss: 0.0415 - val_loss: 0.0413
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0393
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1022
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1901
 560/6530 [=>............................] - ETA: 0s - loss: 0.0383
3792/6530 [================>.............] - ETA: 0s - loss: 0.1019
4096/6530 [=================>............] - ETA: 0s - loss: 0.1906
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0394
4048/6530 [=================>............] - ETA: 0s - loss: 0.1022
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1866
6530/6530 [==============================] - 0s 26us/step - loss: 0.1860 - val_loss: 0.1630
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1676
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0392
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1022
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1746
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0392
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1022
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1732
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0385
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1026
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1745
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0392
6530/6530 [==============================] - 0s 26us/step - loss: 0.1745 - val_loss: 0.1594
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1736
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1023
4016/6530 [=================>............] - ETA: 0s - loss: 0.0394
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1689
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1023
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0397
3904/6530 [================>.............] - ETA: 0s - loss: 0.1683
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1019
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0402
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1687
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1019
6530/6530 [==============================] - 0s 27us/step - loss: 0.1679 - val_loss: 0.1541
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1605
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0406
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1019
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1647
6432/6530 [============================>.] - ETA: 0s - loss: 0.0401
6384/6530 [============================>.] - ETA: 0s - loss: 0.1021
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1658
6530/6530 [==============================] - 1s 91us/step - loss: 0.0399 - val_loss: 0.0402
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0513
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1668
 576/6530 [=>............................] - ETA: 0s - loss: 0.0356
6530/6530 [==============================] - 1s 208us/step - loss: 0.1023 - val_loss: 0.0824

6530/6530 [==============================] - 0s 26us/step - loss: 0.1659 - val_loss: 0.1514
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1631
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0399
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1621
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0394
4032/6530 [=================>............] - ETA: 0s - loss: 0.1641
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0383
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1640
6530/6530 [==============================] - 0s 27us/step - loss: 0.1640 - val_loss: 0.1492
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1806
3008/6530 [============>.................] - ETA: 0s - loss: 0.0384
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1624
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0383
3904/6530 [================>.............] - ETA: 0s - loss: 0.1607
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0387
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1602
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0386
6530/6530 [==============================] - 0s 27us/step - loss: 0.1592 - val_loss: 0.1500
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1824
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0383
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1609
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0381
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1581
6464/6530 [============================>.] - ETA: 0s - loss: 0.1594
6530/6530 [==============================] - 1s 88us/step - loss: 0.0384 - val_loss: 0.0386
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0246
6530/6530 [==============================] - 0s 25us/step - loss: 0.1592 - val_loss: 0.1474
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1728
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0385
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1531
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0358
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1531
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0371
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1541
6530/6530 [==============================] - 0s 26us/step - loss: 0.1543 - val_loss: 0.1450

2624/6530 [===========>..................] - ETA: 0s - loss: 0.0368
3264/6530 [=============>................] - ETA: 0s - loss: 0.0364
3920/6530 [=================>............] - ETA: 0s - loss: 0.0362
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0366
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0369
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0367
6336/6530 [============================>.] - ETA: 0s - loss: 0.0370
6530/6530 [==============================] - 1s 84us/step - loss: 0.0371 - val_loss: 0.0374
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0220
 624/6530 [=>............................] - ETA: 0s - loss: 0.0369
# training | RMSE: 0.0994, MAE: 0.0768
worker 1  xfile  [1, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28944113514402675}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3686040864853135}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17200213178174123}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.09936450308844873, 'rmse': 0.09936450308844873, 'mae': 0.07676579741601613, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:   7 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  55 | activation: sigmoid | extras: None 
layer 3 | size:  98 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9f4eb8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 42s - loss: 0.3838
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0369
1408/6530 [=====>........................] - ETA: 1s - loss: 0.2550 
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0368
2752/6530 [===========>..................] - ETA: 0s - loss: 0.2400
# training | RMSE: 0.1827, MAE: 0.1411
worker 0  xfile  [7, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4970046503928497}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4171571693762234}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 49, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.18269336624169624, 'rmse': 0.18269336624169624, 'mae': 0.1411296737587013, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  19 | activation: tanh    | extras: None 
layer 2 | size:  62 | activation: sigmoid | extras: dropout - rate: 42.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f03c4698208>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 25s - loss: 1.5934
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0365
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2318
2112/6530 [========>.....................] - ETA: 0s - loss: 0.6031 
2880/6530 [============>.................] - ETA: 0s - loss: 0.0366
4800/6530 [=====================>........] - ETA: 0s - loss: 0.2229
4352/6530 [==================>...........] - ETA: 0s - loss: 0.4132
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0359
6016/6530 [==========================>...] - ETA: 0s - loss: 0.2151
6528/6530 [============================>.] - ETA: 0s - loss: 0.3456
4112/6530 [=================>............] - ETA: 0s - loss: 0.0366
6530/6530 [==============================] - 0s 67us/step - loss: 0.3455 - val_loss: 0.2026
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2141
6530/6530 [==============================] - 1s 113us/step - loss: 0.2110 - val_loss: 0.1636
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1810
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0363
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1969
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1611
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0361
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1893
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1635
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0359
6530/6530 [==============================] - 0s 23us/step - loss: 0.1844 - val_loss: 0.1684
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1851
3840/6530 [================>.............] - ETA: 0s - loss: 0.1619
6480/6530 [============================>.] - ETA: 0s - loss: 0.0358
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1706
6530/6530 [==============================] - 1s 90us/step - loss: 0.0357 - val_loss: 0.0369
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0629
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1631
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1693
 592/6530 [=>............................] - ETA: 0s - loss: 0.0375
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1623
6530/6530 [==============================] - 0s 43us/step - loss: 0.1620 - val_loss: 0.1684

6530/6530 [==============================] - 0s 23us/step - loss: 0.1678 - val_loss: 0.1648
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1768Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1732
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0366
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1625
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1611
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0358
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1632
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1628
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0351
6530/6530 [==============================] - 0s 24us/step - loss: 0.1629 - val_loss: 0.2131

3648/6530 [===============>..............] - ETA: 0s - loss: 0.1625Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2076
2976/6530 [============>.................] - ETA: 0s - loss: 0.0343
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1624
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0345
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1628
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1628
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0349
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1628
6530/6530 [==============================] - 0s 45us/step - loss: 0.1623 - val_loss: 0.1688
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1734
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0348
6530/6530 [==============================] - 0s 24us/step - loss: 0.1624 - val_loss: 0.2184
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2110
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1628
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0345
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1622
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1641
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0345
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1622
3840/6530 [================>.............] - ETA: 0s - loss: 0.1625
6530/6530 [==============================] - 0s 24us/step - loss: 0.1621 - val_loss: 0.2159
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2087
6530/6530 [==============================] - 1s 89us/step - loss: 0.0345 - val_loss: 0.0361

5056/6530 [======================>.......] - ETA: 0s - loss: 0.1636
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1620
6336/6530 [============================>.] - ETA: 0s - loss: 0.1627
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1618
6530/6530 [==============================] - 0s 43us/step - loss: 0.1624 - val_loss: 0.1692
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1732
6528/6530 [============================>.] - ETA: 0s - loss: 0.1617
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1618
6530/6530 [==============================] - 0s 25us/step - loss: 0.1617 - val_loss: 0.2094
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2029
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1633
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1615
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1615
3776/6530 [================>.............] - ETA: 0s - loss: 0.1621
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1634
6530/6530 [==============================] - 0s 25us/step - loss: 0.1615 - val_loss: 0.2070

6400/6530 [============================>.] - ETA: 0s - loss: 0.1624
6530/6530 [==============================] - 0s 42us/step - loss: 0.1623 - val_loss: 0.1690
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1738
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1615
# training | RMSE: 0.2452, MAE: 0.2060
worker 0  xfile  [9, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42824272704555977}, 'layer_2_size': 62, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 2, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4527782389368925}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.24524154465223566, 'rmse': 0.24524154465223566, 'mae': 0.20597509371057451, 'early_stop': True}
vggnet done  0

2624/6530 [===========>..................] - ETA: 0s - loss: 0.1643
4032/6530 [=================>............] - ETA: 0s - loss: 0.1626
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1632
6530/6530 [==============================] - 0s 41us/step - loss: 0.1622 - val_loss: 0.1667

# training | RMSE: 0.2048, MAE: 0.1645
worker 1  xfile  [8, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.20476823990991855, 'rmse': 0.20476823990991855, 'mae': 0.1645259441502952, 'early_stop': True}
vggnet done  1

# training | RMSE: 0.1868, MAE: 0.1519
worker 2  xfile  [6, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 63, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 23, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.1868449985543841, 'rmse': 0.1868449985543841, 'mae': 0.15192041520684665, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:   2 | activation: tanh    | extras: dropout - rate: 15.3% 
layer 2 | size: 100 | activation: relu    | extras: None 
layer 3 | size:  42 | activation: tanh    | extras: None 
layer 4 | size:  81 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f03d86fd668>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 36s - loss: 0.7043
1408/6530 [=====>........................] - ETA: 1s - loss: 0.2673 
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1812
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1502
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1304
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1205
6530/6530 [==============================] - 1s 111us/step - loss: 0.1148 - val_loss: 0.0651
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0605
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0677
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0598
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0558
2944/6530 [============>.................] - ETA: 0s - loss: 0.0531
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0507
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0495
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0486
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0482
6528/6530 [============================>.] - ETA: 0s - loss: 0.0475
6530/6530 [==============================] - 0s 76us/step - loss: 0.0475 - val_loss: 0.0415
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0486
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0445
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0441
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0442
3264/6530 [=============>................] - ETA: 0s - loss: 0.0442
4032/6530 [=================>............] - ETA: 0s - loss: 0.0440
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0439
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0434
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0431
6530/6530 [==============================] - 0s 70us/step - loss: 0.0432 - val_loss: 0.0416
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0393
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0441
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0427
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0422
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0423
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0418
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0428
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0428
6530/6530 [==============================] - 0s 64us/step - loss: 0.0429 - val_loss: 0.0416
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0338
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0419
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0426
3200/6530 [=============>................] - ETA: 0s - loss: 0.0427
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0429
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0423
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0425
6530/6530 [==============================] - 0s 56us/step - loss: 0.0427 - val_loss: 0.0415
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0337
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0418
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0423
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0426
3776/6530 [================>.............] - ETA: 0s - loss: 0.0428
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0430
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0428
6530/6530 [==============================] - 0s 57us/step - loss: 0.0429 - val_loss: 0.0410
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0438
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0431
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0432
3136/6530 [=============>................] - ETA: 0s - loss: 0.0441
4096/6530 [=================>............] - ETA: 0s - loss: 0.0427
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0428
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0429
6530/6530 [==============================] - 0s 58us/step - loss: 0.0430 - val_loss: 0.0411
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0479
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0422
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0416
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0418
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0414
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0409
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0417
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0419
6530/6530 [==============================] - 0s 65us/step - loss: 0.0416 - val_loss: 0.0404
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0462
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0361
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0378
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0394
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0407
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0407
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0405
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0407
6530/6530 [==============================] - 0s 62us/step - loss: 0.0410 - val_loss: 0.0391

# training | RMSE: 0.1937, MAE: 0.1522
worker 2  xfile  [10, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15264298881590707}, 'layer_1_size': 2, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 100, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17174719681706674}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.19368357234420236, 'rmse': 0.19368357234420236, 'mae': 0.15224367289879703, 'early_stop': False}
vggnet done  2
all of workers have been done
calculation finished
#2 epoch=9.0 loss={'loss': 0.15680852390421243, 'rmse': 0.15680852390421243, 'mae': 0.12888819905021745, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19732678881540575}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20791950143380367}, 'layer_3_size': 85, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#0 epoch=9.0 loss={'loss': 0.10752888348474429, 'rmse': 0.10752888348474429, 'mae': 0.08040341390386829, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20166402653594076}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26587831508262944}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1965905869310106}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#3 epoch=9.0 loss={'loss': 0.1255249415905743, 'rmse': 0.1255249415905743, 'mae': 0.0985766344446653, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4538953694584704}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1250234434350652}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#5 epoch=9.0 loss={'loss': 0.15862556303552547, 'rmse': 0.15862556303552547, 'mae': 0.12410039825840798, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4870497906088097}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11777333135437362}, 'layer_5_size': 7, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#4 epoch=9.0 loss={'loss': 0.17062629871549945, 'rmse': 0.17062629871549945, 'mae': 0.13594530985060033, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#1 epoch=9.0 loss={'loss': 0.09936450308844873, 'rmse': 0.09936450308844873, 'mae': 0.07676579741601613, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28944113514402675}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3686040864853135}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17200213178174123}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#7 epoch=9.0 loss={'loss': 0.18269336624169624, 'rmse': 0.18269336624169624, 'mae': 0.1411296737587013, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4970046503928497}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4171571693762234}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 49, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#6 epoch=9.0 loss={'loss': 0.1868449985543841, 'rmse': 0.1868449985543841, 'mae': 0.15192041520684665, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 63, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 23, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#9 epoch=9.0 loss={'loss': 0.24524154465223566, 'rmse': 0.24524154465223566, 'mae': 0.20597509371057451, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42824272704555977}, 'layer_2_size': 62, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 2, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4527782389368925}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#8 epoch=9.0 loss={'loss': 0.20476823990991855, 'rmse': 0.20476823990991855, 'mae': 0.1645259441502952, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#10 epoch=9.0 loss={'loss': 0.19368357234420236, 'rmse': 0.19368357234420236, 'mae': 0.15224367289879703, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15264298881590707}, 'layer_1_size': 2, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 100, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17174719681706674}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
get a list [results] of length 166
get a list [loss] of length 11
get a list [val_loss] of length 11
length of indices is (1, 0, 3, 2, 5, 4, 7, 6, 10, 8, 9)
length of indices is 11
length of T is 11
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28944113514402675}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3686040864853135}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17200213178174123}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [1, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20166402653594076}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26587831508262944}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1965905869310106}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [2, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4538953694584704}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1250234434350652}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]] 

*** 3.7777777777777777 configurations x 27.0 iterations each

11 | Fri Sep 28 01:21:57 2018 | lowest loss so far: 0.0705 (run 0)

{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  15 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  68 | activation: tanh    | extras: None 
layer 3 | size:  63 | activation: relu    | extras: dropout - rate: 45.4% 
layer 4 | size:   7 | activation: sigmoid | extras: dropout - rate: 12.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  64/6530 [..............................] - ETA: 1:34 - loss: 0.1464
1024/6530 [===>..........................] - ETA: 5s - loss: 0.0898  {'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  96 | activation: relu    | extras: dropout - rate: 20.2% 
layer 2 | size:  55 | activation: relu    | extras: batchnorm 
layer 3 | size:  93 | activation: tanh    | extras: dropout - rate: 26.6% 
layer 4 | size:  86 | activation: sigmoid | extras: dropout - rate: 19.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 2:55 - loss: 0.7709
2048/6530 [========>.....................] - ETA: 2s - loss: 0.0746
 576/6530 [=>............................] - ETA: 9s - loss: 0.2675  
3264/6530 [=============>................] - ETA: 1s - loss: 0.0663
1184/6530 [====>.........................] - ETA: 4s - loss: 0.2206
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0616
1824/6530 [=======>......................] - ETA: 2s - loss: 0.2026
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0579
2464/6530 [==========>...................] - ETA: 1s - loss: 0.1918
3168/6530 [=============>................] - ETA: 1s - loss: 0.1831
6530/6530 [==============================] - 1s 194us/step - loss: 0.0561 - val_loss: 0.0381
Epoch 2/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0384
3776/6530 [================>.............] - ETA: 0s - loss: 0.1791{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  96 | activation: relu    | extras: dropout - rate: 28.9% 
layer 2 | size:  85 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  48 | activation: tanh    | extras: None 
layer 4 | size:  59 | activation: relu    | extras: dropout - rate: 36.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 7:05 - loss: 0.6945
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0410
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1738
 272/6530 [>.............................] - ETA: 25s - loss: 0.3721 
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0409
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1701
 528/6530 [=>............................] - ETA: 13s - loss: 0.3300
3904/6530 [================>.............] - ETA: 0s - loss: 0.0394
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1664
 784/6530 [==>...........................] - ETA: 8s - loss: 0.3034 
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0384
6368/6530 [============================>.] - ETA: 0s - loss: 0.1639
1024/6530 [===>..........................] - ETA: 6s - loss: 0.2883
6528/6530 [============================>.] - ETA: 0s - loss: 0.0382
6530/6530 [==============================] - 0s 42us/step - loss: 0.0382 - val_loss: 0.0314
Epoch 3/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0354
1280/6530 [====>.........................] - ETA: 5s - loss: 0.2795
6530/6530 [==============================] - 1s 223us/step - loss: 0.1631 - val_loss: 0.1326

1408/6530 [=====>........................] - ETA: 0s - loss: 0.0357Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1451
1536/6530 [======>.......................] - ETA: 4s - loss: 0.2720
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0359
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1354
1792/6530 [=======>......................] - ETA: 3s - loss: 0.2633
4032/6530 [=================>............] - ETA: 0s - loss: 0.0352
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1318
2032/6530 [========>.....................] - ETA: 3s - loss: 0.2590
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0348
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1319
2288/6530 [=========>....................] - ETA: 2s - loss: 0.2540
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1318
6530/6530 [==============================] - 0s 41us/step - loss: 0.0347 - val_loss: 0.0289
Epoch 4/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0262
2544/6530 [==========>...................] - ETA: 2s - loss: 0.2516
3136/6530 [=============>................] - ETA: 0s - loss: 0.1313
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0329
2784/6530 [===========>..................] - ETA: 2s - loss: 0.2472
3744/6530 [================>.............] - ETA: 0s - loss: 0.1313
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0323
3040/6530 [============>.................] - ETA: 1s - loss: 0.2426
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1305
3968/6530 [=================>............] - ETA: 0s - loss: 0.0325
3296/6530 [==============>...............] - ETA: 1s - loss: 0.2385
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1308
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0324
3552/6530 [===============>..............] - ETA: 1s - loss: 0.2350
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1298
6530/6530 [==============================] - 0s 41us/step - loss: 0.0325 - val_loss: 0.0277
Epoch 5/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0277
3792/6530 [================>.............] - ETA: 1s - loss: 0.2321
6336/6530 [============================>.] - ETA: 0s - loss: 0.1297
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0321
6530/6530 [==============================] - 1s 84us/step - loss: 0.1295 - val_loss: 0.1223
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1238
4048/6530 [=================>............] - ETA: 1s - loss: 0.2295
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0319
4304/6530 [==================>...........] - ETA: 0s - loss: 0.2271
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1255
4032/6530 [=================>............] - ETA: 0s - loss: 0.0310
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1214
4560/6530 [===================>..........] - ETA: 0s - loss: 0.2250
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0308
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1204
4800/6530 [=====================>........] - ETA: 0s - loss: 0.2225
6530/6530 [==============================] - 0s 41us/step - loss: 0.0309 - val_loss: 0.0272
Epoch 6/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0245
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1221
5056/6530 [======================>.......] - ETA: 0s - loss: 0.2203
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0287
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1206
5312/6530 [=======================>......] - ETA: 0s - loss: 0.2178
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0296
3840/6530 [================>.............] - ETA: 0s - loss: 0.1200
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2164
4096/6530 [=================>............] - ETA: 0s - loss: 0.0296
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1203
5824/6530 [=========================>....] - ETA: 0s - loss: 0.2154
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0297
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1206
6064/6530 [==========================>...] - ETA: 0s - loss: 0.2138
6530/6530 [==============================] - 0s 40us/step - loss: 0.0297 - val_loss: 0.0241
Epoch 7/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0259
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1204
6320/6530 [============================>.] - ETA: 0s - loss: 0.2119
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1208
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0277
6530/6530 [==============================] - 1s 84us/step - loss: 0.1208 - val_loss: 0.1063
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1039
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0279
6530/6530 [==============================] - 2s 376us/step - loss: 0.2105 - val_loss: 0.1735
Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1672
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1147
3968/6530 [=================>............] - ETA: 0s - loss: 0.0282
 272/6530 [>.............................] - ETA: 1s - loss: 0.1802
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1148
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0280
 528/6530 [=>............................] - ETA: 1s - loss: 0.1667
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1144
6530/6530 [==============================] - 0s 40us/step - loss: 0.0277 - val_loss: 0.0234
Epoch 8/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0238
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1691
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1155
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0270
1040/6530 [===>..........................] - ETA: 1s - loss: 0.1690
3232/6530 [=============>................] - ETA: 0s - loss: 0.1143
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0271
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1677
3904/6530 [================>.............] - ETA: 0s - loss: 0.1160
4096/6530 [=================>............] - ETA: 0s - loss: 0.0269
1568/6530 [======>.......................] - ETA: 1s - loss: 0.1671
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1166
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0270
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1670
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1162
6530/6530 [==============================] - 0s 40us/step - loss: 0.0270 - val_loss: 0.0220
Epoch 9/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0222
2064/6530 [========>.....................] - ETA: 0s - loss: 0.1660
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1157
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0253
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1652
6368/6530 [============================>.] - ETA: 0s - loss: 0.1164
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0258
6530/6530 [==============================] - 1s 85us/step - loss: 0.1162 - val_loss: 0.1061
Epoch 5/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1293
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1649
4032/6530 [=================>............] - ETA: 0s - loss: 0.0259
 640/6530 [=>............................] - ETA: 0s - loss: 0.1132
2832/6530 [============>.................] - ETA: 0s - loss: 0.1636
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0258
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1119
3072/6530 [=============>................] - ETA: 0s - loss: 0.1617
6530/6530 [==============================] - 0s 40us/step - loss: 0.0260 - val_loss: 0.0210
Epoch 10/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0237
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1123
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1615
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0253
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1129
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1618
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0255
3232/6530 [=============>................] - ETA: 0s - loss: 0.1118
3856/6530 [================>.............] - ETA: 0s - loss: 0.1614
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0253
3872/6530 [================>.............] - ETA: 0s - loss: 0.1130
4112/6530 [=================>............] - ETA: 0s - loss: 0.1616
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0253
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1136
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1606
6530/6530 [==============================] - 0s 39us/step - loss: 0.0252 - val_loss: 0.0202
Epoch 11/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0219
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1133
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1606
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0239
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1126
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1605
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0244
6432/6530 [============================>.] - ETA: 0s - loss: 0.1126
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1600
6530/6530 [==============================] - 1s 83us/step - loss: 0.1124 - val_loss: 0.0920
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0907
4032/6530 [=================>............] - ETA: 0s - loss: 0.0247
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1593
 640/6530 [=>............................] - ETA: 0s - loss: 0.1062
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0247
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1583
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1071
6530/6530 [==============================] - 0s 40us/step - loss: 0.0247 - val_loss: 0.0203
Epoch 12/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0235
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1584
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1082
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0225
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1576
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1082
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0231
6400/6530 [============================>.] - ETA: 0s - loss: 0.1572
3136/6530 [=============>................] - ETA: 0s - loss: 0.1068
4096/6530 [=================>............] - ETA: 0s - loss: 0.0234
6530/6530 [==============================] - 1s 209us/step - loss: 0.1574 - val_loss: 0.1403

3776/6530 [================>.............] - ETA: 0s - loss: 0.1069Epoch 3/27

  16/6530 [..............................] - ETA: 2s - loss: 0.1606
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0235
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1074
 272/6530 [>.............................] - ETA: 1s - loss: 0.1433
6530/6530 [==============================] - 0s 40us/step - loss: 0.0236 - val_loss: 0.0188
Epoch 13/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0210
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1081
 528/6530 [=>............................] - ETA: 1s - loss: 0.1399
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0233
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1079
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1406
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0224
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1085
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1421
3840/6530 [================>.............] - ETA: 0s - loss: 0.0227
6530/6530 [==============================] - 1s 84us/step - loss: 0.1085 - val_loss: 0.0953
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1283
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1425
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0228
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1096
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1434
6336/6530 [============================>.] - ETA: 0s - loss: 0.0228
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1064
6530/6530 [==============================] - 0s 43us/step - loss: 0.0227 - val_loss: 0.0181
Epoch 14/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0166
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1429
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1078
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0225
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1422
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1068
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0227
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1427
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1067
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0222
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1428
3968/6530 [=================>............] - ETA: 0s - loss: 0.1072
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0224
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1430
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1077
6530/6530 [==============================] - 0s 39us/step - loss: 0.0225 - val_loss: 0.0173
Epoch 15/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0182
3056/6530 [=============>................] - ETA: 0s - loss: 0.1420
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1078
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0221
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1417
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1076
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0218
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1419
3776/6530 [================>.............] - ETA: 0s - loss: 0.0217
3824/6530 [================>.............] - ETA: 0s - loss: 0.1416
6530/6530 [==============================] - 1s 82us/step - loss: 0.1076 - val_loss: 0.0827
Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1315
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0218
4080/6530 [=================>............] - ETA: 0s - loss: 0.1424
 640/6530 [=>............................] - ETA: 0s - loss: 0.1064
6400/6530 [============================>.] - ETA: 0s - loss: 0.0222
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1416
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1049
6530/6530 [==============================] - 0s 42us/step - loss: 0.0222 - val_loss: 0.0167
Epoch 16/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0212
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1418
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1039
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0217
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1416
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1046
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0216
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1413
3232/6530 [=============>................] - ETA: 0s - loss: 0.1035
4032/6530 [=================>............] - ETA: 0s - loss: 0.0217
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1408
3872/6530 [================>.............] - ETA: 0s - loss: 0.1046
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0216
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1405
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1046
6530/6530 [==============================] - 0s 41us/step - loss: 0.0216 - val_loss: 0.0164
Epoch 17/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0231
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1048
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1400
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0212
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1049
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1401
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0209
6432/6530 [============================>.] - ETA: 0s - loss: 0.1055
6416/6530 [============================>.] - ETA: 0s - loss: 0.1397
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0210
6530/6530 [==============================] - 1s 83us/step - loss: 0.1054 - val_loss: 0.0832
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1014
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0211
6530/6530 [==============================] - 1s 209us/step - loss: 0.1397 - val_loss: 0.1093
Epoch 4/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1015
 640/6530 [=>............................] - ETA: 0s - loss: 0.1035
6530/6530 [==============================] - 0s 40us/step - loss: 0.0212 - val_loss: 0.0161
Epoch 18/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0211
 272/6530 [>.............................] - ETA: 1s - loss: 0.1267
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1035
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0211
 544/6530 [=>............................] - ETA: 1s - loss: 0.1237
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1036
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0206
 800/6530 [==>...........................] - ETA: 1s - loss: 0.1297
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1039
3968/6530 [=================>............] - ETA: 0s - loss: 0.0208
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1325
3264/6530 [=============>................] - ETA: 0s - loss: 0.1038
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0209
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1335
3936/6530 [=================>............] - ETA: 0s - loss: 0.1043
6530/6530 [==============================] - 0s 41us/step - loss: 0.0210 - val_loss: 0.0161

1568/6530 [======>.......................] - ETA: 0s - loss: 0.1305Epoch 19/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0211
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1041
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0211
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1042
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1305
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0208
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1039
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1303
4096/6530 [=================>............] - ETA: 0s - loss: 0.0209
6528/6530 [============================>.] - ETA: 0s - loss: 0.1045
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1309
6530/6530 [==============================] - 1s 82us/step - loss: 0.1044 - val_loss: 0.0865
Epoch 10/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0922
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0206
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1308
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1015
6530/6530 [==============================] - 0s 40us/step - loss: 0.0206 - val_loss: 0.0158
Epoch 20/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0181
2864/6530 [============>.................] - ETA: 0s - loss: 0.1302
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1003
3120/6530 [=============>................] - ETA: 0s - loss: 0.1287
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0206
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1019
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0205
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1290
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1022
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0203
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1286
3264/6530 [=============>................] - ETA: 0s - loss: 0.1009
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0202
3888/6530 [================>.............] - ETA: 0s - loss: 0.1282
3936/6530 [=================>............] - ETA: 0s - loss: 0.1017
6530/6530 [==============================] - 0s 39us/step - loss: 0.0201 - val_loss: 0.0152
Epoch 21/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0196
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1283
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1022
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0202
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1273
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1027
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0206
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1280
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1030
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0202
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1277
6530/6530 [==============================] - 1s 82us/step - loss: 0.1032 - val_loss: 0.0827
Epoch 11/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1103
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0203
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1278
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0993
6530/6530 [==============================] - 0s 40us/step - loss: 0.0204 - val_loss: 0.0151
Epoch 22/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0175
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1277
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1017
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0198
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1274
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1009
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0196
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1273
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1016
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0195
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1275
3168/6530 [=============>................] - ETA: 0s - loss: 0.1005
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0195
6384/6530 [============================>.] - ETA: 0s - loss: 0.1276
3808/6530 [================>.............] - ETA: 0s - loss: 0.1008
6530/6530 [==============================] - 0s 39us/step - loss: 0.0196 - val_loss: 0.0150
Epoch 23/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0187
6530/6530 [==============================] - 1s 209us/step - loss: 0.1276 - val_loss: 0.1013
Epoch 5/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1376
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1010
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0204
 240/6530 [>.............................] - ETA: 1s - loss: 0.1194
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1021
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0200
 480/6530 [=>............................] - ETA: 1s - loss: 0.1165
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1015
3968/6530 [=================>............] - ETA: 0s - loss: 0.0195
 736/6530 [==>...........................] - ETA: 1s - loss: 0.1193
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1020
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0195
 960/6530 [===>..........................] - ETA: 1s - loss: 0.1177
6530/6530 [==============================] - 1s 85us/step - loss: 0.1017 - val_loss: 0.0854
Epoch 12/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0898
6530/6530 [==============================] - 0s 40us/step - loss: 0.0195 - val_loss: 0.0146
Epoch 24/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0191
1216/6530 [====>.........................] - ETA: 1s - loss: 0.1193
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0999
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0199
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1199
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0979
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0195
1728/6530 [======>.......................] - ETA: 1s - loss: 0.1197
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0992
3904/6530 [================>.............] - ETA: 0s - loss: 0.0193
1968/6530 [========>.....................] - ETA: 0s - loss: 0.1183
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0998
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0190
2224/6530 [=========>....................] - ETA: 0s - loss: 0.1184
3232/6530 [=============>................] - ETA: 0s - loss: 0.0996
6528/6530 [============================>.] - ETA: 0s - loss: 0.0190
6530/6530 [==============================] - 0s 41us/step - loss: 0.0190 - val_loss: 0.0142
Epoch 25/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0157
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1192
3872/6530 [================>.............] - ETA: 0s - loss: 0.1006
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0187
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1197
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1008
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0186
3008/6530 [============>.................] - ETA: 0s - loss: 0.1191
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1009
3968/6530 [=================>............] - ETA: 0s - loss: 0.0185
3264/6530 [=============>................] - ETA: 0s - loss: 0.1188
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1007
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0185
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1194
6368/6530 [============================>.] - ETA: 0s - loss: 0.1012
6530/6530 [==============================] - 1s 84us/step - loss: 0.1009 - val_loss: 0.0856
Epoch 13/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1004
6530/6530 [==============================] - 0s 40us/step - loss: 0.0186 - val_loss: 0.0140
Epoch 26/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0186
3760/6530 [================>.............] - ETA: 0s - loss: 0.1194
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1020
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0186
4016/6530 [=================>............] - ETA: 0s - loss: 0.1192
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1020
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0185
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1192
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1012
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0186
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1191
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1005
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0186
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1191
3264/6530 [=============>................] - ETA: 0s - loss: 0.0995
6530/6530 [==============================] - 0s 40us/step - loss: 0.0189 - val_loss: 0.0140

5024/6530 [======================>.......] - ETA: 0s - loss: 0.1189Epoch 27/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0179
3872/6530 [================>.............] - ETA: 0s - loss: 0.1003
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0182
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1188
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1000
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1181
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0185
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1002
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1181
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0183
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1001
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1181
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0184
6432/6530 [============================>.] - ETA: 0s - loss: 0.1001
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1183
6530/6530 [==============================] - 0s 41us/step - loss: 0.0183 - val_loss: 0.0138

6530/6530 [==============================] - 1s 83us/step - loss: 0.1001 - val_loss: 0.0899
Epoch 14/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1055
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0964
6530/6530 [==============================] - 1s 212us/step - loss: 0.1180 - val_loss: 0.0879
Epoch 6/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0995
# training | RMSE: 0.1076, MAE: 0.0845
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4538953694584704}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1250234434350652}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.10759036653681571, 'rmse': 0.10759036653681571, 'mae': 0.08452099595735524, 'early_stop': False}
vggnet done  2

1344/6530 [=====>........................] - ETA: 0s - loss: 0.0974
 272/6530 [>.............................] - ETA: 1s - loss: 0.1192
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0982
 544/6530 [=>............................] - ETA: 1s - loss: 0.1122
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0976
 816/6530 [==>...........................] - ETA: 1s - loss: 0.1149
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0980
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1174
4064/6530 [=================>............] - ETA: 0s - loss: 0.0985
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1158
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0987
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1137
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0990
1936/6530 [=======>......................] - ETA: 0s - loss: 0.1131
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0990
2224/6530 [=========>....................] - ETA: 0s - loss: 0.1134
6530/6530 [==============================] - 1s 77us/step - loss: 0.0987 - val_loss: 0.0793
Epoch 15/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1080
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1121
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0984
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1113
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0956
3056/6530 [=============>................] - ETA: 0s - loss: 0.1112
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0972
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1114
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0973
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1121
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0981
3840/6530 [================>.............] - ETA: 0s - loss: 0.1120
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0977
4112/6530 [=================>............] - ETA: 0s - loss: 0.1120
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0979
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1118
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0976
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1128
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0980
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1125
6530/6530 [==============================] - 1s 78us/step - loss: 0.0978 - val_loss: 0.0784
Epoch 16/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0986
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1124
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0958
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1124
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0954
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1123
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0961
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1122
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0969
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1122
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0972
4128/6530 [=================>............] - ETA: 0s - loss: 0.0961
6530/6530 [==============================] - 1s 193us/step - loss: 0.1121 - val_loss: 0.0966
Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1450
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0961
 288/6530 [>.............................] - ETA: 1s - loss: 0.1151
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0958
 544/6530 [=>............................] - ETA: 1s - loss: 0.1098
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0962
 816/6530 [==>...........................] - ETA: 1s - loss: 0.1113
6530/6530 [==============================] - 1s 77us/step - loss: 0.0963 - val_loss: 0.0769
Epoch 17/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0967
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1146
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0945
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1126
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0959
1648/6530 [======>.......................] - ETA: 0s - loss: 0.1117
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0962
1904/6530 [=======>......................] - ETA: 0s - loss: 0.1113
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0970
2160/6530 [========>.....................] - ETA: 0s - loss: 0.1113
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0962
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1106
4128/6530 [=================>............] - ETA: 0s - loss: 0.0961
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1108
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0963
3008/6530 [============>.................] - ETA: 0s - loss: 0.1104
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0963
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1098
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0960
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1100
6530/6530 [==============================] - 1s 78us/step - loss: 0.0960 - val_loss: 0.0768
Epoch 18/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1114
3824/6530 [================>.............] - ETA: 0s - loss: 0.1098
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0968
4096/6530 [=================>............] - ETA: 0s - loss: 0.1097
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0958
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1091
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0960
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1099
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0967
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1098
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0967
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1096
4128/6530 [=================>............] - ETA: 0s - loss: 0.0958
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1093
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0966
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1089
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0970
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1091
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0967
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1089
6530/6530 [==============================] - 1s 79us/step - loss: 0.0965 - val_loss: 0.0787
Epoch 19/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0976
6464/6530 [============================>.] - ETA: 0s - loss: 0.1088
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0960
6530/6530 [==============================] - 1s 196us/step - loss: 0.1089 - val_loss: 0.0857
Epoch 8/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0569
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0970
 304/6530 [>.............................] - ETA: 1s - loss: 0.1053
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0962
 592/6530 [=>............................] - ETA: 1s - loss: 0.1036
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0961
 848/6530 [==>...........................] - ETA: 1s - loss: 0.1051
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0955
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1089
4000/6530 [=================>............] - ETA: 0s - loss: 0.0957
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1081
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0960
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1066
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0964
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1063
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0960
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1054
6530/6530 [==============================] - 1s 80us/step - loss: 0.0960 - val_loss: 0.0785
Epoch 20/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0909
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1051
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0971
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1057
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0965
2912/6530 [============>.................] - ETA: 0s - loss: 0.1044
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0953
3184/6530 [=============>................] - ETA: 0s - loss: 0.1034
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0948
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1047
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0943
3744/6530 [================>.............] - ETA: 0s - loss: 0.1047
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0939
4032/6530 [=================>............] - ETA: 0s - loss: 0.1053
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0947
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1049
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0944
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1053
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0950
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1054
6530/6530 [==============================] - 1s 77us/step - loss: 0.0949 - val_loss: 0.0776
Epoch 21/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0975
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1054
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0936
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1053
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0927
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1052
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0921
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1051
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0931
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1047
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0935
6432/6530 [============================>.] - ETA: 0s - loss: 0.1050
4096/6530 [=================>............] - ETA: 0s - loss: 0.0935
6530/6530 [==============================] - 1s 198us/step - loss: 0.1051 - val_loss: 0.0880
Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1183
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0941
 288/6530 [>.............................] - ETA: 1s - loss: 0.1055
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0940
 560/6530 [=>............................] - ETA: 1s - loss: 0.1014
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0943
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1015
6530/6530 [==============================] - 1s 78us/step - loss: 0.0943 - val_loss: 0.0758
Epoch 22/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0821
1120/6530 [====>.........................] - ETA: 1s - loss: 0.1035
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0948
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1032
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0947
1648/6530 [======>.......................] - ETA: 0s - loss: 0.1034
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0942
1904/6530 [=======>......................] - ETA: 0s - loss: 0.1026
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0945
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0940
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1031
4128/6530 [=================>............] - ETA: 0s - loss: 0.0943
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1029
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0944
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1034
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0948
2992/6530 [============>.................] - ETA: 0s - loss: 0.1028
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0949
3264/6530 [=============>................] - ETA: 0s - loss: 0.1025
6530/6530 [==============================] - 0s 76us/step - loss: 0.0947 - val_loss: 0.0772
Epoch 23/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1010
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1034
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0973
3808/6530 [================>.............] - ETA: 0s - loss: 0.1037
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0950
4080/6530 [=================>............] - ETA: 0s - loss: 0.1036
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0941
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1033
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0945
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1036
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0940
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1041
4064/6530 [=================>............] - ETA: 0s - loss: 0.0945
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1034
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0939
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1033
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0941
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1027
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0935
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1027
6530/6530 [==============================] - 1s 80us/step - loss: 0.0934 - val_loss: 0.0760
Epoch 24/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0968
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1025
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0954
6480/6530 [============================>.] - ETA: 0s - loss: 0.1025
6530/6530 [==============================] - 1s 196us/step - loss: 0.1026 - val_loss: 0.0794
Epoch 10/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0728
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0940
 288/6530 [>.............................] - ETA: 1s - loss: 0.1037
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0938
 560/6530 [=>............................] - ETA: 1s - loss: 0.1019
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0938
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0937
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1034
4128/6530 [=================>............] - ETA: 0s - loss: 0.0936
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1062
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0939
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1047
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0936
1648/6530 [======>.......................] - ETA: 0s - loss: 0.1038
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1033
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0936
6530/6530 [==============================] - 1s 78us/step - loss: 0.0939 - val_loss: 0.0760
Epoch 25/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0774
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1026
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0922
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1023
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0928
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1025
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0927
2976/6530 [============>.................] - ETA: 0s - loss: 0.1021
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0930
3248/6530 [=============>................] - ETA: 0s - loss: 0.1018
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0924
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1025
4128/6530 [=================>............] - ETA: 0s - loss: 0.0924
3792/6530 [================>.............] - ETA: 0s - loss: 0.1024
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0925
4064/6530 [=================>............] - ETA: 0s - loss: 0.1025
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0925
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1022
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0923
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1023
6530/6530 [==============================] - 1s 78us/step - loss: 0.0925 - val_loss: 0.0765
Epoch 26/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1091
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1021
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0933
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1018
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0923
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1018
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0922
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1016
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0925
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1016
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0919
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1012
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0917
6496/6530 [============================>.] - ETA: 0s - loss: 0.1014
6530/6530 [==============================] - 1s 196us/step - loss: 0.1014 - val_loss: 0.0806
Epoch 11/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0863
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0922
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0919
 288/6530 [>.............................] - ETA: 1s - loss: 0.0949
 560/6530 [=>............................] - ETA: 1s - loss: 0.0947
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0921
6530/6530 [==============================] - 1s 77us/step - loss: 0.0920 - val_loss: 0.0732
Epoch 27/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0831
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0963
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0901
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0991
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0895
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0994
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0891
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0985
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0897
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0981
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0891
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0981
4064/6530 [=================>............] - ETA: 0s - loss: 0.0899
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0977
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0904
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0984
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0906
2992/6530 [============>.................] - ETA: 0s - loss: 0.0985
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0911
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0980
6530/6530 [==============================] - 1s 78us/step - loss: 0.0913 - val_loss: 0.0733

3552/6530 [===============>..............] - ETA: 0s - loss: 0.0985
3840/6530 [================>.............] - ETA: 0s - loss: 0.0989
4112/6530 [=================>............] - ETA: 0s - loss: 0.0987
# training | RMSE: 0.0888, MAE: 0.0679
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20166402653594076}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26587831508262944}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1965905869310106}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.08881311863644993, 'rmse': 0.08881311863644993, 'mae': 0.06792874305154584, 'early_stop': False}
vggnet done  1

4400/6530 [===================>..........] - ETA: 0s - loss: 0.0983
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0992
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0989
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0987
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0989
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0989
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0990
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0991
6528/6530 [============================>.] - ETA: 0s - loss: 0.0992
6530/6530 [==============================] - 1s 197us/step - loss: 0.0992 - val_loss: 0.0892
Epoch 12/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0898
 224/6530 [>.............................] - ETA: 1s - loss: 0.0982
 448/6530 [=>............................] - ETA: 1s - loss: 0.0995
 656/6530 [==>...........................] - ETA: 1s - loss: 0.1006
 896/6530 [===>..........................] - ETA: 1s - loss: 0.1001
1152/6530 [====>.........................] - ETA: 1s - loss: 0.1018
1392/6530 [=====>........................] - ETA: 1s - loss: 0.1010
1632/6530 [======>.......................] - ETA: 1s - loss: 0.1004
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0991
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0983
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0981
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0983
2864/6530 [============>.................] - ETA: 0s - loss: 0.0980
3040/6530 [============>.................] - ETA: 0s - loss: 0.0980
3232/6530 [=============>................] - ETA: 0s - loss: 0.0976
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0980
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0990
3776/6530 [================>.............] - ETA: 0s - loss: 0.0988
3968/6530 [=================>............] - ETA: 0s - loss: 0.0989
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0988
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0986
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0991
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0988
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0989
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0991
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0990
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0987
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0984
6352/6530 [============================>.] - ETA: 0s - loss: 0.0984
6530/6530 [==============================] - 2s 236us/step - loss: 0.0984 - val_loss: 0.0750
Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0954
 224/6530 [>.............................] - ETA: 1s - loss: 0.0923
 416/6530 [>.............................] - ETA: 1s - loss: 0.0925
 592/6530 [=>............................] - ETA: 1s - loss: 0.0918
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0935
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0947
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0965
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0972
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0979
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0978
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0972
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0971
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0962
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0958
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0955
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0955
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0955
2592/6530 [==========>...................] - ETA: 1s - loss: 0.0960
2752/6530 [===========>..................] - ETA: 1s - loss: 0.0954
2928/6530 [============>.................] - ETA: 1s - loss: 0.0945
3104/6530 [=============>................] - ETA: 1s - loss: 0.0943
3280/6530 [==============>...............] - ETA: 1s - loss: 0.0944
3472/6530 [==============>...............] - ETA: 1s - loss: 0.0953
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0951
3776/6530 [================>.............] - ETA: 0s - loss: 0.0954
3920/6530 [=================>............] - ETA: 0s - loss: 0.0955
4064/6530 [=================>............] - ETA: 0s - loss: 0.0955
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0955
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0953
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0959
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0960
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0961
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0957
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0957
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0956
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0957
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0960
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0958
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0959
6432/6530 [============================>.] - ETA: 0s - loss: 0.0958
6530/6530 [==============================] - 2s 328us/step - loss: 0.0959 - val_loss: 0.0796
Epoch 14/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0739
 208/6530 [..............................] - ETA: 1s - loss: 0.0907
 384/6530 [>.............................] - ETA: 1s - loss: 0.0925
 544/6530 [=>............................] - ETA: 1s - loss: 0.0931
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0936
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0939
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0955
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0964
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0966
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0963
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0966
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0960
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0954
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0945
2512/6530 [==========>...................] - ETA: 1s - loss: 0.0951
2688/6530 [===========>..................] - ETA: 1s - loss: 0.0949
2864/6530 [============>.................] - ETA: 1s - loss: 0.0946
3040/6530 [============>.................] - ETA: 1s - loss: 0.0939
3200/6530 [=============>................] - ETA: 0s - loss: 0.0934
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0945
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0949
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0949
3824/6530 [================>.............] - ETA: 0s - loss: 0.0951
3968/6530 [=================>............] - ETA: 0s - loss: 0.0949
4128/6530 [=================>............] - ETA: 0s - loss: 0.0951
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0952
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0955
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0958
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0956
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0955
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0955
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0954
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0953
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0952
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0953
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0952
6320/6530 [============================>.] - ETA: 0s - loss: 0.0952
6480/6530 [============================>.] - ETA: 0s - loss: 0.0950
6530/6530 [==============================] - 2s 307us/step - loss: 0.0951 - val_loss: 0.0936
Epoch 15/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0994
 176/6530 [..............................] - ETA: 2s - loss: 0.1027
 352/6530 [>.............................] - ETA: 1s - loss: 0.0969
 512/6530 [=>............................] - ETA: 1s - loss: 0.0946
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0962
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0949
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0954
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0962
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0960
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0956
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0959
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0951
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0954
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0948
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0944
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0941
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0936
2608/6530 [==========>...................] - ETA: 1s - loss: 0.0945
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0940
2928/6530 [============>.................] - ETA: 1s - loss: 0.0935
3104/6530 [=============>................] - ETA: 1s - loss: 0.0933
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0938
3440/6530 [==============>...............] - ETA: 1s - loss: 0.0942
3584/6530 [===============>..............] - ETA: 1s - loss: 0.0942
3760/6530 [================>.............] - ETA: 0s - loss: 0.0938
3936/6530 [=================>............] - ETA: 0s - loss: 0.0942
4064/6530 [=================>............] - ETA: 0s - loss: 0.0943
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0943
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0944
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0947
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0951
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0945
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0946
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0943
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0942
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0940
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0940
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0937
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0938
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0938
6400/6530 [============================>.] - ETA: 0s - loss: 0.0937
6530/6530 [==============================] - 2s 341us/step - loss: 0.0938 - val_loss: 0.0810
Epoch 16/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0922
 176/6530 [..............................] - ETA: 2s - loss: 0.0985
 304/6530 [>.............................] - ETA: 2s - loss: 0.0995
 448/6530 [=>............................] - ETA: 2s - loss: 0.0985
 624/6530 [=>............................] - ETA: 2s - loss: 0.0977
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0971
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0977
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0988
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0986
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0970
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0967
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0959
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0955
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0953
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0948
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0946
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0947
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0943
2928/6530 [============>.................] - ETA: 1s - loss: 0.0935
3104/6530 [=============>................] - ETA: 1s - loss: 0.0927
3264/6530 [=============>................] - ETA: 1s - loss: 0.0928
3408/6530 [==============>...............] - ETA: 1s - loss: 0.0932
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0931
3712/6530 [================>.............] - ETA: 0s - loss: 0.0931
3872/6530 [================>.............] - ETA: 0s - loss: 0.0931
4032/6530 [=================>............] - ETA: 0s - loss: 0.0935
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0934
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0929
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0935
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0936
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0934
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0937
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0933
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0935
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0934
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0935
6464/6530 [============================>.] - ETA: 0s - loss: 0.0934
6530/6530 [==============================] - 2s 300us/step - loss: 0.0935 - val_loss: 0.0781
Epoch 17/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0740
 208/6530 [..............................] - ETA: 1s - loss: 0.0893
 368/6530 [>.............................] - ETA: 1s - loss: 0.0906
 512/6530 [=>............................] - ETA: 1s - loss: 0.0907
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0918
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0902
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0917
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0933
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0943
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0945
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0939
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0936
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0927
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0922
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0919
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0928
2784/6530 [===========>..................] - ETA: 1s - loss: 0.0927
2928/6530 [============>.................] - ETA: 1s - loss: 0.0925
3088/6530 [=============>................] - ETA: 1s - loss: 0.0925
3264/6530 [=============>................] - ETA: 1s - loss: 0.0925
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0930
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0934
3760/6530 [================>.............] - ETA: 0s - loss: 0.0929
3904/6530 [================>.............] - ETA: 0s - loss: 0.0931
4064/6530 [=================>............] - ETA: 0s - loss: 0.0931
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0933
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0929
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0934
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0935
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0935
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0936
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0936
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0934
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0931
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0931
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0929
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0927
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0929
6464/6530 [============================>.] - ETA: 0s - loss: 0.0929
6530/6530 [==============================] - 2s 319us/step - loss: 0.0928 - val_loss: 0.0772

# training | RMSE: 0.0900, MAE: 0.0708
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28944113514402675}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3686040864853135}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17200213178174123}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.089955101980893, 'rmse': 0.089955101980893, 'mae': 0.07084755980537764, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#2 epoch=27.0 loss={'loss': 0.10759036653681571, 'rmse': 0.10759036653681571, 'mae': 0.08452099595735524, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4538953694584704}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1250234434350652}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#1 epoch=27.0 loss={'loss': 0.08881311863644993, 'rmse': 0.08881311863644993, 'mae': 0.06792874305154584, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20166402653594076}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26587831508262944}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1965905869310106}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#0 epoch=27.0 loss={'loss': 0.089955101980893, 'rmse': 0.089955101980893, 'mae': 0.07084755980537764, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28944113514402675}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3686040864853135}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17200213178174123}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
get a list [results] of length 169
get a list [loss] of length 3
get a list [val_loss] of length 3
length of indices is (1, 0, 2)
length of indices is 3
length of T is 3
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20166402653594076}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26587831508262944}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1965905869310106}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]] 

*** 1.259259259259259 configurations x 81.0 iterations each

1 | Fri Sep 28 01:22:26 2018 | lowest loss so far: 0.0705 (run 0)

vggnet done  1
vggnet done  2
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  96 | activation: relu    | extras: dropout - rate: 20.2% 
layer 2 | size:  55 | activation: relu    | extras: batchnorm 
layer 3 | size:  93 | activation: tanh    | extras: dropout - rate: 26.6% 
layer 4 | size:  86 | activation: sigmoid | extras: dropout - rate: 19.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  32/6530 [..............................] - ETA: 2:51 - loss: 0.7709
 384/6530 [>.............................] - ETA: 14s - loss: 0.2979 
 704/6530 [==>...........................] - ETA: 7s - loss: 0.2521 
1120/6530 [====>.........................] - ETA: 4s - loss: 0.2242
1504/6530 [=====>........................] - ETA: 3s - loss: 0.2089
1952/6530 [=======>......................] - ETA: 2s - loss: 0.2004
2464/6530 [==========>...................] - ETA: 1s - loss: 0.1918
3008/6530 [============>.................] - ETA: 1s - loss: 0.1852
3616/6530 [===============>..............] - ETA: 1s - loss: 0.1802
4128/6530 [=================>............] - ETA: 0s - loss: 0.1763
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1727
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1694
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1666
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1645
6530/6530 [==============================] - 2s 247us/step - loss: 0.1631 - val_loss: 0.1326
Epoch 2/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1451
 512/6530 [=>............................] - ETA: 0s - loss: 0.1353
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1344
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1314
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1315
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1318
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1310
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1317
3008/6530 [============>.................] - ETA: 0s - loss: 0.1313
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1317
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1312
4128/6530 [=================>............] - ETA: 0s - loss: 0.1308
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1307
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1306
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1293
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1297
6530/6530 [==============================] - 1s 132us/step - loss: 0.1294 - val_loss: 0.1197
Epoch 3/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1273
 448/6530 [=>............................] - ETA: 0s - loss: 0.1237
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1222
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1203
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1194
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1203
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1208
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1218
3104/6530 [=============>................] - ETA: 0s - loss: 0.1200
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1204
3936/6530 [=================>............] - ETA: 0s - loss: 0.1202
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1199
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1211
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1209
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1204
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1208
6530/6530 [==============================] - 1s 127us/step - loss: 0.1208 - val_loss: 0.1059
Epoch 4/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1275
 480/6530 [=>............................] - ETA: 0s - loss: 0.1130
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1128
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1117
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1118
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1131
3040/6530 [============>.................] - ETA: 0s - loss: 0.1129
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1145
4128/6530 [=================>............] - ETA: 0s - loss: 0.1149
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1154
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1150
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1147
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1149
6368/6530 [============================>.] - ETA: 0s - loss: 0.1152
6530/6530 [==============================] - 1s 112us/step - loss: 0.1151 - val_loss: 0.1080
Epoch 5/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1293
 448/6530 [=>............................] - ETA: 0s - loss: 0.1140
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1106
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1117
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1119
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1124
3136/6530 [=============>................] - ETA: 0s - loss: 0.1114
3776/6530 [================>.............] - ETA: 0s - loss: 0.1123
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1128
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1132
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1126
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1124
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1126
6530/6530 [==============================] - 1s 106us/step - loss: 0.1122 - val_loss: 0.0908
Epoch 6/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1010
 544/6530 [=>............................] - ETA: 0s - loss: 0.1057
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1083
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1079
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1073
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1082
3264/6530 [=============>................] - ETA: 0s - loss: 0.1069
3712/6530 [================>.............] - ETA: 0s - loss: 0.1069
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1072
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1077
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1081
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1081
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1079
6464/6530 [============================>.] - ETA: 0s - loss: 0.1084
6530/6530 [==============================] - 1s 108us/step - loss: 0.1084 - val_loss: 0.0908
Epoch 7/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1070
 544/6530 [=>............................] - ETA: 0s - loss: 0.1123
 992/6530 [===>..........................] - ETA: 0s - loss: 0.1088
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1083
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1092
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1072
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1069
3168/6530 [=============>................] - ETA: 0s - loss: 0.1062
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1069
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1070
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1072
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1073
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1071
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1073
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1079
6530/6530 [==============================] - 1s 121us/step - loss: 0.1074 - val_loss: 0.0826
Epoch 8/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1254
 480/6530 [=>............................] - ETA: 0s - loss: 0.1064
 992/6530 [===>..........................] - ETA: 0s - loss: 0.1048
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1038
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1041
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1041
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1045
3072/6530 [=============>................] - ETA: 0s - loss: 0.1035
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1041
3808/6530 [================>.............] - ETA: 0s - loss: 0.1044
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1041
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1043
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1045
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1046
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1047
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1048
6496/6530 [============================>.] - ETA: 0s - loss: 0.1051
6530/6530 [==============================] - 1s 136us/step - loss: 0.1051 - val_loss: 0.0857
Epoch 9/81

  32/6530 [..............................] - ETA: 1s - loss: 0.1065
 384/6530 [>.............................] - ETA: 0s - loss: 0.1036
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1041
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1038
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1033
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1040
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1035
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1039
3008/6530 [============>.................] - ETA: 0s - loss: 0.1035
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1035
3744/6530 [================>.............] - ETA: 0s - loss: 0.1035
4096/6530 [=================>............] - ETA: 0s - loss: 0.1038
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1040
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1040
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1039
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1037
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1036
6368/6530 [============================>.] - ETA: 0s - loss: 0.1043
6530/6530 [==============================] - 1s 145us/step - loss: 0.1042 - val_loss: 0.0855
Epoch 10/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1044
 448/6530 [=>............................] - ETA: 0s - loss: 0.0992
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1006
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1008
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1015
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1021
3072/6530 [=============>................] - ETA: 0s - loss: 0.1011
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1018
4096/6530 [=================>............] - ETA: 0s - loss: 0.1017
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1023
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1026
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1025
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1029
6530/6530 [==============================] - 1s 105us/step - loss: 0.1032 - val_loss: 0.0898
Epoch 11/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1135
 576/6530 [=>............................] - ETA: 0s - loss: 0.0987
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1011
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0999
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1002
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1006
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1007
3072/6530 [=============>................] - ETA: 0s - loss: 0.1001
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1002
3808/6530 [================>.............] - ETA: 0s - loss: 0.1004
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1001
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1010
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1014
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1011
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1010
6530/6530 [==============================] - 1s 123us/step - loss: 0.1012 - val_loss: 0.0842
Epoch 12/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0907
 512/6530 [=>............................] - ETA: 0s - loss: 0.1014
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0980
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0977
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0991
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0996
3040/6530 [============>.................] - ETA: 0s - loss: 0.0991
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1000
3968/6530 [=================>............] - ETA: 0s - loss: 0.0999
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0999
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1002
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1003
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1002
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1004
6496/6530 [============================>.] - ETA: 0s - loss: 0.1005
6530/6530 [==============================] - 1s 114us/step - loss: 0.1004 - val_loss: 0.0859

# training | RMSE: 0.1069, MAE: 0.0841
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20166402653594076}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26587831508262944}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1965905869310106}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.1068976530178067, 'rmse': 0.1068976530178067, 'mae': 0.08413814344205026, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=81.0 loss={'loss': 0.1068976530178067, 'rmse': 0.1068976530178067, 'mae': 0.08413814344205026, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20166402653594076}, 'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26587831508262944}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1965905869310106}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
get a list [results] of length 170
get a list [loss] of length 1
get a list [val_loss] of length 1
length of indices is (0,)
length of indices is 1
length of T is 1
s=2
T is of size 15
T=[{'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4802424283625154}, 'layer_1_size': 47, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38460543082365073}, 'layer_3_size': 88, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4998246419108864}, 'layer_4_size': 4, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1192090463413142}, 'layer_4_size': 47, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23040836054459668}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2680780636062813}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 45, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13492815742173062}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 69, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.440102233913089}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10947132527782243}, 'layer_2_size': 96, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3858775033138824}, 'layer_3_size': 17, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.27212081010555467}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3006015944095446}, 'layer_1_size': 57, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 26, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.20952434789544042}, 'layer_4_size': 92, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.490753343048291}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18765862401836017}, 'layer_3_size': 26, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4329524849559595}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1363927772286982}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11948090427468112}, 'layer_3_size': 48, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3349483602683996}, 'layer_1_size': 29, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 25, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3970860647381087}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 16, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21350832101767667}, 'layer_4_size': 30, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14988470208751936}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.32783201871512857}, 'layer_1_size': 12, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 89, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1747291073747546}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 53, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23447433164356346}, 'layer_4_size': 18, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]
newly formed T structure is:[[0, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4802424283625154}, 'layer_1_size': 47, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38460543082365073}, 'layer_3_size': 88, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4998246419108864}, 'layer_4_size': 4, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [1, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1192090463413142}, 'layer_4_size': 47, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [2, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23040836054459668}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [3, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2680780636062813}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 45, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13492815742173062}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [4, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 69, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [5, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.440102233913089}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10947132527782243}, 'layer_2_size': 96, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3858775033138824}, 'layer_3_size': 17, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.27212081010555467}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [6, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3006015944095446}, 'layer_1_size': 57, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 26, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [7, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.20952434789544042}, 'layer_4_size': 92, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.490753343048291}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [8, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18765862401836017}, 'layer_3_size': 26, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4329524849559595}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [9, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1363927772286982}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11948090427468112}, 'layer_3_size': 48, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [10, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3349483602683996}, 'layer_1_size': 29, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 25, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3970860647381087}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [11, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 16, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21350832101767667}, 'layer_4_size': 30, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14988470208751936}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [12, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.32783201871512857}, 'layer_1_size': 12, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 89, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1747291073747546}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 53, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [13, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [14, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23447433164356346}, 'layer_4_size': 18, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]] 

*** 15 configurations x 9.0 iterations each

1 | Fri Sep 28 01:22:38 2018 | lowest loss so far: 0.0705 (run 0)

{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  47 | activation: sigmoid | extras: dropout - rate: 48.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 5:04 - loss: 0.6244
 480/6530 [=>............................] - ETA: 10s - loss: 0.3343 
 960/6530 [===>..........................] - ETA: 4s - loss: 0.2057 
1504/6530 [=====>........................] - ETA: 3s - loss: 0.1544{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  53 | activation: tanh    | extras: None 
layer 2 | size:  77 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9ece80>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 2:58 - loss: 0.4978
2048/6530 [========>.....................] - ETA: 2s - loss: 0.1297
1056/6530 [===>..........................] - ETA: 4s - loss: 0.2418  
2608/6530 [==========>...................] - ETA: 1s - loss: 0.1145
2080/6530 [========>.....................] - ETA: 2s - loss: 0.2025
3216/6530 [=============>................] - ETA: 1s - loss: 0.1028
2944/6530 [============>.................] - ETA: 1s - loss: 0.1927
3792/6530 [================>.............] - ETA: 0s - loss: 0.0955
3904/6530 [================>.............] - ETA: 0s - loss: 0.1853
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0891{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  38 | activation: tanh    | extras: None 
layer 2 | size:  51 | activation: relu    | extras: batchnorm 
layer 3 | size:  91 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9eceb8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 44s - loss: 0.9542
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1816
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0840
2688/6530 [===========>..................] - ETA: 1s - loss: 0.2575 
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1784
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0805
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1708
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0777
6530/6530 [==============================] - 1s 193us/step - loss: 0.1765 - val_loss: 0.1642
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1711
6530/6530 [==============================] - 1s 164us/step - loss: 0.1413 - val_loss: 0.0487
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0496
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1599
6530/6530 [==============================] - 1s 211us/step - loss: 0.0755 - val_loss: 0.0455
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0582
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0423
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1617
 592/6530 [=>............................] - ETA: 0s - loss: 0.0476
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0409
3040/6530 [============>.................] - ETA: 0s - loss: 0.1617
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0463
6530/6530 [==============================] - 0s 20us/step - loss: 0.0402 - val_loss: 0.0389
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0375
4096/6530 [=================>............] - ETA: 0s - loss: 0.1614
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0431
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0351
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1621
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0433
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0347
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1622
6530/6530 [==============================] - 0s 21us/step - loss: 0.0344 - val_loss: 0.0348
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0329
2992/6530 [============>.................] - ETA: 0s - loss: 0.0426
6530/6530 [==============================] - 0s 52us/step - loss: 0.1626 - val_loss: 0.1596
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1656
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0315
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0426
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1600
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0426
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0312
6530/6530 [==============================] - 0s 20us/step - loss: 0.0311 - val_loss: 0.0318

2048/6530 [========>.....................] - ETA: 0s - loss: 0.1617Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0294
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0427
3040/6530 [============>.................] - ETA: 0s - loss: 0.1604
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0288
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0426
4032/6530 [=================>............] - ETA: 0s - loss: 0.1600
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0286
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0425
6530/6530 [==============================] - 0s 20us/step - loss: 0.0286 - val_loss: 0.0295
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0268
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1595
6512/6530 [============================>.] - ETA: 0s - loss: 0.0423
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0265
6530/6530 [==============================] - 1s 89us/step - loss: 0.0422 - val_loss: 0.0415
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0493
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1587
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0265
6530/6530 [==============================] - 0s 52us/step - loss: 0.1587 - val_loss: 0.1516
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1343
 624/6530 [=>............................] - ETA: 0s - loss: 0.0434
6530/6530 [==============================] - 0s 20us/step - loss: 0.0265 - val_loss: 0.0274
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0246
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1487
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0417
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0248
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1465
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0408
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0249
3040/6530 [============>.................] - ETA: 0s - loss: 0.1483
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0410
6530/6530 [==============================] - 0s 22us/step - loss: 0.0248 - val_loss: 0.0258
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0226
4064/6530 [=================>............] - ETA: 0s - loss: 0.1474
3104/6530 [=============>................] - ETA: 0s - loss: 0.0402
2944/6530 [============>.................] - ETA: 0s - loss: 0.0230
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1467
3728/6530 [================>.............] - ETA: 0s - loss: 0.0408
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0233
6530/6530 [==============================] - 0s 20us/step - loss: 0.0234 - val_loss: 0.0243
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0210
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1460
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0409
6530/6530 [==============================] - 0s 52us/step - loss: 0.1457 - val_loss: 0.1402
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1260
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0219
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0412
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1343
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0222
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0414
6530/6530 [==============================] - 0s 21us/step - loss: 0.0222 - val_loss: 0.0230

2048/6530 [========>.....................] - ETA: 0s - loss: 0.1314
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0414
3072/6530 [=============>................] - ETA: 0s - loss: 0.1282
6530/6530 [==============================] - 1s 89us/step - loss: 0.0412 - val_loss: 0.0412
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0492
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1284
 640/6530 [=>............................] - ETA: 0s - loss: 0.0434
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1275
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0411
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1274
6530/6530 [==============================] - 0s 50us/step - loss: 0.1270 - val_loss: 0.1224
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1169
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0408
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1168
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0413
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1167
3152/6530 [=============>................] - ETA: 0s - loss: 0.0401
3200/6530 [=============>................] - ETA: 0s - loss: 0.1167
3728/6530 [================>.............] - ETA: 0s - loss: 0.0408
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1156
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0409
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1152
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0412
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1143
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0412
6530/6530 [==============================] - 0s 50us/step - loss: 0.1143 - val_loss: 0.1200
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1091
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0414
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1113
6530/6530 [==============================] - 1s 86us/step - loss: 0.0412 - val_loss: 0.0411
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0494
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1096
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0432
3136/6530 [=============>................] - ETA: 0s - loss: 0.1095
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0413
4128/6530 [=================>............] - ETA: 0s - loss: 0.1087
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0407
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1076
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0410
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1069
6530/6530 [==============================] - 0s 51us/step - loss: 0.1069 - val_loss: 0.1105
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1043
3120/6530 [=============>................] - ETA: 0s - loss: 0.0402
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1084
3760/6530 [================>.............] - ETA: 0s - loss: 0.0407
# training | RMSE: 0.1434, MAE: 0.1120
worker 1  xfile  [1, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1192090463413142}, 'layer_4_size': 47, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1433806243355196, 'rmse': 0.1433806243355196, 'mae': 0.11195617467151867, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  50 | activation: tanh    | extras: batchnorm 
layer 2 | size:  35 | activation: tanh    | extras: dropout - rate: 26.8% 
layer 3 | size:  71 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9ecb00>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 20s - loss: 0.9776
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1057
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0409
2304/6530 [=========>....................] - ETA: 0s - loss: 0.6726 
3072/6530 [=============>................] - ETA: 0s - loss: 0.1060
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0412
4480/6530 [===================>..........] - ETA: 0s - loss: 0.5624
4128/6530 [=================>............] - ETA: 0s - loss: 0.1039
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0412
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1033
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0414
6530/6530 [==============================] - 1s 93us/step - loss: 0.4920 - val_loss: 0.2331
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.3289
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1026
2048/6530 [========>.....................] - ETA: 0s - loss: 0.2952
6530/6530 [==============================] - 0s 52us/step - loss: 0.1024 - val_loss: 0.1029
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1108
6530/6530 [==============================] - 1s 87us/step - loss: 0.0412 - val_loss: 0.0411
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0495
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2804
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0967
 608/6530 [=>............................] - ETA: 0s - loss: 0.0435
6528/6530 [============================>.] - ETA: 0s - loss: 0.2706
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0990
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0414
6530/6530 [==============================] - 0s 26us/step - loss: 0.2706 - val_loss: 0.1881
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.2572
3104/6530 [=============>................] - ETA: 0s - loss: 0.0983
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0409
2304/6530 [=========>....................] - ETA: 0s - loss: 0.2359
4128/6530 [=================>............] - ETA: 0s - loss: 0.0980
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0407
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2295
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0980
2992/6530 [============>.................] - ETA: 0s - loss: 0.0406
6530/6530 [==============================] - 0s 25us/step - loss: 0.2232 - val_loss: 0.2037
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.2196
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0409
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0978
2304/6530 [=========>....................] - ETA: 0s - loss: 0.2123
6530/6530 [==============================] - 0s 51us/step - loss: 0.0981 - val_loss: 0.1016

4208/6530 [==================>...........] - ETA: 0s - loss: 0.0410
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2092
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0413
6530/6530 [==============================] - 0s 25us/step - loss: 0.2053 - val_loss: 0.1543
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1760
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0414
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1911
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0414
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1890
6530/6530 [==============================] - 1s 86us/step - loss: 0.0412 - val_loss: 0.0411
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0496
6530/6530 [==============================] - 0s 24us/step - loss: 0.1858 - val_loss: 0.1682
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1781
 608/6530 [=>............................] - ETA: 0s - loss: 0.0436
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1744
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0414
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1740
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0409
6530/6530 [==============================] - 0s 24us/step - loss: 0.1740 - val_loss: 0.1781
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.2089
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0411
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1711
3072/6530 [=============>................] - ETA: 0s - loss: 0.0404
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1687
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0410
6530/6530 [==============================] - 0s 23us/step - loss: 0.1660 - val_loss: 0.1695
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1617
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0410
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1563
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0413
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1562
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0412
6530/6530 [==============================] - 0s 25us/step - loss: 0.1573 - val_loss: 0.1484
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1680
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0414
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1568
6530/6530 [==============================] - 1s 87us/step - loss: 0.0412 - val_loss: 0.0411
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0496
# training | RMSE: 0.1265, MAE: 0.0976
worker 2  xfile  [2, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23040836054459668}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.12646634725699943, 'rmse': 0.12646634725699943, 'mae': 0.09757215494172035, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  69 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  91 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9ec978>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 55s - loss: 2.3878
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1542
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0433
 800/6530 [==>...........................] - ETA: 2s - loss: 0.6800 
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1527
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0420
6530/6530 [==============================] - 0s 26us/step - loss: 0.1530 - val_loss: 0.1679

1600/6530 [======>.......................] - ETA: 1s - loss: 0.3660
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0402
2400/6530 [==========>...................] - ETA: 0s - loss: 0.2603
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0408
3168/6530 [=============>................] - ETA: 0s - loss: 0.2073
3024/6530 [============>.................] - ETA: 0s - loss: 0.0405
3968/6530 [=================>............] - ETA: 0s - loss: 0.1754
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0410
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1527
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0411
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1371
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0412
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1263
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0414
6530/6530 [==============================] - 1s 115us/step - loss: 0.1232 - val_loss: 0.0379
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0567
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0414
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0404
6530/6530 [==============================] - 1s 87us/step - loss: 0.0412 - val_loss: 0.0411
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0496
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0378
 640/6530 [=>............................] - ETA: 0s - loss: 0.0435
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0373
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0416
3104/6530 [=============>................] - ETA: 0s - loss: 0.0358
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0409
3840/6530 [================>.............] - ETA: 0s - loss: 0.0359
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0412
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0357
3152/6530 [=============>................] - ETA: 0s - loss: 0.0402
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0351
# training | RMSE: 0.2041, MAE: 0.1603
worker 1  xfile  [3, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2680780636062813}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 45, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13492815742173062}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2041044027372604, 'rmse': 0.2041044027372604, 'mae': 0.1603393027066329, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  77 | activation: tanh    | extras: dropout - rate: 44.0% 
layer 2 | size:  96 | activation: sigmoid | extras: dropout - rate: 10.9% 
layer 3 | size:  17 | activation: sigmoid | extras: dropout - rate: 38.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c4a5eb8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 1:00 - loss: 0.6587
3760/6530 [================>.............] - ETA: 0s - loss: 0.0408
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0346
 736/6530 [==>...........................] - ETA: 2s - loss: 0.4508  
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0409
6530/6530 [==============================] - 0s 71us/step - loss: 0.0345 - val_loss: 0.0295
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0489
1472/6530 [=====>........................] - ETA: 1s - loss: 0.2962
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0413
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0319
2208/6530 [=========>....................] - ETA: 0s - loss: 0.2243
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0412
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0296
2912/6530 [============>.................] - ETA: 0s - loss: 0.1865
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0414
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0289
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1633
6530/6530 [==============================] - 1s 87us/step - loss: 0.0412 - val_loss: 0.0411

3136/6530 [=============>................] - ETA: 0s - loss: 0.0281
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1469
3872/6530 [================>.............] - ETA: 0s - loss: 0.0284
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1348
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0282
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1268
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0277
6530/6530 [==============================] - 1s 121us/step - loss: 0.1210 - val_loss: 0.0694
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0815
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0276
6530/6530 [==============================] - 0s 68us/step - loss: 0.0276 - val_loss: 0.0265
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0406
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0757
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0254
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0749
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0246
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0735
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0240
# training | RMSE: 0.2015, MAE: 0.1624
worker 0  xfile  [0, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4802424283625154}, 'layer_1_size': 47, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38460543082365073}, 'layer_3_size': 88, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4998246419108864}, 'layer_4_size': 4, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.20152233269177042, 'rmse': 0.20152233269177042, 'mae': 0.16240338237292382, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  57 | activation: sigmoid | extras: dropout - rate: 30.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9ecb00>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 13s - loss: 0.4898
2912/6530 [============>.................] - ETA: 0s - loss: 0.0721
3264/6530 [=============>................] - ETA: 0s - loss: 0.0233
2304/6530 [=========>....................] - ETA: 0s - loss: 0.2658 
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0720
4064/6530 [=================>............] - ETA: 0s - loss: 0.0235
4608/6530 [====================>.........] - ETA: 0s - loss: 0.2220
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0710
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0235
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0701
6530/6530 [==============================] - 0s 45us/step - loss: 0.2069 - val_loss: 0.1696
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1740
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0230
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0704
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1664
6464/6530 [============================>.] - ETA: 0s - loss: 0.0231
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1668
6530/6530 [==============================] - 0s 72us/step - loss: 0.0700 - val_loss: 0.0695
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0815
6530/6530 [==============================] - 0s 67us/step - loss: 0.0231 - val_loss: 0.0283
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0351
6530/6530 [==============================] - 0s 24us/step - loss: 0.1672 - val_loss: 0.1680
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1725
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0750
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0224
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1653
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0746
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0214
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1651
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0740
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0210
6530/6530 [==============================] - 0s 24us/step - loss: 0.1657 - val_loss: 0.1668
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1719
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0725
3072/6530 [=============>................] - ETA: 0s - loss: 0.0204
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1639
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0717
3808/6530 [================>.............] - ETA: 0s - loss: 0.0205
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1637
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0712
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0206
6530/6530 [==============================] - 0s 24us/step - loss: 0.1643 - val_loss: 0.1651
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1704
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0702
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0204
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1628
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0701
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0204
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1628
6530/6530 [==============================] - 0s 70us/step - loss: 0.0204 - val_loss: 0.0305

6432/6530 [============================>.] - ETA: 0s - loss: 0.0700Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0328
6530/6530 [==============================] - 0s 74us/step - loss: 0.0697 - val_loss: 0.0691
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0813
6530/6530 [==============================] - 0s 23us/step - loss: 0.1629 - val_loss: 0.1637
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1692
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0198
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0747
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1613
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0192
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0742
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1606
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0191
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0728
6530/6530 [==============================] - 0s 24us/step - loss: 0.1615 - val_loss: 0.1638
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1691
3072/6530 [=============>................] - ETA: 0s - loss: 0.0185
2848/6530 [============>.................] - ETA: 0s - loss: 0.0717
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1598
3808/6530 [================>.............] - ETA: 0s - loss: 0.0187
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0711
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1599
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0187
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0702
6530/6530 [==============================] - 0s 23us/step - loss: 0.1602 - val_loss: 0.1622
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1677
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0186
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0693
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1584
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0186
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0694
6530/6530 [==============================] - 0s 69us/step - loss: 0.0187 - val_loss: 0.0280
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0299
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1586
6530/6530 [==============================] - 0s 72us/step - loss: 0.0690 - val_loss: 0.0678
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0807
6530/6530 [==============================] - 0s 23us/step - loss: 0.1588 - val_loss: 0.1606

 736/6530 [==>...........................] - ETA: 0s - loss: 0.0182Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1663
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0723
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0178
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1566
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0714
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0175
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1571
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0709
3072/6530 [=============>................] - ETA: 0s - loss: 0.0171
6530/6530 [==============================] - 0s 23us/step - loss: 0.1574 - val_loss: 0.1593

3008/6530 [============>.................] - ETA: 0s - loss: 0.0690
3840/6530 [================>.............] - ETA: 0s - loss: 0.0172
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0688
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0173
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0676
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0170
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0666
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0173
6530/6530 [==============================] - 0s 68us/step - loss: 0.0173 - val_loss: 0.0234
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0265
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0661
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0164
6530/6530 [==============================] - 0s 72us/step - loss: 0.0656 - val_loss: 0.0614
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0745
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0162
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0661
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0162
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0619
3104/6530 [=============>................] - ETA: 0s - loss: 0.0159
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0603
3840/6530 [================>.............] - ETA: 0s - loss: 0.0161
3136/6530 [=============>................] - ETA: 0s - loss: 0.0580
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0162
3872/6530 [================>.............] - ETA: 0s - loss: 0.0569
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0159
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0553
# training | RMSE: 0.1977, MAE: 0.1575
worker 0  xfile  [6, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3006015944095446}, 'layer_1_size': 57, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 26, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.19766927572926624, 'rmse': 0.19766927572926624, 'mae': 0.15745894806239677, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  50 | activation: tanh    | extras: None 
layer 2 | size:  75 | activation: sigmoid | extras: None 
layer 3 | size:  39 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c4e5208>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 1:27 - loss: 1.1772
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0162
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0541
 416/6530 [>.............................] - ETA: 3s - loss: 0.1616  
6530/6530 [==============================] - 0s 68us/step - loss: 0.0162 - val_loss: 0.0194
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0236
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0529
 832/6530 [==>...........................] - ETA: 2s - loss: 0.1024
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0155
6530/6530 [==============================] - 0s 70us/step - loss: 0.0520 - val_loss: 0.0476
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0593
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0836
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0152
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0453
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0723
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0153
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0426
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0668
3104/6530 [=============>................] - ETA: 0s - loss: 0.0149
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0424
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0621
3904/6530 [================>.............] - ETA: 0s - loss: 0.0152
3072/6530 [=============>................] - ETA: 0s - loss: 0.0417
2832/6530 [============>.................] - ETA: 0s - loss: 0.0601
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0152
3808/6530 [================>.............] - ETA: 0s - loss: 0.0419
3232/6530 [=============>................] - ETA: 0s - loss: 0.0575
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0150
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0422
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0573
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0154
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0423
6530/6530 [==============================] - 0s 68us/step - loss: 0.0154 - val_loss: 0.0165

4048/6530 [=================>............] - ETA: 0s - loss: 0.0554
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0423
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0540
6530/6530 [==============================] - 0s 72us/step - loss: 0.0420 - val_loss: 0.0452
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0574
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0536
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0447
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0526
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0421
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0514
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0416
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0500
2944/6530 [============>.................] - ETA: 0s - loss: 0.0420
6496/6530 [============================>.] - ETA: 0s - loss: 0.0493
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0417
6530/6530 [==============================] - 1s 168us/step - loss: 0.0494 - val_loss: 0.0610
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0596
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0416
 368/6530 [>.............................] - ETA: 0s - loss: 0.0371
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0418
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0363
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0417
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0370
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0420
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0363
6530/6530 [==============================] - 1s 78us/step - loss: 0.0417 - val_loss: 0.0464
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0586
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0346
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0443
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0338
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0413
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0327
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0417
3136/6530 [=============>................] - ETA: 0s - loss: 0.0322
3136/6530 [=============>................] - ETA: 0s - loss: 0.0410
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0321
3872/6530 [================>.............] - ETA: 0s - loss: 0.0415
3936/6530 [=================>............] - ETA: 0s - loss: 0.0323
# training | RMSE: 0.1225, MAE: 0.0990
worker 2  xfile  [4, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 69, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.12251142140393727, 'rmse': 0.12251142140393727, 'mae': 0.09899345813141994, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  61 | activation: relu    | extras: batchnorm 
layer 2 | size:  90 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c657c88>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 35s - loss: 0.6745
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0417
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0319
1344/6530 [=====>........................] - ETA: 1s - loss: 0.3191 
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0418
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0316
2624/6530 [===========>..................] - ETA: 0s - loss: 0.2431
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0418
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0315
3968/6530 [=================>............] - ETA: 0s - loss: 0.2162
6530/6530 [==============================] - 0s 71us/step - loss: 0.0415 - val_loss: 0.0468

5504/6530 [========================>.....] - ETA: 0s - loss: 0.0311
5184/6530 [======================>.......] - ETA: 0s - loss: 0.2004
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0307
6400/6530 [============================>.] - ETA: 0s - loss: 0.1923
6336/6530 [============================>.] - ETA: 0s - loss: 0.0304
6530/6530 [==============================] - 1s 100us/step - loss: 0.1909 - val_loss: 0.1933
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1783
6530/6530 [==============================] - 1s 133us/step - loss: 0.0303 - val_loss: 0.0264
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0353
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1461
 416/6530 [>.............................] - ETA: 0s - loss: 0.0260
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1430
 848/6530 [==>...........................] - ETA: 0s - loss: 0.0239
3968/6530 [=================>............] - ETA: 0s - loss: 0.1391
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0250
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1363
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0248
6530/6530 [==============================] - 0s 42us/step - loss: 0.1353 - val_loss: 0.1411
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1583
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0246
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1258
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0248
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1262
2880/6530 [============>.................] - ETA: 0s - loss: 0.0247
# training | RMSE: 0.2143, MAE: 0.1762
worker 1  xfile  [5, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.440102233913089}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10947132527782243}, 'layer_2_size': 96, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3858775033138824}, 'layer_3_size': 17, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.27212081010555467}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.21425025889611057, 'rmse': 0.21425025889611057, 'mae': 0.17618226729002057, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  85 | activation: tanh    | extras: dropout - rate: 13.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f03c063eba8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 4s - loss: 0.7356
3904/6530 [================>.............] - ETA: 0s - loss: 0.1266
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0247
6530/6530 [==============================] - 0s 38us/step - loss: 0.4609 - val_loss: 0.1874
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1848
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1250
3712/6530 [================>.............] - ETA: 0s - loss: 0.0244
6530/6530 [==============================] - 0s 6us/step - loss: 0.1690 - val_loss: 0.1605
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1608
6464/6530 [============================>.] - ETA: 0s - loss: 0.1241
6530/6530 [==============================] - 0s 6us/step - loss: 0.1610 - val_loss: 0.1599
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1582
6530/6530 [==============================] - 0s 41us/step - loss: 0.1240 - val_loss: 0.2432
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1685
4128/6530 [=================>............] - ETA: 0s - loss: 0.0242
6530/6530 [==============================] - 0s 6us/step - loss: 0.1602 - val_loss: 0.1599
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1580
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1209
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0245
6530/6530 [==============================] - 0s 5us/step - loss: 0.1600 - val_loss: 0.1594
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1578
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1184
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0245
6530/6530 [==============================] - 0s 5us/step - loss: 0.1598 - val_loss: 0.1594
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1576
6530/6530 [==============================] - 0s 6us/step - loss: 0.1595 - val_loss: 0.1590
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1576
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0244
3840/6530 [================>.............] - ETA: 0s - loss: 0.1171
6530/6530 [==============================] - 0s 6us/step - loss: 0.1595 - val_loss: 0.1589
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1573
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0243
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1174
6530/6530 [==============================] - 0s 6us/step - loss: 0.1591 - val_loss: 0.1586

6160/6530 [===========================>..] - ETA: 0s - loss: 0.0239
6336/6530 [============================>.] - ETA: 0s - loss: 0.1161
6530/6530 [==============================] - 0s 42us/step - loss: 0.1162 - val_loss: 0.1847
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1321
6530/6530 [==============================] - 1s 128us/step - loss: 0.0240 - val_loss: 0.0426
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0274
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1122
 432/6530 [>.............................] - ETA: 0s - loss: 0.0220
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1111
 848/6530 [==>...........................] - ETA: 0s - loss: 0.0216
3968/6530 [=================>............] - ETA: 0s - loss: 0.1097
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0205
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1094
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0201
6530/6530 [==============================] - 0s 40us/step - loss: 0.1089 - val_loss: 0.1229
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1736
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0204
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1069
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0201
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1039
2928/6530 [============>.................] - ETA: 0s - loss: 0.0209
3840/6530 [================>.............] - ETA: 0s - loss: 0.1042
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0210
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1044
# training | RMSE: 0.2014, MAE: 0.1584
worker 1  xfile  [9, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1363927772286982}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11948090427468112}, 'layer_3_size': 48, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20138584107163854, 'rmse': 0.20138584107163854, 'mae': 0.15841766720362738, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  29 | activation: relu    | extras: dropout - rate: 33.5% 
layer 2 | size:  42 | activation: relu    | extras: None 
layer 3 | size:  91 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03e00cc710>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 1:40 - loss: 2.0362
3776/6530 [================>.............] - ETA: 0s - loss: 0.0212
6528/6530 [============================>.] - ETA: 0s - loss: 0.1043
6530/6530 [==============================] - 0s 42us/step - loss: 0.1044 - val_loss: 0.2197
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2664
 528/6530 [=>............................] - ETA: 3s - loss: 0.3523  
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0211
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1054
 976/6530 [===>..........................] - ETA: 1s - loss: 0.2923
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0212
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1034
1488/6530 [=====>........................] - ETA: 1s - loss: 0.2573
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0211
3904/6530 [================>.............] - ETA: 0s - loss: 0.1030
2000/6530 [========>.....................] - ETA: 1s - loss: 0.2408
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0208
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1015
2480/6530 [==========>...................] - ETA: 0s - loss: 0.2315
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0203
6400/6530 [============================>.] - ETA: 0s - loss: 0.1000
3008/6530 [============>.................] - ETA: 0s - loss: 0.2217
6530/6530 [==============================] - 0s 42us/step - loss: 0.1001 - val_loss: 0.1785
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1231
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0203
3520/6530 [===============>..............] - ETA: 0s - loss: 0.2157
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0949
6530/6530 [==============================] - 1s 127us/step - loss: 0.0202 - val_loss: 0.0297
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0432
4032/6530 [=================>............] - ETA: 0s - loss: 0.2112
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0969
 416/6530 [>.............................] - ETA: 0s - loss: 0.0205
4528/6530 [===================>..........] - ETA: 0s - loss: 0.2074
3840/6530 [================>.............] - ETA: 0s - loss: 0.0980
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0171
5024/6530 [======================>.......] - ETA: 0s - loss: 0.2044
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0986
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0170
5520/6530 [========================>.....] - ETA: 0s - loss: 0.2019
6400/6530 [============================>.] - ETA: 0s - loss: 0.0990
6530/6530 [==============================] - 0s 42us/step - loss: 0.0991 - val_loss: 0.1515
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1655
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0173
6000/6530 [==========================>...] - ETA: 0s - loss: 0.2002
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1001
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0167
6512/6530 [============================>.] - ETA: 0s - loss: 0.1982
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1029
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0170
6530/6530 [==============================] - 1s 150us/step - loss: 0.1981 - val_loss: 0.1706
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1947
3904/6530 [================>.............] - ETA: 0s - loss: 0.0997
2912/6530 [============>.................] - ETA: 0s - loss: 0.0168
 528/6530 [=>............................] - ETA: 0s - loss: 0.1808
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0962
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0172
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1807
6464/6530 [============================>.] - ETA: 0s - loss: 0.0952
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0171
6530/6530 [==============================] - 0s 42us/step - loss: 0.0950 - val_loss: 0.1322

1456/6530 [=====>........................] - ETA: 0s - loss: 0.1746
4000/6530 [=================>............] - ETA: 0s - loss: 0.0172
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1757
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0170
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1751
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0168
3008/6530 [============>.................] - ETA: 0s - loss: 0.1733
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0166
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1723
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0167
4032/6530 [=================>............] - ETA: 0s - loss: 0.1721
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0165
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1721
6512/6530 [============================>.] - ETA: 0s - loss: 0.0164
6530/6530 [==============================] - 1s 130us/step - loss: 0.0164 - val_loss: 0.0185
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0200
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1721
 448/6530 [=>............................] - ETA: 0s - loss: 0.0169
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1714
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0169
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1709
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0161
6530/6530 [==============================] - 1s 104us/step - loss: 0.1704 - val_loss: 0.1697
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1795
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0162
 512/6530 [=>............................] - ETA: 0s - loss: 0.1715
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0163
1040/6530 [===>..........................] - ETA: 0s - loss: 0.1723
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0165
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1650
3056/6530 [=============>................] - ETA: 0s - loss: 0.0167
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1657
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0168
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1677
3904/6530 [================>.............] - ETA: 0s - loss: 0.0166
3136/6530 [=============>................] - ETA: 0s - loss: 0.1658
# training | RMSE: 0.1509, MAE: 0.1260
worker 2  xfile  [8, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18765862401836017}, 'layer_3_size': 26, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4329524849559595}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.1509010426040011, 'rmse': 0.1509010426040011, 'mae': 0.1260299719815674, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  90 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  16 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f03f2732f98>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 16s - loss: 0.7064
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0162
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1656
3072/6530 [=============>................] - ETA: 0s - loss: 0.5555 
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0159
4032/6530 [=================>............] - ETA: 0s - loss: 0.1652
6144/6530 [===========================>..] - ETA: 0s - loss: 0.4349
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0159
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1644
6530/6530 [==============================] - 0s 75us/step - loss: 0.4217 - val_loss: 0.1901
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.2081
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0159
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1637
3072/6530 [=============>................] - ETA: 0s - loss: 0.1702
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0156
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1642
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1667
6336/6530 [============================>.] - ETA: 0s - loss: 0.0156
6530/6530 [==============================] - 0s 18us/step - loss: 0.1665 - val_loss: 0.1724
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1613
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1640
6530/6530 [==============================] - 1s 125us/step - loss: 0.0155 - val_loss: 0.0151
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0066
2944/6530 [============>.................] - ETA: 0s - loss: 0.1626
6352/6530 [============================>.] - ETA: 0s - loss: 0.1638
 432/6530 [>.............................] - ETA: 0s - loss: 0.0172
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1602
6530/6530 [==============================] - 1s 108us/step - loss: 0.1635 - val_loss: 0.1520
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.2034
6530/6530 [==============================] - 0s 19us/step - loss: 0.1604 - val_loss: 0.1614
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1652
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0161
 528/6530 [=>............................] - ETA: 0s - loss: 0.1659
3200/6530 [=============>................] - ETA: 0s - loss: 0.1580
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0153
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1640
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1571
6530/6530 [==============================] - 0s 18us/step - loss: 0.1575 - val_loss: 0.1652
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1667
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0151
1520/6530 [=====>........................] - ETA: 0s - loss: 0.1614
3200/6530 [=============>................] - ETA: 0s - loss: 0.1524
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0150
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1619
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1494
6530/6530 [==============================] - 0s 18us/step - loss: 0.1495 - val_loss: 0.1500
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1423
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0145
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1625
2944/6530 [============>.................] - ETA: 0s - loss: 0.1438
2896/6530 [============>.................] - ETA: 0s - loss: 0.0144
3008/6530 [============>.................] - ETA: 0s - loss: 0.1606
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1407
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0142
6530/6530 [==============================] - 0s 18us/step - loss: 0.1405 - val_loss: 0.1423
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1468
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1596
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0142
3072/6530 [=============>................] - ETA: 0s - loss: 0.1394
4032/6530 [=================>............] - ETA: 0s - loss: 0.1596
4096/6530 [=================>............] - ETA: 0s - loss: 0.0143
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1362
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1592
6530/6530 [==============================] - 0s 19us/step - loss: 0.1359 - val_loss: 0.1385
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1292
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0142
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1591
3072/6530 [=============>................] - ETA: 0s - loss: 0.1356
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0142
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1594
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1347
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0140
6530/6530 [==============================] - 0s 18us/step - loss: 0.1342 - val_loss: 0.1378
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1333
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1599
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0143
3200/6530 [=============>................] - ETA: 0s - loss: 0.1315
6512/6530 [============================>.] - ETA: 0s - loss: 0.1596
6530/6530 [==============================] - 1s 106us/step - loss: 0.1596 - val_loss: 0.1594

6144/6530 [===========================>..] - ETA: 0s - loss: 0.0142Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1586
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1323
6530/6530 [==============================] - 0s 18us/step - loss: 0.1322 - val_loss: 0.1426

 528/6530 [=>............................] - ETA: 0s - loss: 0.1575
6530/6530 [==============================] - 1s 129us/step - loss: 0.0142 - val_loss: 0.0214
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0123
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1604
 432/6530 [>.............................] - ETA: 0s - loss: 0.0135
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1538
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0142
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1546
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0132
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1558
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0137
3136/6530 [=============>................] - ETA: 0s - loss: 0.1534
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0131
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1547
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0133
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1545
3008/6530 [============>.................] - ETA: 0s - loss: 0.0135
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1545
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0135
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1544
3856/6530 [================>.............] - ETA: 0s - loss: 0.0136
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1539
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0139
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1542
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0137
6530/6530 [==============================] - 1s 101us/step - loss: 0.1537 - val_loss: 0.1406
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1874
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0137
 528/6530 [=>............................] - ETA: 0s - loss: 0.1582
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0136
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1552
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0136
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1504
6368/6530 [============================>.] - ETA: 0s - loss: 0.0137
2064/6530 [========>.....................] - ETA: 0s - loss: 0.1520
6530/6530 [==============================] - 1s 125us/step - loss: 0.0137 - val_loss: 0.0180
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0201
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1532
 432/6530 [>.............................] - ETA: 0s - loss: 0.0137
3024/6530 [============>.................] - ETA: 0s - loss: 0.1518
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0126
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1521
# training | RMSE: 0.1828, MAE: 0.1396
worker 2  xfile  [11, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 16, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21350832101767667}, 'layer_4_size': 30, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14988470208751936}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.18283151854957994, 'rmse': 0.18283151854957994, 'mae': 0.13963405901913534, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  12 | activation: relu    | extras: dropout - rate: 32.8% 
layer 2 | size:  74 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  89 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03dc1ecc50>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 22s - loss: 0.6081
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0131
4016/6530 [=================>............] - ETA: 0s - loss: 0.1518
2304/6530 [=========>....................] - ETA: 0s - loss: 0.3694 
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0126
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1510
4608/6530 [====================>.........] - ETA: 0s - loss: 0.2078
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0125
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1505
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0124
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1503
6530/6530 [==============================] - 1s 101us/step - loss: 0.1600 - val_loss: 0.0416
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0551
2880/6530 [============>.................] - ETA: 0s - loss: 0.0124
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1508
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0125
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0444
6512/6530 [============================>.] - ETA: 0s - loss: 0.1501
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0127
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0421
6530/6530 [==============================] - 1s 106us/step - loss: 0.1501 - val_loss: 0.1477
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1622
4096/6530 [=================>............] - ETA: 0s - loss: 0.0130
6530/6530 [==============================] - 0s 25us/step - loss: 0.0408 - val_loss: 0.0348

 528/6530 [=>............................] - ETA: 0s - loss: 0.1482Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0394
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0131
1040/6530 [===>..........................] - ETA: 0s - loss: 0.1491
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0364
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0129
1552/6530 [======>.......................] - ETA: 0s - loss: 0.1458
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0369
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0127
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1479
6530/6530 [==============================] - 0s 24us/step - loss: 0.0367 - val_loss: 0.0332
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0415
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0127
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1475
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0375
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0128
2992/6530 [============>.................] - ETA: 0s - loss: 0.1454
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0365
6464/6530 [============================>.] - ETA: 0s - loss: 0.0127
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1455
6530/6530 [==============================] - 0s 25us/step - loss: 0.0364 - val_loss: 0.0302
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0342
6530/6530 [==============================] - 1s 130us/step - loss: 0.0127 - val_loss: 0.0188

3984/6530 [=================>............] - ETA: 0s - loss: 0.1452
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0351
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1458
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0350
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1457
6528/6530 [============================>.] - ETA: 0s - loss: 0.0351
6530/6530 [==============================] - 0s 25us/step - loss: 0.0350 - val_loss: 0.0307
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0410
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1458
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0349
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1466
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0351
6528/6530 [============================>.] - ETA: 0s - loss: 0.1458
6530/6530 [==============================] - 1s 105us/step - loss: 0.1458 - val_loss: 0.1390
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1356
6530/6530 [==============================] - 0s 25us/step - loss: 0.0356 - val_loss: 0.0307
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0414
 560/6530 [=>............................] - ETA: 0s - loss: 0.1456
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0348
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1467
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0348
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1440
6530/6530 [==============================] - 0s 24us/step - loss: 0.0348 - val_loss: 0.0279
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0364
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1434
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0351
# training | RMSE: 0.1310, MAE: 0.1038
worker 0  xfile  [7, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.20952434789544042}, 'layer_4_size': 92, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.490753343048291}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.1310494592036145, 'rmse': 0.1310494592036145, 'mae': 0.10376544324658829, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  25 | activation: relu    | extras: None 
layer 2 | size:  55 | activation: tanh    | extras: None 
layer 3 | size:  55 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c131a20>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 1:13 - loss: 0.5368
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1428
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0342
 512/6530 [=>............................] - ETA: 2s - loss: 0.2219  
2880/6530 [============>.................] - ETA: 0s - loss: 0.1430
6400/6530 [============================>.] - ETA: 0s - loss: 0.0345
6530/6530 [==============================] - 0s 27us/step - loss: 0.0344 - val_loss: 0.0285
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0494
1008/6530 [===>..........................] - ETA: 1s - loss: 0.1356
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1426
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0336
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1048
3840/6530 [================>.............] - ETA: 0s - loss: 0.1433
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0337
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0892
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1434
6530/6530 [==============================] - 0s 25us/step - loss: 0.0340 - val_loss: 0.0285

2480/6530 [==========>...................] - ETA: 0s - loss: 0.0798
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1438
3008/6530 [============>.................] - ETA: 0s - loss: 0.0739
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1437
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0695
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1433
4064/6530 [=================>............] - ETA: 0s - loss: 0.0654
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1438
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0624
6530/6530 [==============================] - 1s 110us/step - loss: 0.1432 - val_loss: 0.1335
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1431
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0597
 512/6530 [=>............................] - ETA: 0s - loss: 0.1421
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0584
 992/6530 [===>..........................] - ETA: 0s - loss: 0.1391
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0570
1488/6530 [=====>........................] - ETA: 0s - loss: 0.1379
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1371
6530/6530 [==============================] - 1s 134us/step - loss: 0.0561 - val_loss: 0.0536
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0861
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1395
 464/6530 [=>............................] - ETA: 0s - loss: 0.0385
3088/6530 [=============>................] - ETA: 0s - loss: 0.1384
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0380
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1386
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0375
4128/6530 [=================>............] - ETA: 0s - loss: 0.1387
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0368
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1386
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0362
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1393
3088/6530 [=============>................] - ETA: 0s - loss: 0.0359
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1392
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0358
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1401
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0355
6530/6530 [==============================] - 1s 104us/step - loss: 0.1400 - val_loss: 0.1341

4688/6530 [====================>.........] - ETA: 0s - loss: 0.0349
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0347
# training | RMSE: 0.1623, MAE: 0.1292
worker 2  xfile  [12, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.32783201871512857}, 'layer_1_size': 12, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 89, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1747291073747546}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 53, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.16226374778305688, 'rmse': 0.16226374778305688, 'mae': 0.12918728154970746, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:   7 | activation: relu    | extras: batchnorm 
layer 2 | size:  93 | activation: relu    | extras: None 
layer 3 | size:  73 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f03c4446978>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 47s - loss: 0.9315
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0346
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1278 
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0342
2944/6530 [============>.................] - ETA: 0s - loss: 0.0957
6530/6530 [==============================] - 1s 101us/step - loss: 0.0342 - val_loss: 0.0396
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0494
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0816
# training | RMSE: 0.1645, MAE: 0.1317
worker 1  xfile  [10, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3349483602683996}, 'layer_1_size': 29, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 25, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3970860647381087}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.16449039597204201, 'rmse': 0.16449039597204201, 'mae': 0.13165854943802716, 'early_stop': False}
vggnet done  1

 512/6530 [=>............................] - ETA: 0s - loss: 0.0297
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0750
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0306
6530/6530 [==============================] - 1s 116us/step - loss: 0.0729 - val_loss: 0.0623
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0545
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0301
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0480
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0295
3200/6530 [=============>................] - ETA: 0s - loss: 0.0468
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0292
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0478
3200/6530 [=============>................] - ETA: 0s - loss: 0.0292
6400/6530 [============================>.] - ETA: 0s - loss: 0.0470
3760/6530 [================>.............] - ETA: 0s - loss: 0.0289
6530/6530 [==============================] - 0s 35us/step - loss: 0.0469 - val_loss: 0.0476
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0541
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0285
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0450
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0286
3200/6530 [=============>................] - ETA: 0s - loss: 0.0451
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0282
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0437
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0278
6336/6530 [============================>.] - ETA: 0s - loss: 0.0432
6530/6530 [==============================] - 0s 34us/step - loss: 0.0432 - val_loss: 0.0734
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0932
6496/6530 [============================>.] - ETA: 0s - loss: 0.0278
6530/6530 [==============================] - 1s 98us/step - loss: 0.0277 - val_loss: 0.0301
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0411
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0462
 576/6530 [=>............................] - ETA: 0s - loss: 0.0236
3264/6530 [=============>................] - ETA: 0s - loss: 0.0430
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0231
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0422
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0226
6400/6530 [============================>.] - ETA: 0s - loss: 0.0420
6530/6530 [==============================] - 0s 34us/step - loss: 0.0418 - val_loss: 0.0501
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0552
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0223
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0406
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0216
2880/6530 [============>.................] - ETA: 0s - loss: 0.0399
3152/6530 [=============>................] - ETA: 0s - loss: 0.0214
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0400
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0208
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0402
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0203
6530/6530 [==============================] - 0s 37us/step - loss: 0.0404 - val_loss: 0.0434
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0365
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0203
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0356
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0203
3072/6530 [=============>................] - ETA: 0s - loss: 0.0376
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0201
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0394
6352/6530 [============================>.] - ETA: 0s - loss: 0.0199
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0389
6530/6530 [==============================] - 0s 35us/step - loss: 0.0392 - val_loss: 0.0409
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0401
6530/6530 [==============================] - 1s 99us/step - loss: 0.0199 - val_loss: 0.0191
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0225
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0393
 512/6530 [=>............................] - ETA: 0s - loss: 0.0170
3136/6530 [=============>................] - ETA: 0s - loss: 0.0387
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0182
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0382
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0183
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0383
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0177
6530/6530 [==============================] - 0s 35us/step - loss: 0.0383 - val_loss: 0.0429
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0310
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0176
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0367
3104/6530 [=============>................] - ETA: 0s - loss: 0.0172
3072/6530 [=============>................] - ETA: 0s - loss: 0.0376
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0170
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0379
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0169
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0377
6530/6530 [==============================] - 0s 35us/step - loss: 0.0378 - val_loss: 0.0553
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0506
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0168
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0422
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0166
2944/6530 [============>.................] - ETA: 0s - loss: 0.0387
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0167
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0381
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0165
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0378
6530/6530 [==============================] - 1s 102us/step - loss: 0.0164 - val_loss: 0.0258
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0176
6530/6530 [==============================] - 0s 36us/step - loss: 0.0376 - val_loss: 0.0495

 576/6530 [=>............................] - ETA: 0s - loss: 0.0147
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0149
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0151
# training | RMSE: 0.2173, MAE: 0.1722
worker 2  xfile  [14, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23447433164356346}, 'layer_4_size': 18, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.21729329465124916, 'rmse': 0.21729329465124916, 'mae': 0.172214393170926, 'early_stop': False}
vggnet done  2

2144/6530 [========>.....................] - ETA: 0s - loss: 0.0151
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0151
3248/6530 [=============>................] - ETA: 0s - loss: 0.0152
3760/6530 [================>.............] - ETA: 0s - loss: 0.0152
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0153
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0152
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0152
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0153
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0152
6530/6530 [==============================] - 1s 104us/step - loss: 0.0153 - val_loss: 0.0211
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0090
 480/6530 [=>............................] - ETA: 0s - loss: 0.0150
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0148
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0152
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0151
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0149
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0151
2896/6530 [============>.................] - ETA: 0s - loss: 0.0149
3264/6530 [=============>................] - ETA: 0s - loss: 0.0149
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0151
3904/6530 [================>.............] - ETA: 0s - loss: 0.0147
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0146
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0145
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0146
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0145
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0145
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0146
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0145
6530/6530 [==============================] - 1s 144us/step - loss: 0.0145 - val_loss: 0.0271
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0352
 432/6530 [>.............................] - ETA: 0s - loss: 0.0161
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0140
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0145
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0148
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0144
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0141
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0139
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0141
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0140
2912/6530 [============>.................] - ETA: 0s - loss: 0.0136
3184/6530 [=============>................] - ETA: 0s - loss: 0.0136
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0136
3792/6530 [================>.............] - ETA: 0s - loss: 0.0136
4080/6530 [=================>............] - ETA: 0s - loss: 0.0135
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0135
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0138
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0140
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0139
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0139
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0139
6368/6530 [============================>.] - ETA: 0s - loss: 0.0139
6530/6530 [==============================] - 1s 174us/step - loss: 0.0138 - val_loss: 0.0210
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0159
 400/6530 [>.............................] - ETA: 0s - loss: 0.0129
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0127
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0132
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0134
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0132
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0129
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0132
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0132
3104/6530 [=============>................] - ETA: 0s - loss: 0.0131
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0130
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0130
4000/6530 [=================>............] - ETA: 0s - loss: 0.0130
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0131
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0132
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0134
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0132
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0133
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0132
6530/6530 [==============================] - 1s 153us/step - loss: 0.0132 - val_loss: 0.0142

# training | RMSE: 0.1102, MAE: 0.0852
worker 0  xfile  [13, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.11016416796959301, 'rmse': 0.11016416796959301, 'mae': 0.08517072858563703, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#1 epoch=9.0 loss={'loss': 0.1433806243355196, 'rmse': 0.1433806243355196, 'mae': 0.11195617467151867, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1192090463413142}, 'layer_4_size': 47, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#2 epoch=9.0 loss={'loss': 0.12646634725699943, 'rmse': 0.12646634725699943, 'mae': 0.09757215494172035, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23040836054459668}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#3 epoch=9.0 loss={'loss': 0.2041044027372604, 'rmse': 0.2041044027372604, 'mae': 0.1603393027066329, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2680780636062813}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 45, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13492815742173062}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#0 epoch=9.0 loss={'loss': 0.20152233269177042, 'rmse': 0.20152233269177042, 'mae': 0.16240338237292382, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4802424283625154}, 'layer_1_size': 47, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38460543082365073}, 'layer_3_size': 88, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4998246419108864}, 'layer_4_size': 4, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#6 epoch=9.0 loss={'loss': 0.19766927572926624, 'rmse': 0.19766927572926624, 'mae': 0.15745894806239677, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3006015944095446}, 'layer_1_size': 57, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 26, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#4 epoch=9.0 loss={'loss': 0.12251142140393727, 'rmse': 0.12251142140393727, 'mae': 0.09899345813141994, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 69, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#5 epoch=9.0 loss={'loss': 0.21425025889611057, 'rmse': 0.21425025889611057, 'mae': 0.17618226729002057, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.440102233913089}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10947132527782243}, 'layer_2_size': 96, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3858775033138824}, 'layer_3_size': 17, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.27212081010555467}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#9 epoch=9.0 loss={'loss': 0.20138584107163854, 'rmse': 0.20138584107163854, 'mae': 0.15841766720362738, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1363927772286982}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11948090427468112}, 'layer_3_size': 48, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#8 epoch=9.0 loss={'loss': 0.1509010426040011, 'rmse': 0.1509010426040011, 'mae': 0.1260299719815674, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18765862401836017}, 'layer_3_size': 26, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4329524849559595}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#11 epoch=9.0 loss={'loss': 0.18283151854957994, 'rmse': 0.18283151854957994, 'mae': 0.13963405901913534, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 16, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21350832101767667}, 'layer_4_size': 30, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14988470208751936}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#7 epoch=9.0 loss={'loss': 0.1310494592036145, 'rmse': 0.1310494592036145, 'mae': 0.10376544324658829, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.20952434789544042}, 'layer_4_size': 92, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.490753343048291}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
#12 epoch=9.0 loss={'loss': 0.16226374778305688, 'rmse': 0.16226374778305688, 'mae': 0.12918728154970746, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.32783201871512857}, 'layer_1_size': 12, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 89, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1747291073747546}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 53, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#10 epoch=9.0 loss={'loss': 0.16449039597204201, 'rmse': 0.16449039597204201, 'mae': 0.13165854943802716, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3349483602683996}, 'layer_1_size': 29, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 25, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3970860647381087}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#14 epoch=9.0 loss={'loss': 0.21729329465124916, 'rmse': 0.21729329465124916, 'mae': 0.172214393170926, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23447433164356346}, 'layer_4_size': 18, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#13 epoch=9.0 loss={'loss': 0.11016416796959301, 'rmse': 0.11016416796959301, 'mae': 0.08517072858563703, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
get a list [results] of length 185
get a list [loss] of length 15
get a list [val_loss] of length 15
length of indices is (13, 4, 2, 7, 1, 8, 12, 10, 11, 6, 9, 0, 3, 5, 14)
length of indices is 15
length of T is 15
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [1, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 69, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [2, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23040836054459668}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [3, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.20952434789544042}, 'layer_4_size': 92, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.490753343048291}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [4, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1192090463413142}, 'layer_4_size': 47, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]] 

*** 5.0 configurations x 27.0 iterations each

14 | Fri Sep 28 01:23:02 2018 | lowest loss so far: 0.0705 (run 0)

{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  25 | activation: relu    | extras: None 
layer 2 | size:  55 | activation: tanh    | extras: None 
layer 3 | size:  55 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9f1668>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 5:30 - loss: 0.5631{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  53 | activation: tanh    | extras: None 
layer 2 | size:  77 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9f1748>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 2:48 - loss: 0.4978
 480/6530 [=>............................] - ETA: 10s - loss: 0.2247 
1056/6530 [===>..........................] - ETA: 4s - loss: 0.2418  
 944/6530 [===>..........................] - ETA: 5s - loss: 0.1386 
1952/6530 [=======>......................] - ETA: 2s - loss: 0.2050
1424/6530 [=====>........................] - ETA: 3s - loss: 0.1057
2912/6530 [============>.................] - ETA: 1s - loss: 0.1933
1904/6530 [=======>......................] - ETA: 2s - loss: 0.0884
3840/6530 [================>.............] - ETA: 0s - loss: 0.1858
2416/6530 [==========>...................] - ETA: 1s - loss: 0.0793
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1818
2928/6530 [============>.................] - ETA: 1s - loss: 0.0724
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1784
3392/6530 [==============>...............] - ETA: 1s - loss: 0.0689{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  69 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  91 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9f16d8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 2:59 - loss: 0.2316
3904/6530 [================>.............] - ETA: 0s - loss: 0.0652
6530/6530 [==============================] - 1s 183us/step - loss: 0.1765 - val_loss: 0.1642
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1711
 800/6530 [==>...........................] - ETA: 6s - loss: 0.0580  
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0627
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1611
1568/6530 [======>.......................] - ETA: 3s - loss: 0.0493
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0602
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1622
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0477
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0584
2976/6530 [============>.................] - ETA: 0s - loss: 0.1620
3104/6530 [=============>................] - ETA: 1s - loss: 0.0454
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0566
4000/6530 [=================>............] - ETA: 0s - loss: 0.1605
3904/6530 [================>.............] - ETA: 0s - loss: 0.0449
6448/6530 [============================>.] - ETA: 0s - loss: 0.0553
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1623
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0441
6530/6530 [==============================] - 2s 234us/step - loss: 0.0551 - val_loss: 0.0393
Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0391
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1624
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0429
6530/6530 [==============================] - 0s 52us/step - loss: 0.1626 - val_loss: 0.1596
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1656
 528/6530 [=>............................] - ETA: 0s - loss: 0.0386
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0420
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1600
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0388
6530/6530 [==============================] - 1s 207us/step - loss: 0.0418 - val_loss: 0.0287
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0445
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1617
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0373
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0352
3072/6530 [=============>................] - ETA: 0s - loss: 0.1603
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0378
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0322
4096/6530 [=================>............] - ETA: 0s - loss: 0.1599
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0372
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0315
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1595
3024/6530 [============>.................] - ETA: 0s - loss: 0.0384
3072/6530 [=============>................] - ETA: 0s - loss: 0.0306
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1587
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0393
6530/6530 [==============================] - 0s 52us/step - loss: 0.1587 - val_loss: 0.1516
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1343
3872/6530 [================>.............] - ETA: 0s - loss: 0.0307
4032/6530 [=================>............] - ETA: 0s - loss: 0.0391
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1478
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0306
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0388
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1473
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0302
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0385
2976/6530 [============>.................] - ETA: 0s - loss: 0.1481
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0300
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0390
4000/6530 [=================>............] - ETA: 0s - loss: 0.1475
6530/6530 [==============================] - 0s 70us/step - loss: 0.0298 - val_loss: 0.0269
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0415
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0391
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1464
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0281
6530/6530 [==============================] - 1s 103us/step - loss: 0.0394 - val_loss: 0.0465
Epoch 3/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0277
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1461
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0260
6530/6530 [==============================] - 0s 52us/step - loss: 0.1457 - val_loss: 0.1402
Epoch 5/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1260
 496/6530 [=>............................] - ETA: 0s - loss: 0.0378
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0252
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1343
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0371
3168/6530 [=============>................] - ETA: 0s - loss: 0.0246
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0375
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1315
3968/6530 [=================>............] - ETA: 0s - loss: 0.0246
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0370
3136/6530 [=============>................] - ETA: 0s - loss: 0.1287
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0246
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0376
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1284
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0240
3120/6530 [=============>................] - ETA: 0s - loss: 0.0381
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1277
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0242
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0382
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1272
6530/6530 [==============================] - 0s 68us/step - loss: 0.0242 - val_loss: 0.0234
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0341
6530/6530 [==============================] - 0s 52us/step - loss: 0.1270 - val_loss: 0.1224
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1169
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0383
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0227
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1160
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0386
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0217
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1171
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0386
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0212
3008/6530 [============>.................] - ETA: 0s - loss: 0.1171
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0389
3104/6530 [=============>................] - ETA: 0s - loss: 0.0207
4064/6530 [=================>............] - ETA: 0s - loss: 0.1157
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0388
3808/6530 [================>.............] - ETA: 0s - loss: 0.0206
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1154
6530/6530 [==============================] - 1s 102us/step - loss: 0.0386 - val_loss: 0.0404
Epoch 4/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0248
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0207
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1144
 544/6530 [=>............................] - ETA: 0s - loss: 0.0378
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0205
6530/6530 [==============================] - 0s 53us/step - loss: 0.1143 - val_loss: 0.1200
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1091
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0390
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0205
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1121
6530/6530 [==============================] - 0s 70us/step - loss: 0.0205 - val_loss: 0.0226
Epoch 5/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0303
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0388
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1097
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0196
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0390
3040/6530 [============>.................] - ETA: 0s - loss: 0.1095
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0192
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0384
4064/6530 [=================>............] - ETA: 0s - loss: 0.1088
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0189
3088/6530 [=============>................] - ETA: 0s - loss: 0.0379
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1074
3072/6530 [=============>................] - ETA: 0s - loss: 0.0184
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0379
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1071
6530/6530 [==============================] - 0s 52us/step - loss: 0.1069 - val_loss: 0.1105

3808/6530 [================>.............] - ETA: 0s - loss: 0.0183
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0375Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1043
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0185
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0376
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1084
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0183
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1057
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0379
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0183
3104/6530 [=============>................] - ETA: 0s - loss: 0.1060
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0376
4128/6530 [=================>............] - ETA: 0s - loss: 0.1039
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0373
6530/6530 [==============================] - 0s 71us/step - loss: 0.0184 - val_loss: 0.0198
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0265
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1033
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0175
6530/6530 [==============================] - 1s 102us/step - loss: 0.0370 - val_loss: 0.0439
Epoch 5/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0368
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1027
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0175
 544/6530 [=>............................] - ETA: 0s - loss: 0.0352
6530/6530 [==============================] - 0s 52us/step - loss: 0.1024 - val_loss: 0.1029
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1108
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0171
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0336
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0967
3168/6530 [=============>................] - ETA: 0s - loss: 0.0167
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0341
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0986
3968/6530 [=================>............] - ETA: 0s - loss: 0.0169
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0333
3168/6530 [=============>................] - ETA: 0s - loss: 0.0985
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0170
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0335
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0980
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0167
3072/6530 [=============>................] - ETA: 0s - loss: 0.0330
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0981
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0170
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0326
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0980
6530/6530 [==============================] - 0s 67us/step - loss: 0.0170 - val_loss: 0.0169
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0234
4048/6530 [=================>............] - ETA: 0s - loss: 0.0323
6530/6530 [==============================] - 0s 52us/step - loss: 0.0981 - val_loss: 0.1016
Epoch 10/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0968
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0161
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0320
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1013
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0160
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0320
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1009
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0159
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0316
3200/6530 [=============>................] - ETA: 0s - loss: 0.0993
3136/6530 [=============>................] - ETA: 0s - loss: 0.0157
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0315
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0975
3872/6530 [================>.............] - ETA: 0s - loss: 0.0158
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0969
6530/6530 [==============================] - 1s 103us/step - loss: 0.0314 - val_loss: 0.0347
Epoch 6/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0176
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0159
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0957
 544/6530 [=>............................] - ETA: 0s - loss: 0.0301
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0157
6530/6530 [==============================] - 0s 50us/step - loss: 0.0956 - val_loss: 0.0987
Epoch 11/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0801
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0297
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0160
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0954
6530/6530 [==============================] - 0s 68us/step - loss: 0.0160 - val_loss: 0.0145

1600/6530 [======>.......................] - ETA: 0s - loss: 0.0295Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0210
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0955
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0305
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0151
3136/6530 [=============>................] - ETA: 0s - loss: 0.0954
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0303
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0153
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0946
3168/6530 [=============>................] - ETA: 0s - loss: 0.0293
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0150
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0940
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0291
3168/6530 [=============>................] - ETA: 0s - loss: 0.0148
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0931
6530/6530 [==============================] - 0s 51us/step - loss: 0.0931 - val_loss: 0.0994
Epoch 12/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0961
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0288
3904/6530 [================>.............] - ETA: 0s - loss: 0.0149
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0922
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0286
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0150
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0919
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0283
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0147
3072/6530 [=============>................] - ETA: 0s - loss: 0.0921
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0280
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0152
6530/6530 [==============================] - 0s 68us/step - loss: 0.0152 - val_loss: 0.0129
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0193
3936/6530 [=================>............] - ETA: 0s - loss: 0.0915
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0279
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0145
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0914
6530/6530 [==============================] - 1s 101us/step - loss: 0.0279 - val_loss: 0.0311
Epoch 7/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0411
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0144
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0912
 496/6530 [=>............................] - ETA: 0s - loss: 0.0293
6530/6530 [==============================] - 0s 53us/step - loss: 0.0909 - val_loss: 0.0958
Epoch 13/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0760
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0144
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0278
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0957
3136/6530 [=============>................] - ETA: 0s - loss: 0.0142
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0264
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0939
3936/6530 [=================>............] - ETA: 0s - loss: 0.0143
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0260
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0144
3008/6530 [============>.................] - ETA: 0s - loss: 0.0926
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0251
4032/6530 [=================>............] - ETA: 0s - loss: 0.0923
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0141
3072/6530 [=============>................] - ETA: 0s - loss: 0.0250
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0913
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0146
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0246
6530/6530 [==============================] - 0s 68us/step - loss: 0.0146 - val_loss: 0.0119
Epoch 10/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0183
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0894
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0244
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0139
6530/6530 [==============================] - 0s 52us/step - loss: 0.0890 - val_loss: 0.0912
Epoch 14/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0800
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0243
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0139
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0853
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0239
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0140
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0875
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0235
3040/6530 [============>.................] - ETA: 0s - loss: 0.0138
3008/6530 [============>.................] - ETA: 0s - loss: 0.0864
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0237
3744/6530 [================>.............] - ETA: 0s - loss: 0.0137
3968/6530 [=================>............] - ETA: 0s - loss: 0.0863
6530/6530 [==============================] - 1s 102us/step - loss: 0.0235 - val_loss: 0.0239
Epoch 8/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0298
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0139
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0871
 544/6530 [=>............................] - ETA: 0s - loss: 0.0198
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0137
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0869
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0198
6530/6530 [==============================] - 0s 53us/step - loss: 0.0868 - val_loss: 0.0945
Epoch 15/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0806
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0140
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0190
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0837
6530/6530 [==============================] - 0s 70us/step - loss: 0.0141 - val_loss: 0.0112
Epoch 11/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0177
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0195
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0848
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0133
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0196
2912/6530 [============>.................] - ETA: 0s - loss: 0.0838
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0138
3104/6530 [=============>................] - ETA: 0s - loss: 0.0194
3904/6530 [================>.............] - ETA: 0s - loss: 0.0845
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0136
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0191
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0851
3072/6530 [=============>................] - ETA: 0s - loss: 0.0134
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0188
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0851
3872/6530 [================>.............] - ETA: 0s - loss: 0.0135
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0187
6530/6530 [==============================] - 0s 54us/step - loss: 0.0850 - val_loss: 0.0899
Epoch 16/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0584
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0135
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0187
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0826
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0134
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0185
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0821
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0136
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0184
3168/6530 [=============>................] - ETA: 0s - loss: 0.0829
6530/6530 [==============================] - 0s 69us/step - loss: 0.0138 - val_loss: 0.0108
Epoch 12/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0175
6530/6530 [==============================] - 1s 102us/step - loss: 0.0182 - val_loss: 0.0423

4192/6530 [==================>...........] - ETA: 0s - loss: 0.0834Epoch 9/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0270
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0130
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0826
 528/6530 [=>............................] - ETA: 0s - loss: 0.0160
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0133
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0836
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0161
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0133
6530/6530 [==============================] - 0s 52us/step - loss: 0.0835 - val_loss: 0.0965
Epoch 17/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1008
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0174
3008/6530 [============>.................] - ETA: 0s - loss: 0.0132
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0832
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0170
3744/6530 [================>.............] - ETA: 0s - loss: 0.0130
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0829
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0165
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0132
3072/6530 [=============>................] - ETA: 0s - loss: 0.0829
3040/6530 [============>.................] - ETA: 0s - loss: 0.0165
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0131
4096/6530 [=================>............] - ETA: 0s - loss: 0.0841
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0165
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0133
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0838
4112/6530 [=================>............] - ETA: 0s - loss: 0.0164
6530/6530 [==============================] - 0s 71us/step - loss: 0.0134 - val_loss: 0.0105
Epoch 13/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0175
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0836
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0163
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0127
6530/6530 [==============================] - 0s 53us/step - loss: 0.0835 - val_loss: 0.0890
Epoch 18/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0764
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0164
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0132
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0798
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0163
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0129
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0819
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0162
3168/6530 [=============>................] - ETA: 0s - loss: 0.0127
3008/6530 [============>.................] - ETA: 0s - loss: 0.0813
6530/6530 [==============================] - 1s 102us/step - loss: 0.0161 - val_loss: 0.0183
Epoch 10/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0143
3936/6530 [=================>............] - ETA: 0s - loss: 0.0129
4064/6530 [=================>............] - ETA: 0s - loss: 0.0817
 528/6530 [=>............................] - ETA: 0s - loss: 0.0160
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0129
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0815
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0161
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0126
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0808
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0159
6530/6530 [==============================] - 0s 52us/step - loss: 0.0810 - val_loss: 0.0879
Epoch 19/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0694
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0131
6530/6530 [==============================] - 0s 68us/step - loss: 0.0132 - val_loss: 0.0103
Epoch 14/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0177
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0160
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0842
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0125
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0156
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0821
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0130
3072/6530 [=============>................] - ETA: 0s - loss: 0.0156
2880/6530 [============>.................] - ETA: 0s - loss: 0.0822
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0127
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0156
3936/6530 [=================>............] - ETA: 0s - loss: 0.0823
4112/6530 [=================>............] - ETA: 0s - loss: 0.0153
3168/6530 [=============>................] - ETA: 0s - loss: 0.0125
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0814
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0152
3968/6530 [=================>............] - ETA: 0s - loss: 0.0127
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0812
6530/6530 [==============================] - 0s 53us/step - loss: 0.0813 - val_loss: 0.0868
Epoch 20/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0655
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0152
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0127
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0842
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0152
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0124
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0808
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0151
6336/6530 [============================>.] - ETA: 0s - loss: 0.0129
6530/6530 [==============================] - 0s 68us/step - loss: 0.0130 - val_loss: 0.0101
Epoch 15/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0179
3008/6530 [============>.................] - ETA: 0s - loss: 0.0795
6530/6530 [==============================] - 1s 102us/step - loss: 0.0151 - val_loss: 0.0138
Epoch 11/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0193
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0123
4032/6530 [=================>............] - ETA: 0s - loss: 0.0785
 544/6530 [=>............................] - ETA: 0s - loss: 0.0145
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0125
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0785
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0139
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0126
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0787
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0138
6530/6530 [==============================] - 0s 52us/step - loss: 0.0788 - val_loss: 0.0864
Epoch 21/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0863
3104/6530 [=============>................] - ETA: 0s - loss: 0.0124
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0141
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0815
3904/6530 [================>.............] - ETA: 0s - loss: 0.0125
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0146
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0815
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0125
2992/6530 [============>.................] - ETA: 0s - loss: 0.0147
3104/6530 [=============>................] - ETA: 0s - loss: 0.0803
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0122
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0144
4064/6530 [=================>............] - ETA: 0s - loss: 0.0790
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0127
4064/6530 [=================>............] - ETA: 0s - loss: 0.0145
6530/6530 [==============================] - 0s 69us/step - loss: 0.0128 - val_loss: 0.0100
Epoch 16/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0181
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0786
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0143
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0119
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0785
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0143
6530/6530 [==============================] - 0s 52us/step - loss: 0.0782 - val_loss: 0.0833
Epoch 22/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0708
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0126
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0142
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0749
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0123
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0143
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0762
3200/6530 [=============>................] - ETA: 0s - loss: 0.0121
2976/6530 [============>.................] - ETA: 0s - loss: 0.0781
6530/6530 [==============================] - 1s 103us/step - loss: 0.0142 - val_loss: 0.0155
Epoch 12/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0167
3904/6530 [================>.............] - ETA: 0s - loss: 0.0123
3968/6530 [=================>............] - ETA: 0s - loss: 0.0777
 544/6530 [=>............................] - ETA: 0s - loss: 0.0136
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0123
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0775
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0142
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0121
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0774
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0139
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0125
6530/6530 [==============================] - 0s 54us/step - loss: 0.0773 - val_loss: 0.0974
Epoch 23/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1047
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0138
6530/6530 [==============================] - 0s 70us/step - loss: 0.0126 - val_loss: 0.0099
Epoch 17/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0184
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0810
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0139
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0119
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0776
3120/6530 [=============>................] - ETA: 0s - loss: 0.0138
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0121
3072/6530 [=============>................] - ETA: 0s - loss: 0.0777
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0138
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0122
4096/6530 [=================>............] - ETA: 0s - loss: 0.0774
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0138
3072/6530 [=============>................] - ETA: 0s - loss: 0.0122
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0766
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0136
3808/6530 [================>.............] - ETA: 0s - loss: 0.0122
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0763
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0137
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0122
6530/6530 [==============================] - 0s 51us/step - loss: 0.0761 - val_loss: 0.0780
Epoch 24/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0842
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0138
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0120
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0849
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0137
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0123
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0807
6530/6530 [==============================] - 0s 69us/step - loss: 0.0124 - val_loss: 0.0099
Epoch 18/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0186
6530/6530 [==============================] - 1s 102us/step - loss: 0.0137 - val_loss: 0.0162
Epoch 13/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0111
3008/6530 [============>.................] - ETA: 0s - loss: 0.0783
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0116
 544/6530 [=>............................] - ETA: 0s - loss: 0.0138
4064/6530 [=================>............] - ETA: 0s - loss: 0.0776
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0123
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0139
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0770
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0120
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0135
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0767
3072/6530 [=============>................] - ETA: 0s - loss: 0.0120
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0140
6530/6530 [==============================] - 0s 53us/step - loss: 0.0762 - val_loss: 0.0845
Epoch 25/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0785
3872/6530 [================>.............] - ETA: 0s - loss: 0.0120
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0138
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0751
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0120
3104/6530 [=============>................] - ETA: 0s - loss: 0.0135
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0747
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0118
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0134
2976/6530 [============>.................] - ETA: 0s - loss: 0.0744
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0121
4112/6530 [=================>............] - ETA: 0s - loss: 0.0134
3872/6530 [================>.............] - ETA: 0s - loss: 0.0736
6530/6530 [==============================] - 0s 70us/step - loss: 0.0123 - val_loss: 0.0098
Epoch 19/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0189
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0134
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0739
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0120
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0134
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0744
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0119
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0133
6530/6530 [==============================] - 0s 54us/step - loss: 0.0745 - val_loss: 0.0785
Epoch 26/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0875
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0120
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0132
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0751
3040/6530 [============>.................] - ETA: 0s - loss: 0.0119
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0748
6530/6530 [==============================] - 1s 103us/step - loss: 0.0132 - val_loss: 0.0149
Epoch 14/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0121
3840/6530 [================>.............] - ETA: 0s - loss: 0.0119
3040/6530 [============>.................] - ETA: 0s - loss: 0.0741
 544/6530 [=>............................] - ETA: 0s - loss: 0.0121
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0119
3776/6530 [================>.............] - ETA: 0s - loss: 0.0746
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0118
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0117
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0742
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0115
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0120
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0737
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0122
6530/6530 [==============================] - 0s 69us/step - loss: 0.0121 - val_loss: 0.0098
Epoch 20/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0191
6530/6530 [==============================] - 0s 54us/step - loss: 0.0734 - val_loss: 0.0742
Epoch 27/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0599
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0122
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0115
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0768
3104/6530 [=============>................] - ETA: 0s - loss: 0.0124
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0117
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0764
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0125
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0118
3104/6530 [=============>................] - ETA: 0s - loss: 0.0752
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0126
3104/6530 [=============>................] - ETA: 0s - loss: 0.0117
4128/6530 [=================>............] - ETA: 0s - loss: 0.0759
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0125
3776/6530 [================>.............] - ETA: 0s - loss: 0.0117
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0758
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0126
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0117
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0750
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0127
6530/6530 [==============================] - 0s 52us/step - loss: 0.0746 - val_loss: 0.0733

5376/6530 [=======================>......] - ETA: 0s - loss: 0.0116
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0128
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0118
6530/6530 [==============================] - 1s 102us/step - loss: 0.0128 - val_loss: 0.0172
Epoch 15/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0141
6530/6530 [==============================] - 0s 69us/step - loss: 0.0120 - val_loss: 0.0098
Epoch 21/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0193
 544/6530 [=>............................] - ETA: 0s - loss: 0.0131
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0114
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0131
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0119
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0130
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0116
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0125
3200/6530 [=============>................] - ETA: 0s - loss: 0.0115
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0124
4032/6530 [=================>............] - ETA: 0s - loss: 0.0117
3184/6530 [=============>................] - ETA: 0s - loss: 0.0125
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0116
3712/6530 [================>.............] - ETA: 0s - loss: 0.0124
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0113
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0125
6464/6530 [============================>.] - ETA: 0s - loss: 0.0119
6530/6530 [==============================] - 0s 66us/step - loss: 0.0119 - val_loss: 0.0099
Epoch 22/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0195
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0124
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0109
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0125
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0118
# training | RMSE: 0.0910, MAE: 0.0681
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23040836054459668}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.09101404549530169, 'rmse': 0.09101404549530169, 'mae': 0.06807528063763053, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  50 | activation: tanh    | extras: None 
layer 2 | size:  75 | activation: sigmoid | extras: None 
layer 3 | size:  39 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 1:24 - loss: 0.1741
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0125
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0117
 432/6530 [>.............................] - ETA: 3s - loss: 0.0579  
6336/6530 [============================>.] - ETA: 0s - loss: 0.0126
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0114
6530/6530 [==============================] - 1s 101us/step - loss: 0.0126 - val_loss: 0.0142

 848/6530 [==>...........................] - ETA: 2s - loss: 0.0512
4064/6530 [=================>............] - ETA: 0s - loss: 0.0115
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0480
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0115
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0467
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0112
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0453
6400/6530 [============================>.] - ETA: 0s - loss: 0.0117
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0443
6530/6530 [==============================] - 0s 67us/step - loss: 0.0118 - val_loss: 0.0099
Epoch 23/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0198
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0433
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0112
3200/6530 [=============>................] - ETA: 0s - loss: 0.0420
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0117
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0418
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0114
4016/6530 [=================>............] - ETA: 0s - loss: 0.0410
3136/6530 [=============>................] - ETA: 0s - loss: 0.0114
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0400
3936/6530 [=================>............] - ETA: 0s - loss: 0.0115
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0396
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0114
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0393
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0112
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0388
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0115
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0382
6530/6530 [==============================] - 0s 69us/step - loss: 0.0116 - val_loss: 0.0099
Epoch 24/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0200
6528/6530 [============================>.] - ETA: 0s - loss: 0.0377
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0110
6530/6530 [==============================] - 1s 162us/step - loss: 0.0377 - val_loss: 0.0784
Epoch 2/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0661
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0116
 448/6530 [=>............................] - ETA: 0s - loss: 0.0305
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0115
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0285
3264/6530 [=============>................] - ETA: 0s - loss: 0.0112
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0278
# training | RMSE: 0.1112, MAE: 0.0873
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.11117919147139543, 'rmse': 0.11117919147139543, 'mae': 0.08730057768193428, 'early_stop': True}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  38 | activation: tanh    | extras: None 
layer 2 | size:  51 | activation: relu    | extras: batchnorm 
layer 3 | size:  91 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9f1908>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 128/6530 [..............................] - ETA: 16s - loss: 0.3523
4064/6530 [=================>............] - ETA: 0s - loss: 0.0113
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0261
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0982 
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0113
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0257
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0684
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0111
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0268
6530/6530 [==============================] - 0s 75us/step - loss: 0.0640 - val_loss: 0.0354

6464/6530 [============================>.] - ETA: 0s - loss: 0.0115Epoch 2/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0349
6530/6530 [==============================] - 0s 66us/step - loss: 0.0115 - val_loss: 0.0100

2976/6530 [============>.................] - ETA: 0s - loss: 0.0265
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0323
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0259
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0314
3808/6530 [================>.............] - ETA: 0s - loss: 0.0254
6530/6530 [==============================] - 0s 20us/step - loss: 0.0312 - val_loss: 0.0309
Epoch 3/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0286
# training | RMSE: 0.0931, MAE: 0.0742
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 69, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.0931157240851333, 'rmse': 0.0931157240851333, 'mae': 0.07416125491043915, 'early_stop': True}
vggnet done  1

4224/6530 [==================>...........] - ETA: 0s - loss: 0.0253
2944/6530 [============>.................] - ETA: 0s - loss: 0.0277
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0253
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0273
6530/6530 [==============================] - 0s 19us/step - loss: 0.0273 - val_loss: 0.0278
Epoch 4/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0250
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0246
3072/6530 [=============>................] - ETA: 0s - loss: 0.0248
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0246
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0249
6530/6530 [==============================] - 0s 19us/step - loss: 0.0247 - val_loss: 0.0256
Epoch 5/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0225
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0241
2944/6530 [============>.................] - ETA: 0s - loss: 0.0229
6336/6530 [============================>.] - ETA: 0s - loss: 0.0241
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0226
6530/6530 [==============================] - 1s 124us/step - loss: 0.0241 - val_loss: 0.0396
Epoch 3/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0482
6530/6530 [==============================] - 0s 19us/step - loss: 0.0227 - val_loss: 0.0237
Epoch 6/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0205
 448/6530 [=>............................] - ETA: 0s - loss: 0.0227
2944/6530 [============>.................] - ETA: 0s - loss: 0.0212
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0253
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0209
6530/6530 [==============================] - 0s 20us/step - loss: 0.0210 - val_loss: 0.0221
Epoch 7/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0188
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0224
2944/6530 [============>.................] - ETA: 0s - loss: 0.0198
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0218
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0195
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0216
6530/6530 [==============================] - 0s 19us/step - loss: 0.0196 - val_loss: 0.0208
Epoch 8/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0174
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0211
2944/6530 [============>.................] - ETA: 0s - loss: 0.0186
2976/6530 [============>.................] - ETA: 0s - loss: 0.0201
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0184
6530/6530 [==============================] - 0s 19us/step - loss: 0.0184 - val_loss: 0.0197
Epoch 9/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0162
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0203
3072/6530 [=============>................] - ETA: 0s - loss: 0.0175
3808/6530 [================>.............] - ETA: 0s - loss: 0.0204
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0175
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0206
6530/6530 [==============================] - 0s 19us/step - loss: 0.0175 - val_loss: 0.0188
Epoch 10/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0153
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0205
3072/6530 [=============>................] - ETA: 0s - loss: 0.0167
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0207
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0167
6530/6530 [==============================] - 0s 19us/step - loss: 0.0167 - val_loss: 0.0180
Epoch 11/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0145
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0205
3072/6530 [=============>................] - ETA: 0s - loss: 0.0160
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0204
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0159
6448/6530 [============================>.] - ETA: 0s - loss: 0.0202
6530/6530 [==============================] - 0s 20us/step - loss: 0.0160 - val_loss: 0.0174
Epoch 12/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0139
6530/6530 [==============================] - 1s 122us/step - loss: 0.0201 - val_loss: 0.0373
Epoch 4/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0302
3072/6530 [=============>................] - ETA: 0s - loss: 0.0154
 464/6530 [=>............................] - ETA: 0s - loss: 0.0201
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0154
6530/6530 [==============================] - 0s 19us/step - loss: 0.0153 - val_loss: 0.0169
Epoch 13/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0135
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0183
3072/6530 [=============>................] - ETA: 0s - loss: 0.0149
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0188
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0148
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0186
6530/6530 [==============================] - 0s 18us/step - loss: 0.0148 - val_loss: 0.0164
Epoch 14/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0131
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0185
3072/6530 [=============>................] - ETA: 0s - loss: 0.0144
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0183
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0144
6530/6530 [==============================] - 0s 19us/step - loss: 0.0143 - val_loss: 0.0160
Epoch 15/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0127
3040/6530 [============>.................] - ETA: 0s - loss: 0.0186
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0142
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0184
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0139
3872/6530 [================>.............] - ETA: 0s - loss: 0.0187
6530/6530 [==============================] - 0s 20us/step - loss: 0.0139 - val_loss: 0.0156
Epoch 16/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0123
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0185
2944/6530 [============>.................] - ETA: 0s - loss: 0.0136
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0184
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0135
6530/6530 [==============================] - 0s 19us/step - loss: 0.0135 - val_loss: 0.0152
Epoch 17/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0120
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0187
2944/6530 [============>.................] - ETA: 0s - loss: 0.0132
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0182
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0131
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0179
6530/6530 [==============================] - 0s 19us/step - loss: 0.0131 - val_loss: 0.0149
Epoch 18/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0117
6448/6530 [============================>.] - ETA: 0s - loss: 0.0178
3072/6530 [=============>................] - ETA: 0s - loss: 0.0128
6530/6530 [==============================] - 1s 122us/step - loss: 0.0178 - val_loss: 0.0182
Epoch 5/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0181
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0128
6530/6530 [==============================] - 0s 19us/step - loss: 0.0128 - val_loss: 0.0146
Epoch 19/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0115
 448/6530 [=>............................] - ETA: 0s - loss: 0.0142
3072/6530 [=============>................] - ETA: 0s - loss: 0.0125
 880/6530 [===>..........................] - ETA: 0s - loss: 0.0142
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0124
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0149
6530/6530 [==============================] - 0s 19us/step - loss: 0.0124 - val_loss: 0.0143
Epoch 20/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0113
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0150
3072/6530 [=============>................] - ETA: 0s - loss: 0.0122
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0156
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0121
6530/6530 [==============================] - 0s 18us/step - loss: 0.0121 - val_loss: 0.0141
Epoch 21/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0110
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0162
3072/6530 [=============>................] - ETA: 0s - loss: 0.0119
3088/6530 [=============>................] - ETA: 0s - loss: 0.0160
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0119
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0161
6530/6530 [==============================] - 0s 19us/step - loss: 0.0118 - val_loss: 0.0139
Epoch 22/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0108
3952/6530 [=================>............] - ETA: 0s - loss: 0.0161
2944/6530 [============>.................] - ETA: 0s - loss: 0.0117
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0159
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0116
6530/6530 [==============================] - 0s 19us/step - loss: 0.0116 - val_loss: 0.0137
Epoch 23/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0107
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0157
2944/6530 [============>.................] - ETA: 0s - loss: 0.0114
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0160
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0113
6530/6530 [==============================] - 0s 19us/step - loss: 0.0113 - val_loss: 0.0135
Epoch 24/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0106
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0159
2944/6530 [============>.................] - ETA: 0s - loss: 0.0112
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0160
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0111
6530/6530 [==============================] - 0s 19us/step - loss: 0.0111 - val_loss: 0.0133
Epoch 25/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0104
6530/6530 [==============================] - 1s 121us/step - loss: 0.0159 - val_loss: 0.0168
Epoch 6/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0050
2944/6530 [============>.................] - ETA: 0s - loss: 0.0109
 448/6530 [=>............................] - ETA: 0s - loss: 0.0145
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0109
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0141
6530/6530 [==============================] - 0s 19us/step - loss: 0.0109 - val_loss: 0.0131
Epoch 26/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0103
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0143
3072/6530 [=============>................] - ETA: 0s - loss: 0.0107
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0151
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0107
6530/6530 [==============================] - 0s 20us/step - loss: 0.0107 - val_loss: 0.0130
Epoch 27/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0101
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0144
3072/6530 [=============>................] - ETA: 0s - loss: 0.0105
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0144
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0105
6530/6530 [==============================] - 0s 19us/step - loss: 0.0105 - val_loss: 0.0128

3072/6530 [=============>................] - ETA: 0s - loss: 0.0143
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0143
# training | RMSE: 0.0986, MAE: 0.0776
worker 0  xfile  [4, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1192090463413142}, 'layer_4_size': 47, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.09855593755136743, 'rmse': 0.09855593755136743, 'mae': 0.0775504480447567, 'early_stop': False}
vggnet done  0

3936/6530 [=================>............] - ETA: 0s - loss: 0.0144
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0147
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0149
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0148
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0148
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0148
6530/6530 [==============================] - 1s 121us/step - loss: 0.0146 - val_loss: 0.0149
Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0044
 368/6530 [>.............................] - ETA: 0s - loss: 0.0134
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0134
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0149
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0148
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0151
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0147
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0141
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0142
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0143
2880/6530 [============>.................] - ETA: 0s - loss: 0.0144
3152/6530 [=============>................] - ETA: 0s - loss: 0.0142
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0143
3776/6530 [================>.............] - ETA: 0s - loss: 0.0142
4096/6530 [=================>............] - ETA: 0s - loss: 0.0145
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0145
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0146
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0143
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0144
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0143
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0142
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0141
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0143
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0143
6480/6530 [============================>.] - ETA: 0s - loss: 0.0142
6530/6530 [==============================] - 1s 199us/step - loss: 0.0142 - val_loss: 0.0143
Epoch 8/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0134
 240/6530 [>.............................] - ETA: 1s - loss: 0.0120
 432/6530 [>.............................] - ETA: 1s - loss: 0.0145
 656/6530 [==>...........................] - ETA: 1s - loss: 0.0147
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0149
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0141
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0135
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0135
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0142
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0141
2896/6530 [============>.................] - ETA: 0s - loss: 0.0141
3152/6530 [=============>................] - ETA: 0s - loss: 0.0140
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0139
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0141
3824/6530 [================>.............] - ETA: 0s - loss: 0.0140
4096/6530 [=================>............] - ETA: 0s - loss: 0.0139
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0137
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0136
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0135
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0136
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0137
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0136
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0135
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0135
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0135
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0135
6480/6530 [============================>.] - ETA: 0s - loss: 0.0134
6530/6530 [==============================] - 1s 219us/step - loss: 0.0134 - val_loss: 0.0159
Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0087
 224/6530 [>.............................] - ETA: 1s - loss: 0.0134
 416/6530 [>.............................] - ETA: 1s - loss: 0.0132
 624/6530 [=>............................] - ETA: 1s - loss: 0.0147
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0147
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0144
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0141
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0142
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0139
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0137
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0135
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0137
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0135
2880/6530 [============>.................] - ETA: 0s - loss: 0.0133
3088/6530 [=============>................] - ETA: 0s - loss: 0.0133
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0133
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0131
3824/6530 [================>.............] - ETA: 0s - loss: 0.0131
4128/6530 [=================>............] - ETA: 0s - loss: 0.0130
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0129
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0130
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0131
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0130
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0130
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0130
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0131
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0129
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0130
6368/6530 [============================>.] - ETA: 0s - loss: 0.0129
6530/6530 [==============================] - 2s 238us/step - loss: 0.0128 - val_loss: 0.0179
Epoch 10/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0122
 208/6530 [..............................] - ETA: 1s - loss: 0.0087
 400/6530 [>.............................] - ETA: 1s - loss: 0.0091
 576/6530 [=>............................] - ETA: 1s - loss: 0.0111
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0121
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0120
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0121
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0125
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0126
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0130
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0128
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0127
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0127
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0130
2976/6530 [============>.................] - ETA: 0s - loss: 0.0127
3200/6530 [=============>................] - ETA: 0s - loss: 0.0127
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0126
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0124
3856/6530 [================>.............] - ETA: 0s - loss: 0.0126
4080/6530 [=================>............] - ETA: 0s - loss: 0.0126
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0125
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0124
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0124
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0127
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0127
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0126
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0124
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0126
6432/6530 [============================>.] - ETA: 0s - loss: 0.0125
6530/6530 [==============================] - 1s 228us/step - loss: 0.0125 - val_loss: 0.0372
Epoch 11/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0322
 368/6530 [>.............................] - ETA: 0s - loss: 0.0114
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0122
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0117
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0121
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0120
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0123
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0121
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0122
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0124
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0124
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0122
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0119
2992/6530 [============>.................] - ETA: 0s - loss: 0.0119
3184/6530 [=============>................] - ETA: 0s - loss: 0.0120
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0119
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0119
3792/6530 [================>.............] - ETA: 0s - loss: 0.0118
4016/6530 [=================>............] - ETA: 0s - loss: 0.0117
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0117
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0117
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0116
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0117
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0117
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0116
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0116
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0117
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0116
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0116
6400/6530 [============================>.] - ETA: 0s - loss: 0.0116
6530/6530 [==============================] - 2s 244us/step - loss: 0.0116 - val_loss: 0.0156
Epoch 12/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0107
 224/6530 [>.............................] - ETA: 1s - loss: 0.0114
 416/6530 [>.............................] - ETA: 1s - loss: 0.0117
 656/6530 [==>...........................] - ETA: 1s - loss: 0.0105
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0109
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0109
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0108
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0118
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0119
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0120
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0119
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0118
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0116
2928/6530 [============>.................] - ETA: 0s - loss: 0.0117
3168/6530 [=============>................] - ETA: 0s - loss: 0.0119
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0118
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0118
3840/6530 [================>.............] - ETA: 0s - loss: 0.0116
4048/6530 [=================>............] - ETA: 0s - loss: 0.0116
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0115
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0115
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0116
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0115
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0116
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0115
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0113
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0114
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0114
6320/6530 [============================>.] - ETA: 0s - loss: 0.0115
6528/6530 [============================>.] - ETA: 0s - loss: 0.0115
6530/6530 [==============================] - 2s 237us/step - loss: 0.0115 - val_loss: 0.0114
Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0072
 224/6530 [>.............................] - ETA: 1s - loss: 0.0102
 464/6530 [=>............................] - ETA: 1s - loss: 0.0106
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0098
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0099
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0102
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0103
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0103
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0103
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0102
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0101
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0106
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0104
2848/6530 [============>.................] - ETA: 0s - loss: 0.0104
3104/6530 [=============>................] - ETA: 0s - loss: 0.0104
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0104
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0106
3888/6530 [================>.............] - ETA: 0s - loss: 0.0107
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0110
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0110
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0110
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0109
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0109
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0108
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0108
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0108
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0108
6336/6530 [============================>.] - ETA: 0s - loss: 0.0108
6530/6530 [==============================] - 2s 230us/step - loss: 0.0108 - val_loss: 0.0253
Epoch 14/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0313
 224/6530 [>.............................] - ETA: 1s - loss: 0.0136
 432/6530 [>.............................] - ETA: 1s - loss: 0.0115
 656/6530 [==>...........................] - ETA: 1s - loss: 0.0113
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0109
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0113
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0115
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0114
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0114
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0113
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0110
2912/6530 [============>.................] - ETA: 0s - loss: 0.0111
3168/6530 [=============>................] - ETA: 0s - loss: 0.0111
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0110
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0110
3968/6530 [=================>............] - ETA: 0s - loss: 0.0109
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0108
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0108
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0107
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0107
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0107
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0106
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0105
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0105
6512/6530 [============================>.] - ETA: 0s - loss: 0.0105
6530/6530 [==============================] - 1s 196us/step - loss: 0.0105 - val_loss: 0.0225
Epoch 15/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0214
 288/6530 [>.............................] - ETA: 1s - loss: 0.0121
 528/6530 [=>............................] - ETA: 1s - loss: 0.0103
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0097
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0098
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0094
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0092
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0095
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0096
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0096
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0095
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0096
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0100
3104/6530 [=============>................] - ETA: 0s - loss: 0.0098
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0098
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0099
3840/6530 [================>.............] - ETA: 0s - loss: 0.0098
4064/6530 [=================>............] - ETA: 0s - loss: 0.0098
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0096
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0097
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0097
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0097
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0097
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0098
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0099
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0099
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0100
6480/6530 [============================>.] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 1s 222us/step - loss: 0.0100 - val_loss: 0.0243
Epoch 16/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0233
 256/6530 [>.............................] - ETA: 1s - loss: 0.0126
 512/6530 [=>............................] - ETA: 1s - loss: 0.0106
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0101
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0101
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0096
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0095
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0096
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0095
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0096
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0097
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0097
2928/6530 [============>.................] - ETA: 0s - loss: 0.0099
3136/6530 [=============>................] - ETA: 0s - loss: 0.0098
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0097
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0097
3728/6530 [================>.............] - ETA: 0s - loss: 0.0096
3936/6530 [=================>............] - ETA: 0s - loss: 0.0095
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0095
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0096
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0095
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0095
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0094
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0095
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0095
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0096
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0096
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0097
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0098
6480/6530 [============================>.] - ETA: 0s - loss: 0.0097
6530/6530 [==============================] - 2s 238us/step - loss: 0.0097 - val_loss: 0.0258
Epoch 17/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0255
 288/6530 [>.............................] - ETA: 1s - loss: 0.0101
 544/6530 [=>............................] - ETA: 1s - loss: 0.0099
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0099
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0092
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0092
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0094
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0094
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0093
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0093
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0092
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0091
2992/6530 [============>.................] - ETA: 0s - loss: 0.0091
3200/6530 [=============>................] - ETA: 0s - loss: 0.0092
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0093
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0094
3920/6530 [=================>............] - ETA: 0s - loss: 0.0095
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0096
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0095
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0095
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0095
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0095
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0095
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0095
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0095
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0095
6530/6530 [==============================] - 1s 211us/step - loss: 0.0095 - val_loss: 0.0263

# training | RMSE: 0.1571, MAE: 0.1343
worker 2  xfile  [3, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.20952434789544042}, 'layer_4_size': 92, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.490753343048291}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.15714019059652568, 'rmse': 0.15714019059652568, 'mae': 0.13427194845249443, 'early_stop': True}
vggnet done  2
all of workers have been done
calculation finished
#2 epoch=27.0 loss={'loss': 0.09101404549530169, 'rmse': 0.09101404549530169, 'mae': 0.06807528063763053, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23040836054459668}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#0 epoch=27.0 loss={'loss': 0.11117919147139543, 'rmse': 0.11117919147139543, 'mae': 0.08730057768193428, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#1 epoch=27.0 loss={'loss': 0.0931157240851333, 'rmse': 0.0931157240851333, 'mae': 0.07416125491043915, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 69, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#4 epoch=27.0 loss={'loss': 0.09855593755136743, 'rmse': 0.09855593755136743, 'mae': 0.0775504480447567, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1192090463413142}, 'layer_4_size': 47, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#3 epoch=27.0 loss={'loss': 0.15714019059652568, 'rmse': 0.15714019059652568, 'mae': 0.13427194845249443, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.20952434789544042}, 'layer_4_size': 92, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.490753343048291}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
get a list [results] of length 190
get a list [loss] of length 5
get a list [val_loss] of length 5
length of indices is (2, 1, 4, 0, 3)
length of indices is 5
length of T is 5
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]] 

*** 1.6666666666666665 configurations x 81.0 iterations each

4 | Fri Sep 28 01:23:34 2018 | lowest loss so far: 0.0705 (run 0)

vggnet done  1
vggnet done  2
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  25 | activation: relu    | extras: None 
layer 2 | size:  55 | activation: tanh    | extras: None 
layer 3 | size:  55 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9f1a90>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 4:29 - loss: 0.5631
 432/6530 [>.............................] - ETA: 10s - loss: 0.2454 
 848/6530 [==>...........................] - ETA: 5s - loss: 0.1496 
1264/6530 [====>.........................] - ETA: 3s - loss: 0.1146
1648/6530 [======>.......................] - ETA: 2s - loss: 0.0965
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0840
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0754
3120/6530 [=============>................] - ETA: 1s - loss: 0.0707
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0673
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0641
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0616
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0601
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0587
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0571
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0559
6530/6530 [==============================] - 1s 223us/step - loss: 0.0551 - val_loss: 0.0393
Epoch 2/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0391
 464/6530 [=>............................] - ETA: 0s - loss: 0.0367
 848/6530 [==>...........................] - ETA: 0s - loss: 0.0374
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0380
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0371
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0373
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0374
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0380
3104/6530 [=============>................] - ETA: 0s - loss: 0.0386
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0390
3712/6530 [================>.............] - ETA: 0s - loss: 0.0392
4064/6530 [=================>............] - ETA: 0s - loss: 0.0390
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0390
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0387
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0384
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0389
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0388
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0391
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0392
6530/6530 [==============================] - 1s 156us/step - loss: 0.0394 - val_loss: 0.0465
Epoch 3/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0277
 320/6530 [>.............................] - ETA: 1s - loss: 0.0390
 592/6530 [=>............................] - ETA: 1s - loss: 0.0367
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0369
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0381
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0376
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0378
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0372
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0377
2976/6530 [============>.................] - ETA: 0s - loss: 0.0380
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0381
3952/6530 [=================>............] - ETA: 0s - loss: 0.0380
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0385
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0388
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0387
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0387
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0390
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0387
6530/6530 [==============================] - 1s 146us/step - loss: 0.0386 - val_loss: 0.0404
Epoch 4/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0248
 384/6530 [>.............................] - ETA: 0s - loss: 0.0382
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0383
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0390
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0390
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0391
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0393
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0383
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0383
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0379
2992/6530 [============>.................] - ETA: 0s - loss: 0.0379
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0376
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0378
3968/6530 [=================>............] - ETA: 0s - loss: 0.0377
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0373
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0375
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0374
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0380
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0377
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0376
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0376
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0372
6530/6530 [==============================] - 1s 178us/step - loss: 0.0370 - val_loss: 0.0439
Epoch 5/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0368
 352/6530 [>.............................] - ETA: 0s - loss: 0.0364
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0340
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0336
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0341
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0340
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0330
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0335
3232/6530 [=============>................] - ETA: 0s - loss: 0.0332
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0327
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0323
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0320
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0320
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0317
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0316
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0315
6400/6530 [============================>.] - ETA: 0s - loss: 0.0315
6530/6530 [==============================] - 1s 135us/step - loss: 0.0314 - val_loss: 0.0347
Epoch 6/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0176
 368/6530 [>.............................] - ETA: 0s - loss: 0.0314
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0302
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0297
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0300
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0297
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0303
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0303
2864/6530 [============>.................] - ETA: 0s - loss: 0.0299
3216/6530 [=============>................] - ETA: 0s - loss: 0.0292
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0290
4000/6530 [=================>............] - ETA: 0s - loss: 0.0289
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0288
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0285
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0283
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0280
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0280
6530/6530 [==============================] - 1s 135us/step - loss: 0.0279 - val_loss: 0.0311
Epoch 7/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0411
 464/6530 [=>............................] - ETA: 0s - loss: 0.0293
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0275
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0274
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0269
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0264
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0258
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0251
2880/6530 [============>.................] - ETA: 0s - loss: 0.0251
3184/6530 [=============>................] - ETA: 0s - loss: 0.0249
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0246
3920/6530 [=================>............] - ETA: 0s - loss: 0.0243
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0245
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0243
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0238
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0236
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0237
6480/6530 [============================>.] - ETA: 0s - loss: 0.0235
6530/6530 [==============================] - 1s 141us/step - loss: 0.0235 - val_loss: 0.0239
Epoch 8/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0298
 320/6530 [>.............................] - ETA: 1s - loss: 0.0196
 592/6530 [=>............................] - ETA: 1s - loss: 0.0200
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0202
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0192
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0191
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0194
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0199
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0196
2928/6530 [============>.................] - ETA: 0s - loss: 0.0194
3200/6530 [=============>................] - ETA: 0s - loss: 0.0192
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0191
3760/6530 [================>.............] - ETA: 0s - loss: 0.0190
4048/6530 [=================>............] - ETA: 0s - loss: 0.0188
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0189
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0186
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0187
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0186
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0186
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0185
6400/6530 [============================>.] - ETA: 0s - loss: 0.0183
6530/6530 [==============================] - 1s 168us/step - loss: 0.0182 - val_loss: 0.0423
Epoch 9/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0270
 336/6530 [>.............................] - ETA: 1s - loss: 0.0166
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0160
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0162
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0171
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0173
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0172
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0166
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0165
3056/6530 [=============>................] - ETA: 0s - loss: 0.0165
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0166
3824/6530 [================>.............] - ETA: 0s - loss: 0.0165
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0165
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0162
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0165
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0164
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0163
6448/6530 [============================>.] - ETA: 0s - loss: 0.0162
6530/6530 [==============================] - 1s 140us/step - loss: 0.0161 - val_loss: 0.0183
Epoch 10/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0143
 368/6530 [>.............................] - ETA: 0s - loss: 0.0157
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0162
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0160
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0160
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0157
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0158
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0157
3024/6530 [============>.................] - ETA: 0s - loss: 0.0157
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0157
3792/6530 [================>.............] - ETA: 0s - loss: 0.0155
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0153
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0151
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0152
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0152
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0152
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0152
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0151
6416/6530 [============================>.] - ETA: 0s - loss: 0.0151
6530/6530 [==============================] - 1s 151us/step - loss: 0.0151 - val_loss: 0.0138
Epoch 11/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0193
 304/6530 [>.............................] - ETA: 1s - loss: 0.0136
 608/6530 [=>............................] - ETA: 1s - loss: 0.0143
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0140
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0139
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0140
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0140
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0143
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0143
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0145
3024/6530 [============>.................] - ETA: 0s - loss: 0.0147
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0144
3744/6530 [================>.............] - ETA: 0s - loss: 0.0144
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0144
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0143
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0142
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0142
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0143
6352/6530 [============================>.] - ETA: 0s - loss: 0.0143
6530/6530 [==============================] - 1s 150us/step - loss: 0.0142 - val_loss: 0.0155
Epoch 12/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0167
 432/6530 [>.............................] - ETA: 0s - loss: 0.0138
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0139
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0142
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0139
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0139
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0139
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0140
3168/6530 [=============>................] - ETA: 0s - loss: 0.0138
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0138
3952/6530 [=================>............] - ETA: 0s - loss: 0.0139
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0137
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0136
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0137
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0138
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0137
6320/6530 [============================>.] - ETA: 0s - loss: 0.0137
6530/6530 [==============================] - 1s 138us/step - loss: 0.0137 - val_loss: 0.0162
Epoch 13/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0111
 336/6530 [>.............................] - ETA: 1s - loss: 0.0143
 608/6530 [=>............................] - ETA: 1s - loss: 0.0133
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0129
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0139
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0136
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0134
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0140
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0140
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0137
2928/6530 [============>.................] - ETA: 0s - loss: 0.0135
3232/6530 [=============>................] - ETA: 0s - loss: 0.0134
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0134
3792/6530 [================>.............] - ETA: 0s - loss: 0.0133
4096/6530 [=================>............] - ETA: 0s - loss: 0.0134
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0134
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0134
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0133
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0132
6368/6530 [============================>.] - ETA: 0s - loss: 0.0132
6530/6530 [==============================] - 1s 157us/step - loss: 0.0132 - val_loss: 0.0149
Epoch 14/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0121
 400/6530 [>.............................] - ETA: 0s - loss: 0.0122
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0119
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0119
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0117
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0118
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0122
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0122
2848/6530 [============>.................] - ETA: 0s - loss: 0.0124
3168/6530 [=============>................] - ETA: 0s - loss: 0.0123
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0125
3728/6530 [================>.............] - ETA: 0s - loss: 0.0125
4000/6530 [=================>............] - ETA: 0s - loss: 0.0125
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0126
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0126
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0126
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0127
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0127
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0128
6400/6530 [============================>.] - ETA: 0s - loss: 0.0127
6530/6530 [==============================] - 1s 157us/step - loss: 0.0128 - val_loss: 0.0172
Epoch 15/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0141
 384/6530 [>.............................] - ETA: 0s - loss: 0.0127
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0139
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0131
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0131
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0129
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0125
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0125
3008/6530 [============>.................] - ETA: 0s - loss: 0.0125
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0125
3728/6530 [================>.............] - ETA: 0s - loss: 0.0124
4064/6530 [=================>............] - ETA: 0s - loss: 0.0125
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0125
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0125
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0125
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0125
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0125
6416/6530 [============================>.] - ETA: 0s - loss: 0.0126
6530/6530 [==============================] - 1s 141us/step - loss: 0.0126 - val_loss: 0.0142

# training | RMSE: 0.1112, MAE: 0.0873
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.11118163190711337, 'rmse': 0.11118163190711337, 'mae': 0.08730193790864778, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=81.0 loss={'loss': 0.11118163190711337, 'rmse': 0.11118163190711337, 'mae': 0.08730193790864778, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
get a list [results] of length 191
get a list [loss] of length 1
get a list [val_loss] of length 1
length of indices is (0,)
length of indices is 1
length of T is 1
s=1
T is of size 8
T=[{'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 10, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14759338769004177}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3801219225105922}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2007328291273842}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.14033998426882086}, 'layer_4_size': 15, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12685547129234936}, 'layer_1_size': 43, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.24677886952239084}, 'layer_4_size': 72, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18053653660587218}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 100, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3211261514163377}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 92, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16969009309750988}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 29, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 31, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 74, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 10, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14759338769004177}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [1, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [2, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3801219225105922}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [3, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2007328291273842}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.14033998426882086}, 'layer_4_size': 15, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [4, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12685547129234936}, 'layer_1_size': 43, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.24677886952239084}, 'layer_4_size': 72, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18053653660587218}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [5, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 100, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3211261514163377}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [6, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 92, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16969009309750988}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 29, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}], [7, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 31, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 74, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]] 

*** 8 configurations x 27.0 iterations each

1 | Fri Sep 28 01:23:50 2018 | lowest loss so far: 0.0705 (run 0)

{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  42 | activation: relu    | extras: None 
layer 2 | size:  35 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  64/6530 [..............................] - ETA: 1:25 - loss: 0.5039{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  10 | activation: tanh    | extras: None 
layer 2 | size:  80 | activation: tanh    | extras: dropout - rate: 14.8% 
layer 3 | size:  65 | activation: sigmoid | extras: None 
layer 4 | size:  62 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 128/6530 [..............................] - ETA: 36s - loss: 0.3800
1536/6530 [======>.......................] - ETA: 2s - loss: 0.3744  
2944/6530 [============>.................] - ETA: 0s - loss: 0.0914 
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1965
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0677
6530/6530 [==============================] - 1s 134us/step - loss: 0.0660 - val_loss: 0.0405
Epoch 2/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0429
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1415
3712/6530 [================>.............] - ETA: 0s - loss: 0.0412
6530/6530 [==============================] - 1s 162us/step - loss: 0.1200 - val_loss: 0.0443
Epoch 2/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0397
6530/6530 [==============================] - 0s 15us/step - loss: 0.0413 - val_loss: 0.0402
Epoch 3/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0406
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0393
3072/6530 [=============>................] - ETA: 0s - loss: 0.0391
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0387
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0400
6530/6530 [==============================] - 0s 18us/step - loss: 0.0397 - val_loss: 0.0392
Epoch 4/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0426
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0386
6530/6530 [==============================] - 0s 30us/step - loss: 0.0384 - val_loss: 0.0384
Epoch 3/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0393
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0385
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0364
6530/6530 [==============================] - 0s 16us/step - loss: 0.0384 - val_loss: 0.0379
Epoch 5/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0383
3968/6530 [=================>............] - ETA: 0s - loss: 0.0344
2944/6530 [============>.................] - ETA: 0s - loss: 0.0367
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0332
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0367
6530/6530 [==============================] - 0s 18us/step - loss: 0.0365 - val_loss: 0.0360
Epoch 6/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0352
6530/6530 [==============================] - 0s 28us/step - loss: 0.0329 - val_loss: 0.0418
Epoch 4/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0336
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0342
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0290
3968/6530 [=================>............] - ETA: 0s - loss: 0.0288
6530/6530 [==============================] - 0s 17us/step - loss: 0.0345 - val_loss: 0.0338
Epoch 7/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0309
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0288
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0325
6530/6530 [==============================] - 0s 29us/step - loss: 0.0285 - val_loss: 0.0393
Epoch 5/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0507
6530/6530 [==============================] - 0s 17us/step - loss: 0.0324 - val_loss: 0.0317
Epoch 8/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0307
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0271
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0302
3968/6530 [=================>............] - ETA: 0s - loss: 0.0260
6400/6530 [============================>.] - ETA: 0s - loss: 0.0304
6530/6530 [==============================] - 0s 17us/step - loss: 0.0302 - val_loss: 0.0296
Epoch 9/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0288
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0256
6530/6530 [==============================] - 0s 28us/step - loss: 0.0253 - val_loss: 0.0263
Epoch 6/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0293
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0282
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0237
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0284
6530/6530 [==============================] - 0s 17us/step - loss: 0.0282 - val_loss: 0.0282
Epoch 10/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0251
3712/6530 [================>.............] - ETA: 0s - loss: 0.0231{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  98 | activation: tanh    | extras: dropout - rate: 38.0% 
layer 2 | size:  14 | activation: tanh    | extras: None 
layer 3 | size:  25 | activation: tanh    | extras: batchnorm 
layer 4 | size:  50 | activation: tanh    | extras: batchnorm 
layer 5 | size:  35 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 7:57 - loss: 0.2862
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0263
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0228
6530/6530 [==============================] - 0s 16us/step - loss: 0.0262 - val_loss: 0.0268

 224/6530 [>.............................] - ETA: 34s - loss: 0.2818 Epoch 11/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0245
6530/6530 [==============================] - 0s 29us/step - loss: 0.0224 - val_loss: 0.0236
Epoch 7/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0239
 432/6530 [>.............................] - ETA: 18s - loss: 0.2760
2944/6530 [============>.................] - ETA: 0s - loss: 0.0248
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0208
6400/6530 [============================>.] - ETA: 0s - loss: 0.0251
 656/6530 [==>...........................] - ETA: 11s - loss: 0.2705
6530/6530 [==============================] - 0s 17us/step - loss: 0.0250 - val_loss: 0.0257
Epoch 12/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0247
3968/6530 [=================>............] - ETA: 0s - loss: 0.0202
 864/6530 [==>...........................] - ETA: 9s - loss: 0.2632 
3200/6530 [=============>................] - ETA: 0s - loss: 0.0240
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0199
6530/6530 [==============================] - 0s 27us/step - loss: 0.0197 - val_loss: 0.0284
Epoch 8/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0261
1072/6530 [===>..........................] - ETA: 7s - loss: 0.2615
6528/6530 [============================>.] - ETA: 0s - loss: 0.0243
6530/6530 [==============================] - 0s 17us/step - loss: 0.0243 - val_loss: 0.0252
Epoch 13/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0246
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0187
1296/6530 [====>.........................] - ETA: 6s - loss: 0.2537
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0239
3712/6530 [================>.............] - ETA: 0s - loss: 0.0180
1520/6530 [=====>........................] - ETA: 5s - loss: 0.2497
6528/6530 [============================>.] - ETA: 0s - loss: 0.0235
6530/6530 [==============================] - 0s 17us/step - loss: 0.0235 - val_loss: 0.0243
Epoch 14/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0220
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0175
1728/6530 [======>.......................] - ETA: 4s - loss: 0.2436
6530/6530 [==============================] - 0s 28us/step - loss: 0.0174 - val_loss: 0.0366
Epoch 9/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0409
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0227
1920/6530 [=======>......................] - ETA: 3s - loss: 0.2422
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0176
6528/6530 [============================>.] - ETA: 0s - loss: 0.0229
6530/6530 [==============================] - 0s 17us/step - loss: 0.0229 - val_loss: 0.0241
Epoch 15/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0229
2128/6530 [========>.....................] - ETA: 3s - loss: 0.2391
4032/6530 [=================>............] - ETA: 0s - loss: 0.0166
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0224
2336/6530 [=========>....................] - ETA: 3s - loss: 0.2357
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0162
6530/6530 [==============================] - 0s 16us/step - loss: 0.0221 - val_loss: 0.0222
Epoch 16/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0206
6530/6530 [==============================] - 0s 27us/step - loss: 0.0162 - val_loss: 0.0234
Epoch 10/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0148
2544/6530 [==========>...................] - ETA: 2s - loss: 0.2343
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0208
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0165
2768/6530 [===========>..................] - ETA: 2s - loss: 0.2327
6528/6530 [============================>.] - ETA: 0s - loss: 0.0209
4032/6530 [=================>............] - ETA: 0s - loss: 0.0155
6530/6530 [==============================] - 0s 17us/step - loss: 0.0209 - val_loss: 0.0220
Epoch 17/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0198
2992/6530 [============>.................] - ETA: 2s - loss: 0.2301
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0152
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0206
6530/6530 [==============================] - 0s 27us/step - loss: 0.0151 - val_loss: 0.0279
Epoch 11/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0194
3200/6530 [=============>................] - ETA: 2s - loss: 0.2287
6530/6530 [==============================] - 0s 16us/step - loss: 0.0206 - val_loss: 0.0214
Epoch 18/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0188
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0145
3424/6530 [==============>...............] - ETA: 1s - loss: 0.2282
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0200
3968/6530 [=================>............] - ETA: 0s - loss: 0.0149
3648/6530 [===============>..............] - ETA: 1s - loss: 0.2279
6530/6530 [==============================] - 0s 16us/step - loss: 0.0201 - val_loss: 0.0204
Epoch 19/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0184
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0146
3856/6530 [================>.............] - ETA: 1s - loss: 0.2264
6530/6530 [==============================] - 0s 27us/step - loss: 0.0143 - val_loss: 0.0205
Epoch 12/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0169
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0193
4064/6530 [=================>............] - ETA: 1s - loss: 0.2245
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0142
6530/6530 [==============================] - 0s 16us/step - loss: 0.0195 - val_loss: 0.0206
Epoch 20/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0179
4272/6530 [==================>...........] - ETA: 1s - loss: 0.2229
3776/6530 [================>.............] - ETA: 0s - loss: 0.0135
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0193
4480/6530 [===================>..........] - ETA: 1s - loss: 0.2224
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0136
6530/6530 [==============================] - 0s 16us/step - loss: 0.0193 - val_loss: 0.0196
Epoch 21/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0170
6530/6530 [==============================] - 0s 28us/step - loss: 0.0135 - val_loss: 0.0201
Epoch 13/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0179
4704/6530 [====================>.........] - ETA: 0s - loss: 0.2205
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0188
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0132
4912/6530 [=====================>........] - ETA: 0s - loss: 0.2194
6530/6530 [==============================] - 0s 17us/step - loss: 0.0188 - val_loss: 0.0189
Epoch 22/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0153
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0134
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2187
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0134
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0179
5328/6530 [=======================>......] - ETA: 0s - loss: 0.2184
6530/6530 [==============================] - 0s 29us/step - loss: 0.0131 - val_loss: 0.0190
Epoch 14/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0192
6530/6530 [==============================] - 0s 16us/step - loss: 0.0182 - val_loss: 0.0183
Epoch 23/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0145
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2177
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0124
3072/6530 [=============>................] - ETA: 0s - loss: 0.0171
5744/6530 [=========================>....] - ETA: 0s - loss: 0.2170
3776/6530 [================>.............] - ETA: 0s - loss: 0.0122
6528/6530 [============================>.] - ETA: 0s - loss: 0.0177
6530/6530 [==============================] - 0s 17us/step - loss: 0.0177 - val_loss: 0.0179
Epoch 24/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0152
5968/6530 [==========================>...] - ETA: 0s - loss: 0.2157
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0123
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0174
6530/6530 [==============================] - 0s 28us/step - loss: 0.0124 - val_loss: 0.0168

6176/6530 [===========================>..] - ETA: 0s - loss: 0.2150Epoch 15/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0124
6530/6530 [==============================] - 0s 16us/step - loss: 0.0175 - val_loss: 0.0177

6400/6530 [============================>.] - ETA: 0s - loss: 0.2143
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0115Epoch 25/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0146
3840/6530 [================>.............] - ETA: 0s - loss: 0.0119
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0168
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0119
6530/6530 [==============================] - 0s 16us/step - loss: 0.0169 - val_loss: 0.0175
Epoch 26/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0151
6530/6530 [==============================] - 3s 438us/step - loss: 0.2136 - val_loss: 0.1673

6530/6530 [==============================] - 0s 28us/step - loss: 0.0120 - val_loss: 0.0135
Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.2137Epoch 16/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0086
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0164
 224/6530 [>.............................] - ETA: 1s - loss: 0.2009
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0117
6530/6530 [==============================] - 0s 16us/step - loss: 0.0164 - val_loss: 0.0170
Epoch 27/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0140
3904/6530 [================>.............] - ETA: 0s - loss: 0.0115
 448/6530 [=>............................] - ETA: 1s - loss: 0.2009
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0157
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0115
 656/6530 [==>...........................] - ETA: 1s - loss: 0.2043
6530/6530 [==============================] - 0s 28us/step - loss: 0.0116 - val_loss: 0.0206
Epoch 17/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0219
6530/6530 [==============================] - 0s 16us/step - loss: 0.0160 - val_loss: 0.0165

 864/6530 [==>...........................] - ETA: 1s - loss: 0.1995
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0112
1072/6530 [===>..........................] - ETA: 1s - loss: 0.2020
3968/6530 [=================>............] - ETA: 0s - loss: 0.0116
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1994
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0114
6530/6530 [==============================] - 0s 27us/step - loss: 0.0114 - val_loss: 0.0137
Epoch 18/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0108
1488/6530 [=====>........................] - ETA: 1s - loss: 0.1995
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0106
1712/6530 [======>.......................] - ETA: 1s - loss: 0.1984
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0108
1936/6530 [=======>......................] - ETA: 1s - loss: 0.1980
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0107
2144/6530 [========>.....................] - ETA: 1s - loss: 0.1966
6530/6530 [==============================] - 0s 26us/step - loss: 0.0108 - val_loss: 0.0135
Epoch 19/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0107
2352/6530 [=========>....................] - ETA: 1s - loss: 0.1965
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0103
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1964
4096/6530 [=================>............] - ETA: 0s - loss: 0.0107
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1961
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0105
6530/6530 [==============================] - 0s 26us/step - loss: 0.0105 - val_loss: 0.0121
Epoch 20/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0077
2992/6530 [============>.................] - ETA: 0s - loss: 0.1942
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0106
3200/6530 [=============>................] - ETA: 0s - loss: 0.1938
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0106
# training | RMSE: 0.1213, MAE: 0.0930
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 10, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14759338769004177}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.12132806163199679, 'rmse': 0.12132806163199679, 'mae': 0.09301086690344386, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  15 | activation: relu    | extras: dropout - rate: 20.1% 
layer 2 | size:  47 | activation: relu    | extras: None 
layer 3 | size:  14 | activation: tanh    | extras: None 
layer 4 | size:  15 | activation: tanh    | extras: dropout - rate: 14.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9f12b0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 256/6530 [>.............................] - ETA: 5s - loss: 0.9518
3408/6530 [==============>...............] - ETA: 0s - loss: 0.1940
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0106
6530/6530 [==============================] - 0s 27us/step - loss: 0.0107 - val_loss: 0.0154
Epoch 21/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0115
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1942
6530/6530 [==============================] - 0s 48us/step - loss: 0.4417 - val_loss: 0.2233
Epoch 2/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.2337
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0108
3824/6530 [================>.............] - ETA: 0s - loss: 0.1934
6144/6530 [===========================>..] - ETA: 0s - loss: 0.2269
6530/6530 [==============================] - 0s 9us/step - loss: 0.2258 - val_loss: 0.1884
Epoch 3/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.2187
3776/6530 [================>.............] - ETA: 0s - loss: 0.0103
4032/6530 [=================>............] - ETA: 0s - loss: 0.1935
6400/6530 [============================>.] - ETA: 0s - loss: 0.2013
6530/6530 [==============================] - 0s 9us/step - loss: 0.2010 - val_loss: 0.1789
Epoch 4/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1944
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0100
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1927
6530/6530 [==============================] - 0s 8us/step - loss: 0.1900 - val_loss: 0.1711
Epoch 5/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1944
6530/6530 [==============================] - 0s 29us/step - loss: 0.0099 - val_loss: 0.0157
Epoch 22/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0159
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1925
6400/6530 [============================>.] - ETA: 0s - loss: 0.1837
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0101
6530/6530 [==============================] - 0s 9us/step - loss: 0.1837 - val_loss: 0.1650
Epoch 6/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1851
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1920
3904/6530 [================>.............] - ETA: 0s - loss: 0.0099
6530/6530 [==============================] - 0s 8us/step - loss: 0.1777 - val_loss: 0.1603
Epoch 7/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1698
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1918
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0097
6400/6530 [============================>.] - ETA: 0s - loss: 0.1729
6530/6530 [==============================] - 0s 9us/step - loss: 0.1728 - val_loss: 0.1564
Epoch 8/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1698
6530/6530 [==============================] - 0s 28us/step - loss: 0.0097 - val_loss: 0.0337
Epoch 23/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0399
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1920
6530/6530 [==============================] - 0s 8us/step - loss: 0.1671 - val_loss: 0.1530
Epoch 9/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1637
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1916
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0097
6530/6530 [==============================] - 0s 8us/step - loss: 0.1630 - val_loss: 0.1491
Epoch 10/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1643
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1910
4032/6530 [=================>............] - ETA: 0s - loss: 0.0097
6530/6530 [==============================] - 0s 8us/step - loss: 0.1590 - val_loss: 0.1457
Epoch 11/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1496
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1910
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0097
6530/6530 [==============================] - 0s 28us/step - loss: 0.0096 - val_loss: 0.0117
Epoch 24/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0141
6530/6530 [==============================] - 0s 8us/step - loss: 0.1565 - val_loss: 0.1415
Epoch 12/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1456
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1909
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0097
6530/6530 [==============================] - 0s 8us/step - loss: 0.1533 - val_loss: 0.1394
Epoch 13/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1444
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1907
3904/6530 [================>.............] - ETA: 0s - loss: 0.0095
6530/6530 [==============================] - 0s 8us/step - loss: 0.1504 - val_loss: 0.1372
Epoch 14/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1459
6352/6530 [============================>.] - ETA: 0s - loss: 0.1901
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0094
6530/6530 [==============================] - 0s 28us/step - loss: 0.0094 - val_loss: 0.0173

6530/6530 [==============================] - 0s 8us/step - loss: 0.1479 - val_loss: 0.1332
Epoch 25/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0162Epoch 15/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1528
6530/6530 [==============================] - 2s 253us/step - loss: 0.1898 - val_loss: 0.1630
Epoch 3/27

  16/6530 [..............................] - ETA: 1s - loss: 0.2289
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0092
6530/6530 [==============================] - 0s 9us/step - loss: 0.1429 - val_loss: 0.1303
Epoch 16/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1440
 224/6530 [>.............................] - ETA: 1s - loss: 0.1876
3904/6530 [================>.............] - ETA: 0s - loss: 0.0088
6530/6530 [==============================] - 0s 8us/step - loss: 0.1434 - val_loss: 0.1279
Epoch 17/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1383
 432/6530 [>.............................] - ETA: 1s - loss: 0.1889
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0090
6530/6530 [==============================] - 0s 8us/step - loss: 0.1392 - val_loss: 0.1264
Epoch 18/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1355
6530/6530 [==============================] - 0s 27us/step - loss: 0.0090 - val_loss: 0.0326
Epoch 26/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0289
 656/6530 [==>...........................] - ETA: 1s - loss: 0.1907
6530/6530 [==============================] - 0s 9us/step - loss: 0.1381 - val_loss: 0.1226
Epoch 19/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1451
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0095
 864/6530 [==>...........................] - ETA: 1s - loss: 0.1874
6530/6530 [==============================] - 0s 8us/step - loss: 0.1341 - val_loss: 0.1208
Epoch 20/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1203
3904/6530 [================>.............] - ETA: 0s - loss: 0.0092
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1895
6530/6530 [==============================] - 0s 8us/step - loss: 0.1339 - val_loss: 0.1189
Epoch 21/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1305
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0089
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1852
6530/6530 [==============================] - 0s 28us/step - loss: 0.0088 - val_loss: 0.0142
Epoch 27/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0106
6400/6530 [============================>.] - ETA: 0s - loss: 0.1323
6530/6530 [==============================] - 0s 9us/step - loss: 0.1322 - val_loss: 0.1173
Epoch 22/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1247
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1829
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0091
6530/6530 [==============================] - 0s 8us/step - loss: 0.1285 - val_loss: 0.1159
Epoch 23/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1298
1728/6530 [======>.......................] - ETA: 1s - loss: 0.1824
3840/6530 [================>.............] - ETA: 0s - loss: 0.0089
6530/6530 [==============================] - 0s 9us/step - loss: 0.1289 - val_loss: 0.1142
Epoch 24/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1294
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0088
1952/6530 [=======>......................] - ETA: 1s - loss: 0.1831
6530/6530 [==============================] - 0s 28us/step - loss: 0.0087 - val_loss: 0.0100

6530/6530 [==============================] - 0s 8us/step - loss: 0.1286 - val_loss: 0.1132
Epoch 25/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1341
2160/6530 [========>.....................] - ETA: 1s - loss: 0.1813
6530/6530 [==============================] - 0s 8us/step - loss: 0.1284 - val_loss: 0.1128
Epoch 26/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1343
2384/6530 [=========>....................] - ETA: 1s - loss: 0.1808
6400/6530 [============================>.] - ETA: 0s - loss: 0.1255
6530/6530 [==============================] - 0s 9us/step - loss: 0.1255 - val_loss: 0.1105
Epoch 27/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1278
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1806
6530/6530 [==============================] - 0s 8us/step - loss: 0.1253 - val_loss: 0.1096

2832/6530 [============>.................] - ETA: 0s - loss: 0.1796
3040/6530 [============>.................] - ETA: 0s - loss: 0.1773
3264/6530 [=============>................] - ETA: 0s - loss: 0.1780
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1787
3696/6530 [===============>..............] - ETA: 0s - loss: 0.1784
# training | RMSE: 0.0912, MAE: 0.0710
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.0911742643278679, 'rmse': 0.0911742643278679, 'mae': 0.07099675097236031, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  43 | activation: relu    | extras: dropout - rate: 12.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f041f9f1b70>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 256/6530 [>.............................] - ETA: 2s - loss: 0.7335
3888/6530 [================>.............] - ETA: 0s - loss: 0.1787
6530/6530 [==============================] - 0s 26us/step - loss: 0.6247 - val_loss: 0.4386
Epoch 2/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.4427
6530/6530 [==============================] - 0s 6us/step - loss: 0.2325 - val_loss: 0.1645
Epoch 3/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1619
4096/6530 [=================>............] - ETA: 0s - loss: 0.1788
6530/6530 [==============================] - 0s 6us/step - loss: 0.1602 - val_loss: 0.1593
Epoch 4/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1552
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1782
6530/6530 [==============================] - 0s 6us/step - loss: 0.1544 - val_loss: 0.1548
Epoch 5/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1487
# training | RMSE: 0.1338, MAE: 0.1029
worker 0  xfile  [3, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2007328291273842}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.14033998426882086}, 'layer_4_size': 15, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.13382600607499123, 'rmse': 0.13382600607499123, 'mae': 0.10287794569994442, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:   7 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f041667c3c8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 33s - loss: 1.3185
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1784
6530/6530 [==============================] - 0s 7us/step - loss: 0.1478 - val_loss: 0.1475
Epoch 6/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1412
1184/6530 [====>.........................] - ETA: 0s - loss: 1.1874 
6530/6530 [==============================] - 0s 7us/step - loss: 0.1388 - val_loss: 0.1388

4736/6530 [====================>.........] - ETA: 0s - loss: 0.1780Epoch 7/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1337
2368/6530 [=========>....................] - ETA: 0s - loss: 1.0314
6530/6530 [==============================] - 0s 6us/step - loss: 0.1295 - val_loss: 0.1301
Epoch 8/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1252
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1778
3584/6530 [===============>..............] - ETA: 0s - loss: 0.8766
6530/6530 [==============================] - 0s 6us/step - loss: 0.1211 - val_loss: 0.1222
Epoch 9/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1154
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1784
6530/6530 [==============================] - 0s 6us/step - loss: 0.1136 - val_loss: 0.1163
Epoch 10/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1079
4768/6530 [====================>.........] - ETA: 0s - loss: 0.7600
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1786
6530/6530 [==============================] - 0s 6us/step - loss: 0.1069 - val_loss: 0.1112
Epoch 11/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1019
5952/6530 [==========================>...] - ETA: 0s - loss: 0.6599
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1780
6530/6530 [==============================] - 0s 8us/step - loss: 0.1012 - val_loss: 0.1073
Epoch 12/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0977
6530/6530 [==============================] - 0s 73us/step - loss: 0.6198 - val_loss: 0.1997
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1718
6530/6530 [==============================] - 0s 6us/step - loss: 0.0972 - val_loss: 0.1034
Epoch 13/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0931
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1778
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1740
6530/6530 [==============================] - 0s 6us/step - loss: 0.0932 - val_loss: 0.1006
Epoch 14/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0913
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1776
6530/6530 [==============================] - 0s 6us/step - loss: 0.0910 - val_loss: 0.0981
Epoch 15/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0887
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1553
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1773
6530/6530 [==============================] - 0s 7us/step - loss: 0.0888 - val_loss: 0.0967
Epoch 16/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0878
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1406
6530/6530 [==============================] - 0s 6us/step - loss: 0.0875 - val_loss: 0.0943
Epoch 17/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0884
6400/6530 [============================>.] - ETA: 0s - loss: 0.1773
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1283
6530/6530 [==============================] - 0s 6us/step - loss: 0.0863 - val_loss: 0.0969
Epoch 18/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0922
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1174
6530/6530 [==============================] - 2s 250us/step - loss: 0.1769 - val_loss: 0.1556

6530/6530 [==============================] - 0s 6us/step - loss: 0.0854 - val_loss: 0.0945
Epoch 19/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0900Epoch 4/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1843
6530/6530 [==============================] - 0s 45us/step - loss: 0.1137 - val_loss: 0.0714
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0697
6530/6530 [==============================] - 0s 6us/step - loss: 0.0841 - val_loss: 0.0954
Epoch 20/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0877
 208/6530 [..............................] - ETA: 1s - loss: 0.1676
6530/6530 [==============================] - 0s 7us/step - loss: 0.0834 - val_loss: 0.0930
Epoch 21/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0889
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0725
 416/6530 [>.............................] - ETA: 1s - loss: 0.1792
6530/6530 [==============================] - 0s 6us/step - loss: 0.0831 - val_loss: 0.0930
Epoch 22/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0895
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0684
 624/6530 [=>............................] - ETA: 1s - loss: 0.1817
6530/6530 [==============================] - 0s 7us/step - loss: 0.0831 - val_loss: 0.0922
Epoch 23/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0883
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0662
6530/6530 [==============================] - 0s 6us/step - loss: 0.0821 - val_loss: 0.0916
Epoch 24/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0875
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1808
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0638
6530/6530 [==============================] - 0s 6us/step - loss: 0.0813 - val_loss: 0.0907
Epoch 25/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0859
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1818
6530/6530 [==============================] - 0s 6us/step - loss: 0.0807 - val_loss: 0.0919

5920/6530 [==========================>...] - ETA: 0s - loss: 0.0613Epoch 26/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0875
1232/6530 [====>.........................] - ETA: 1s - loss: 0.1784
6530/6530 [==============================] - 0s 45us/step - loss: 0.0602 - val_loss: 0.0494

6530/6530 [==============================] - 0s 6us/step - loss: 0.0805 - val_loss: 0.0883
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0532Epoch 27/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0821
6530/6530 [==============================] - 0s 6us/step - loss: 0.0797 - val_loss: 0.0893

1440/6530 [=====>........................] - ETA: 1s - loss: 0.1768
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0518
1664/6530 [======>.......................] - ETA: 1s - loss: 0.1739
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0495
1856/6530 [=======>......................] - ETA: 1s - loss: 0.1739
3712/6530 [================>.............] - ETA: 0s - loss: 0.0486
2064/6530 [========>.....................] - ETA: 1s - loss: 0.1733
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0482
2272/6530 [=========>....................] - ETA: 1s - loss: 0.1721
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0472
6530/6530 [==============================] - 0s 43us/step - loss: 0.0469 - val_loss: 0.0428
Epoch 5/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0486
2496/6530 [==========>...................] - ETA: 1s - loss: 0.1731
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0446
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1721
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0432
2928/6530 [============>.................] - ETA: 0s - loss: 0.1717
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0429
3120/6530 [=============>................] - ETA: 0s - loss: 0.1707
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0430
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1716
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0428
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1726
6530/6530 [==============================] - 0s 45us/step - loss: 0.0425 - val_loss: 0.0411
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0481
3792/6530 [================>.............] - ETA: 0s - loss: 0.1719
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0422
4000/6530 [=================>............] - ETA: 0s - loss: 0.1715
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0411
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1712
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0410
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1713
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0414
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1708
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0414
6530/6530 [==============================] - 0s 44us/step - loss: 0.0411 - val_loss: 0.0408
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0485
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1710
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0414
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1706
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0407
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1706
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0404
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1703
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0409
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1702
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0410
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1700
6530/6530 [==============================] - 0s 43us/step - loss: 0.0407 - val_loss: 0.0407
Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0489
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1699
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0408
6320/6530 [============================>.] - ETA: 0s - loss: 0.1704
# training | RMSE: 0.1112, MAE: 0.0816
worker 1  xfile  [4, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12685547129234936}, 'layer_1_size': 43, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.24677886952239084}, 'layer_4_size': 72, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18053653660587218}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.11116343926870298, 'rmse': 0.11116343926870298, 'mae': 0.08161443715578336, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  92 | activation: relu    | extras: batchnorm 
layer 2 | size:  87 | activation: sigmoid | extras: dropout - rate: 17.0% 
layer 3 | size:  19 | activation: tanh    | extras: None 
layer 4 | size:  29 | activation: tanh    | extras: batchnorm 
layer 5 | size:  33 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041c4b9ef0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 3:29 - loss: 0.6578
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0405
3712/6530 [================>.............] - ETA: 0s - loss: 0.0402
 272/6530 [>.............................] - ETA: 13s - loss: 0.5106 
6530/6530 [==============================] - 2s 254us/step - loss: 0.1700 - val_loss: 0.1458
Epoch 5/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1991
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0407
 528/6530 [=>............................] - ETA: 7s - loss: 0.3924 
 224/6530 [>.............................] - ETA: 1s - loss: 0.1663
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0409
 752/6530 [==>...........................] - ETA: 5s - loss: 0.3428
6530/6530 [==============================] - 0s 44us/step - loss: 0.0406 - val_loss: 0.0407
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0491
 432/6530 [>.............................] - ETA: 1s - loss: 0.1650
 992/6530 [===>..........................] - ETA: 4s - loss: 0.3094
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0408
 640/6530 [=>............................] - ETA: 1s - loss: 0.1651
1248/6530 [====>.........................] - ETA: 3s - loss: 0.2830
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0401
 848/6530 [==>...........................] - ETA: 1s - loss: 0.1618
1488/6530 [=====>........................] - ETA: 2s - loss: 0.2687
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0400
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1611
1696/6530 [======>.......................] - ETA: 2s - loss: 0.2590
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0405
1264/6530 [====>.........................] - ETA: 1s - loss: 0.1601
1936/6530 [=======>......................] - ETA: 2s - loss: 0.2480
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0406
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1597
2176/6530 [========>.....................] - ETA: 1s - loss: 0.2394
6530/6530 [==============================] - 0s 46us/step - loss: 0.0404 - val_loss: 0.0406
Epoch 10/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0490
1680/6530 [======>.......................] - ETA: 1s - loss: 0.1587
2400/6530 [==========>...................] - ETA: 1s - loss: 0.2345
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0417
1888/6530 [=======>......................] - ETA: 1s - loss: 0.1590
2656/6530 [===========>..................] - ETA: 1s - loss: 0.2291
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0399
2096/6530 [========>.....................] - ETA: 1s - loss: 0.1589
2912/6530 [============>.................] - ETA: 1s - loss: 0.2229
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0396
2272/6530 [=========>....................] - ETA: 1s - loss: 0.1585
3152/6530 [=============>................] - ETA: 1s - loss: 0.2184
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0402
2480/6530 [==========>...................] - ETA: 1s - loss: 0.1599
3408/6530 [==============>...............] - ETA: 1s - loss: 0.2161
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0403
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1604
3632/6530 [===============>..............] - ETA: 1s - loss: 0.2136
6530/6530 [==============================] - 0s 47us/step - loss: 0.0403 - val_loss: 0.0405
Epoch 11/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0489
2896/6530 [============>.................] - ETA: 0s - loss: 0.1592
3856/6530 [================>.............] - ETA: 0s - loss: 0.2110
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0405
3104/6530 [=============>................] - ETA: 0s - loss: 0.1586
4096/6530 [=================>............] - ETA: 0s - loss: 0.2087
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0399
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1598
4336/6530 [==================>...........] - ETA: 0s - loss: 0.2061
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0398
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1598
4576/6530 [====================>.........] - ETA: 0s - loss: 0.2045
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0401
3712/6530 [================>.............] - ETA: 0s - loss: 0.1596
4832/6530 [=====================>........] - ETA: 0s - loss: 0.2025
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0403
3920/6530 [=================>............] - ETA: 0s - loss: 0.1599
5072/6530 [======================>.......] - ETA: 0s - loss: 0.2007
6530/6530 [==============================] - 0s 46us/step - loss: 0.0401 - val_loss: 0.0403
Epoch 12/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0487
4128/6530 [=================>............] - ETA: 0s - loss: 0.1594
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1986
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0403
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1588
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1962
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0396
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1591
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1944
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0395
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1592
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1930
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0400
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1593
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1914
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0401
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1597
6530/6530 [==============================] - 0s 46us/step - loss: 0.0400 - val_loss: 0.0401
Epoch 13/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0485
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1597
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0401
6530/6530 [==============================] - 2s 308us/step - loss: 0.1897 - val_loss: 0.2342

5616/6530 [========================>.....] - ETA: 0s - loss: 0.1590Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1214
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0394
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1586
 256/6530 [>.............................] - ETA: 1s - loss: 0.1466
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0393
 512/6530 [=>............................] - ETA: 1s - loss: 0.1492
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1585
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0399
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1500
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0400
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1584
6530/6530 [==============================] - 0s 45us/step - loss: 0.0398 - val_loss: 0.0399
Epoch 14/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0483
1008/6530 [===>..........................] - ETA: 1s - loss: 0.1495
6512/6530 [============================>.] - ETA: 0s - loss: 0.1580
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0407
6530/6530 [==============================] - 2s 256us/step - loss: 0.1580 - val_loss: 0.1336

1248/6530 [====>.........................] - ETA: 1s - loss: 0.1488Epoch 6/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1857
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0391
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1468
 224/6530 [>.............................] - ETA: 1s - loss: 0.1603
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0393
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1463
 432/6530 [>.............................] - ETA: 1s - loss: 0.1596
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0395
 640/6530 [=>............................] - ETA: 1s - loss: 0.1566
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1470
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0398
2256/6530 [=========>....................] - ETA: 0s - loss: 0.1466
 864/6530 [==>...........................] - ETA: 1s - loss: 0.1569
6530/6530 [==============================] - 0s 45us/step - loss: 0.0395 - val_loss: 0.0397
Epoch 15/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0480
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1471
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1596
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0404
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1472
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1572
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0388
2976/6530 [============>.................] - ETA: 0s - loss: 0.1461
1488/6530 [=====>........................] - ETA: 1s - loss: 0.1551
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0387
3200/6530 [=============>................] - ETA: 0s - loss: 0.1453
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1541
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0392
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1463
1904/6530 [=======>......................] - ETA: 1s - loss: 0.1525
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0394
3712/6530 [================>.............] - ETA: 0s - loss: 0.1461
2112/6530 [========>.....................] - ETA: 1s - loss: 0.1520
6530/6530 [==============================] - 0s 46us/step - loss: 0.0392 - val_loss: 0.0394
Epoch 16/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0476
3952/6530 [=================>............] - ETA: 0s - loss: 0.1463
2320/6530 [=========>....................] - ETA: 1s - loss: 0.1516
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0400
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1460
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1514
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0385
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1463
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1511
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0385
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1463
2944/6530 [============>.................] - ETA: 0s - loss: 0.1501
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0388
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1462
3152/6530 [=============>................] - ETA: 0s - loss: 0.1502
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0391
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1503
6530/6530 [==============================] - 0s 46us/step - loss: 0.0389 - val_loss: 0.0390

5200/6530 [======================>.......] - ETA: 0s - loss: 0.1458Epoch 17/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0472
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1457
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1506
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0398
3760/6530 [================>.............] - ETA: 0s - loss: 0.1500
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1449
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0383
3968/6530 [=================>............] - ETA: 0s - loss: 0.1498
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0380
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1444
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0383
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1495
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1442
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0385
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1496
6384/6530 [============================>.] - ETA: 0s - loss: 0.1439
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1502
6530/6530 [==============================] - 0s 47us/step - loss: 0.0385 - val_loss: 0.0386
Epoch 18/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0468
6530/6530 [==============================] - 1s 218us/step - loss: 0.1436 - val_loss: 0.2815
Epoch 3/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1265
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1502
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0387
 272/6530 [>.............................] - ETA: 1s - loss: 0.1270
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0376
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1497
 528/6530 [=>............................] - ETA: 1s - loss: 0.1293
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0377
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1497
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1331
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0380
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1498
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1338
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0381
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1493
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1327
6530/6530 [==============================] - 0s 47us/step - loss: 0.0380 - val_loss: 0.0381
Epoch 19/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0462
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1491
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1311
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0387
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1488
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1318
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0371
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1485
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1319
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0371
6464/6530 [============================>.] - ETA: 0s - loss: 0.1482
2256/6530 [=========>....................] - ETA: 0s - loss: 0.1324
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0374
6530/6530 [==============================] - 2s 256us/step - loss: 0.1484 - val_loss: 0.1211
Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1590
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1340
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0376
 224/6530 [>.............................] - ETA: 1s - loss: 0.1431
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1342
6530/6530 [==============================] - 0s 46us/step - loss: 0.0375 - val_loss: 0.0376
Epoch 20/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0456
 432/6530 [>.............................] - ETA: 1s - loss: 0.1443
3008/6530 [============>.................] - ETA: 0s - loss: 0.1327
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0384
 640/6530 [=>............................] - ETA: 1s - loss: 0.1419
3248/6530 [=============>................] - ETA: 0s - loss: 0.1327
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0365
 848/6530 [==>...........................] - ETA: 1s - loss: 0.1407
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1335
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0367
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1426
3760/6530 [================>.............] - ETA: 0s - loss: 0.1332
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0368
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1425
3968/6530 [=================>............] - ETA: 0s - loss: 0.1339
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0369
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1424
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1337
6530/6530 [==============================] - 0s 46us/step - loss: 0.0369 - val_loss: 0.0369
Epoch 21/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0448
1664/6530 [======>.......................] - ETA: 1s - loss: 0.1408
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1341
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0374
1872/6530 [=======>......................] - ETA: 1s - loss: 0.1413
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1343
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0358
2080/6530 [========>.....................] - ETA: 1s - loss: 0.1397
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1344
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0357
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1339
2272/6530 [=========>....................] - ETA: 1s - loss: 0.1401
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0360
2480/6530 [==========>...................] - ETA: 1s - loss: 0.1410
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1340
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0363
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1336
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1411
6530/6530 [==============================] - 0s 47us/step - loss: 0.0362 - val_loss: 0.0362
Epoch 22/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0440
2912/6530 [============>.................] - ETA: 0s - loss: 0.1409
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1337
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0357
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1335
3120/6530 [=============>................] - ETA: 0s - loss: 0.1412
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0351
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1411
6384/6530 [============================>.] - ETA: 0s - loss: 0.1331
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0352
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1414
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0355
6530/6530 [==============================] - 1s 217us/step - loss: 0.1330 - val_loss: 0.1281
Epoch 4/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1284
3744/6530 [================>.............] - ETA: 0s - loss: 0.1413
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0356
 256/6530 [>.............................] - ETA: 1s - loss: 0.1230
6530/6530 [==============================] - 0s 45us/step - loss: 0.0354 - val_loss: 0.0354
Epoch 23/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0431
3952/6530 [=================>............] - ETA: 0s - loss: 0.1414
 512/6530 [=>............................] - ETA: 1s - loss: 0.1240
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0346
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1414
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1254
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0343
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1412
 992/6530 [===>..........................] - ETA: 1s - loss: 0.1263
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0343
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1411
1232/6530 [====>.........................] - ETA: 1s - loss: 0.1262
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0345
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1411
1488/6530 [=====>........................] - ETA: 1s - loss: 0.1265
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0346
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1413
1728/6530 [======>.......................] - ETA: 1s - loss: 0.1260
6530/6530 [==============================] - 0s 46us/step - loss: 0.0346 - val_loss: 0.0345
Epoch 24/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0420
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1414
1968/6530 [========>.....................] - ETA: 0s - loss: 0.1261
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0343
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1418
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1270
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0333
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1411
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1273
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0335
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1407
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1284
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0336
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1408
2912/6530 [============>.................] - ETA: 0s - loss: 0.1270
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0337
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1410
3168/6530 [=============>................] - ETA: 0s - loss: 0.1256
6530/6530 [==============================] - 0s 48us/step - loss: 0.0336 - val_loss: 0.0336
Epoch 25/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0409
6480/6530 [============================>.] - ETA: 0s - loss: 0.1407
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1261
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0328
6530/6530 [==============================] - 2s 256us/step - loss: 0.1408 - val_loss: 0.1167
Epoch 8/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1496
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1264
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0325
 224/6530 [>.............................] - ETA: 1s - loss: 0.1322
3904/6530 [================>.............] - ETA: 0s - loss: 0.1263
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0326
 416/6530 [>.............................] - ETA: 1s - loss: 0.1375
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1265
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0327
 624/6530 [=>............................] - ETA: 1s - loss: 0.1385
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1268
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0329
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1392
6530/6530 [==============================] - 0s 46us/step - loss: 0.0327 - val_loss: 0.0326
Epoch 26/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0397
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1274
1040/6530 [===>..........................] - ETA: 1s - loss: 0.1377
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0321
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1273
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1388
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0315
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1271
1456/6530 [=====>........................] - ETA: 1s - loss: 0.1375
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0316
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1271
1680/6530 [======>.......................] - ETA: 1s - loss: 0.1356
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0317
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1269
1888/6530 [=======>......................] - ETA: 1s - loss: 0.1351
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0319
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1267
6530/6530 [==============================] - 0s 46us/step - loss: 0.0317 - val_loss: 0.0317

2112/6530 [========>.....................] - ETA: 1s - loss: 0.1348Epoch 27/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0384
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1267
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0306
2304/6530 [=========>....................] - ETA: 1s - loss: 0.1358
6400/6530 [============================>.] - ETA: 0s - loss: 0.1266
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0304
2512/6530 [==========>...................] - ETA: 1s - loss: 0.1361
6530/6530 [==============================] - 1s 217us/step - loss: 0.1267 - val_loss: 0.1168
Epoch 5/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1081
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0307
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1361
 272/6530 [>.............................] - ETA: 1s - loss: 0.1167
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0307
2944/6530 [============>.................] - ETA: 0s - loss: 0.1349
 528/6530 [=>............................] - ETA: 1s - loss: 0.1180
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0308
3152/6530 [=============>................] - ETA: 0s - loss: 0.1345
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1195
6530/6530 [==============================] - 0s 46us/step - loss: 0.0308 - val_loss: 0.0307

3360/6530 [==============>...............] - ETA: 0s - loss: 0.1349
1040/6530 [===>..........................] - ETA: 1s - loss: 0.1202
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1352
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1206
3808/6530 [================>.............] - ETA: 0s - loss: 0.1341
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1201
4032/6530 [=================>............] - ETA: 0s - loss: 0.1346
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1206
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1341
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1211
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1349
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1213
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1346
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1220
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1348
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1222
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1350
3024/6530 [============>.................] - ETA: 0s - loss: 0.1209
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1352
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1206
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1347
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1217
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1343
3760/6530 [================>.............] - ETA: 0s - loss: 0.1215
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1342
4016/6530 [=================>............] - ETA: 0s - loss: 0.1220
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1341
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1218
6336/6530 [============================>.] - ETA: 0s - loss: 0.1344
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1222
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1225
6530/6530 [==============================] - 2s 254us/step - loss: 0.1345 - val_loss: 0.1178
Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1318
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1224
 240/6530 [>.............................] - ETA: 1s - loss: 0.1284
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1221
 448/6530 [=>............................] - ETA: 1s - loss: 0.1297
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1226
 672/6530 [==>...........................] - ETA: 1s - loss: 0.1318
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1221
 896/6530 [===>..........................] - ETA: 1s - loss: 0.1317
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1220
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1333
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1222
1328/6530 [=====>........................] - ETA: 1s - loss: 0.1320
6496/6530 [============================>.] - ETA: 0s - loss: 0.1218
1552/6530 [======>.......................] - ETA: 1s - loss: 0.1307
6530/6530 [==============================] - 1s 215us/step - loss: 0.1219 - val_loss: 0.1464
Epoch 6/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0974
1760/6530 [=======>......................] - ETA: 1s - loss: 0.1303
 256/6530 [>.............................] - ETA: 1s - loss: 0.1151
1968/6530 [========>.....................] - ETA: 1s - loss: 0.1300
 512/6530 [=>............................] - ETA: 1s - loss: 0.1168
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1306
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1184
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1308
1008/6530 [===>..........................] - ETA: 1s - loss: 0.1194
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1312
1264/6530 [====>.........................] - ETA: 1s - loss: 0.1213
2864/6530 [============>.................] - ETA: 0s - loss: 0.1300
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1191
3072/6530 [=============>................] - ETA: 0s - loss: 0.1297
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1191
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1297
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1191
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1305
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1195
3712/6530 [================>.............] - ETA: 0s - loss: 0.1300
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1203
3936/6530 [=================>............] - ETA: 0s - loss: 0.1306
# training | RMSE: 0.1741, MAE: 0.1401
worker 0  xfile  [5, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 100, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3211261514163377}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.17408661222544236, 'rmse': 0.17408661222544236, 'mae': 0.14007628102151323, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  41 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  98 | activation: relu    | extras: batchnorm 
layer 3 | size:  31 | activation: tanh    | extras: batchnorm 
layer 4 | size:  74 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f04141d9668>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 2:35 - loss: 0.9256
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1200
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1306
 384/6530 [>.............................] - ETA: 13s - loss: 0.7543 
3040/6530 [============>.................] - ETA: 0s - loss: 0.1191
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1300
 736/6530 [==>...........................] - ETA: 6s - loss: 0.6465 
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1190
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1303
1088/6530 [===>..........................] - ETA: 4s - loss: 0.5677
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1194
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1304
1408/6530 [=====>........................] - ETA: 3s - loss: 0.5212
3776/6530 [================>.............] - ETA: 0s - loss: 0.1195
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1304
1760/6530 [=======>......................] - ETA: 2s - loss: 0.4834
4016/6530 [=================>............] - ETA: 0s - loss: 0.1200
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1310
2048/6530 [========>.....................] - ETA: 2s - loss: 0.4609
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1197
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1311
2400/6530 [==========>...................] - ETA: 1s - loss: 0.4338
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1205
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1312
2752/6530 [===========>..................] - ETA: 1s - loss: 0.4120
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1207
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1313
3104/6530 [=============>................] - ETA: 1s - loss: 0.3950
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1203
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1309
3456/6530 [==============>...............] - ETA: 1s - loss: 0.3786
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1201
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1309
3808/6530 [================>.............] - ETA: 0s - loss: 0.3672
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1201
6448/6530 [============================>.] - ETA: 0s - loss: 0.1306
4128/6530 [=================>............] - ETA: 0s - loss: 0.3568
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1194
6530/6530 [==============================] - 2s 250us/step - loss: 0.1306 - val_loss: 0.1059
Epoch 10/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1330
4480/6530 [===================>..........] - ETA: 0s - loss: 0.3471
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1195
 224/6530 [>.............................] - ETA: 1s - loss: 0.1284
4832/6530 [=====================>........] - ETA: 0s - loss: 0.3388
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1196
 432/6530 [>.............................] - ETA: 1s - loss: 0.1244
5184/6530 [======================>.......] - ETA: 0s - loss: 0.3308
6512/6530 [============================>.] - ETA: 0s - loss: 0.1193
 656/6530 [==>...........................] - ETA: 1s - loss: 0.1251
5536/6530 [========================>.....] - ETA: 0s - loss: 0.3232
6530/6530 [==============================] - 1s 214us/step - loss: 0.1193 - val_loss: 0.2966
Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1349
 864/6530 [==>...........................] - ETA: 1s - loss: 0.1245
5888/6530 [==========================>...] - ETA: 0s - loss: 0.3167
 272/6530 [>.............................] - ETA: 1s - loss: 0.1167
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1264
6208/6530 [===========================>..] - ETA: 0s - loss: 0.3119
 496/6530 [=>............................] - ETA: 1s - loss: 0.1154
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1262
 736/6530 [==>...........................] - ETA: 1s - loss: 0.1158
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1240
 976/6530 [===>..........................] - ETA: 1s - loss: 0.1154
6530/6530 [==============================] - 2s 284us/step - loss: 0.3071 - val_loss: 0.2020
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.2093
1744/6530 [=======>......................] - ETA: 1s - loss: 0.1247
1232/6530 [====>.........................] - ETA: 1s - loss: 0.1167
1952/6530 [=======>......................] - ETA: 1s - loss: 0.1243
 384/6530 [>.............................] - ETA: 0s - loss: 0.2075
1488/6530 [=====>........................] - ETA: 1s - loss: 0.1158
 736/6530 [==>...........................] - ETA: 0s - loss: 0.2032
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1250
1744/6530 [=======>......................] - ETA: 1s - loss: 0.1150
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1972
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1258
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1154
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1915
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1268
2256/6530 [=========>....................] - ETA: 0s - loss: 0.1153
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1901
2832/6530 [============>.................] - ETA: 0s - loss: 0.1258
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1155
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1864
3040/6530 [============>.................] - ETA: 0s - loss: 0.1254
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1160
3232/6530 [=============>................] - ETA: 0s - loss: 0.1253
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1854
2960/6530 [============>.................] - ETA: 0s - loss: 0.1154
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1257
2848/6530 [============>.................] - ETA: 0s - loss: 0.1828
3216/6530 [=============>................] - ETA: 0s - loss: 0.1149
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1254
3200/6530 [=============>................] - ETA: 0s - loss: 0.1813
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1160
3888/6530 [================>.............] - ETA: 0s - loss: 0.1253
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1810
3712/6530 [================>.............] - ETA: 0s - loss: 0.1158
4096/6530 [=================>............] - ETA: 0s - loss: 0.1253
3904/6530 [================>.............] - ETA: 0s - loss: 0.1793
3952/6530 [=================>............] - ETA: 0s - loss: 0.1163
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1250
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1783
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1162
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1256
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1772
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1164
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1258
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1760
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1168
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1254
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1749
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1165
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1258
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1732
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1165
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1262
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1731
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1169
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1264
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1725
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1165
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1264
6530/6530 [==============================] - 1s 155us/step - loss: 0.1723 - val_loss: 0.2175

5952/6530 [==========================>...] - ETA: 0s - loss: 0.1164Epoch 3/27

  32/6530 [..............................] - ETA: 1s - loss: 0.2893
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1261
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1165
 384/6530 [>.............................] - ETA: 0s - loss: 0.1722
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1260
6448/6530 [============================>.] - ETA: 0s - loss: 0.1163
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1667
6464/6530 [============================>.] - ETA: 0s - loss: 0.1253
6530/6530 [==============================] - 1s 217us/step - loss: 0.1163 - val_loss: 0.1127

1056/6530 [===>..........................] - ETA: 0s - loss: 0.1591Epoch 8/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1210
6530/6530 [==============================] - 2s 249us/step - loss: 0.1253 - val_loss: 0.1071
Epoch 11/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1198
 272/6530 [>.............................] - ETA: 1s - loss: 0.1142
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1552
 208/6530 [..............................] - ETA: 1s - loss: 0.1198
 528/6530 [=>............................] - ETA: 1s - loss: 0.1113
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1548
 416/6530 [>.............................] - ETA: 1s - loss: 0.1198
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1127
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1510
 640/6530 [=>............................] - ETA: 1s - loss: 0.1211
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1142
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1515
 848/6530 [==>...........................] - ETA: 1s - loss: 0.1203
1264/6530 [====>.........................] - ETA: 1s - loss: 0.1144
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1505
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1211
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1128
3168/6530 [=============>................] - ETA: 0s - loss: 0.1491
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1211
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1124
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1493
1488/6530 [=====>........................] - ETA: 1s - loss: 0.1199
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1118
3872/6530 [================>.............] - ETA: 0s - loss: 0.1484
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1196
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1120
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1476
1920/6530 [=======>......................] - ETA: 1s - loss: 0.1194
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1130
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1470
2128/6530 [========>.....................] - ETA: 1s - loss: 0.1186
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1128
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1461
2352/6530 [=========>....................] - ETA: 1s - loss: 0.1195
3056/6530 [=============>................] - ETA: 0s - loss: 0.1117
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1450
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1207
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1113
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1447
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1212
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1120
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1446
2960/6530 [============>.................] - ETA: 0s - loss: 0.1207
3792/6530 [================>.............] - ETA: 0s - loss: 0.1120
6336/6530 [============================>.] - ETA: 0s - loss: 0.1448
3168/6530 [=============>................] - ETA: 0s - loss: 0.1205
4048/6530 [=================>............] - ETA: 0s - loss: 0.1129
6530/6530 [==============================] - 1s 154us/step - loss: 0.1447 - val_loss: 0.1464
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1424
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1212
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1125
 384/6530 [>.............................] - ETA: 0s - loss: 0.1366
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1217
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1132
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1349
3792/6530 [================>.............] - ETA: 0s - loss: 0.1213
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1133
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1334
4016/6530 [=================>............] - ETA: 0s - loss: 0.1217
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1134
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1317
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1216
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1130
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1335
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1213
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1128
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1309
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1215
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1126
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1317
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1218
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1125
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1314
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1219
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1127
3136/6530 [=============>................] - ETA: 0s - loss: 0.1312
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1223
6528/6530 [============================>.] - ETA: 0s - loss: 0.1126
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1312
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1223
6530/6530 [==============================] - 1s 214us/step - loss: 0.1125 - val_loss: 0.1275
Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1069
3872/6530 [================>.............] - ETA: 0s - loss: 0.1313
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1224
 272/6530 [>.............................] - ETA: 1s - loss: 0.1090
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1310
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1225
 496/6530 [=>............................] - ETA: 1s - loss: 0.1109
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1310
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1226
 736/6530 [==>...........................] - ETA: 1s - loss: 0.1110
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1309
6352/6530 [============================>.] - ETA: 0s - loss: 0.1223
 976/6530 [===>..........................] - ETA: 1s - loss: 0.1096
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1302
1216/6530 [====>.........................] - ETA: 1s - loss: 0.1109
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1299
6530/6530 [==============================] - 2s 252us/step - loss: 0.1220 - val_loss: 0.1003
Epoch 12/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1560
1456/6530 [=====>........................] - ETA: 1s - loss: 0.1098
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1298
 224/6530 [>.............................] - ETA: 1s - loss: 0.1223
1712/6530 [======>.......................] - ETA: 1s - loss: 0.1108
6336/6530 [============================>.] - ETA: 0s - loss: 0.1301
 432/6530 [>.............................] - ETA: 1s - loss: 0.1273
1968/6530 [========>.....................] - ETA: 0s - loss: 0.1097
6530/6530 [==============================] - 1s 153us/step - loss: 0.1302 - val_loss: 0.1898

 640/6530 [=>............................] - ETA: 1s - loss: 0.1227Epoch 5/27

  32/6530 [..............................] - ETA: 1s - loss: 0.2136
2224/6530 [=========>....................] - ETA: 0s - loss: 0.1098
 864/6530 [==>...........................] - ETA: 1s - loss: 0.1198
 384/6530 [>.............................] - ETA: 0s - loss: 0.1330
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1103
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1232
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1303
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1110
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1237
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1249
2976/6530 [============>.................] - ETA: 0s - loss: 0.1103
1488/6530 [=====>........................] - ETA: 1s - loss: 0.1221
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1232
3232/6530 [=============>................] - ETA: 0s - loss: 0.1092
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1202
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1221
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1102
1904/6530 [=======>......................] - ETA: 1s - loss: 0.1191
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1209
3712/6530 [================>.............] - ETA: 0s - loss: 0.1100
2128/6530 [========>.....................] - ETA: 1s - loss: 0.1185
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1214
3952/6530 [=================>............] - ETA: 0s - loss: 0.1101
2336/6530 [=========>....................] - ETA: 1s - loss: 0.1194
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1207
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1099
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1206
3136/6530 [=============>................] - ETA: 0s - loss: 0.1205
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1105
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1207
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1205
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1108
2944/6530 [============>.................] - ETA: 0s - loss: 0.1203
3840/6530 [================>.............] - ETA: 0s - loss: 0.1209
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1105
3152/6530 [=============>................] - ETA: 0s - loss: 0.1203
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1205
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1105
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1207
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1202
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1108
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1212
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1200
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1104
3776/6530 [================>.............] - ETA: 0s - loss: 0.1206
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1192
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1101
3984/6530 [=================>............] - ETA: 0s - loss: 0.1206
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1186
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1102
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1204
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1184
6368/6530 [============================>.] - ETA: 0s - loss: 0.1100
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1202
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1185
6530/6530 [==============================] - 1s 218us/step - loss: 0.1099 - val_loss: 0.1777
Epoch 10/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1139
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1206
6530/6530 [==============================] - 1s 157us/step - loss: 0.1188 - val_loss: 0.1539
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1652
 256/6530 [>.............................] - ETA: 1s - loss: 0.1052
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1207
 352/6530 [>.............................] - ETA: 0s - loss: 0.1234
 496/6530 [=>............................] - ETA: 1s - loss: 0.1075
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1203
 736/6530 [==>...........................] - ETA: 1s - loss: 0.1087
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1196
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1205
 992/6530 [===>..........................] - ETA: 1s - loss: 0.1095
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1155
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1208
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1110
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1152
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1203
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1162
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1097
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1199
1744/6530 [=======>......................] - ETA: 0s - loss: 0.1095
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1137
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1200
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1088
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1138
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1198
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1094
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1133
6480/6530 [============================>.] - ETA: 0s - loss: 0.1198
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1100
3168/6530 [=============>................] - ETA: 0s - loss: 0.1125
6530/6530 [==============================] - 2s 255us/step - loss: 0.1199 - val_loss: 0.1047
Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1444
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1104
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1126
 240/6530 [>.............................] - ETA: 1s - loss: 0.1284
2992/6530 [============>.................] - ETA: 0s - loss: 0.1097
3872/6530 [================>.............] - ETA: 0s - loss: 0.1129
 464/6530 [=>............................] - ETA: 1s - loss: 0.1288
3248/6530 [=============>................] - ETA: 0s - loss: 0.1092
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1125
 672/6530 [==>...........................] - ETA: 1s - loss: 0.1226
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1101
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1122
 880/6530 [===>..........................] - ETA: 1s - loss: 0.1225
3728/6530 [================>.............] - ETA: 0s - loss: 0.1097
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1122
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1256
3984/6530 [=================>............] - ETA: 0s - loss: 0.1097
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1118
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1244
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1096
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1113
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1225
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1103
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1114
1728/6530 [======>.......................] - ETA: 1s - loss: 0.1210
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1105
6336/6530 [============================>.] - ETA: 0s - loss: 0.1115
1952/6530 [=======>......................] - ETA: 1s - loss: 0.1207
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1103
2144/6530 [========>.....................] - ETA: 1s - loss: 0.1207
6530/6530 [==============================] - 1s 156us/step - loss: 0.1115 - val_loss: 0.1275
Epoch 7/27

  32/6530 [..............................] - ETA: 1s - loss: 0.1353
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1103
2352/6530 [=========>....................] - ETA: 1s - loss: 0.1206
 384/6530 [>.............................] - ETA: 0s - loss: 0.1127
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1105
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1212
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1127
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1099
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1206
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1094
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1097
2992/6530 [============>.................] - ETA: 0s - loss: 0.1196
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1087
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1096
3200/6530 [=============>................] - ETA: 0s - loss: 0.1191
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1088
6416/6530 [============================>.] - ETA: 0s - loss: 0.1094
3408/6530 [==============>...............] - ETA: 0s - loss: 0.1193
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1076
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1195
6530/6530 [==============================] - 1s 218us/step - loss: 0.1095 - val_loss: 0.1462
Epoch 11/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1024
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1080
3824/6530 [================>.............] - ETA: 0s - loss: 0.1200
 272/6530 [>.............................] - ETA: 1s - loss: 0.1040
2848/6530 [============>.................] - ETA: 0s - loss: 0.1079
4048/6530 [=================>............] - ETA: 0s - loss: 0.1203
 512/6530 [=>............................] - ETA: 1s - loss: 0.1053
3200/6530 [=============>................] - ETA: 0s - loss: 0.1073
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1195
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1062
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1072
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1198
1008/6530 [===>..........................] - ETA: 1s - loss: 0.1069
3904/6530 [================>.............] - ETA: 0s - loss: 0.1075
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1196
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1083
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1069
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1195
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1072
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1067
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1195
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1069
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1064
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1198
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1063
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1060
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1196
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1066
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1057
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1189
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1069
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1060
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1186
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1068
6336/6530 [============================>.] - ETA: 0s - loss: 0.1063
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1183
3008/6530 [============>.................] - ETA: 0s - loss: 0.1059
6530/6530 [==============================] - 1s 155us/step - loss: 0.1065 - val_loss: 0.1405
Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1594
6352/6530 [============================>.] - ETA: 0s - loss: 0.1185
3264/6530 [=============>................] - ETA: 0s - loss: 0.1052
 352/6530 [>.............................] - ETA: 0s - loss: 0.1213
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1059
6530/6530 [==============================] - 2s 251us/step - loss: 0.1183 - val_loss: 0.0995
Epoch 14/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1431
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1156
3760/6530 [================>.............] - ETA: 0s - loss: 0.1058
 240/6530 [>.............................] - ETA: 1s - loss: 0.1127
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1098
3984/6530 [=================>............] - ETA: 0s - loss: 0.1060
 448/6530 [=>............................] - ETA: 1s - loss: 0.1149
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1082
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1061
 656/6530 [==>...........................] - ETA: 1s - loss: 0.1136
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1079
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1071
 864/6530 [==>...........................] - ETA: 1s - loss: 0.1135
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1064
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1070
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1135
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1061
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1069
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1141
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1051
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1069
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1134
3168/6530 [=============>................] - ETA: 0s - loss: 0.1044
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1071
1728/6530 [======>.......................] - ETA: 1s - loss: 0.1124
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1047
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1066
1936/6530 [=======>......................] - ETA: 1s - loss: 0.1119
3904/6530 [================>.............] - ETA: 0s - loss: 0.1048
5968/6530 [==========================>...] - ETA: 0s - loss: 0.1065
2144/6530 [========>.....................] - ETA: 1s - loss: 0.1121
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1044
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1066
2352/6530 [=========>....................] - ETA: 1s - loss: 0.1128
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1040
6448/6530 [============================>.] - ETA: 0s - loss: 0.1063
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1137
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1040
6530/6530 [==============================] - 1s 215us/step - loss: 0.1064 - val_loss: 0.1004
Epoch 12/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0806
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1134
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1033
 256/6530 [>.............................] - ETA: 1s - loss: 0.1021
2976/6530 [============>.................] - ETA: 0s - loss: 0.1137
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1029
 496/6530 [=>............................] - ETA: 1s - loss: 0.1051
3184/6530 [=============>................] - ETA: 0s - loss: 0.1136
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1031
 736/6530 [==>...........................] - ETA: 1s - loss: 0.1070
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1141
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1033
 960/6530 [===>..........................] - ETA: 1s - loss: 0.1055
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1144
1184/6530 [====>.........................] - ETA: 1s - loss: 0.1075
6530/6530 [==============================] - 1s 155us/step - loss: 0.1035 - val_loss: 0.1136
Epoch 9/27

  32/6530 [..............................] - ETA: 1s - loss: 0.1113
3808/6530 [================>.............] - ETA: 0s - loss: 0.1144
1424/6530 [=====>........................] - ETA: 1s - loss: 0.1060
 352/6530 [>.............................] - ETA: 0s - loss: 0.1088
4032/6530 [=================>............] - ETA: 0s - loss: 0.1149
1680/6530 [======>.......................] - ETA: 1s - loss: 0.1059
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1056
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1145
1936/6530 [=======>......................] - ETA: 0s - loss: 0.1053
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1024
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1149
2160/6530 [========>.....................] - ETA: 0s - loss: 0.1057
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1022
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1151
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1059
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1022
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1151
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1062
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1006
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1154
2912/6530 [============>.................] - ETA: 0s - loss: 0.1050
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1009
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1156
3168/6530 [=============>................] - ETA: 0s - loss: 0.1043
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1004
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1153
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1050
3168/6530 [=============>................] - ETA: 0s - loss: 0.1001
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1151
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1052
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1001
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1154
3904/6530 [================>.............] - ETA: 0s - loss: 0.1053
3872/6530 [================>.............] - ETA: 0s - loss: 0.1001
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1151
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1054
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0996
6368/6530 [============================>.] - ETA: 0s - loss: 0.1150
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1053
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0994
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1061
6530/6530 [==============================] - 2s 253us/step - loss: 0.1152 - val_loss: 0.0902

4928/6530 [=====================>........] - ETA: 0s - loss: 0.0997Epoch 15/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1354
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1058
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0992
 224/6530 [>.............................] - ETA: 1s - loss: 0.1092
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1061
 432/6530 [>.............................] - ETA: 1s - loss: 0.1094
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0988
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1062
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0992
 640/6530 [=>............................] - ETA: 1s - loss: 0.1110
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1059
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0992
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1097
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1058
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1108
6530/6530 [==============================] - 1s 155us/step - loss: 0.0996 - val_loss: 0.1024
Epoch 10/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0978
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1055
1264/6530 [====>.........................] - ETA: 1s - loss: 0.1123
 416/6530 [>.............................] - ETA: 0s - loss: 0.1107
6384/6530 [============================>.] - ETA: 0s - loss: 0.1057
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1114
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1032
1680/6530 [======>.......................] - ETA: 1s - loss: 0.1107
6530/6530 [==============================] - 1s 218us/step - loss: 0.1057 - val_loss: 0.0953
Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1064
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1017
1888/6530 [=======>......................] - ETA: 1s - loss: 0.1106
 240/6530 [>.............................] - ETA: 1s - loss: 0.0971
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1008
2096/6530 [========>.....................] - ETA: 1s - loss: 0.1097
 480/6530 [=>............................] - ETA: 1s - loss: 0.1027
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1004
2288/6530 [=========>....................] - ETA: 1s - loss: 0.1108
 720/6530 [==>...........................] - ETA: 1s - loss: 0.1060
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1000
2480/6530 [==========>...................] - ETA: 1s - loss: 0.1114
 960/6530 [===>..........................] - ETA: 1s - loss: 0.1046
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0996
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1120
1216/6530 [====>.........................] - ETA: 1s - loss: 0.1066
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0987
2880/6530 [============>.................] - ETA: 0s - loss: 0.1113
1456/6530 [=====>........................] - ETA: 1s - loss: 0.1051
3104/6530 [=============>................] - ETA: 0s - loss: 0.0983
3088/6530 [=============>................] - ETA: 0s - loss: 0.1115
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1048
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0990
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1120
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1039
3808/6530 [================>.............] - ETA: 0s - loss: 0.0987
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1124
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1042
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0986
3728/6530 [================>.............] - ETA: 0s - loss: 0.1127
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1040
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0982
3952/6530 [=================>............] - ETA: 0s - loss: 0.1129
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1040
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0984
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1132
2960/6530 [============>.................] - ETA: 0s - loss: 0.1035
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0979
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1126
3216/6530 [=============>................] - ETA: 0s - loss: 0.1030
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0972
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1128
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1037
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0976
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1133
3728/6530 [================>.............] - ETA: 0s - loss: 0.1031
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0977
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1131
3984/6530 [=================>............] - ETA: 0s - loss: 0.1032
6530/6530 [==============================] - 1s 155us/step - loss: 0.0980 - val_loss: 0.1352
Epoch 11/27

  32/6530 [..............................] - ETA: 1s - loss: 0.1145
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1134
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1032
 352/6530 [>.............................] - ETA: 1s - loss: 0.1098
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1136
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1037
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1028
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1134
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1039
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1136
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0987
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1042
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0982
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1135
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1043
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0982
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1133
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1045
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0975
6496/6530 [============================>.] - ETA: 0s - loss: 0.1132
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1041
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0974
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1039
6530/6530 [==============================] - 2s 256us/step - loss: 0.1133 - val_loss: 0.1006
Epoch 16/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1440
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0964
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1039
 224/6530 [>.............................] - ETA: 1s - loss: 0.1141
3136/6530 [=============>................] - ETA: 0s - loss: 0.0961
6320/6530 [============================>.] - ETA: 0s - loss: 0.1040
 432/6530 [>.............................] - ETA: 1s - loss: 0.1156
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0963
 640/6530 [=>............................] - ETA: 1s - loss: 0.1126
6530/6530 [==============================] - 1s 219us/step - loss: 0.1037 - val_loss: 0.1149
Epoch 14/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1077
3840/6530 [================>.............] - ETA: 0s - loss: 0.0958
 864/6530 [==>...........................] - ETA: 1s - loss: 0.1118
 256/6530 [>.............................] - ETA: 1s - loss: 0.1008
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0957
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1132
 496/6530 [=>............................] - ETA: 1s - loss: 0.1016
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0954
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1138
 720/6530 [==>...........................] - ETA: 1s - loss: 0.1042
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0954
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1130
 976/6530 [===>..........................] - ETA: 1s - loss: 0.1019
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0949
1712/6530 [======>.......................] - ETA: 1s - loss: 0.1124
1200/6530 [====>.........................] - ETA: 1s - loss: 0.1036
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0941
1904/6530 [=======>......................] - ETA: 1s - loss: 0.1119
1424/6530 [=====>........................] - ETA: 1s - loss: 0.1031
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0946
2112/6530 [========>.....................] - ETA: 1s - loss: 0.1117
1664/6530 [======>.......................] - ETA: 1s - loss: 0.1022
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0950
2304/6530 [=========>....................] - ETA: 1s - loss: 0.1118
1904/6530 [=======>......................] - ETA: 1s - loss: 0.1013
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1128
6530/6530 [==============================] - 1s 156us/step - loss: 0.0953 - val_loss: 0.1312
Epoch 12/27

  32/6530 [..............................] - ETA: 1s - loss: 0.1389
2160/6530 [========>.....................] - ETA: 0s - loss: 0.1013
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1125
 384/6530 [>.............................] - ETA: 0s - loss: 0.1081
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1013
2944/6530 [============>.................] - ETA: 0s - loss: 0.1118
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1029
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1020
3152/6530 [=============>................] - ETA: 0s - loss: 0.1116
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0979
2896/6530 [============>.................] - ETA: 0s - loss: 0.1011
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1119
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0966
3136/6530 [=============>................] - ETA: 0s - loss: 0.1003
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1123
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0960
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1005
3776/6530 [================>.............] - ETA: 0s - loss: 0.1122
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0948
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1013
4000/6530 [=================>............] - ETA: 0s - loss: 0.1124
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0949
3872/6530 [================>.............] - ETA: 0s - loss: 0.1012
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1124
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0937
4112/6530 [=================>............] - ETA: 0s - loss: 0.1015
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1122
3136/6530 [=============>................] - ETA: 0s - loss: 0.0934
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1014
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1125
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0942
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1024
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1125
3840/6530 [================>.............] - ETA: 0s - loss: 0.0938
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1023
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1126
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0935
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1026
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1129
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0932
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1028
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1129
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0933
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1023
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1130
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0930
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1021
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1128
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0924
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1019
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1127
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0926
6368/6530 [============================>.] - ETA: 0s - loss: 0.1019
6336/6530 [============================>.] - ETA: 0s - loss: 0.1128
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0927
6530/6530 [==============================] - 1s 219us/step - loss: 0.1017 - val_loss: 0.0871
Epoch 15/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0717
6530/6530 [==============================] - 2s 252us/step - loss: 0.1127 - val_loss: 0.0966

6530/6530 [==============================] - 1s 155us/step - loss: 0.0932 - val_loss: 0.1058
Epoch 17/27

  16/6530 [..............................] - ETA: 2s - loss: 0.1190Epoch 13/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0934
 176/6530 [..............................] - ETA: 1s - loss: 0.0984
 224/6530 [>.............................] - ETA: 1s - loss: 0.1077
 384/6530 [>.............................] - ETA: 0s - loss: 0.0994
 432/6530 [>.............................] - ETA: 1s - loss: 0.0964
 432/6530 [>.............................] - ETA: 1s - loss: 0.1080
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0961
 688/6530 [==>...........................] - ETA: 1s - loss: 0.1010
 640/6530 [=>............................] - ETA: 1s - loss: 0.1061
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0930
 928/6530 [===>..........................] - ETA: 1s - loss: 0.1035
 848/6530 [==>...........................] - ETA: 1s - loss: 0.1077
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0934
1184/6530 [====>.........................] - ETA: 1s - loss: 0.1044
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1091
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0933
1440/6530 [=====>........................] - ETA: 1s - loss: 0.1031
1264/6530 [====>.........................] - ETA: 1s - loss: 0.1111
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0921
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1030
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1088
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0928
1936/6530 [=======>......................] - ETA: 0s - loss: 0.1017
1680/6530 [======>.......................] - ETA: 1s - loss: 0.1079
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0923
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1015
1872/6530 [=======>......................] - ETA: 1s - loss: 0.1086
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1015
3136/6530 [=============>................] - ETA: 0s - loss: 0.0919
2064/6530 [========>.....................] - ETA: 1s - loss: 0.1080
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1023
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0921
2272/6530 [=========>....................] - ETA: 1s - loss: 0.1088
2928/6530 [============>.................] - ETA: 0s - loss: 0.1011
3808/6530 [================>.............] - ETA: 0s - loss: 0.0920
2480/6530 [==========>...................] - ETA: 1s - loss: 0.1101
3184/6530 [=============>................] - ETA: 0s - loss: 0.1004
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0919
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1099
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1011
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0915
2880/6530 [============>.................] - ETA: 0s - loss: 0.1089
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1013
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0915
3088/6530 [=============>................] - ETA: 0s - loss: 0.1084
3920/6530 [=================>............] - ETA: 0s - loss: 0.1013
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0911
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1093
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1017
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0907
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1095
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1019
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0911
3696/6530 [===============>..............] - ETA: 0s - loss: 0.1092
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1027
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0913
3904/6530 [================>.............] - ETA: 0s - loss: 0.1093
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1025
6530/6530 [==============================] - 1s 158us/step - loss: 0.0916 - val_loss: 0.1081
Epoch 14/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1067
4112/6530 [=================>............] - ETA: 0s - loss: 0.1097
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1024
 384/6530 [>.............................] - ETA: 0s - loss: 0.1003
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1099
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1027
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0997
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1100
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1022
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0939
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1100
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1020
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0927
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1099
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1019
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0925
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1101
6400/6530 [============================>.] - ETA: 0s - loss: 0.1020
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0922
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1103
6530/6530 [==============================] - 1s 219us/step - loss: 0.1020 - val_loss: 0.1040
Epoch 16/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0972
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0926
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1099
 256/6530 [>.............................] - ETA: 1s - loss: 0.0966
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0917
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1097
 512/6530 [=>............................] - ETA: 1s - loss: 0.0973
3104/6530 [=============>................] - ETA: 0s - loss: 0.0910
5968/6530 [==========================>...] - ETA: 0s - loss: 0.1099
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0992
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0915
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1096
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1002
3808/6530 [================>.............] - ETA: 0s - loss: 0.0910
6400/6530 [============================>.] - ETA: 0s - loss: 0.1096
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1010
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0909
6530/6530 [==============================] - 2s 259us/step - loss: 0.1096 - val_loss: 0.0915
Epoch 18/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1356
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0996
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0907
 224/6530 [>.............................] - ETA: 1s - loss: 0.1013
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0992
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0907
 432/6530 [>.............................] - ETA: 1s - loss: 0.1042
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0985
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0903
 640/6530 [=>............................] - ETA: 1s - loss: 0.1071
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0989
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0899
 848/6530 [==>...........................] - ETA: 1s - loss: 0.1065
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0991
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0900
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1075
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0990
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0902
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1102
2976/6530 [============>.................] - ETA: 0s - loss: 0.0980
6528/6530 [============================>.] - ETA: 0s - loss: 0.0908
1456/6530 [=====>........................] - ETA: 1s - loss: 0.1086
6530/6530 [==============================] - 1s 158us/step - loss: 0.0908 - val_loss: 0.1298

3232/6530 [=============>................] - ETA: 0s - loss: 0.0974
1648/6530 [======>.......................] - ETA: 1s - loss: 0.1091
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0985
1856/6530 [=======>......................] - ETA: 1s - loss: 0.1088
3744/6530 [================>.............] - ETA: 0s - loss: 0.0983
2064/6530 [========>.....................] - ETA: 1s - loss: 0.1088
3984/6530 [=================>............] - ETA: 0s - loss: 0.0986
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0987
2288/6530 [=========>....................] - ETA: 1s - loss: 0.1095
# training | RMSE: 0.1510, MAE: 0.1210
worker 0  xfile  [7, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 31, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 74, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.15095705232728132, 'rmse': 0.15095705232728132, 'mae': 0.12095576194895331, 'early_stop': True}
vggnet done  0

4480/6530 [===================>..........] - ETA: 0s - loss: 0.0999
2496/6530 [==========>...................] - ETA: 1s - loss: 0.1098
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1000
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1099
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1000
2928/6530 [============>.................] - ETA: 0s - loss: 0.1091
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1001
3152/6530 [=============>................] - ETA: 0s - loss: 0.1092
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1003
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1095
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0999
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1099
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0997
3824/6530 [================>.............] - ETA: 0s - loss: 0.1098
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1000
4048/6530 [=================>............] - ETA: 0s - loss: 0.1096
6528/6530 [============================>.] - ETA: 0s - loss: 0.0998
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1092
6530/6530 [==============================] - 1s 212us/step - loss: 0.0998 - val_loss: 0.1239
Epoch 17/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0982
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1095
 288/6530 [>.............................] - ETA: 1s - loss: 0.0955
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1094
 560/6530 [=>............................] - ETA: 1s - loss: 0.0974
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1091
 816/6530 [==>...........................] - ETA: 1s - loss: 0.1017
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1097
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1037
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1099
1344/6530 [=====>........................] - ETA: 1s - loss: 0.1015
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1096
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1010
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1097
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0996
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1095
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0988
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1095
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0989
6512/6530 [============================>.] - ETA: 0s - loss: 0.1095
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0997
6530/6530 [==============================] - 2s 248us/step - loss: 0.1096 - val_loss: 0.0922
Epoch 19/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1126
2896/6530 [============>.................] - ETA: 0s - loss: 0.0983
 240/6530 [>.............................] - ETA: 1s - loss: 0.1040
3152/6530 [=============>................] - ETA: 0s - loss: 0.0980
 448/6530 [=>............................] - ETA: 1s - loss: 0.1081
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0983
 656/6530 [==>...........................] - ETA: 1s - loss: 0.1063
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0991
 880/6530 [===>..........................] - ETA: 1s - loss: 0.1054
3936/6530 [=================>............] - ETA: 0s - loss: 0.0989
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1071
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0992
1328/6530 [=====>........................] - ETA: 1s - loss: 0.1073
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0996
1568/6530 [======>.......................] - ETA: 1s - loss: 0.1064
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0996
1792/6530 [=======>......................] - ETA: 1s - loss: 0.1068
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0997
2016/6530 [========>.....................] - ETA: 1s - loss: 0.1057
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0997
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1064
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0999
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1072
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0995
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1070
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0994
2912/6530 [============>.................] - ETA: 0s - loss: 0.1062
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0997
3120/6530 [=============>................] - ETA: 0s - loss: 0.1057
6464/6530 [============================>.] - ETA: 0s - loss: 0.0994
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1058
6530/6530 [==============================] - 1s 207us/step - loss: 0.0995 - val_loss: 0.0906
Epoch 18/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0797
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1064
 256/6530 [>.............................] - ETA: 1s - loss: 0.0974
3744/6530 [================>.............] - ETA: 0s - loss: 0.1069
 512/6530 [=>............................] - ETA: 1s - loss: 0.0963
3952/6530 [=================>............] - ETA: 0s - loss: 0.1072
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0992
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1073
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1022
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1068
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1030
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1075
1552/6530 [======>.......................] - ETA: 1s - loss: 0.1014
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1073
1808/6530 [=======>......................] - ETA: 0s - loss: 0.1008
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1073
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0995
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1076
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0993
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1079
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1001
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1076
2864/6530 [============>.................] - ETA: 0s - loss: 0.0990
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1075
3120/6530 [=============>................] - ETA: 0s - loss: 0.0982
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1073
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0984
6368/6530 [============================>.] - ETA: 0s - loss: 0.1074
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0990
6530/6530 [==============================] - 2s 242us/step - loss: 0.1073 - val_loss: 0.0901
Epoch 20/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1412
3936/6530 [=================>............] - ETA: 0s - loss: 0.0986
 240/6530 [>.............................] - ETA: 1s - loss: 0.1054
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0989
 464/6530 [=>............................] - ETA: 1s - loss: 0.1076
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0994
 688/6530 [==>...........................] - ETA: 1s - loss: 0.1082
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0995
 880/6530 [===>..........................] - ETA: 1s - loss: 0.1078
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0993
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1099
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0993
1328/6530 [=====>........................] - ETA: 1s - loss: 0.1104
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0994
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1090
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0991
1744/6530 [=======>......................] - ETA: 1s - loss: 0.1087
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0988
1968/6530 [========>.....................] - ETA: 1s - loss: 0.1080
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0990
2192/6530 [=========>....................] - ETA: 1s - loss: 0.1081
6528/6530 [============================>.] - ETA: 0s - loss: 0.0986
6530/6530 [==============================] - 1s 203us/step - loss: 0.0986 - val_loss: 0.0833
Epoch 19/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0858
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1085
 288/6530 [>.............................] - ETA: 1s - loss: 0.0936
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1089
 544/6530 [=>............................] - ETA: 1s - loss: 0.0955
2864/6530 [============>.................] - ETA: 0s - loss: 0.1078
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0990
3056/6530 [=============>................] - ETA: 0s - loss: 0.1075
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0992
3264/6530 [=============>................] - ETA: 0s - loss: 0.1076
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0997
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1080
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0984
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1079
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0975
3872/6530 [================>.............] - ETA: 0s - loss: 0.1075
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0968
4096/6530 [=================>............] - ETA: 0s - loss: 0.1076
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0974
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1076
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0978
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1078
2832/6530 [============>.................] - ETA: 0s - loss: 0.0974
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1080
3088/6530 [=============>................] - ETA: 0s - loss: 0.0966
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1077
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0961
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1078
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0972
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1081
3856/6530 [================>.............] - ETA: 0s - loss: 0.0970
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1079
4128/6530 [=================>............] - ETA: 0s - loss: 0.0974
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1079
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0974
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1077
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0976
6336/6530 [============================>.] - ETA: 0s - loss: 0.1081
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0976
6530/6530 [==============================] - 2s 245us/step - loss: 0.1080 - val_loss: 0.0870

5200/6530 [======================>.......] - ETA: 0s - loss: 0.0980Epoch 21/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1283
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0984
 240/6530 [>.............................] - ETA: 1s - loss: 0.1027
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0979
 464/6530 [=>............................] - ETA: 1s - loss: 0.1039
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0979
 688/6530 [==>...........................] - ETA: 1s - loss: 0.1036
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0977
 912/6530 [===>..........................] - ETA: 1s - loss: 0.1023
6480/6530 [============================>.] - ETA: 0s - loss: 0.0972
1136/6530 [====>.........................] - ETA: 1s - loss: 0.1037
6530/6530 [==============================] - 1s 205us/step - loss: 0.0974 - val_loss: 0.1124
Epoch 20/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0974
1344/6530 [=====>........................] - ETA: 1s - loss: 0.1053
 272/6530 [>.............................] - ETA: 1s - loss: 0.0925
1568/6530 [======>.......................] - ETA: 1s - loss: 0.1047
 528/6530 [=>............................] - ETA: 1s - loss: 0.0939
1792/6530 [=======>......................] - ETA: 1s - loss: 0.1046
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0994
2016/6530 [========>.....................] - ETA: 1s - loss: 0.1048
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1008
2240/6530 [=========>....................] - ETA: 1s - loss: 0.1058
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1014
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1072
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0996
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1074
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0991
2912/6530 [============>.................] - ETA: 0s - loss: 0.1071
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0975
3152/6530 [=============>................] - ETA: 0s - loss: 0.1063
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0972
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1070
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0977
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1076
2880/6530 [============>.................] - ETA: 0s - loss: 0.0964
3792/6530 [================>.............] - ETA: 0s - loss: 0.1071
3120/6530 [=============>................] - ETA: 0s - loss: 0.0959
4016/6530 [=================>............] - ETA: 0s - loss: 0.1071
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0958
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1071
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0965
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1070
3888/6530 [================>.............] - ETA: 0s - loss: 0.0964
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1070
4112/6530 [=================>............] - ETA: 0s - loss: 0.0967
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1066
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0964
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1069
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0970
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1071
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0967
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1070
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0971
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1066
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0974
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1067
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0970
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1066
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0967
6384/6530 [============================>.] - ETA: 0s - loss: 0.1067
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0965
6530/6530 [==============================] - 2s 244us/step - loss: 0.1068 - val_loss: 0.0914
Epoch 22/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1402
6432/6530 [============================>.] - ETA: 0s - loss: 0.0965
 240/6530 [>.............................] - ETA: 1s - loss: 0.1014
6530/6530 [==============================] - 1s 207us/step - loss: 0.0965 - val_loss: 0.1124
Epoch 21/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0771
 464/6530 [=>............................] - ETA: 1s - loss: 0.1013
 288/6530 [>.............................] - ETA: 1s - loss: 0.0927
 704/6530 [==>...........................] - ETA: 1s - loss: 0.1025
 560/6530 [=>............................] - ETA: 1s - loss: 0.0941
 928/6530 [===>..........................] - ETA: 1s - loss: 0.1010
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0993
1136/6530 [====>.........................] - ETA: 1s - loss: 0.1019
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1009
1344/6530 [=====>........................] - ETA: 1s - loss: 0.1023
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0989
1552/6530 [======>.......................] - ETA: 1s - loss: 0.1017
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0980
1760/6530 [=======>......................] - ETA: 1s - loss: 0.1016
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0974
1968/6530 [========>.....................] - ETA: 1s - loss: 0.1018
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0962
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1026
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0960
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1031
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0961
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1043
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0955
2848/6530 [============>.................] - ETA: 0s - loss: 0.1035
3072/6530 [=============>................] - ETA: 0s - loss: 0.0947
3072/6530 [=============>................] - ETA: 0s - loss: 0.1038
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0946
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1037
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0956
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1045
3856/6530 [================>.............] - ETA: 0s - loss: 0.0954
3744/6530 [================>.............] - ETA: 0s - loss: 0.1044
4128/6530 [=================>............] - ETA: 0s - loss: 0.0957
3952/6530 [=================>............] - ETA: 0s - loss: 0.1047
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0957
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1049
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0966
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1046
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0964
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1049
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0968
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1045
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0971
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1041
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0966
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1041
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0963
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1046
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0962
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1043
6448/6530 [============================>.] - ETA: 0s - loss: 0.0962
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1040
6530/6530 [==============================] - 1s 207us/step - loss: 0.0963 - val_loss: 0.1089
Epoch 22/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1088
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1039
 288/6530 [>.............................] - ETA: 1s - loss: 0.0954
6320/6530 [============================>.] - ETA: 0s - loss: 0.1041
 544/6530 [=>............................] - ETA: 1s - loss: 0.0948
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0992
6530/6530 [==============================] - 2s 244us/step - loss: 0.1040 - val_loss: 0.0844
Epoch 23/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1311
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0986
 224/6530 [>.............................] - ETA: 1s - loss: 0.1074
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0979
 448/6530 [=>............................] - ETA: 1s - loss: 0.1054
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0974
 672/6530 [==>...........................] - ETA: 1s - loss: 0.1033
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0962
 896/6530 [===>..........................] - ETA: 1s - loss: 0.1043
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0950
1120/6530 [====>.........................] - ETA: 1s - loss: 0.1055
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0950
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1059
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0954
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1043
2896/6530 [============>.................] - ETA: 0s - loss: 0.0947
1728/6530 [======>.......................] - ETA: 1s - loss: 0.1041
3152/6530 [=============>................] - ETA: 0s - loss: 0.0942
1968/6530 [========>.....................] - ETA: 1s - loss: 0.1036
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0947
2192/6530 [=========>....................] - ETA: 1s - loss: 0.1038
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0949
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1036
3968/6530 [=================>............] - ETA: 0s - loss: 0.0948
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1042
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0951
2864/6530 [============>.................] - ETA: 0s - loss: 0.1035
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0958
3072/6530 [=============>................] - ETA: 0s - loss: 0.1034
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0959
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1029
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0959
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1033
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0958
3712/6530 [================>.............] - ETA: 0s - loss: 0.1034
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0961
3936/6530 [=================>............] - ETA: 0s - loss: 0.1036
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0957
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1036
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0956
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1032
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0956
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1038
6448/6530 [============================>.] - ETA: 0s - loss: 0.0954
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1037
6530/6530 [==============================] - 1s 207us/step - loss: 0.0955 - val_loss: 0.0833
Epoch 23/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0746
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1038
 272/6530 [>.............................] - ETA: 1s - loss: 0.0953
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1039
 560/6530 [=>............................] - ETA: 1s - loss: 0.0950
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1040
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0989
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1040
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1002
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1043
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0976
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1040
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0967
6400/6530 [============================>.] - ETA: 0s - loss: 0.1039
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0953
6530/6530 [==============================] - 2s 243us/step - loss: 0.1041 - val_loss: 0.0874
Epoch 24/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1234
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0939
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0942
 240/6530 [>.............................] - ETA: 1s - loss: 0.1007
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0947
 448/6530 [=>............................] - ETA: 1s - loss: 0.1028
2864/6530 [============>.................] - ETA: 0s - loss: 0.0941
 672/6530 [==>...........................] - ETA: 1s - loss: 0.1024
3120/6530 [=============>................] - ETA: 0s - loss: 0.0931
 896/6530 [===>..........................] - ETA: 1s - loss: 0.1028
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0927
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1042
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0936
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1045
3856/6530 [================>.............] - ETA: 0s - loss: 0.0935
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1041
4096/6530 [=================>............] - ETA: 0s - loss: 0.0938
1712/6530 [======>.......................] - ETA: 1s - loss: 0.1035
1920/6530 [=======>......................] - ETA: 1s - loss: 0.1030
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0935
2128/6530 [========>.....................] - ETA: 1s - loss: 0.1029
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0944
2336/6530 [=========>....................] - ETA: 1s - loss: 0.1031
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0942
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1044
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0947
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1040
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0949
2944/6530 [============>.................] - ETA: 0s - loss: 0.1036
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0946
3136/6530 [=============>................] - ETA: 0s - loss: 0.1036
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0943
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1045
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0942
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1046
6352/6530 [============================>.] - ETA: 0s - loss: 0.0944
3712/6530 [================>.............] - ETA: 0s - loss: 0.1044
6530/6530 [==============================] - 1s 210us/step - loss: 0.0943 - val_loss: 0.0959

3904/6530 [================>.............] - ETA: 0s - loss: 0.1042
4112/6530 [=================>............] - ETA: 0s - loss: 0.1042
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1041
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1044
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1040
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1037
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1043
# training | RMSE: 0.1226, MAE: 0.0931
worker 1  xfile  [6, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 92, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16969009309750988}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 29, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.12259614093138961, 'rmse': 0.12259614093138961, 'mae': 0.09312988199929306, 'early_stop': True}
vggnet done  1

5392/6530 [=======================>......] - ETA: 0s - loss: 0.1046
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1045
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1042
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1040
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1039
6528/6530 [============================>.] - ETA: 0s - loss: 0.1041
6530/6530 [==============================] - 2s 251us/step - loss: 0.1041 - val_loss: 0.0934
Epoch 25/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1067
 224/6530 [>.............................] - ETA: 1s - loss: 0.1086
 432/6530 [>.............................] - ETA: 1s - loss: 0.1073
 576/6530 [=>............................] - ETA: 1s - loss: 0.1052
 736/6530 [==>...........................] - ETA: 1s - loss: 0.1057
 848/6530 [==>...........................] - ETA: 1s - loss: 0.1049
 976/6530 [===>..........................] - ETA: 1s - loss: 0.1046
1120/6530 [====>.........................] - ETA: 1s - loss: 0.1066
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1070
1424/6530 [=====>........................] - ETA: 1s - loss: 0.1051
1584/6530 [======>.......................] - ETA: 1s - loss: 0.1043
1760/6530 [=======>......................] - ETA: 1s - loss: 0.1040
1952/6530 [=======>......................] - ETA: 1s - loss: 0.1038
2144/6530 [========>.....................] - ETA: 1s - loss: 0.1032
2320/6530 [=========>....................] - ETA: 1s - loss: 0.1030
2480/6530 [==========>...................] - ETA: 1s - loss: 0.1033
2608/6530 [==========>...................] - ETA: 1s - loss: 0.1034
2736/6530 [===========>..................] - ETA: 1s - loss: 0.1031
2880/6530 [============>.................] - ETA: 1s - loss: 0.1023
3024/6530 [============>.................] - ETA: 1s - loss: 0.1020
3184/6530 [=============>................] - ETA: 1s - loss: 0.1018
3360/6530 [==============>...............] - ETA: 1s - loss: 0.1023
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1025
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1023
3776/6530 [================>.............] - ETA: 0s - loss: 0.1024
3888/6530 [================>.............] - ETA: 0s - loss: 0.1023
4016/6530 [=================>............] - ETA: 0s - loss: 0.1020
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1019
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1018
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1014
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1019
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1018
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1018
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1018
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1017
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1017
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1019
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1022
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1020
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1018
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1016
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1017
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1013
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1011
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1011
6448/6530 [============================>.] - ETA: 0s - loss: 0.1011
6530/6530 [==============================] - 2s 380us/step - loss: 0.1012 - val_loss: 0.0864
Epoch 26/27

  16/6530 [..............................] - ETA: 2s - loss: 0.1137
 176/6530 [..............................] - ETA: 2s - loss: 0.0916
 336/6530 [>.............................] - ETA: 2s - loss: 0.0950
 464/6530 [=>............................] - ETA: 2s - loss: 0.0973
 592/6530 [=>............................] - ETA: 2s - loss: 0.0953
 720/6530 [==>...........................] - ETA: 2s - loss: 0.0976
 832/6530 [==>...........................] - ETA: 2s - loss: 0.0973
 944/6530 [===>..........................] - ETA: 2s - loss: 0.0973
1056/6530 [===>..........................] - ETA: 2s - loss: 0.0989
1168/6530 [====>.........................] - ETA: 2s - loss: 0.0992
1280/6530 [====>.........................] - ETA: 2s - loss: 0.1000
1408/6530 [=====>........................] - ETA: 2s - loss: 0.0999
1536/6530 [======>.......................] - ETA: 2s - loss: 0.0999
1648/6530 [======>.......................] - ETA: 2s - loss: 0.0997
1776/6530 [=======>......................] - ETA: 2s - loss: 0.0996
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0993
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0994
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0995
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0993
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0995
2544/6530 [==========>...................] - ETA: 1s - loss: 0.1002
2656/6530 [===========>..................] - ETA: 1s - loss: 0.1002
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0999
2880/6530 [============>.................] - ETA: 1s - loss: 0.0996
2992/6530 [============>.................] - ETA: 1s - loss: 0.1000
3120/6530 [=============>................] - ETA: 1s - loss: 0.0999
3280/6530 [==============>...............] - ETA: 1s - loss: 0.1002
3440/6530 [==============>...............] - ETA: 1s - loss: 0.1011
3584/6530 [===============>..............] - ETA: 1s - loss: 0.1009
3728/6530 [================>.............] - ETA: 1s - loss: 0.1007
3840/6530 [================>.............] - ETA: 1s - loss: 0.1009
3984/6530 [=================>............] - ETA: 1s - loss: 0.1009
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1011
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1010
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1013
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1011
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1012
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1010
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1012
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1014
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1013
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1011
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1010
5968/6530 [==========================>...] - ETA: 0s - loss: 0.1010
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1009
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1010
6416/6530 [============================>.] - ETA: 0s - loss: 0.1010
6530/6530 [==============================] - 3s 391us/step - loss: 0.1010 - val_loss: 0.0855
Epoch 27/27

  16/6530 [..............................] - ETA: 2s - loss: 0.1314
 144/6530 [..............................] - ETA: 2s - loss: 0.1017
 240/6530 [>.............................] - ETA: 2s - loss: 0.1010
 368/6530 [>.............................] - ETA: 2s - loss: 0.1001
 496/6530 [=>............................] - ETA: 2s - loss: 0.1002
 640/6530 [=>............................] - ETA: 2s - loss: 0.0991
 768/6530 [==>...........................] - ETA: 2s - loss: 0.0994
 880/6530 [===>..........................] - ETA: 2s - loss: 0.0991
1024/6530 [===>..........................] - ETA: 2s - loss: 0.0998
1152/6530 [====>.........................] - ETA: 2s - loss: 0.1003
1280/6530 [====>.........................] - ETA: 2s - loss: 0.1013
1408/6530 [=====>........................] - ETA: 2s - loss: 0.1005
1536/6530 [======>.......................] - ETA: 2s - loss: 0.1003
1664/6530 [======>.......................] - ETA: 2s - loss: 0.1002
1760/6530 [=======>......................] - ETA: 2s - loss: 0.1005
1856/6530 [=======>......................] - ETA: 2s - loss: 0.1006
1984/6530 [========>.....................] - ETA: 1s - loss: 0.1000
2144/6530 [========>.....................] - ETA: 1s - loss: 0.1003
2304/6530 [=========>....................] - ETA: 1s - loss: 0.1007
2464/6530 [==========>...................] - ETA: 1s - loss: 0.1012
2624/6530 [===========>..................] - ETA: 1s - loss: 0.1014
2784/6530 [===========>..................] - ETA: 1s - loss: 0.1008
2944/6530 [============>.................] - ETA: 1s - loss: 0.1006
3088/6530 [=============>................] - ETA: 1s - loss: 0.1002
3232/6530 [=============>................] - ETA: 1s - loss: 0.0999
3360/6530 [==============>...............] - ETA: 1s - loss: 0.0998
3504/6530 [===============>..............] - ETA: 1s - loss: 0.0999
3664/6530 [===============>..............] - ETA: 1s - loss: 0.0999
3792/6530 [================>.............] - ETA: 1s - loss: 0.0999
3952/6530 [=================>............] - ETA: 1s - loss: 0.0999
4080/6530 [=================>............] - ETA: 0s - loss: 0.1001
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0998
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1002
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1003
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1003
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1002
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0999
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0999
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1002
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1005
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1006
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1007
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1006
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1005
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1005
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1003
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1005
6352/6530 [============================>.] - ETA: 0s - loss: 0.1004
6480/6530 [============================>.] - ETA: 0s - loss: 0.1002
6530/6530 [==============================] - 3s 407us/step - loss: 0.1003 - val_loss: 0.0854

# training | RMSE: 0.1033, MAE: 0.0799
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3801219225105922}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.1032992999968466, 'rmse': 0.1032992999968466, 'mae': 0.07986127306205022, 'early_stop': True}
vggnet done  2
all of workers have been done
calculation finished
#0 epoch=27.0 loss={'loss': 0.12132806163199679, 'rmse': 0.12132806163199679, 'mae': 0.09301086690344386, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 10, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14759338769004177}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#1 epoch=27.0 loss={'loss': 0.0911742643278679, 'rmse': 0.0911742643278679, 'mae': 0.07099675097236031, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#3 epoch=27.0 loss={'loss': 0.13382600607499123, 'rmse': 0.13382600607499123, 'mae': 0.10287794569994442, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2007328291273842}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.14033998426882086}, 'layer_4_size': 15, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#4 epoch=27.0 loss={'loss': 0.11116343926870298, 'rmse': 0.11116343926870298, 'mae': 0.08161443715578336, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12685547129234936}, 'layer_1_size': 43, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.24677886952239084}, 'layer_4_size': 72, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18053653660587218}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#5 epoch=27.0 loss={'loss': 0.17408661222544236, 'rmse': 0.17408661222544236, 'mae': 0.14007628102151323, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 100, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3211261514163377}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#7 epoch=27.0 loss={'loss': 0.15095705232728132, 'rmse': 0.15095705232728132, 'mae': 0.12095576194895331, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 31, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 74, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#6 epoch=27.0 loss={'loss': 0.12259614093138961, 'rmse': 0.12259614093138961, 'mae': 0.09312988199929306, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 92, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16969009309750988}, 'layer_2_size': 87, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 29, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}
#2 epoch=27.0 loss={'loss': 0.1032992999968466, 'rmse': 0.1032992999968466, 'mae': 0.07986127306205022, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3801219225105922}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
get a list [results] of length 199
get a list [loss] of length 8
get a list [val_loss] of length 8
length of indices is (1, 2, 4, 0, 6, 3, 7, 5)
length of indices is 8
length of T is 8
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [1, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3801219225105922}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]] 

*** 2.6666666666666665 configurations x 81.0 iterations each

3 | Fri Sep 28 01:24:40 2018 | lowest loss so far: 0.0705 (run 0)

vggnet done  2
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  42 | activation: relu    | extras: None 
layer 2 | size:  35 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  64/6530 [..............................] - ETA: 1:06 - loss: 0.5039
1344/6530 [=====>........................] - ETA: 2s - loss: 0.4190  
2048/6530 [========>.....................] - ETA: 1s - loss: 0.2916
3264/6530 [=============>................] - ETA: 0s - loss: 0.1995
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1453
6530/6530 [==============================] - 1s 142us/step - loss: 0.1200 - val_loss: 0.0443
Epoch 2/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0397
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0383
3904/6530 [================>.............] - ETA: 0s - loss: 0.0381
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0387
6530/6530 [==============================] - 0s 30us/step - loss: 0.0384 - val_loss: 0.0384
Epoch 3/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0393
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0369
3904/6530 [================>.............] - ETA: 0s - loss: 0.0345
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0330
6530/6530 [==============================] - 0s 27us/step - loss: 0.0329 - val_loss: 0.0418
Epoch 4/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0336
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0290
3904/6530 [================>.............] - ETA: 0s - loss: 0.0288
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0288
6530/6530 [==============================] - 0s 27us/step - loss: 0.0285 - val_loss: 0.0393
Epoch 5/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0507
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0274
3904/6530 [================>.............] - ETA: 0s - loss: 0.0260
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0255
6530/6530 [==============================] - 0s 29us/step - loss: 0.0253 - val_loss: 0.0263
Epoch 6/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0293
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0240{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  98 | activation: tanh    | extras: dropout - rate: 38.0% 
layer 2 | size:  14 | activation: tanh    | extras: None 
layer 3 | size:  25 | activation: tanh    | extras: batchnorm 
layer 4 | size:  50 | activation: tanh    | extras: batchnorm 
layer 5 | size:  35 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 7:12 - loss: 0.2862
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0233
 176/6530 [..............................] - ETA: 40s - loss: 0.2977 
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0228
 336/6530 [>.............................] - ETA: 21s - loss: 0.2685
6530/6530 [==============================] - 0s 32us/step - loss: 0.0224 - val_loss: 0.0236
Epoch 7/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0239
 544/6530 [=>............................] - ETA: 13s - loss: 0.2737
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0205
 736/6530 [==>...........................] - ETA: 10s - loss: 0.2693
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0202
 944/6530 [===>..........................] - ETA: 7s - loss: 0.2610 
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0200
6530/6530 [==============================] - 0s 28us/step - loss: 0.0197 - val_loss: 0.0284
Epoch 8/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0261
1152/6530 [====>.........................] - ETA: 6s - loss: 0.2586
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0187
1360/6530 [=====>........................] - ETA: 5s - loss: 0.2521
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0177
1584/6530 [======>.......................] - ETA: 4s - loss: 0.2469
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0175
6530/6530 [==============================] - 0s 26us/step - loss: 0.0174 - val_loss: 0.0366

1808/6530 [=======>......................] - ETA: 3s - loss: 0.2430Epoch 9/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0409
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0176
2032/6530 [========>.....................] - ETA: 3s - loss: 0.2404
4032/6530 [=================>............] - ETA: 0s - loss: 0.0166
2256/6530 [=========>....................] - ETA: 3s - loss: 0.2376
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0163
2480/6530 [==========>...................] - ETA: 2s - loss: 0.2348
6530/6530 [==============================] - 0s 26us/step - loss: 0.0162 - val_loss: 0.0234
Epoch 10/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0148
2672/6530 [===========>..................] - ETA: 2s - loss: 0.2340
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0165
2880/6530 [============>.................] - ETA: 2s - loss: 0.2311
3968/6530 [=================>............] - ETA: 0s - loss: 0.0155
3104/6530 [=============>................] - ETA: 2s - loss: 0.2293
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0153
6530/6530 [==============================] - 0s 27us/step - loss: 0.0151 - val_loss: 0.0279
Epoch 11/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0194
3312/6530 [==============>...............] - ETA: 1s - loss: 0.2289
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0144
3520/6530 [===============>..............] - ETA: 1s - loss: 0.2280
3968/6530 [=================>............] - ETA: 0s - loss: 0.0149
3728/6530 [================>.............] - ETA: 1s - loss: 0.2271
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0146
6530/6530 [==============================] - 0s 27us/step - loss: 0.0143 - val_loss: 0.0205
Epoch 12/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0169
3952/6530 [=================>............] - ETA: 1s - loss: 0.2257
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0141
4144/6530 [==================>...........] - ETA: 1s - loss: 0.2239
3776/6530 [================>.............] - ETA: 0s - loss: 0.0135
4336/6530 [==================>...........] - ETA: 1s - loss: 0.2225
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0136
4544/6530 [===================>..........] - ETA: 0s - loss: 0.2215
6530/6530 [==============================] - 0s 28us/step - loss: 0.0135 - val_loss: 0.0201
Epoch 13/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0179
4752/6530 [====================>.........] - ETA: 0s - loss: 0.2203
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0134
4960/6530 [=====================>........] - ETA: 0s - loss: 0.2195
4032/6530 [=================>............] - ETA: 0s - loss: 0.0136
5184/6530 [======================>.......] - ETA: 0s - loss: 0.2188
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0132
6530/6530 [==============================] - 0s 26us/step - loss: 0.0131 - val_loss: 0.0190
Epoch 14/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0192
5392/6530 [=======================>......] - ETA: 0s - loss: 0.2183
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0124
5600/6530 [========================>.....] - ETA: 0s - loss: 0.2172
4096/6530 [=================>............] - ETA: 0s - loss: 0.0122
5824/6530 [=========================>....] - ETA: 0s - loss: 0.2167
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0124
6016/6530 [==========================>...] - ETA: 0s - loss: 0.2156
6530/6530 [==============================] - 0s 27us/step - loss: 0.0124 - val_loss: 0.0168
Epoch 15/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0124
6240/6530 [===========================>..] - ETA: 0s - loss: 0.2150
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0115
6448/6530 [============================>.] - ETA: 0s - loss: 0.2142
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0119
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0120
6530/6530 [==============================] - 0s 26us/step - loss: 0.0120 - val_loss: 0.0135
Epoch 16/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0086
6530/6530 [==============================] - 3s 427us/step - loss: 0.2136 - val_loss: 0.1673
Epoch 2/81

  16/6530 [..............................] - ETA: 1s - loss: 0.2137
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0116
 240/6530 [>.............................] - ETA: 1s - loss: 0.1945
4096/6530 [=================>............] - ETA: 0s - loss: 0.0116
 464/6530 [=>............................] - ETA: 1s - loss: 0.2017
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0116
6530/6530 [==============================] - 0s 26us/step - loss: 0.0116 - val_loss: 0.0206
Epoch 17/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0219
 656/6530 [==>...........................] - ETA: 1s - loss: 0.2043
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0112
 880/6530 [===>..........................] - ETA: 1s - loss: 0.1996
3968/6530 [=================>............] - ETA: 0s - loss: 0.0116
1088/6530 [===>..........................] - ETA: 1s - loss: 0.2024
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0114
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1994
6530/6530 [==============================] - 0s 28us/step - loss: 0.0114 - val_loss: 0.0137
Epoch 18/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0109
1488/6530 [=====>........................] - ETA: 1s - loss: 0.1995
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0107
1712/6530 [======>.......................] - ETA: 1s - loss: 0.1984
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0108
1936/6530 [=======>......................] - ETA: 1s - loss: 0.1980
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0107
6530/6530 [==============================] - 0s 26us/step - loss: 0.0108 - val_loss: 0.0134
Epoch 19/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0106
2160/6530 [========>.....................] - ETA: 1s - loss: 0.1967
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0102
2384/6530 [=========>....................] - ETA: 1s - loss: 0.1963
4032/6530 [=================>............] - ETA: 0s - loss: 0.0107
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1967
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0105
6530/6530 [==============================] - 0s 26us/step - loss: 0.0105 - val_loss: 0.0122
Epoch 20/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0077
2832/6530 [============>.................] - ETA: 0s - loss: 0.1956
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0106
3040/6530 [============>.................] - ETA: 0s - loss: 0.1939
4096/6530 [=================>............] - ETA: 0s - loss: 0.0105
3248/6530 [=============>................] - ETA: 0s - loss: 0.1939
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0106
6530/6530 [==============================] - 0s 26us/step - loss: 0.0107 - val_loss: 0.0154
Epoch 21/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0115
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1944
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0109
3696/6530 [===============>..............] - ETA: 0s - loss: 0.1939
3968/6530 [=================>............] - ETA: 0s - loss: 0.0103
3904/6530 [================>.............] - ETA: 0s - loss: 0.1937
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0100
4128/6530 [=================>............] - ETA: 0s - loss: 0.1935
6530/6530 [==============================] - 0s 27us/step - loss: 0.0099 - val_loss: 0.0157
Epoch 22/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0159
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1928
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0101
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1921
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0099
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1916
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0097
6530/6530 [==============================] - 0s 26us/step - loss: 0.0097 - val_loss: 0.0341
Epoch 23/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0403
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1917
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0098
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1917
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0097
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1914
6336/6530 [============================>.] - ETA: 0s - loss: 0.0096
6530/6530 [==============================] - 0s 26us/step - loss: 0.0096 - val_loss: 0.0117
Epoch 24/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0141
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1912
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0098
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1910
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0095
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1907
6400/6530 [============================>.] - ETA: 0s - loss: 0.0094
6368/6530 [============================>.] - ETA: 0s - loss: 0.1900
6530/6530 [==============================] - 0s 26us/step - loss: 0.0094 - val_loss: 0.0173
Epoch 25/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0161
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0092
6530/6530 [==============================] - 2s 244us/step - loss: 0.1898 - val_loss: 0.1630
Epoch 3/81

  16/6530 [..............................] - ETA: 1s - loss: 0.2288
3840/6530 [================>.............] - ETA: 0s - loss: 0.0089
 224/6530 [>.............................] - ETA: 1s - loss: 0.1876
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0090
6530/6530 [==============================] - 0s 28us/step - loss: 0.0090 - val_loss: 0.0328
Epoch 26/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0292
 432/6530 [>.............................] - ETA: 1s - loss: 0.1889
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0095
 640/6530 [=>............................] - ETA: 1s - loss: 0.1901
4096/6530 [=================>............] - ETA: 0s - loss: 0.0092
 880/6530 [===>..........................] - ETA: 1s - loss: 0.1878
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0089
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1894
6530/6530 [==============================] - 0s 26us/step - loss: 0.0088 - val_loss: 0.0142
Epoch 27/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0106
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1850
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0090
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1827
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0089
1760/6530 [=======>......................] - ETA: 1s - loss: 0.1821
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0088
6530/6530 [==============================] - 0s 27us/step - loss: 0.0087 - val_loss: 0.0100
Epoch 28/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0100
1968/6530 [========>.....................] - ETA: 1s - loss: 0.1830
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0087
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1807
4096/6530 [=================>............] - ETA: 0s - loss: 0.0085
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1809
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0085
6530/6530 [==============================] - 0s 26us/step - loss: 0.0087 - val_loss: 0.0158
Epoch 29/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0117
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1806
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0084
2848/6530 [============>.................] - ETA: 0s - loss: 0.1793
3840/6530 [================>.............] - ETA: 0s - loss: 0.0082
3056/6530 [=============>................] - ETA: 0s - loss: 0.1769
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0085
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1783
6530/6530 [==============================] - 0s 27us/step - loss: 0.0085 - val_loss: 0.0095
Epoch 30/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0071
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1789
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0075
3696/6530 [===============>..............] - ETA: 0s - loss: 0.1784
3840/6530 [================>.............] - ETA: 0s - loss: 0.0086
3904/6530 [================>.............] - ETA: 0s - loss: 0.1787
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0084
6530/6530 [==============================] - 0s 27us/step - loss: 0.0083 - val_loss: 0.0186
Epoch 31/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0147
4112/6530 [=================>............] - ETA: 0s - loss: 0.1785
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0083
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1782
3904/6530 [================>.............] - ETA: 0s - loss: 0.0080
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1782
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0081
6530/6530 [==============================] - 0s 28us/step - loss: 0.0081 - val_loss: 0.0116

4752/6530 [====================>.........] - ETA: 0s - loss: 0.1781Epoch 32/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0108
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1779
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0077
3968/6530 [=================>............] - ETA: 0s - loss: 0.0077
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1786
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0079
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1786
6530/6530 [==============================] - 0s 26us/step - loss: 0.0080 - val_loss: 0.0111
Epoch 33/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0108
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1779
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0073
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1779
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0078
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1775
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0079
6530/6530 [==============================] - 0s 26us/step - loss: 0.0079 - val_loss: 0.0280
Epoch 34/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0365
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1772
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0081
6512/6530 [============================>.] - ETA: 0s - loss: 0.1770
4096/6530 [=================>............] - ETA: 0s - loss: 0.0076
6530/6530 [==============================] - 2s 249us/step - loss: 0.1769 - val_loss: 0.1560
Epoch 4/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1831
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0076
6530/6530 [==============================] - 0s 26us/step - loss: 0.0077 - val_loss: 0.0096

 224/6530 [>.............................] - ETA: 1s - loss: 0.1675
# training | RMSE: 0.0893, MAE: 0.0696
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.08927234942504737, 'rmse': 0.08927234942504737, 'mae': 0.0695971704523707, 'early_stop': True}
vggnet done  0

 432/6530 [>.............................] - ETA: 1s - loss: 0.1778
 672/6530 [==>...........................] - ETA: 1s - loss: 0.1787
 880/6530 [===>..........................] - ETA: 1s - loss: 0.1791
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1800
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1779
1392/6530 [=====>........................] - ETA: 1s - loss: 0.1756
1552/6530 [======>.......................] - ETA: 1s - loss: 0.1733
1728/6530 [======>.......................] - ETA: 1s - loss: 0.1731
1872/6530 [=======>......................] - ETA: 1s - loss: 0.1734
2032/6530 [========>.....................] - ETA: 1s - loss: 0.1732
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1721
2336/6530 [=========>....................] - ETA: 1s - loss: 0.1717
2528/6530 [==========>...................] - ETA: 1s - loss: 0.1725
2688/6530 [===========>..................] - ETA: 1s - loss: 0.1716
2832/6530 [============>.................] - ETA: 1s - loss: 0.1715
3008/6530 [============>.................] - ETA: 1s - loss: 0.1707
3200/6530 [=============>................] - ETA: 0s - loss: 0.1704
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1708
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1715
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1714
3824/6530 [================>.............] - ETA: 0s - loss: 0.1710
3936/6530 [=================>............] - ETA: 0s - loss: 0.1713
4032/6530 [=================>............] - ETA: 0s - loss: 0.1709
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1708
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1703
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1707
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1706
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1703
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1707
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1704
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1704
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1703
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1704
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1705
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1699
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1699
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1694
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1696
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1693
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1692
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1695
6416/6530 [============================>.] - ETA: 0s - loss: 0.1696
6530/6530 [==============================] - 2s 366us/step - loss: 0.1693 - val_loss: 0.1476
Epoch 5/81

  16/6530 [..............................] - ETA: 2s - loss: 0.2069
 176/6530 [..............................] - ETA: 2s - loss: 0.1591
 352/6530 [>.............................] - ETA: 1s - loss: 0.1574
 512/6530 [=>............................] - ETA: 1s - loss: 0.1646
 704/6530 [==>...........................] - ETA: 1s - loss: 0.1624
 880/6530 [===>..........................] - ETA: 1s - loss: 0.1592
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1605
1232/6530 [====>.........................] - ETA: 1s - loss: 0.1593
1408/6530 [=====>........................] - ETA: 1s - loss: 0.1594
1584/6530 [======>.......................] - ETA: 1s - loss: 0.1570
1744/6530 [=======>......................] - ETA: 1s - loss: 0.1582
1888/6530 [=======>......................] - ETA: 1s - loss: 0.1586
2048/6530 [========>.....................] - ETA: 1s - loss: 0.1589
2192/6530 [=========>....................] - ETA: 1s - loss: 0.1590
2336/6530 [=========>....................] - ETA: 1s - loss: 0.1589
2480/6530 [==========>...................] - ETA: 1s - loss: 0.1594
2624/6530 [===========>..................] - ETA: 1s - loss: 0.1603
2752/6530 [===========>..................] - ETA: 1s - loss: 0.1597
2896/6530 [============>.................] - ETA: 1s - loss: 0.1592
3024/6530 [============>.................] - ETA: 1s - loss: 0.1590
3168/6530 [=============>................] - ETA: 1s - loss: 0.1589
3280/6530 [==============>...............] - ETA: 1s - loss: 0.1594
3392/6530 [==============>...............] - ETA: 1s - loss: 0.1594
3520/6530 [===============>..............] - ETA: 1s - loss: 0.1592
3648/6530 [===============>..............] - ETA: 1s - loss: 0.1595
3776/6530 [================>.............] - ETA: 0s - loss: 0.1595
3904/6530 [================>.............] - ETA: 0s - loss: 0.1598
4016/6530 [=================>............] - ETA: 0s - loss: 0.1593
4128/6530 [=================>............] - ETA: 0s - loss: 0.1596
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1591
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1591
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1597
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1595
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1594
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1596
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1597
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1598
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1598
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1602
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1595
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1591
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1589
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1587
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1585
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1587
6384/6530 [============================>.] - ETA: 0s - loss: 0.1582
6530/6530 [==============================] - 2s 381us/step - loss: 0.1580 - val_loss: 0.1264
Epoch 6/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1587
 144/6530 [..............................] - ETA: 2s - loss: 0.1582
 288/6530 [>.............................] - ETA: 2s - loss: 0.1532
 432/6530 [>.............................] - ETA: 2s - loss: 0.1558
 560/6530 [=>............................] - ETA: 2s - loss: 0.1528
 688/6530 [==>...........................] - ETA: 2s - loss: 0.1515
 816/6530 [==>...........................] - ETA: 2s - loss: 0.1513
 960/6530 [===>..........................] - ETA: 2s - loss: 0.1520
1104/6530 [====>.........................] - ETA: 2s - loss: 0.1550
1216/6530 [====>.........................] - ETA: 2s - loss: 0.1531
1344/6530 [=====>........................] - ETA: 2s - loss: 0.1529
1488/6530 [=====>........................] - ETA: 2s - loss: 0.1517
1616/6530 [======>.......................] - ETA: 1s - loss: 0.1507
1776/6530 [=======>......................] - ETA: 1s - loss: 0.1503
1952/6530 [=======>......................] - ETA: 1s - loss: 0.1507
2144/6530 [========>.....................] - ETA: 1s - loss: 0.1500
2320/6530 [=========>....................] - ETA: 1s - loss: 0.1494
2496/6530 [==========>...................] - ETA: 1s - loss: 0.1494
2656/6530 [===========>..................] - ETA: 1s - loss: 0.1494
2832/6530 [============>.................] - ETA: 1s - loss: 0.1483
3008/6530 [============>.................] - ETA: 1s - loss: 0.1478
3168/6530 [=============>................] - ETA: 1s - loss: 0.1481
3312/6530 [==============>...............] - ETA: 1s - loss: 0.1484
3456/6530 [==============>...............] - ETA: 1s - loss: 0.1489
3584/6530 [===============>..............] - ETA: 1s - loss: 0.1490
3728/6530 [================>.............] - ETA: 0s - loss: 0.1484
3856/6530 [================>.............] - ETA: 0s - loss: 0.1484
3984/6530 [=================>............] - ETA: 0s - loss: 0.1483
4112/6530 [=================>............] - ETA: 0s - loss: 0.1482
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1477
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1478
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1487
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1485
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1488
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1487
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1487
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1488
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1484
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1480
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1480
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1477
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1476
6448/6530 [============================>.] - ETA: 0s - loss: 0.1473
6530/6530 [==============================] - 2s 354us/step - loss: 0.1474 - val_loss: 0.1269
Epoch 7/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1749
 144/6530 [..............................] - ETA: 2s - loss: 0.1379
 256/6530 [>.............................] - ETA: 2s - loss: 0.1421
 368/6530 [>.............................] - ETA: 2s - loss: 0.1422
 496/6530 [=>............................] - ETA: 2s - loss: 0.1436
 624/6530 [=>............................] - ETA: 2s - loss: 0.1407
 752/6530 [==>...........................] - ETA: 2s - loss: 0.1396
 896/6530 [===>..........................] - ETA: 2s - loss: 0.1389
1040/6530 [===>..........................] - ETA: 2s - loss: 0.1413
1152/6530 [====>.........................] - ETA: 2s - loss: 0.1409
1296/6530 [====>.........................] - ETA: 2s - loss: 0.1418
1440/6530 [=====>........................] - ETA: 2s - loss: 0.1415
1568/6530 [======>.......................] - ETA: 2s - loss: 0.1400
1712/6530 [======>.......................] - ETA: 1s - loss: 0.1410
1856/6530 [=======>......................] - ETA: 1s - loss: 0.1409
2016/6530 [========>.....................] - ETA: 1s - loss: 0.1396
2144/6530 [========>.....................] - ETA: 1s - loss: 0.1395
2288/6530 [=========>....................] - ETA: 1s - loss: 0.1401
2416/6530 [==========>...................] - ETA: 1s - loss: 0.1409
2560/6530 [==========>...................] - ETA: 1s - loss: 0.1408
2736/6530 [===========>..................] - ETA: 1s - loss: 0.1405
2896/6530 [============>.................] - ETA: 1s - loss: 0.1400
3056/6530 [=============>................] - ETA: 1s - loss: 0.1399
3184/6530 [=============>................] - ETA: 1s - loss: 0.1396
3296/6530 [==============>...............] - ETA: 1s - loss: 0.1396
3408/6530 [==============>...............] - ETA: 1s - loss: 0.1400
3552/6530 [===============>..............] - ETA: 1s - loss: 0.1399
3696/6530 [===============>..............] - ETA: 1s - loss: 0.1401
3856/6530 [================>.............] - ETA: 1s - loss: 0.1400
4032/6530 [=================>............] - ETA: 0s - loss: 0.1401
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1400
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1400
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1404
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1402
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1400
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1401
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1403
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1404
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1408
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1409
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1402
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1398
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1397
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1396
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1398
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1400
6400/6530 [============================>.] - ETA: 0s - loss: 0.1397
6530/6530 [==============================] - 3s 393us/step - loss: 0.1398 - val_loss: 0.1186
Epoch 8/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1521
 144/6530 [..............................] - ETA: 2s - loss: 0.1351
 256/6530 [>.............................] - ETA: 2s - loss: 0.1416
 400/6530 [>.............................] - ETA: 2s - loss: 0.1405
 544/6530 [=>............................] - ETA: 2s - loss: 0.1385
 672/6530 [==>...........................] - ETA: 2s - loss: 0.1392
 800/6530 [==>...........................] - ETA: 2s - loss: 0.1386
 912/6530 [===>..........................] - ETA: 2s - loss: 0.1371
1024/6530 [===>..........................] - ETA: 2s - loss: 0.1371
1136/6530 [====>.........................] - ETA: 2s - loss: 0.1373
1264/6530 [====>.........................] - ETA: 2s - loss: 0.1384
1376/6530 [=====>........................] - ETA: 2s - loss: 0.1363
1520/6530 [=====>........................] - ETA: 2s - loss: 0.1351
1680/6530 [======>.......................] - ETA: 2s - loss: 0.1351
1792/6530 [=======>......................] - ETA: 1s - loss: 0.1349
1920/6530 [=======>......................] - ETA: 1s - loss: 0.1345
2080/6530 [========>.....................] - ETA: 1s - loss: 0.1340
2272/6530 [=========>....................] - ETA: 1s - loss: 0.1347
2464/6530 [==========>...................] - ETA: 1s - loss: 0.1349
2656/6530 [===========>..................] - ETA: 1s - loss: 0.1356
2848/6530 [============>.................] - ETA: 1s - loss: 0.1345
3008/6530 [============>.................] - ETA: 1s - loss: 0.1339
3168/6530 [=============>................] - ETA: 1s - loss: 0.1334
3312/6530 [==============>...............] - ETA: 1s - loss: 0.1337
3456/6530 [==============>...............] - ETA: 1s - loss: 0.1344
3632/6530 [===============>..............] - ETA: 1s - loss: 0.1341
3824/6530 [================>.............] - ETA: 0s - loss: 0.1335
4032/6530 [=================>............] - ETA: 0s - loss: 0.1337
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1330
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1332
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1337
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1337
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1338
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1336
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1337
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1339
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1340
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1336
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1332
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1333
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1329
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1332
6400/6530 [============================>.] - ETA: 0s - loss: 0.1330
6530/6530 [==============================] - 2s 353us/step - loss: 0.1330 - val_loss: 0.1124
Epoch 9/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1357
 160/6530 [..............................] - ETA: 2s - loss: 0.1214
 320/6530 [>.............................] - ETA: 2s - loss: 0.1257
 496/6530 [=>............................] - ETA: 1s - loss: 0.1237
 672/6530 [==>...........................] - ETA: 1s - loss: 0.1264
 864/6530 [==>...........................] - ETA: 1s - loss: 0.1286
1040/6530 [===>..........................] - ETA: 1s - loss: 0.1313
1200/6530 [====>.........................] - ETA: 1s - loss: 0.1313
1376/6530 [=====>........................] - ETA: 1s - loss: 0.1295
1552/6530 [======>.......................] - ETA: 1s - loss: 0.1293
1760/6530 [=======>......................] - ETA: 1s - loss: 0.1292
1968/6530 [========>.....................] - ETA: 1s - loss: 0.1294
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1300
2368/6530 [=========>....................] - ETA: 1s - loss: 0.1300
2528/6530 [==========>...................] - ETA: 1s - loss: 0.1307
2656/6530 [===========>..................] - ETA: 1s - loss: 0.1305
2784/6530 [===========>..................] - ETA: 1s - loss: 0.1298
2896/6530 [============>.................] - ETA: 1s - loss: 0.1292
3008/6530 [============>.................] - ETA: 1s - loss: 0.1289
3136/6530 [=============>................] - ETA: 1s - loss: 0.1288
3264/6530 [=============>................] - ETA: 1s - loss: 0.1291
3376/6530 [==============>...............] - ETA: 1s - loss: 0.1289
3504/6530 [===============>..............] - ETA: 1s - loss: 0.1298
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1297
3728/6530 [================>.............] - ETA: 0s - loss: 0.1291
3840/6530 [================>.............] - ETA: 0s - loss: 0.1297
3952/6530 [=================>............] - ETA: 0s - loss: 0.1297
4096/6530 [=================>............] - ETA: 0s - loss: 0.1296
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1294
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1294
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1293
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1293
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1295
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1294
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1294
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1298
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1300
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1300
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1299
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1298
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1296
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1295
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1293
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1293
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1292
6336/6530 [============================>.] - ETA: 0s - loss: 0.1290
6480/6530 [============================>.] - ETA: 0s - loss: 0.1291
6530/6530 [==============================] - 3s 386us/step - loss: 0.1290 - val_loss: 0.1036
Epoch 10/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1270
 176/6530 [..............................] - ETA: 2s - loss: 0.1261
 384/6530 [>.............................] - ETA: 1s - loss: 0.1278
 576/6530 [=>............................] - ETA: 1s - loss: 0.1263
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1273
 928/6530 [===>..........................] - ETA: 1s - loss: 0.1253
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1273
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1269
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1264
1632/6530 [======>.......................] - ETA: 1s - loss: 0.1260
1776/6530 [=======>......................] - ETA: 1s - loss: 0.1255
1936/6530 [=======>......................] - ETA: 1s - loss: 0.1248
2064/6530 [========>.....................] - ETA: 1s - loss: 0.1248
2208/6530 [=========>....................] - ETA: 1s - loss: 0.1253
2320/6530 [=========>....................] - ETA: 1s - loss: 0.1254
2432/6530 [==========>...................] - ETA: 1s - loss: 0.1256
2544/6530 [==========>...................] - ETA: 1s - loss: 0.1262
2656/6530 [===========>..................] - ETA: 1s - loss: 0.1265
2768/6530 [===========>..................] - ETA: 1s - loss: 0.1263
2880/6530 [============>.................] - ETA: 1s - loss: 0.1254
2992/6530 [============>.................] - ETA: 1s - loss: 0.1253
3136/6530 [=============>................] - ETA: 1s - loss: 0.1248
3296/6530 [==============>...............] - ETA: 1s - loss: 0.1251
3456/6530 [==============>...............] - ETA: 1s - loss: 0.1255
3584/6530 [===============>..............] - ETA: 1s - loss: 0.1256
3712/6530 [================>.............] - ETA: 0s - loss: 0.1251
3840/6530 [================>.............] - ETA: 0s - loss: 0.1254
3968/6530 [=================>............] - ETA: 0s - loss: 0.1251
4096/6530 [=================>............] - ETA: 0s - loss: 0.1253
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1249
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1247
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1250
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1251
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1253
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1252
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1250
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1253
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1256
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1257
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1258
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1258
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1256
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1259
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1258
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1256
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1254
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1255
6352/6530 [============================>.] - ETA: 0s - loss: 0.1250
6464/6530 [============================>.] - ETA: 0s - loss: 0.1248
6530/6530 [==============================] - 3s 409us/step - loss: 0.1248 - val_loss: 0.1068
Epoch 11/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1190
 144/6530 [..............................] - ETA: 2s - loss: 0.1210
 288/6530 [>.............................] - ETA: 2s - loss: 0.1196
 416/6530 [>.............................] - ETA: 2s - loss: 0.1207
 560/6530 [=>............................] - ETA: 2s - loss: 0.1212
 688/6530 [==>...........................] - ETA: 2s - loss: 0.1223
 816/6530 [==>...........................] - ETA: 2s - loss: 0.1224
 944/6530 [===>..........................] - ETA: 2s - loss: 0.1215
1072/6530 [===>..........................] - ETA: 2s - loss: 0.1233
1184/6530 [====>.........................] - ETA: 2s - loss: 0.1232
1296/6530 [====>.........................] - ETA: 2s - loss: 0.1245
1408/6530 [=====>........................] - ETA: 2s - loss: 0.1226
1520/6530 [=====>........................] - ETA: 2s - loss: 0.1218
1648/6530 [======>.......................] - ETA: 2s - loss: 0.1220
1776/6530 [=======>......................] - ETA: 2s - loss: 0.1216
1904/6530 [=======>......................] - ETA: 1s - loss: 0.1218
2016/6530 [========>.....................] - ETA: 1s - loss: 0.1212
2128/6530 [========>.....................] - ETA: 1s - loss: 0.1214
2256/6530 [=========>....................] - ETA: 1s - loss: 0.1212
2384/6530 [=========>....................] - ETA: 1s - loss: 0.1224
2512/6530 [==========>...................] - ETA: 1s - loss: 0.1234
2624/6530 [===========>..................] - ETA: 1s - loss: 0.1242
2736/6530 [===========>..................] - ETA: 1s - loss: 0.1240
2848/6530 [============>.................] - ETA: 1s - loss: 0.1232
2960/6530 [============>.................] - ETA: 1s - loss: 0.1232
3088/6530 [=============>................] - ETA: 1s - loss: 0.1229
3216/6530 [=============>................] - ETA: 1s - loss: 0.1230
3360/6530 [==============>...............] - ETA: 1s - loss: 0.1233
3488/6530 [===============>..............] - ETA: 1s - loss: 0.1240
3632/6530 [===============>..............] - ETA: 1s - loss: 0.1234
3792/6530 [================>.............] - ETA: 1s - loss: 0.1232
3936/6530 [=================>............] - ETA: 1s - loss: 0.1237
4064/6530 [=================>............] - ETA: 1s - loss: 0.1237
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1233
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1235
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1232
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1236
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1234
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1233
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1234
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1234
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1235
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1238
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1241
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1241
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1241
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1240
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1242
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1240
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1239
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1239
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1240
6432/6530 [============================>.] - ETA: 0s - loss: 0.1237
6528/6530 [============================>.] - ETA: 0s - loss: 0.1235
6530/6530 [==============================] - 3s 450us/step - loss: 0.1234 - val_loss: 0.1005
Epoch 12/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1371
 144/6530 [..............................] - ETA: 2s - loss: 0.1177
 256/6530 [>.............................] - ETA: 2s - loss: 0.1249
 368/6530 [>.............................] - ETA: 2s - loss: 0.1222
 496/6530 [=>............................] - ETA: 2s - loss: 0.1238
 624/6530 [=>............................] - ETA: 2s - loss: 0.1228
 752/6530 [==>...........................] - ETA: 2s - loss: 0.1213
 880/6530 [===>..........................] - ETA: 2s - loss: 0.1214
1024/6530 [===>..........................] - ETA: 2s - loss: 0.1215
1200/6530 [====>.........................] - ETA: 2s - loss: 0.1215
1360/6530 [=====>........................] - ETA: 2s - loss: 0.1203
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1198
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1190
1840/6530 [=======>......................] - ETA: 1s - loss: 0.1188
1968/6530 [========>.....................] - ETA: 1s - loss: 0.1185
2080/6530 [========>.....................] - ETA: 1s - loss: 0.1179
2208/6530 [=========>....................] - ETA: 1s - loss: 0.1186
2336/6530 [=========>....................] - ETA: 1s - loss: 0.1188
2480/6530 [==========>...................] - ETA: 1s - loss: 0.1199
2640/6530 [===========>..................] - ETA: 1s - loss: 0.1204
2800/6530 [===========>..................] - ETA: 1s - loss: 0.1199
2944/6530 [============>.................] - ETA: 1s - loss: 0.1199
3104/6530 [=============>................] - ETA: 1s - loss: 0.1198
3264/6530 [=============>................] - ETA: 1s - loss: 0.1203
3408/6530 [==============>...............] - ETA: 1s - loss: 0.1199
3552/6530 [===============>..............] - ETA: 1s - loss: 0.1201
3680/6530 [===============>..............] - ETA: 1s - loss: 0.1198
3808/6530 [================>.............] - ETA: 1s - loss: 0.1195
3920/6530 [=================>............] - ETA: 1s - loss: 0.1197
4032/6530 [=================>............] - ETA: 0s - loss: 0.1198
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1196
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1193
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1192
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1196
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1197
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1197
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1197
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1195
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1196
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1197
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1201
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1201
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1200
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1197
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1195
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1194
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1192
6336/6530 [============================>.] - ETA: 0s - loss: 0.1193
6512/6530 [============================>.] - ETA: 0s - loss: 0.1193
6530/6530 [==============================] - 3s 404us/step - loss: 0.1194 - val_loss: 0.1011
Epoch 13/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1475
 176/6530 [..............................] - ETA: 2s - loss: 0.1240
 336/6530 [>.............................] - ETA: 2s - loss: 0.1249
 512/6530 [=>............................] - ETA: 1s - loss: 0.1232
 672/6530 [==>...........................] - ETA: 1s - loss: 0.1206
 816/6530 [==>...........................] - ETA: 1s - loss: 0.1203
 976/6530 [===>..........................] - ETA: 1s - loss: 0.1221
1120/6530 [====>.........................] - ETA: 1s - loss: 0.1239
1264/6530 [====>.........................] - ETA: 1s - loss: 0.1239
1440/6530 [=====>........................] - ETA: 1s - loss: 0.1218
1616/6530 [======>.......................] - ETA: 1s - loss: 0.1204
1792/6530 [=======>......................] - ETA: 1s - loss: 0.1200
1968/6530 [========>.....................] - ETA: 1s - loss: 0.1194
2144/6530 [========>.....................] - ETA: 1s - loss: 0.1197
2304/6530 [=========>....................] - ETA: 1s - loss: 0.1199
2464/6530 [==========>...................] - ETA: 1s - loss: 0.1210
2624/6530 [===========>..................] - ETA: 1s - loss: 0.1211
2784/6530 [===========>..................] - ETA: 1s - loss: 0.1205
2960/6530 [============>.................] - ETA: 1s - loss: 0.1201
3088/6530 [=============>................] - ETA: 1s - loss: 0.1198
3216/6530 [=============>................] - ETA: 1s - loss: 0.1194
3360/6530 [==============>...............] - ETA: 1s - loss: 0.1191
3488/6530 [===============>..............] - ETA: 1s - loss: 0.1195
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1194
3824/6530 [================>.............] - ETA: 0s - loss: 0.1196
4016/6530 [=================>............] - ETA: 0s - loss: 0.1199
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1196
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1191
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1193
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1195
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1192
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1191
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1194
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1196
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1199
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1199
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1193
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1193
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1190
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1189
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1189
6432/6530 [============================>.] - ETA: 0s - loss: 0.1188
6530/6530 [==============================] - 2s 349us/step - loss: 0.1188 - val_loss: 0.0968
Epoch 14/81

  16/6530 [..............................] - ETA: 3s - loss: 0.1364
 144/6530 [..............................] - ETA: 2s - loss: 0.1180
 272/6530 [>.............................] - ETA: 2s - loss: 0.1142
 400/6530 [>.............................] - ETA: 2s - loss: 0.1158
 544/6530 [=>............................] - ETA: 2s - loss: 0.1169
 672/6530 [==>...........................] - ETA: 2s - loss: 0.1179
 816/6530 [==>...........................] - ETA: 2s - loss: 0.1183
 960/6530 [===>..........................] - ETA: 2s - loss: 0.1168
1072/6530 [===>..........................] - ETA: 2s - loss: 0.1183
1184/6530 [====>.........................] - ETA: 2s - loss: 0.1190
1312/6530 [=====>........................] - ETA: 2s - loss: 0.1190
1456/6530 [=====>........................] - ETA: 2s - loss: 0.1182
1584/6530 [======>.......................] - ETA: 2s - loss: 0.1173
1712/6530 [======>.......................] - ETA: 1s - loss: 0.1166
1872/6530 [=======>......................] - ETA: 1s - loss: 0.1163
2032/6530 [========>.....................] - ETA: 1s - loss: 0.1159
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1157
2304/6530 [=========>....................] - ETA: 1s - loss: 0.1155
2448/6530 [==========>...................] - ETA: 1s - loss: 0.1162
2560/6530 [==========>...................] - ETA: 1s - loss: 0.1162
2688/6530 [===========>..................] - ETA: 1s - loss: 0.1158
2832/6530 [============>.................] - ETA: 1s - loss: 0.1153
2960/6530 [============>.................] - ETA: 1s - loss: 0.1153
3072/6530 [=============>................] - ETA: 1s - loss: 0.1157
3184/6530 [=============>................] - ETA: 1s - loss: 0.1155
3312/6530 [==============>...............] - ETA: 1s - loss: 0.1159
3440/6530 [==============>...............] - ETA: 1s - loss: 0.1162
3568/6530 [===============>..............] - ETA: 1s - loss: 0.1164
3712/6530 [================>.............] - ETA: 1s - loss: 0.1161
3872/6530 [================>.............] - ETA: 1s - loss: 0.1162
4032/6530 [=================>............] - ETA: 0s - loss: 0.1163
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1158
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1156
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1159
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1159
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1154
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1157
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1157
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1159
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1153
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1153
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1157
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1155
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1154
6320/6530 [============================>.] - ETA: 0s - loss: 0.1153
6464/6530 [============================>.] - ETA: 0s - loss: 0.1152
6530/6530 [==============================] - 2s 383us/step - loss: 0.1153 - val_loss: 0.0918
Epoch 15/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1442
 144/6530 [..............................] - ETA: 2s - loss: 0.1052
 288/6530 [>.............................] - ETA: 2s - loss: 0.1066
 416/6530 [>.............................] - ETA: 2s - loss: 0.1106
 544/6530 [=>............................] - ETA: 2s - loss: 0.1129
 688/6530 [==>...........................] - ETA: 2s - loss: 0.1140
 864/6530 [==>...........................] - ETA: 2s - loss: 0.1135
1024/6530 [===>..........................] - ETA: 2s - loss: 0.1148
1168/6530 [====>.........................] - ETA: 2s - loss: 0.1153
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1168
1424/6530 [=====>........................] - ETA: 1s - loss: 0.1155
1568/6530 [======>.......................] - ETA: 1s - loss: 0.1143
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1150
1840/6530 [=======>......................] - ETA: 1s - loss: 0.1153
2000/6530 [========>.....................] - ETA: 1s - loss: 0.1138
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1152
2368/6530 [=========>....................] - ETA: 1s - loss: 0.1153
2560/6530 [==========>...................] - ETA: 1s - loss: 0.1161
2752/6530 [===========>..................] - ETA: 1s - loss: 0.1153
2928/6530 [============>.................] - ETA: 1s - loss: 0.1149
3072/6530 [=============>................] - ETA: 1s - loss: 0.1148
3248/6530 [=============>................] - ETA: 1s - loss: 0.1153
3408/6530 [==============>...............] - ETA: 1s - loss: 0.1156
3552/6530 [===============>..............] - ETA: 1s - loss: 0.1157
3712/6530 [================>.............] - ETA: 0s - loss: 0.1156
3872/6530 [================>.............] - ETA: 0s - loss: 0.1158
4016/6530 [=================>............] - ETA: 0s - loss: 0.1159
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1160
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1161
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1156
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1159
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1160
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1157
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1160
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1158
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1162
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1160
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1158
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1163
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1163
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1161
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1160
6368/6530 [============================>.] - ETA: 0s - loss: 0.1156
6480/6530 [============================>.] - ETA: 0s - loss: 0.1156
6530/6530 [==============================] - 2s 370us/step - loss: 0.1157 - val_loss: 0.0933
Epoch 16/81

  16/6530 [..............................] - ETA: 3s - loss: 0.1671
 128/6530 [..............................] - ETA: 3s - loss: 0.1185
 240/6530 [>.............................] - ETA: 3s - loss: 0.1129
 352/6530 [>.............................] - ETA: 3s - loss: 0.1126
 448/6530 [=>............................] - ETA: 3s - loss: 0.1149
 560/6530 [=>............................] - ETA: 2s - loss: 0.1136
 672/6530 [==>...........................] - ETA: 2s - loss: 0.1134
 800/6530 [==>...........................] - ETA: 2s - loss: 0.1139
 912/6530 [===>..........................] - ETA: 2s - loss: 0.1136
1056/6530 [===>..........................] - ETA: 2s - loss: 0.1146
1200/6530 [====>.........................] - ETA: 2s - loss: 0.1152
1360/6530 [=====>........................] - ETA: 2s - loss: 0.1143
1504/6530 [=====>........................] - ETA: 2s - loss: 0.1148
1664/6530 [======>.......................] - ETA: 2s - loss: 0.1141
1840/6530 [=======>......................] - ETA: 1s - loss: 0.1146
2016/6530 [========>.....................] - ETA: 1s - loss: 0.1135
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1143
2304/6530 [=========>....................] - ETA: 1s - loss: 0.1137
2416/6530 [==========>...................] - ETA: 1s - loss: 0.1144
2528/6530 [==========>...................] - ETA: 1s - loss: 0.1143
2640/6530 [===========>..................] - ETA: 1s - loss: 0.1145
2752/6530 [===========>..................] - ETA: 1s - loss: 0.1135
2864/6530 [============>.................] - ETA: 1s - loss: 0.1130
2976/6530 [============>.................] - ETA: 1s - loss: 0.1132
3072/6530 [=============>................] - ETA: 1s - loss: 0.1130
3184/6530 [=============>................] - ETA: 1s - loss: 0.1128
3296/6530 [==============>...............] - ETA: 1s - loss: 0.1133
3408/6530 [==============>...............] - ETA: 1s - loss: 0.1139
3504/6530 [===============>..............] - ETA: 1s - loss: 0.1142
3600/6530 [===============>..............] - ETA: 1s - loss: 0.1144
3712/6530 [================>.............] - ETA: 1s - loss: 0.1142
3840/6530 [================>.............] - ETA: 1s - loss: 0.1145
3968/6530 [=================>............] - ETA: 1s - loss: 0.1145
4080/6530 [=================>............] - ETA: 1s - loss: 0.1146
4192/6530 [==================>...........] - ETA: 1s - loss: 0.1147
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1145
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1145
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1145
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1147
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1146
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1143
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1142
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1144
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1146
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1146
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1145
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1146
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1145
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1144
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1143
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1145
6464/6530 [============================>.] - ETA: 0s - loss: 0.1140
6530/6530 [==============================] - 3s 435us/step - loss: 0.1141 - val_loss: 0.0943
Epoch 17/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1253
 144/6530 [..............................] - ETA: 2s - loss: 0.1084
 272/6530 [>.............................] - ETA: 2s - loss: 0.1090
 400/6530 [>.............................] - ETA: 2s - loss: 0.1095
 560/6530 [=>............................] - ETA: 2s - loss: 0.1088
 720/6530 [==>...........................] - ETA: 2s - loss: 0.1117
 896/6530 [===>..........................] - ETA: 2s - loss: 0.1138
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1133
1232/6530 [====>.........................] - ETA: 1s - loss: 0.1141
1408/6530 [=====>........................] - ETA: 1s - loss: 0.1131
1584/6530 [======>.......................] - ETA: 1s - loss: 0.1122
1760/6530 [=======>......................] - ETA: 1s - loss: 0.1114
1936/6530 [=======>......................] - ETA: 1s - loss: 0.1108
2112/6530 [========>.....................] - ETA: 1s - loss: 0.1102
2272/6530 [=========>....................] - ETA: 1s - loss: 0.1106
2416/6530 [==========>...................] - ETA: 1s - loss: 0.1118
2544/6530 [==========>...................] - ETA: 1s - loss: 0.1120
2656/6530 [===========>..................] - ETA: 1s - loss: 0.1116
2800/6530 [===========>..................] - ETA: 1s - loss: 0.1108
2944/6530 [============>.................] - ETA: 1s - loss: 0.1103
3088/6530 [=============>................] - ETA: 1s - loss: 0.1105
3232/6530 [=============>................] - ETA: 1s - loss: 0.1113
3376/6530 [==============>...............] - ETA: 1s - loss: 0.1118
3504/6530 [===============>..............] - ETA: 1s - loss: 0.1119
3632/6530 [===============>..............] - ETA: 1s - loss: 0.1119
3744/6530 [================>.............] - ETA: 0s - loss: 0.1116
3856/6530 [================>.............] - ETA: 0s - loss: 0.1117
3968/6530 [=================>............] - ETA: 0s - loss: 0.1116
4080/6530 [=================>............] - ETA: 0s - loss: 0.1119
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1118
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1120
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1120
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1123
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1122
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1119
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1120
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1121
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1121
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1116
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1113
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1113
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1113
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1112
6432/6530 [============================>.] - ETA: 0s - loss: 0.1113
6530/6530 [==============================] - 2s 363us/step - loss: 0.1113 - val_loss: 0.0916
Epoch 18/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1223
 128/6530 [..............................] - ETA: 2s - loss: 0.1028
 256/6530 [>.............................] - ETA: 2s - loss: 0.1104
 368/6530 [>.............................] - ETA: 2s - loss: 0.1103
 512/6530 [=>............................] - ETA: 2s - loss: 0.1103
 640/6530 [=>............................] - ETA: 2s - loss: 0.1107
 768/6530 [==>...........................] - ETA: 2s - loss: 0.1112
 928/6530 [===>..........................] - ETA: 2s - loss: 0.1110
1056/6530 [===>..........................] - ETA: 2s - loss: 0.1129
1184/6530 [====>.........................] - ETA: 2s - loss: 0.1145
1328/6530 [=====>........................] - ETA: 2s - loss: 0.1149
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1136
1632/6530 [======>.......................] - ETA: 1s - loss: 0.1132
1792/6530 [=======>......................] - ETA: 1s - loss: 0.1134
1920/6530 [=======>......................] - ETA: 1s - loss: 0.1129
2080/6530 [========>.....................] - ETA: 1s - loss: 0.1125
2240/6530 [=========>....................] - ETA: 1s - loss: 0.1131
2384/6530 [=========>....................] - ETA: 1s - loss: 0.1135
2528/6530 [==========>...................] - ETA: 1s - loss: 0.1136
2672/6530 [===========>..................] - ETA: 1s - loss: 0.1135
2864/6530 [============>.................] - ETA: 1s - loss: 0.1129
3024/6530 [============>.................] - ETA: 1s - loss: 0.1128
3216/6530 [=============>................] - ETA: 1s - loss: 0.1126
3376/6530 [==============>...............] - ETA: 1s - loss: 0.1128
3536/6530 [===============>..............] - ETA: 1s - loss: 0.1130
3696/6530 [===============>..............] - ETA: 1s - loss: 0.1130
3872/6530 [================>.............] - ETA: 0s - loss: 0.1131
4048/6530 [=================>............] - ETA: 0s - loss: 0.1129
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1124
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1125
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1126
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1123
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1119
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1120
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1119
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1120
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1119
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1115
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1113
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1112
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1110
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1110
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1113
6416/6530 [============================>.] - ETA: 0s - loss: 0.1114
6530/6530 [==============================] - 2s 366us/step - loss: 0.1114 - val_loss: 0.0938
Epoch 19/81

  16/6530 [..............................] - ETA: 3s - loss: 0.1169
 160/6530 [..............................] - ETA: 2s - loss: 0.1036
 304/6530 [>.............................] - ETA: 2s - loss: 0.1078
 448/6530 [=>............................] - ETA: 2s - loss: 0.1115
 608/6530 [=>............................] - ETA: 2s - loss: 0.1109
 752/6530 [==>...........................] - ETA: 2s - loss: 0.1105
 880/6530 [===>..........................] - ETA: 2s - loss: 0.1084
 992/6530 [===>..........................] - ETA: 2s - loss: 0.1089
1104/6530 [====>.........................] - ETA: 2s - loss: 0.1098
1216/6530 [====>.........................] - ETA: 2s - loss: 0.1099
1344/6530 [=====>........................] - ETA: 2s - loss: 0.1101
1472/6530 [=====>........................] - ETA: 2s - loss: 0.1098
1616/6530 [======>.......................] - ETA: 2s - loss: 0.1091
1760/6530 [=======>......................] - ETA: 1s - loss: 0.1096
1904/6530 [=======>......................] - ETA: 1s - loss: 0.1087
2048/6530 [========>.....................] - ETA: 1s - loss: 0.1083
2192/6530 [=========>....................] - ETA: 1s - loss: 0.1094
2352/6530 [=========>....................] - ETA: 1s - loss: 0.1095
2528/6530 [==========>...................] - ETA: 1s - loss: 0.1103
2688/6530 [===========>..................] - ETA: 1s - loss: 0.1101
2832/6530 [============>.................] - ETA: 1s - loss: 0.1100
2976/6530 [============>.................] - ETA: 1s - loss: 0.1096
3152/6530 [=============>................] - ETA: 1s - loss: 0.1092
3312/6530 [==============>...............] - ETA: 1s - loss: 0.1092
3472/6530 [==============>...............] - ETA: 1s - loss: 0.1094
3632/6530 [===============>..............] - ETA: 1s - loss: 0.1097
3792/6530 [================>.............] - ETA: 1s - loss: 0.1095
3936/6530 [=================>............] - ETA: 0s - loss: 0.1098
4112/6530 [=================>............] - ETA: 0s - loss: 0.1097
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1092
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1095
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1099
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1098
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1096
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1096
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1099
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1099
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1100
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1101
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1099
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1099
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1098
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1097
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1095
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1098
6384/6530 [============================>.] - ETA: 0s - loss: 0.1096
6530/6530 [==============================] - 3s 385us/step - loss: 0.1096 - val_loss: 0.0920
Epoch 20/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1381
 144/6530 [..............................] - ETA: 2s - loss: 0.1084
 272/6530 [>.............................] - ETA: 2s - loss: 0.1086
 400/6530 [>.............................] - ETA: 2s - loss: 0.1092
 544/6530 [=>............................] - ETA: 2s - loss: 0.1076
 656/6530 [==>...........................] - ETA: 2s - loss: 0.1090
 784/6530 [==>...........................] - ETA: 2s - loss: 0.1086
 928/6530 [===>..........................] - ETA: 2s - loss: 0.1079
1088/6530 [===>..........................] - ETA: 2s - loss: 0.1094
1232/6530 [====>.........................] - ETA: 2s - loss: 0.1098
1392/6530 [=====>........................] - ETA: 1s - loss: 0.1090
1568/6530 [======>.......................] - ETA: 1s - loss: 0.1085
1760/6530 [=======>......................] - ETA: 1s - loss: 0.1086
1952/6530 [=======>......................] - ETA: 1s - loss: 0.1091
2128/6530 [========>.....................] - ETA: 1s - loss: 0.1086
2288/6530 [=========>....................] - ETA: 1s - loss: 0.1099
2432/6530 [==========>...................] - ETA: 1s - loss: 0.1098
2608/6530 [==========>...................] - ETA: 1s - loss: 0.1099
2752/6530 [===========>..................] - ETA: 1s - loss: 0.1090
2896/6530 [============>.................] - ETA: 1s - loss: 0.1087
3040/6530 [============>.................] - ETA: 1s - loss: 0.1088
3168/6530 [=============>................] - ETA: 1s - loss: 0.1089
3296/6530 [==============>...............] - ETA: 1s - loss: 0.1090
3440/6530 [==============>...............] - ETA: 1s - loss: 0.1096
3552/6530 [===============>..............] - ETA: 1s - loss: 0.1096
3664/6530 [===============>..............] - ETA: 1s - loss: 0.1097
3808/6530 [================>.............] - ETA: 1s - loss: 0.1093
3936/6530 [=================>............] - ETA: 0s - loss: 0.1091
4048/6530 [=================>............] - ETA: 0s - loss: 0.1095
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1094
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1092
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1089
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1092
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1092
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1093
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1089
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1085
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1087
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1087
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1090
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1089
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1088
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1086
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1084
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1084
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1085
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1086
6400/6530 [============================>.] - ETA: 0s - loss: 0.1086
6530/6530 [==============================] - 3s 402us/step - loss: 0.1087 - val_loss: 0.0899
Epoch 21/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1318
 160/6530 [..............................] - ETA: 2s - loss: 0.1083
 304/6530 [>.............................] - ETA: 2s - loss: 0.1007
 448/6530 [=>............................] - ETA: 2s - loss: 0.1030
 592/6530 [=>............................] - ETA: 2s - loss: 0.1036
 752/6530 [==>...........................] - ETA: 2s - loss: 0.1062
 896/6530 [===>..........................] - ETA: 2s - loss: 0.1058
1040/6530 [===>..........................] - ETA: 1s - loss: 0.1067
1216/6530 [====>.........................] - ETA: 1s - loss: 0.1082
1376/6530 [=====>........................] - ETA: 1s - loss: 0.1075
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1078
1648/6530 [======>.......................] - ETA: 1s - loss: 0.1067
1808/6530 [=======>......................] - ETA: 1s - loss: 0.1072
1952/6530 [=======>......................] - ETA: 1s - loss: 0.1074
2080/6530 [========>.....................] - ETA: 1s - loss: 0.1067
2192/6530 [=========>....................] - ETA: 1s - loss: 0.1075
2288/6530 [=========>....................] - ETA: 1s - loss: 0.1080
2384/6530 [=========>....................] - ETA: 1s - loss: 0.1083
2496/6530 [==========>...................] - ETA: 1s - loss: 0.1084
2624/6530 [===========>..................] - ETA: 1s - loss: 0.1087
2736/6530 [===========>..................] - ETA: 1s - loss: 0.1088
2864/6530 [============>.................] - ETA: 1s - loss: 0.1084
2960/6530 [============>.................] - ETA: 1s - loss: 0.1082
3088/6530 [=============>................] - ETA: 1s - loss: 0.1078
3216/6530 [=============>................] - ETA: 1s - loss: 0.1077
3344/6530 [==============>...............] - ETA: 1s - loss: 0.1079
3488/6530 [===============>..............] - ETA: 1s - loss: 0.1085
3616/6530 [===============>..............] - ETA: 1s - loss: 0.1083
3776/6530 [================>.............] - ETA: 1s - loss: 0.1080
3888/6530 [================>.............] - ETA: 1s - loss: 0.1082
4000/6530 [=================>............] - ETA: 1s - loss: 0.1080
4128/6530 [=================>............] - ETA: 0s - loss: 0.1080
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1077
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1074
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1077
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1075
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1071
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1073
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1074
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1075
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1069
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1064
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1067
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1066
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1067
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1068
6384/6530 [============================>.] - ETA: 0s - loss: 0.1068
6496/6530 [============================>.] - ETA: 0s - loss: 0.1068
6530/6530 [==============================] - 3s 397us/step - loss: 0.1069 - val_loss: 0.0916
Epoch 22/81

  16/6530 [..............................] - ETA: 3s - loss: 0.1257
 128/6530 [..............................] - ETA: 3s - loss: 0.1053
 240/6530 [>.............................] - ETA: 3s - loss: 0.1055
 352/6530 [>.............................] - ETA: 3s - loss: 0.1050
 464/6530 [=>............................] - ETA: 2s - loss: 0.1079
 576/6530 [=>............................] - ETA: 2s - loss: 0.1059
 720/6530 [==>...........................] - ETA: 2s - loss: 0.1086
 864/6530 [==>...........................] - ETA: 2s - loss: 0.1061
1040/6530 [===>..........................] - ETA: 2s - loss: 0.1060
1184/6530 [====>.........................] - ETA: 2s - loss: 0.1067
1360/6530 [=====>........................] - ETA: 2s - loss: 0.1063
1488/6530 [=====>........................] - ETA: 2s - loss: 0.1066
1616/6530 [======>.......................] - ETA: 1s - loss: 0.1059
1744/6530 [=======>......................] - ETA: 1s - loss: 0.1056
1856/6530 [=======>......................] - ETA: 1s - loss: 0.1054
1968/6530 [========>.....................] - ETA: 1s - loss: 0.1050
2080/6530 [========>.....................] - ETA: 1s - loss: 0.1046
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1053
2272/6530 [=========>....................] - ETA: 1s - loss: 0.1050
2368/6530 [=========>....................] - ETA: 1s - loss: 0.1057
2480/6530 [==========>...................] - ETA: 1s - loss: 0.1063
2592/6530 [==========>...................] - ETA: 1s - loss: 0.1069
2704/6530 [===========>..................] - ETA: 1s - loss: 0.1069
2832/6530 [============>.................] - ETA: 1s - loss: 0.1065
2976/6530 [============>.................] - ETA: 1s - loss: 0.1063
3136/6530 [=============>................] - ETA: 1s - loss: 0.1061
3296/6530 [==============>...............] - ETA: 1s - loss: 0.1058
3456/6530 [==============>...............] - ETA: 1s - loss: 0.1065
3600/6530 [===============>..............] - ETA: 1s - loss: 0.1065
3728/6530 [================>.............] - ETA: 1s - loss: 0.1061
3872/6530 [================>.............] - ETA: 1s - loss: 0.1067
4000/6530 [=================>............] - ETA: 1s - loss: 0.1067
4112/6530 [=================>............] - ETA: 1s - loss: 0.1066
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1066
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1063
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1070
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1069
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1069
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1066
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1062
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1064
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1065
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1065
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1067
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1066
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1066
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1063
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1062
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1062
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1061
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1063
6368/6530 [============================>.] - ETA: 0s - loss: 0.1061
6496/6530 [============================>.] - ETA: 0s - loss: 0.1059
6530/6530 [==============================] - 3s 443us/step - loss: 0.1059 - val_loss: 0.0875
Epoch 23/81

  16/6530 [..............................] - ETA: 3s - loss: 0.1365
 128/6530 [..............................] - ETA: 3s - loss: 0.1108
 240/6530 [>.............................] - ETA: 3s - loss: 0.1070
 336/6530 [>.............................] - ETA: 3s - loss: 0.1057
 448/6530 [=>............................] - ETA: 3s - loss: 0.1061
 544/6530 [=>............................] - ETA: 3s - loss: 0.1049
 672/6530 [==>...........................] - ETA: 2s - loss: 0.1056
 800/6530 [==>...........................] - ETA: 2s - loss: 0.1054
 944/6530 [===>..........................] - ETA: 2s - loss: 0.1041
1072/6530 [===>..........................] - ETA: 2s - loss: 0.1050
1200/6530 [====>.........................] - ETA: 2s - loss: 0.1057
1312/6530 [=====>........................] - ETA: 2s - loss: 0.1056
1424/6530 [=====>........................] - ETA: 2s - loss: 0.1045
1552/6530 [======>.......................] - ETA: 2s - loss: 0.1041
1680/6530 [======>.......................] - ETA: 2s - loss: 0.1045
1792/6530 [=======>......................] - ETA: 2s - loss: 0.1042
1904/6530 [=======>......................] - ETA: 2s - loss: 0.1033
2032/6530 [========>.....................] - ETA: 2s - loss: 0.1031
2144/6530 [========>.....................] - ETA: 2s - loss: 0.1041
2256/6530 [=========>....................] - ETA: 1s - loss: 0.1035
2368/6530 [=========>....................] - ETA: 1s - loss: 0.1041
2480/6530 [==========>...................] - ETA: 1s - loss: 0.1044
2592/6530 [==========>...................] - ETA: 1s - loss: 0.1054
2720/6530 [===========>..................] - ETA: 1s - loss: 0.1058
2848/6530 [============>.................] - ETA: 1s - loss: 0.1054
2992/6530 [============>.................] - ETA: 1s - loss: 0.1055
3120/6530 [=============>................] - ETA: 1s - loss: 0.1053
3248/6530 [=============>................] - ETA: 1s - loss: 0.1050
3408/6530 [==============>...............] - ETA: 1s - loss: 0.1054
3568/6530 [===============>..............] - ETA: 1s - loss: 0.1058
3760/6530 [================>.............] - ETA: 1s - loss: 0.1058
3936/6530 [=================>............] - ETA: 1s - loss: 0.1057
4112/6530 [=================>............] - ETA: 1s - loss: 0.1056
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1056
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1052
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1055
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1054
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1054
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1054
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1057
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1059
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1060
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1060
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1057
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1058
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1058
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1059
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1058
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1056
6448/6530 [============================>.] - ETA: 0s - loss: 0.1056
6530/6530 [==============================] - 3s 412us/step - loss: 0.1058 - val_loss: 0.0860
Epoch 24/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1188
 144/6530 [..............................] - ETA: 2s - loss: 0.1122
 288/6530 [>.............................] - ETA: 2s - loss: 0.1041
 432/6530 [>.............................] - ETA: 2s - loss: 0.1079
 592/6530 [=>............................] - ETA: 2s - loss: 0.1064
 752/6530 [==>...........................] - ETA: 2s - loss: 0.1063
 896/6530 [===>..........................] - ETA: 2s - loss: 0.1059
1024/6530 [===>..........................] - ETA: 2s - loss: 0.1073
1168/6530 [====>.........................] - ETA: 1s - loss: 0.1081
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1087
1424/6530 [=====>........................] - ETA: 1s - loss: 0.1084
1568/6530 [======>.......................] - ETA: 1s - loss: 0.1079
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1077
1840/6530 [=======>......................] - ETA: 1s - loss: 0.1081
2000/6530 [========>.....................] - ETA: 1s - loss: 0.1063
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1063
2352/6530 [=========>....................] - ETA: 1s - loss: 0.1058
2512/6530 [==========>...................] - ETA: 1s - loss: 0.1065
2656/6530 [===========>..................] - ETA: 1s - loss: 0.1063
2784/6530 [===========>..................] - ETA: 1s - loss: 0.1056
2928/6530 [============>.................] - ETA: 1s - loss: 0.1052
3104/6530 [=============>................] - ETA: 1s - loss: 0.1052
3248/6530 [=============>................] - ETA: 1s - loss: 0.1058
3392/6530 [==============>...............] - ETA: 1s - loss: 0.1061
3520/6530 [===============>..............] - ETA: 1s - loss: 0.1064
3648/6530 [===============>..............] - ETA: 1s - loss: 0.1061
3776/6530 [================>.............] - ETA: 1s - loss: 0.1060
3888/6530 [================>.............] - ETA: 0s - loss: 0.1060
4000/6530 [=================>............] - ETA: 0s - loss: 0.1060
4112/6530 [=================>............] - ETA: 0s - loss: 0.1056
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1055
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1054
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1054
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1054
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1052
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1047
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1050
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1051
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1052
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1049
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1048
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1048
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1048
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1048
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1047
6400/6530 [============================>.] - ETA: 0s - loss: 0.1047
6512/6530 [============================>.] - ETA: 0s - loss: 0.1048
6530/6530 [==============================] - 3s 387us/step - loss: 0.1049 - val_loss: 0.0872
Epoch 25/81

  16/6530 [..............................] - ETA: 3s - loss: 0.0951
 144/6530 [..............................] - ETA: 2s - loss: 0.1042
 272/6530 [>.............................] - ETA: 2s - loss: 0.1043
 384/6530 [>.............................] - ETA: 2s - loss: 0.1093
 512/6530 [=>............................] - ETA: 2s - loss: 0.1071
 656/6530 [==>...........................] - ETA: 2s - loss: 0.1089
 800/6530 [==>...........................] - ETA: 2s - loss: 0.1075
 960/6530 [===>..........................] - ETA: 2s - loss: 0.1054
1136/6530 [====>.........................] - ETA: 2s - loss: 0.1065
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1075
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1056
1632/6530 [======>.......................] - ETA: 1s - loss: 0.1047
1744/6530 [=======>......................] - ETA: 1s - loss: 0.1045
1856/6530 [=======>......................] - ETA: 1s - loss: 0.1050
1968/6530 [========>.....................] - ETA: 1s - loss: 0.1043
2064/6530 [========>.....................] - ETA: 1s - loss: 0.1037
2160/6530 [========>.....................] - ETA: 1s - loss: 0.1040
2272/6530 [=========>....................] - ETA: 1s - loss: 0.1036
2384/6530 [=========>....................] - ETA: 1s - loss: 0.1040
2480/6530 [==========>...................] - ETA: 1s - loss: 0.1042
2592/6530 [==========>...................] - ETA: 1s - loss: 0.1044
2704/6530 [===========>..................] - ETA: 1s - loss: 0.1042
2832/6530 [============>.................] - ETA: 1s - loss: 0.1037
2960/6530 [============>.................] - ETA: 1s - loss: 0.1032
3072/6530 [=============>................] - ETA: 1s - loss: 0.1034
3200/6530 [=============>................] - ETA: 1s - loss: 0.1028
3312/6530 [==============>...............] - ETA: 1s - loss: 0.1032
3424/6530 [==============>...............] - ETA: 1s - loss: 0.1037
3568/6530 [===============>..............] - ETA: 1s - loss: 0.1037
3696/6530 [===============>..............] - ETA: 1s - loss: 0.1031
3856/6530 [================>.............] - ETA: 1s - loss: 0.1032
3984/6530 [=================>............] - ETA: 1s - loss: 0.1032
4128/6530 [=================>............] - ETA: 1s - loss: 0.1030
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1031
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1031
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1031
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1032
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1030
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1029
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1030
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1033
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1030
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1027
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1026
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1023
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1022
6352/6530 [============================>.] - ETA: 0s - loss: 0.1020
6528/6530 [============================>.] - ETA: 0s - loss: 0.1023
6530/6530 [==============================] - 3s 394us/step - loss: 0.1023 - val_loss: 0.0857
Epoch 26/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1167
 176/6530 [..............................] - ETA: 2s - loss: 0.0980
 352/6530 [>.............................] - ETA: 1s - loss: 0.0966
 544/6530 [=>............................] - ETA: 1s - loss: 0.0982
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0992
 816/6530 [==>...........................] - ETA: 1s - loss: 0.1008
 944/6530 [===>..........................] - ETA: 1s - loss: 0.1004
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1032
1200/6530 [====>.........................] - ETA: 1s - loss: 0.1025
1344/6530 [=====>........................] - ETA: 1s - loss: 0.1026
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1025
1680/6530 [======>.......................] - ETA: 1s - loss: 0.1020
1856/6530 [=======>......................] - ETA: 1s - loss: 0.1025
2048/6530 [========>.....................] - ETA: 1s - loss: 0.1019
2240/6530 [=========>....................] - ETA: 1s - loss: 0.1018
2432/6530 [==========>...................] - ETA: 1s - loss: 0.1019
2608/6530 [==========>...................] - ETA: 1s - loss: 0.1025
2784/6530 [===========>..................] - ETA: 1s - loss: 0.1020
2960/6530 [============>.................] - ETA: 1s - loss: 0.1018
3136/6530 [=============>................] - ETA: 1s - loss: 0.1015
3296/6530 [==============>...............] - ETA: 1s - loss: 0.1018
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1026
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1027
3792/6530 [================>.............] - ETA: 0s - loss: 0.1026
3952/6530 [=================>............] - ETA: 0s - loss: 0.1029
4096/6530 [=================>............] - ETA: 0s - loss: 0.1029
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1030
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1028
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1028
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1028
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1025
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1028
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1029
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1025
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1024
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1026
5968/6530 [==========================>...] - ETA: 0s - loss: 0.1027
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1025
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1028
6352/6530 [============================>.] - ETA: 0s - loss: 0.1026
6480/6530 [============================>.] - ETA: 0s - loss: 0.1026
6530/6530 [==============================] - 2s 341us/step - loss: 0.1027 - val_loss: 0.0832
Epoch 27/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1381
 144/6530 [..............................] - ETA: 2s - loss: 0.0984
 288/6530 [>.............................] - ETA: 2s - loss: 0.0976
 448/6530 [=>............................] - ETA: 2s - loss: 0.0994
 592/6530 [=>............................] - ETA: 2s - loss: 0.0998
 736/6530 [==>...........................] - ETA: 2s - loss: 0.1001
 896/6530 [===>..........................] - ETA: 2s - loss: 0.1001
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0995
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0994
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0995
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0990
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0998
1856/6530 [=======>......................] - ETA: 1s - loss: 0.1003
2000/6530 [========>.....................] - ETA: 1s - loss: 0.1001
2192/6530 [=========>....................] - ETA: 1s - loss: 0.1012
2368/6530 [=========>....................] - ETA: 1s - loss: 0.1015
2512/6530 [==========>...................] - ETA: 1s - loss: 0.1018
2656/6530 [===========>..................] - ETA: 1s - loss: 0.1020
2800/6530 [===========>..................] - ETA: 1s - loss: 0.1015
2960/6530 [============>.................] - ETA: 1s - loss: 0.1010
3120/6530 [=============>................] - ETA: 1s - loss: 0.1008
3280/6530 [==============>...............] - ETA: 1s - loss: 0.1007
3456/6530 [==============>...............] - ETA: 1s - loss: 0.1010
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1011
3776/6530 [================>.............] - ETA: 0s - loss: 0.1013
3888/6530 [================>.............] - ETA: 0s - loss: 0.1013
4000/6530 [=================>............] - ETA: 0s - loss: 0.1012
4128/6530 [=================>............] - ETA: 0s - loss: 0.1012
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1012
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1013
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1018
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1018
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1016
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1014
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1018
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1019
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1021
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1018
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1020
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1020
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1019
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1018
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1015
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1016
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1017
6432/6530 [============================>.] - ETA: 0s - loss: 0.1018
6530/6530 [==============================] - 2s 378us/step - loss: 0.1018 - val_loss: 0.0868
Epoch 28/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0990
 224/6530 [>.............................] - ETA: 1s - loss: 0.1026
 448/6530 [=>............................] - ETA: 1s - loss: 0.1002
 656/6530 [==>...........................] - ETA: 1s - loss: 0.0990
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0991
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0984
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1006
1232/6530 [====>.........................] - ETA: 1s - loss: 0.1010
1376/6530 [=====>........................] - ETA: 1s - loss: 0.1013
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1015
1616/6530 [======>.......................] - ETA: 1s - loss: 0.1015
1744/6530 [=======>......................] - ETA: 1s - loss: 0.1020
1888/6530 [=======>......................] - ETA: 1s - loss: 0.1018
2016/6530 [========>.....................] - ETA: 1s - loss: 0.1013
2144/6530 [========>.....................] - ETA: 1s - loss: 0.1011
2256/6530 [=========>....................] - ETA: 1s - loss: 0.1010
2368/6530 [=========>....................] - ETA: 1s - loss: 0.1013
2480/6530 [==========>...................] - ETA: 1s - loss: 0.1018
2608/6530 [==========>...................] - ETA: 1s - loss: 0.1020
2720/6530 [===========>..................] - ETA: 1s - loss: 0.1013
2848/6530 [============>.................] - ETA: 1s - loss: 0.1007
2976/6530 [============>.................] - ETA: 1s - loss: 0.1004
3104/6530 [=============>................] - ETA: 1s - loss: 0.1001
3232/6530 [=============>................] - ETA: 1s - loss: 0.0998
3344/6530 [==============>...............] - ETA: 1s - loss: 0.0999
3456/6530 [==============>...............] - ETA: 1s - loss: 0.1008
3584/6530 [===============>..............] - ETA: 1s - loss: 0.1009
3712/6530 [================>.............] - ETA: 1s - loss: 0.1007
3856/6530 [================>.............] - ETA: 1s - loss: 0.1012
4016/6530 [=================>............] - ETA: 0s - loss: 0.1015
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1013
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1012
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1011
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1014
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1012
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1012
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1015
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1017
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1019
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1016
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1015
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1012
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1012
6320/6530 [============================>.] - ETA: 0s - loss: 0.1012
6464/6530 [============================>.] - ETA: 0s - loss: 0.1011
6530/6530 [==============================] - 2s 376us/step - loss: 0.1013 - val_loss: 0.0866
Epoch 29/81

  16/6530 [..............................] - ETA: 3s - loss: 0.1121
 128/6530 [..............................] - ETA: 2s - loss: 0.0947
 240/6530 [>.............................] - ETA: 2s - loss: 0.1037
 352/6530 [>.............................] - ETA: 2s - loss: 0.1030
 464/6530 [=>............................] - ETA: 2s - loss: 0.1038
 576/6530 [=>............................] - ETA: 2s - loss: 0.1013
 704/6530 [==>...........................] - ETA: 2s - loss: 0.1020
 816/6530 [==>...........................] - ETA: 2s - loss: 0.1012
 944/6530 [===>..........................] - ETA: 2s - loss: 0.0996
1056/6530 [===>..........................] - ETA: 2s - loss: 0.1008
1184/6530 [====>.........................] - ETA: 2s - loss: 0.1010
1312/6530 [=====>........................] - ETA: 2s - loss: 0.1020
1440/6530 [=====>........................] - ETA: 2s - loss: 0.1008
1536/6530 [======>.......................] - ETA: 2s - loss: 0.1003
1632/6530 [======>.......................] - ETA: 2s - loss: 0.1009
1744/6530 [=======>......................] - ETA: 2s - loss: 0.1011
1856/6530 [=======>......................] - ETA: 2s - loss: 0.1008
1984/6530 [========>.....................] - ETA: 2s - loss: 0.1003
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0999
2256/6530 [=========>....................] - ETA: 1s - loss: 0.1001
2432/6530 [==========>...................] - ETA: 1s - loss: 0.1005
2592/6530 [==========>...................] - ETA: 1s - loss: 0.1006
2768/6530 [===========>..................] - ETA: 1s - loss: 0.1001
2944/6530 [============>.................] - ETA: 1s - loss: 0.0998
3088/6530 [=============>................] - ETA: 1s - loss: 0.0998
3232/6530 [=============>................] - ETA: 1s - loss: 0.0998
3392/6530 [==============>...............] - ETA: 1s - loss: 0.1001
3568/6530 [===============>..............] - ETA: 1s - loss: 0.1006
3760/6530 [================>.............] - ETA: 1s - loss: 0.1005
3936/6530 [=================>............] - ETA: 1s - loss: 0.1008
4112/6530 [=================>............] - ETA: 0s - loss: 0.1007
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1006
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1005
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1010
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1008
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1007
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1003
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1001
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1002
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1004
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1003
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1000
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0998
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0996
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0993
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0993
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0994
6400/6530 [============================>.] - ETA: 0s - loss: 0.0992
6530/6530 [==============================] - 3s 395us/step - loss: 0.0994 - val_loss: 0.0845
Epoch 30/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1122
 192/6530 [..............................] - ETA: 1s - loss: 0.1043
 384/6530 [>.............................] - ETA: 1s - loss: 0.0993
 560/6530 [=>............................] - ETA: 1s - loss: 0.1015
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1029
 912/6530 [===>..........................] - ETA: 1s - loss: 0.1029
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1030
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1038
1408/6530 [=====>........................] - ETA: 1s - loss: 0.1021
1552/6530 [======>.......................] - ETA: 1s - loss: 0.1016
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1014
1872/6530 [=======>......................] - ETA: 1s - loss: 0.1002
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0998
2224/6530 [=========>....................] - ETA: 1s - loss: 0.1000
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0998
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0996
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0997
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0988
2944/6530 [============>.................] - ETA: 1s - loss: 0.0982
3072/6530 [=============>................] - ETA: 1s - loss: 0.0982
3200/6530 [=============>................] - ETA: 1s - loss: 0.0980
3344/6530 [==============>...............] - ETA: 1s - loss: 0.0982
3488/6530 [===============>..............] - ETA: 1s - loss: 0.0989
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0987
3760/6530 [================>.............] - ETA: 0s - loss: 0.0991
3888/6530 [================>.............] - ETA: 0s - loss: 0.0993
4032/6530 [=================>............] - ETA: 0s - loss: 0.0994
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0991
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0990
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0990
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0993
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0993
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0989
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0989
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0990
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0990
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0987
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0989
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0989
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0988
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0992
6384/6530 [============================>.] - ETA: 0s - loss: 0.0991
6528/6530 [============================>.] - ETA: 0s - loss: 0.0993
6530/6530 [==============================] - 2s 354us/step - loss: 0.0993 - val_loss: 0.0849
Epoch 31/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1197
 160/6530 [..............................] - ETA: 2s - loss: 0.1022
 304/6530 [>.............................] - ETA: 2s - loss: 0.0967
 448/6530 [=>............................] - ETA: 2s - loss: 0.0984
 592/6530 [=>............................] - ETA: 2s - loss: 0.0971
 736/6530 [==>...........................] - ETA: 2s - loss: 0.0989
 880/6530 [===>..........................] - ETA: 2s - loss: 0.0992
1024/6530 [===>..........................] - ETA: 2s - loss: 0.0996
1168/6530 [====>.........................] - ETA: 2s - loss: 0.1000
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1003
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0999
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0989
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0991
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0996
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0987
2224/6530 [=========>....................] - ETA: 1s - loss: 0.0987
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0988
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0992
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0995
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0989
2848/6530 [============>.................] - ETA: 1s - loss: 0.0986
2960/6530 [============>.................] - ETA: 1s - loss: 0.0985
3072/6530 [=============>................] - ETA: 1s - loss: 0.0985
3184/6530 [=============>................] - ETA: 1s - loss: 0.0983
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0985
3424/6530 [==============>...............] - ETA: 1s - loss: 0.0989
3568/6530 [===============>..............] - ETA: 1s - loss: 0.0991
3696/6530 [===============>..............] - ETA: 1s - loss: 0.0988
3824/6530 [================>.............] - ETA: 1s - loss: 0.0991
3952/6530 [=================>............] - ETA: 1s - loss: 0.0991
4096/6530 [=================>............] - ETA: 0s - loss: 0.0990
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0988
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0985
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0986
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0984
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0982
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0983
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0985
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0988
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0986
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0984
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0984
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0983
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0984
6368/6530 [============================>.] - ETA: 0s - loss: 0.0983
6512/6530 [============================>.] - ETA: 0s - loss: 0.0984
6530/6530 [==============================] - 2s 379us/step - loss: 0.0984 - val_loss: 0.0846

# training | RMSE: 0.1003, MAE: 0.0779
worker 1  xfile  [1, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3801219225105922}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.10025293168982731, 'rmse': 0.10025293168982731, 'mae': 0.07791809474294802, 'early_stop': True}
vggnet done  1
all of workers have been done
calculation finished
#0 epoch=81.0 loss={'loss': 0.08927234942504737, 'rmse': 0.08927234942504737, 'mae': 0.0695971704523707, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#1 epoch=81.0 loss={'loss': 0.10025293168982731, 'rmse': 0.10025293168982731, 'mae': 0.07791809474294802, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3801219225105922}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
get a list [results] of length 201
get a list [loss] of length 2
get a list [val_loss] of length 2
length of indices is (0, 1)
length of indices is 2
length of T is 2
s=0
T is of size 5
T=[{'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 95, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2536073059624152}, 'layer_4_size': 79, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 32, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20096170173138628}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27296712342622304}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 99, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13682127961566426}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4355574414439555}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11512374354045503}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]
newly formed T structure is:[[0, 81, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 95, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2536073059624152}, 'layer_4_size': 79, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [1, 81, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 32, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20096170173138628}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [2, 81, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27296712342622304}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 99, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [3, 81, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13682127961566426}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4355574414439555}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [4, 81, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11512374354045503}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]] 

*** 5 configurations x 81.0 iterations each

2 | Fri Sep 28 01:25:58 2018 | lowest loss so far: 0.0705 (run 0)

{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  81 | activation: relu    | extras: None 
layer 2 | size:  47 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041fa06550>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 5:27 - loss: 0.4520
 416/6530 [>.............................] - ETA: 12s - loss: 0.2706 {'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  34 | activation: relu    | extras: None 
layer 2 | size:  32 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 5:30 - loss: 0.5759
 864/6530 [==>...........................] - ETA: 5s - loss: 0.2376 
 416/6530 [>.............................] - ETA: 12s - loss: 0.6811 
1328/6530 [=====>........................] - ETA: 3s - loss: 0.2171
 944/6530 [===>..........................] - ETA: 5s - loss: 0.5167 
1856/6530 [=======>......................] - ETA: 2s - loss: 0.2018
1440/6530 [=====>........................] - ETA: 3s - loss: 0.4217
2448/6530 [==========>...................] - ETA: 1s - loss: 0.1919
1936/6530 [=======>......................] - ETA: 2s - loss: 0.3726
3040/6530 [============>.................] - ETA: 1s - loss: 0.1857
2416/6530 [==========>...................] - ETA: 1s - loss: 0.3468
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1806
2944/6530 [============>.................] - ETA: 1s - loss: 0.3276
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1762
3472/6530 [==============>...............] - ETA: 1s - loss: 0.3095
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1732
3968/6530 [=================>............] - ETA: 0s - loss: 0.2956
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1698
4464/6530 [===================>..........] - ETA: 0s - loss: 0.2855
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1663
4896/6530 [=====================>........] - ETA: 0s - loss: 0.2778
6530/6530 [==============================] - 1s 221us/step - loss: 0.1638 - val_loss: 0.1481
Epoch 2/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1525{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  95 | activation: relu    | extras: batchnorm 
layer 2 | size:  29 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  64 | activation: tanh    | extras: None 
layer 4 | size:  79 | activation: sigmoid | extras: dropout - rate: 25.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

 256/6530 [>.............................] - ETA: 25s - loss: 0.5217
5408/6530 [=======================>......] - ETA: 0s - loss: 0.2691
 608/6530 [=>............................] - ETA: 0s - loss: 0.1362
4096/6530 [=================>............] - ETA: 0s - loss: 0.2121 
5920/6530 [==========================>...] - ETA: 0s - loss: 0.2611
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1371
6448/6530 [============================>.] - ETA: 0s - loss: 0.2540
6530/6530 [==============================] - 1s 177us/step - loss: 0.1777 - val_loss: 0.1405
Epoch 2/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1250
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1379
6530/6530 [==============================] - 2s 232us/step - loss: 0.2530 - val_loss: 0.1719
Epoch 2/81

  16/6530 [..............................] - ETA: 0s - loss: 0.2332
4096/6530 [=================>............] - ETA: 0s - loss: 0.1183
2256/6530 [=========>....................] - ETA: 0s - loss: 0.1342
 528/6530 [=>............................] - ETA: 0s - loss: 0.1732
6530/6530 [==============================] - 0s 14us/step - loss: 0.1164 - val_loss: 0.1310
Epoch 3/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1160
2848/6530 [============>.................] - ETA: 0s - loss: 0.1345
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1709
4096/6530 [=================>............] - ETA: 0s - loss: 0.1104
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1333
1552/6530 [======>.......................] - ETA: 0s - loss: 0.1724
6530/6530 [==============================] - 0s 14us/step - loss: 0.1076 - val_loss: 0.1357
Epoch 4/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0991
4048/6530 [=================>............] - ETA: 0s - loss: 0.1310
2064/6530 [========>.....................] - ETA: 0s - loss: 0.1686
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1025
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1310
6530/6530 [==============================] - 0s 13us/step - loss: 0.0993 - val_loss: 0.1560
Epoch 5/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0935
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1683
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1309
4096/6530 [=================>............] - ETA: 0s - loss: 0.0913
3088/6530 [=============>................] - ETA: 0s - loss: 0.1671
6530/6530 [==============================] - 0s 14us/step - loss: 0.0906 - val_loss: 0.1860
Epoch 6/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0919
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1305
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1670
3840/6530 [================>.............] - ETA: 0s - loss: 0.0849
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1303
4112/6530 [=================>............] - ETA: 0s - loss: 0.1660
6530/6530 [==============================] - 0s 15us/step - loss: 0.0855 - val_loss: 0.1592
Epoch 7/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0882
6530/6530 [==============================] - 1s 94us/step - loss: 0.1302 - val_loss: 0.1267
Epoch 3/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1091
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1664
4096/6530 [=================>............] - ETA: 0s - loss: 0.0846
 608/6530 [=>............................] - ETA: 0s - loss: 0.1237
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1662
6530/6530 [==============================] - 0s 14us/step - loss: 0.0841 - val_loss: 0.1513

1200/6530 [====>.........................] - ETA: 0s - loss: 0.1225
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1670
1808/6530 [=======>......................] - ETA: 0s - loss: 0.1233
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1666
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1201
6530/6530 [==============================] - 1s 102us/step - loss: 0.1667 - val_loss: 0.1613
Epoch 3/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1759
2976/6530 [============>.................] - ETA: 0s - loss: 0.1201
 576/6530 [=>............................] - ETA: 0s - loss: 0.1665
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1200
1104/6530 [====>.........................] - ETA: 0s - loss: 0.1617
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1206
1616/6530 [======>.......................] - ETA: 0s - loss: 0.1635
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1202
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1613
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1191
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1617
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1185
3248/6530 [=============>................] - ETA: 0s - loss: 0.1599
6530/6530 [==============================] - 1s 88us/step - loss: 0.1184 - val_loss: 0.1198
Epoch 4/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0846
3792/6530 [================>.............] - ETA: 0s - loss: 0.1601
 576/6530 [=>............................] - ETA: 0s - loss: 0.1085
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1601
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1080
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1611
1712/6530 [======>.......................] - ETA: 0s - loss: 0.1095
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1607
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1094
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1607
2912/6530 [============>.................] - ETA: 0s - loss: 0.1100
6448/6530 [============================>.] - ETA: 0s - loss: 0.1612
6530/6530 [==============================] - 1s 98us/step - loss: 0.1611 - val_loss: 0.1621
Epoch 4/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1359
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1096
 544/6530 [=>............................] - ETA: 0s - loss: 0.1588
4048/6530 [=================>............] - ETA: 0s - loss: 0.1094
1040/6530 [===>..........................] - ETA: 0s - loss: 0.1604
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1095
1552/6530 [======>.......................] - ETA: 0s - loss: 0.1586
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1091
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1560
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1091
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1581
6432/6530 [============================>.] - ETA: 0s - loss: 0.1089
6530/6530 [==============================] - 1s 90us/step - loss: 0.1089 - val_loss: 0.1193
Epoch 5/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1379
3152/6530 [=============>................] - ETA: 0s - loss: 0.1583
 624/6530 [=>............................] - ETA: 0s - loss: 0.1094
3696/6530 [===============>..............] - ETA: 0s - loss: 0.1590
1232/6530 [====>.........................] - ETA: 0s - loss: 0.1076
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1589
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1070
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1589
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1066
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1599
# training | RMSE: 0.1740, MAE: 0.1487
worker 0  xfile  [0, 81, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 95, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2536073059624152}, 'layer_4_size': 79, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.17395038013619699, 'rmse': 0.17395038013619699, 'mae': 0.14866187228985722, 'early_stop': True}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  83 | activation: relu    | extras: dropout - rate: 13.7% 
layer 2 | size:  51 | activation: relu    | extras: batchnorm 
layer 3 | size:  73 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  84 | activation: tanh    | extras: dropout - rate: 43.6% 
layer 5 | size:  96 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f041fa06780>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 3:34 - loss: 0.2508
3072/6530 [=============>................] - ETA: 0s - loss: 0.1050
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1601
 240/6530 [>.............................] - ETA: 15s - loss: 0.1380 
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1043
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1601
 480/6530 [=>............................] - ETA: 7s - loss: 0.1117 
6530/6530 [==============================] - 1s 101us/step - loss: 0.1600 - val_loss: 0.1612
Epoch 5/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1538
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1032
 720/6530 [==>...........................] - ETA: 5s - loss: 0.1000
 480/6530 [=>............................] - ETA: 0s - loss: 0.1589
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1031
 864/6530 [==>...........................] - ETA: 4s - loss: 0.0932
1008/6530 [===>..........................] - ETA: 0s - loss: 0.1628
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1022
1120/6530 [====>.........................] - ETA: 3s - loss: 0.0860
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1602
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1016
1360/6530 [=====>........................] - ETA: 3s - loss: 0.0812
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1599
6480/6530 [============================>.] - ETA: 0s - loss: 0.1015
1600/6530 [======>.......................] - ETA: 2s - loss: 0.0778
6530/6530 [==============================] - 1s 90us/step - loss: 0.1014 - val_loss: 0.1041
Epoch 6/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0982
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1587
1840/6530 [=======>......................] - ETA: 2s - loss: 0.0744
 640/6530 [=>............................] - ETA: 0s - loss: 0.0956
3040/6530 [============>.................] - ETA: 0s - loss: 0.1582
2080/6530 [========>.....................] - ETA: 2s - loss: 0.0721
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0980
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1576
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0707
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0994
4080/6530 [=================>............] - ETA: 0s - loss: 0.1583
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0987
2592/6530 [==========>...................] - ETA: 1s - loss: 0.0682
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1585
2912/6530 [============>.................] - ETA: 0s - loss: 0.0988
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0667
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1598
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0984
3040/6530 [============>.................] - ETA: 1s - loss: 0.0654
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1589
4016/6530 [=================>............] - ETA: 0s - loss: 0.0981
3264/6530 [=============>................] - ETA: 1s - loss: 0.0644
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1597
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0969
3504/6530 [===============>..............] - ETA: 1s - loss: 0.0634
6530/6530 [==============================] - 1s 104us/step - loss: 0.1599 - val_loss: 0.1618
Epoch 6/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1597
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0965
3744/6530 [================>.............] - ETA: 1s - loss: 0.0626
 528/6530 [=>............................] - ETA: 0s - loss: 0.1591
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0955
3984/6530 [=================>............] - ETA: 0s - loss: 0.0614
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1570
6400/6530 [============================>.] - ETA: 0s - loss: 0.0955
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0606
6530/6530 [==============================] - 1s 91us/step - loss: 0.0951 - val_loss: 0.1045
Epoch 7/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0971
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1567
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0595
 592/6530 [=>............................] - ETA: 0s - loss: 0.0912
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1569
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0589
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0924
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1556
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0583
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0951
3088/6530 [=============>................] - ETA: 0s - loss: 0.1565
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0578
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0931
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1574
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0573
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0929
4064/6530 [=================>............] - ETA: 0s - loss: 0.1582
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0568
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0917
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1585
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0566
3888/6530 [================>.............] - ETA: 0s - loss: 0.0913
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1596
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0561
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0908
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1598
6352/6530 [============================>.] - ETA: 0s - loss: 0.0555
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0906
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1594
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0909
6530/6530 [==============================] - 1s 104us/step - loss: 0.1594 - val_loss: 0.1635
Epoch 7/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1689
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0904
6530/6530 [==============================] - 2s 316us/step - loss: 0.0553 - val_loss: 0.0339
Epoch 2/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0738
 528/6530 [=>............................] - ETA: 0s - loss: 0.1585
6530/6530 [==============================] - 1s 94us/step - loss: 0.0903 - val_loss: 0.1071
Epoch 8/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1194
 256/6530 [>.............................] - ETA: 1s - loss: 0.0451
 992/6530 [===>..........................] - ETA: 0s - loss: 0.1569
 608/6530 [=>............................] - ETA: 0s - loss: 0.0879
 496/6530 [=>............................] - ETA: 1s - loss: 0.0458
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1599
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0893
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0451
1968/6530 [========>.....................] - ETA: 0s - loss: 0.1593
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0878
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0445
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1586
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0892
2976/6530 [============>.................] - ETA: 0s - loss: 0.1587
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0442
2928/6530 [============>.................] - ETA: 0s - loss: 0.0892
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1583
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0434
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0880
4032/6530 [=================>............] - ETA: 0s - loss: 0.1587
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0443
4128/6530 [=================>............] - ETA: 0s - loss: 0.0873
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1582
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0439
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0867
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1577
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0436
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0865
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1573
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0434
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0862
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1576
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0426
6416/6530 [============================>.] - ETA: 0s - loss: 0.0864
6528/6530 [============================>.] - ETA: 0s - loss: 0.1573
2832/6530 [============>.................] - ETA: 0s - loss: 0.0422
6530/6530 [==============================] - 1s 91us/step - loss: 0.0863 - val_loss: 0.1298
Epoch 9/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1157
6530/6530 [==============================] - 1s 105us/step - loss: 0.1573 - val_loss: 0.1602
Epoch 8/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1919
3072/6530 [=============>................] - ETA: 0s - loss: 0.0418
 592/6530 [=>............................] - ETA: 0s - loss: 0.0867
 528/6530 [=>............................] - ETA: 0s - loss: 0.1561
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0416
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0868
1040/6530 [===>..........................] - ETA: 0s - loss: 0.1620
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0415
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0859
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1606
3792/6530 [================>.............] - ETA: 0s - loss: 0.0412
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0853
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1601
2960/6530 [============>.................] - ETA: 0s - loss: 0.0852
4048/6530 [=================>............] - ETA: 0s - loss: 0.0408
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1593
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0851
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0409
3104/6530 [=============>................] - ETA: 0s - loss: 0.1600
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0837
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0411
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1600
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0838
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0408
4080/6530 [=================>............] - ETA: 0s - loss: 0.1589
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0837
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0405
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1585
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0834
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0405
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1577
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0404
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1571
6530/6530 [==============================] - 1s 89us/step - loss: 0.0835 - val_loss: 0.1087
Epoch 10/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0918
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0403
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1574
 640/6530 [=>............................] - ETA: 0s - loss: 0.0833
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0402
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0828
6530/6530 [==============================] - 1s 103us/step - loss: 0.1573 - val_loss: 0.1585
Epoch 9/81

  16/6530 [..............................] - ETA: 0s - loss: 0.2104
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0404
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0830
 544/6530 [=>............................] - ETA: 0s - loss: 0.1522
6496/6530 [============================>.] - ETA: 0s - loss: 0.0401
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0819
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1488
6530/6530 [==============================] - 1s 222us/step - loss: 0.0400 - val_loss: 0.0222
Epoch 3/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0224
3024/6530 [============>.................] - ETA: 0s - loss: 0.0816
1552/6530 [======>.......................] - ETA: 0s - loss: 0.1520
 240/6530 [>.............................] - ETA: 1s - loss: 0.0312
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0817
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1541
 480/6530 [=>............................] - ETA: 1s - loss: 0.0334
4112/6530 [=================>............] - ETA: 0s - loss: 0.0821
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1532
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0348
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0821
3120/6530 [=============>................] - ETA: 0s - loss: 0.1539
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0354
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0815
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1548
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0364
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0813
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1548
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0371
6400/6530 [============================>.] - ETA: 0s - loss: 0.0809
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1553
6530/6530 [==============================] - 1s 91us/step - loss: 0.0808 - val_loss: 0.0894
Epoch 11/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0962
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0377
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1545
 560/6530 [=>............................] - ETA: 0s - loss: 0.0788
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0380
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1546
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0772
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0379
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1555
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0789
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0374
6530/6530 [==============================] - 1s 103us/step - loss: 0.1557 - val_loss: 0.1593
Epoch 10/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1466
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0785
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0372
 448/6530 [=>............................] - ETA: 0s - loss: 0.1487
2912/6530 [============>.................] - ETA: 0s - loss: 0.0789
2832/6530 [============>.................] - ETA: 0s - loss: 0.0376
 976/6530 [===>..........................] - ETA: 0s - loss: 0.1567
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0786
3056/6530 [=============>................] - ETA: 0s - loss: 0.0374
1488/6530 [=====>........................] - ETA: 0s - loss: 0.1541
4096/6530 [=================>............] - ETA: 0s - loss: 0.0798
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0373
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1591
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0793
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0376
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1567
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0794
3760/6530 [================>.............] - ETA: 0s - loss: 0.0376
2992/6530 [============>.................] - ETA: 0s - loss: 0.1572
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0790
4000/6530 [=================>............] - ETA: 0s - loss: 0.0374
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1563
6496/6530 [============================>.] - ETA: 0s - loss: 0.0786
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0371
6530/6530 [==============================] - 1s 90us/step - loss: 0.0786 - val_loss: 0.0985
Epoch 12/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0885
4016/6530 [=================>............] - ETA: 0s - loss: 0.1567
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0368
 608/6530 [=>............................] - ETA: 0s - loss: 0.0782
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1559
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0366
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0773
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1554
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0367
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0772
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1556
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0369
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0758
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1552
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0371
2992/6530 [============>.................] - ETA: 0s - loss: 0.0760
6530/6530 [==============================] - 1s 103us/step - loss: 0.1550 - val_loss: 0.1568
Epoch 11/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1543
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0371
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0760
 528/6530 [=>............................] - ETA: 0s - loss: 0.1557
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0369
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0757
1072/6530 [===>..........................] - ETA: 0s - loss: 0.1531
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0368
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0755
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1549
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0760
6400/6530 [============================>.] - ETA: 0s - loss: 0.0366
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1529
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0759
6530/6530 [==============================] - 1s 225us/step - loss: 0.0364 - val_loss: 0.0244

2640/6530 [===========>..................] - ETA: 0s - loss: 0.1537Epoch 4/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0271
6528/6530 [============================>.] - ETA: 0s - loss: 0.0759
6530/6530 [==============================] - 1s 89us/step - loss: 0.0759 - val_loss: 0.1045
Epoch 13/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1182
3168/6530 [=============>................] - ETA: 0s - loss: 0.1550
 256/6530 [>.............................] - ETA: 1s - loss: 0.0333
 608/6530 [=>............................] - ETA: 0s - loss: 0.0749
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1534
 480/6530 [=>............................] - ETA: 1s - loss: 0.0344
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0745
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1536
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0342
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0739
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1541
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0330
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0750
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1533
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0340
2992/6530 [============>.................] - ETA: 0s - loss: 0.0749
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1531
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0339
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0745
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1522
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0335
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0755
6530/6530 [==============================] - 1s 102us/step - loss: 0.1514 - val_loss: 0.1509
Epoch 12/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1247
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0332
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0754
 528/6530 [=>............................] - ETA: 0s - loss: 0.1428
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0333
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0750
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1487
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0334
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0746
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1483
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0338
2064/6530 [========>.....................] - ETA: 0s - loss: 0.1477
2896/6530 [============>.................] - ETA: 0s - loss: 0.0341
6530/6530 [==============================] - 1s 88us/step - loss: 0.0743 - val_loss: 0.0798
Epoch 14/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0647
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1445
3136/6530 [=============>................] - ETA: 0s - loss: 0.0338
 624/6530 [=>............................] - ETA: 0s - loss: 0.0742
3056/6530 [=============>................] - ETA: 0s - loss: 0.1436
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0733
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0338
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1418
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0731
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0343
4096/6530 [=================>............] - ETA: 0s - loss: 0.1403
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0737
3872/6530 [================>.............] - ETA: 0s - loss: 0.0344
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1390
2960/6530 [============>.................] - ETA: 0s - loss: 0.0734
4112/6530 [=================>............] - ETA: 0s - loss: 0.0343
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1380
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0726
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0343
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1373
4080/6530 [=================>............] - ETA: 0s - loss: 0.0727
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0340
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1365
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0724
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0341
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0727
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0338
6530/6530 [==============================] - 1s 104us/step - loss: 0.1355 - val_loss: 0.1383
Epoch 13/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1870
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0730
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0338
 480/6530 [=>............................] - ETA: 0s - loss: 0.1357
6432/6530 [============================>.] - ETA: 0s - loss: 0.0729
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0337
 992/6530 [===>..........................] - ETA: 0s - loss: 0.1248
6530/6530 [==============================] - 1s 91us/step - loss: 0.0730 - val_loss: 0.0828
Epoch 15/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0571
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0335
1456/6530 [=====>........................] - ETA: 0s - loss: 0.1245
 608/6530 [=>............................] - ETA: 0s - loss: 0.0680
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0335
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1251
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0696
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0333
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1240
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0695
6448/6530 [============================>.] - ETA: 0s - loss: 0.0331
2992/6530 [============>.................] - ETA: 0s - loss: 0.1240
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0698
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1246
6530/6530 [==============================] - 1s 223us/step - loss: 0.0332 - val_loss: 0.0211
Epoch 5/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0189
2992/6530 [============>.................] - ETA: 0s - loss: 0.0693
3968/6530 [=================>............] - ETA: 0s - loss: 0.1246
 240/6530 [>.............................] - ETA: 1s - loss: 0.0303
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0700
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1243
 480/6530 [=>............................] - ETA: 1s - loss: 0.0288
4128/6530 [=================>............] - ETA: 0s - loss: 0.0701
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1240
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0307
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0706
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1228
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0312
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0710
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1227
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0312
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0710
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0308
6530/6530 [==============================] - 1s 105us/step - loss: 0.1224 - val_loss: 0.1266
Epoch 14/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0982
6528/6530 [============================>.] - ETA: 0s - loss: 0.0709
6530/6530 [==============================] - 1s 90us/step - loss: 0.0709 - val_loss: 0.0777
Epoch 16/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0585
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0302
 528/6530 [=>............................] - ETA: 0s - loss: 0.1189
 592/6530 [=>............................] - ETA: 0s - loss: 0.0666
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0307
1008/6530 [===>..........................] - ETA: 0s - loss: 0.1192
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0667
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0305
1520/6530 [=====>........................] - ETA: 0s - loss: 0.1180
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0688
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0311
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1183
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0691
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0310
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1183
2880/6530 [============>.................] - ETA: 0s - loss: 0.0690
2848/6530 [============>.................] - ETA: 0s - loss: 0.0312
3072/6530 [=============>................] - ETA: 0s - loss: 0.1188
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0691
3088/6530 [=============>................] - ETA: 0s - loss: 0.0316
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1181
4064/6530 [=================>............] - ETA: 0s - loss: 0.0686
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0317
4096/6530 [=================>............] - ETA: 0s - loss: 0.1187
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0687
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1186
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0320
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0689
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1187
3808/6530 [================>.............] - ETA: 0s - loss: 0.0319
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0692
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1177
4064/6530 [=================>............] - ETA: 0s - loss: 0.0318
6448/6530 [============================>.] - ETA: 0s - loss: 0.0696
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1177
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0319
6530/6530 [==============================] - 1s 90us/step - loss: 0.0695 - val_loss: 0.0765
Epoch 17/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0802
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0319
 608/6530 [=>............................] - ETA: 0s - loss: 0.0710
6530/6530 [==============================] - 1s 104us/step - loss: 0.1180 - val_loss: 0.1251
Epoch 15/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0838
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0321
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0718
 528/6530 [=>............................] - ETA: 0s - loss: 0.1110
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0321
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0710
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1147
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0320
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0704
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1162
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0320
2944/6530 [============>.................] - ETA: 0s - loss: 0.0704
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1147
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0320
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0704
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1145
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0321
4112/6530 [=================>............] - ETA: 0s - loss: 0.0704
3136/6530 [=============>................] - ETA: 0s - loss: 0.1144
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0321
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0701
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1151
6464/6530 [============================>.] - ETA: 0s - loss: 0.0321
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0697
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1150
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0696
6530/6530 [==============================] - 1s 222us/step - loss: 0.0321 - val_loss: 0.0170
Epoch 6/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0134
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1149
6512/6530 [============================>.] - ETA: 0s - loss: 0.0695
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1158
 272/6530 [>.............................] - ETA: 1s - loss: 0.0296
6530/6530 [==============================] - 1s 89us/step - loss: 0.0695 - val_loss: 0.1136
Epoch 18/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0956
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1165
 512/6530 [=>............................] - ETA: 1s - loss: 0.0292
 592/6530 [=>............................] - ETA: 0s - loss: 0.0662
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1166
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0312
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0672
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0314
6530/6530 [==============================] - 1s 103us/step - loss: 0.1167 - val_loss: 0.1218
Epoch 16/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0808
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0683
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0326
 528/6530 [=>............................] - ETA: 0s - loss: 0.1170
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0681
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0325
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1132
2944/6530 [============>.................] - ETA: 0s - loss: 0.0688
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0324
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1127
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0683
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0327
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1129
4128/6530 [=================>............] - ETA: 0s - loss: 0.0687
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1151
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0338
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0682
3088/6530 [=============>................] - ETA: 0s - loss: 0.1160
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0345
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0682
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1159
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0342
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0682
4128/6530 [=================>............] - ETA: 0s - loss: 0.1157
2912/6530 [============>.................] - ETA: 0s - loss: 0.0339
6448/6530 [============================>.] - ETA: 0s - loss: 0.0683
6530/6530 [==============================] - 1s 90us/step - loss: 0.0683 - val_loss: 0.0776
Epoch 19/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0756
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1149
3152/6530 [=============>................] - ETA: 0s - loss: 0.0337
 608/6530 [=>............................] - ETA: 0s - loss: 0.0638
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1148
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0335
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0656
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1147
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0333
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0665
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1152
3888/6530 [================>.............] - ETA: 0s - loss: 0.0329
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0667
4128/6530 [=================>............] - ETA: 0s - loss: 0.0325
6530/6530 [==============================] - 1s 104us/step - loss: 0.1153 - val_loss: 0.1239
Epoch 17/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1268
3040/6530 [============>.................] - ETA: 0s - loss: 0.0668
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0325
 544/6530 [=>............................] - ETA: 0s - loss: 0.1150
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0671
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0327
1072/6530 [===>..........................] - ETA: 0s - loss: 0.1166
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0673
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0326
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1151
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0673
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0325
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1124
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0670
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0325
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1123
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0672
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0325
3136/6530 [=============>................] - ETA: 0s - loss: 0.1133
6530/6530 [==============================] - 1s 88us/step - loss: 0.0675 - val_loss: 0.0940
Epoch 20/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1025
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0324
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1138
 624/6530 [=>............................] - ETA: 0s - loss: 0.0644
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0325
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1146
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0665
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0324
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1149
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0656
6528/6530 [============================>.] - ETA: 0s - loss: 0.0323
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1144
6530/6530 [==============================] - 1s 220us/step - loss: 0.0323 - val_loss: 0.0166
Epoch 7/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0342
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0663
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1137
3040/6530 [============>.................] - ETA: 0s - loss: 0.0660
 256/6530 [>.............................] - ETA: 1s - loss: 0.0292
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1135
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0660
 496/6530 [=>............................] - ETA: 1s - loss: 0.0295
6530/6530 [==============================] - 1s 102us/step - loss: 0.1136 - val_loss: 0.1190
Epoch 18/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0807
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0661
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0305
 544/6530 [=>............................] - ETA: 0s - loss: 0.1113
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0657
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0300
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1089
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0660
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0301
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1119
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0662
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0296
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1117
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0305
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1111
6530/6530 [==============================] - 1s 88us/step - loss: 0.0666 - val_loss: 0.0733
Epoch 21/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0728
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0301
3120/6530 [=============>................] - ETA: 0s - loss: 0.1127
 592/6530 [=>............................] - ETA: 0s - loss: 0.0675
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0302
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1122
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0673
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0305
4128/6530 [=================>............] - ETA: 0s - loss: 0.1130
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0681
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0302
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1121
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0683
2896/6530 [============>.................] - ETA: 0s - loss: 0.0298
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1123
2928/6530 [============>.................] - ETA: 0s - loss: 0.0675
3120/6530 [=============>................] - ETA: 0s - loss: 0.0299
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1123
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0670
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0296
4112/6530 [=================>............] - ETA: 0s - loss: 0.0667
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1117
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0294
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0663
6530/6530 [==============================] - 1s 104us/step - loss: 0.1119 - val_loss: 0.1183
Epoch 19/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1195
3840/6530 [================>.............] - ETA: 0s - loss: 0.0297
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0661
 496/6530 [=>............................] - ETA: 0s - loss: 0.1069
4080/6530 [=================>............] - ETA: 0s - loss: 0.0298
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0661
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1102
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0298
6400/6530 [============================>.] - ETA: 0s - loss: 0.0663
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1110
6530/6530 [==============================] - 1s 91us/step - loss: 0.0663 - val_loss: 0.0697
Epoch 22/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0577
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0296
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1084
 608/6530 [=>............................] - ETA: 0s - loss: 0.0647
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0298
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1099
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0636
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0297
3104/6530 [=============>................] - ETA: 0s - loss: 0.1109
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0644
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0296
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1097
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0648
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0300
4128/6530 [=================>............] - ETA: 0s - loss: 0.1101
2992/6530 [============>.................] - ETA: 0s - loss: 0.0652
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0299
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1103
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0661
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0302
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1102
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0665
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0302
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1103
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0661
6464/6530 [============================>.] - ETA: 0s - loss: 0.0303
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1105
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0659
6530/6530 [==============================] - 1s 223us/step - loss: 0.0303 - val_loss: 0.0171
Epoch 8/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0186
6530/6530 [==============================] - 1s 102us/step - loss: 0.1107 - val_loss: 0.1143
Epoch 20/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1007
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0658
 256/6530 [>.............................] - ETA: 1s - loss: 0.0339
 528/6530 [=>............................] - ETA: 0s - loss: 0.1044
6530/6530 [==============================] - 1s 89us/step - loss: 0.0659 - val_loss: 0.0910
Epoch 23/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1091
 496/6530 [=>............................] - ETA: 1s - loss: 0.0291
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1110
 624/6530 [=>............................] - ETA: 0s - loss: 0.0634
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0304
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1083
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0640
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0297
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1087
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0634
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0298
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1094
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0639
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0296
3024/6530 [============>.................] - ETA: 0s - loss: 0.1090
3024/6530 [============>.................] - ETA: 0s - loss: 0.0648
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0303
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1087
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0650
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0300
4032/6530 [=================>............] - ETA: 0s - loss: 0.1079
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0653
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0305
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1077
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0655
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0301
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1079
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0653
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0299
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1080
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0655
2896/6530 [============>.................] - ETA: 0s - loss: 0.0298
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1079
3152/6530 [=============>................] - ETA: 0s - loss: 0.0294
6530/6530 [==============================] - 1s 89us/step - loss: 0.0654 - val_loss: 0.0713
Epoch 24/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0721
6530/6530 [==============================] - 1s 105us/step - loss: 0.1079 - val_loss: 0.1125
Epoch 21/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1080
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0299
 576/6530 [=>............................] - ETA: 0s - loss: 0.0673
 528/6530 [=>............................] - ETA: 0s - loss: 0.1137
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0295
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0668
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1096
3872/6530 [================>.............] - ETA: 0s - loss: 0.0292
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0659
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1077
4112/6530 [=================>............] - ETA: 0s - loss: 0.0292
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0656
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1084
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0292
2992/6530 [============>.................] - ETA: 0s - loss: 0.0647
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1065
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0292
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0653
3104/6530 [=============>................] - ETA: 0s - loss: 0.1060
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0294
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0648
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1056
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0294
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0646
4064/6530 [=================>............] - ETA: 0s - loss: 0.1057
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0649
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0293
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1061
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0646
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0296
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1057
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0298
6530/6530 [==============================] - 1s 88us/step - loss: 0.0646 - val_loss: 0.0856
Epoch 25/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0876
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1059
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0298
 608/6530 [=>............................] - ETA: 0s - loss: 0.0645
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1061
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0298
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0640
6530/6530 [==============================] - 1s 104us/step - loss: 0.1058 - val_loss: 0.1128
Epoch 22/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1135
6512/6530 [============================>.] - ETA: 0s - loss: 0.0296
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0643
 528/6530 [=>............................] - ETA: 0s - loss: 0.1063
6530/6530 [==============================] - 1s 221us/step - loss: 0.0297 - val_loss: 0.0176
Epoch 9/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0507
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0643
1040/6530 [===>..........................] - ETA: 0s - loss: 0.1039
 256/6530 [>.............................] - ETA: 1s - loss: 0.0272
2960/6530 [============>.................] - ETA: 0s - loss: 0.0640
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1045
 496/6530 [=>............................] - ETA: 1s - loss: 0.0270
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0638
2064/6530 [========>.....................] - ETA: 0s - loss: 0.1034
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0279
4080/6530 [=================>............] - ETA: 0s - loss: 0.0639
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1040
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0283
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0641
3040/6530 [============>.................] - ETA: 0s - loss: 0.1049
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0290
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0641
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1045
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0284
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0641
4048/6530 [=================>............] - ETA: 0s - loss: 0.1048
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0287
6464/6530 [============================>.] - ETA: 0s - loss: 0.0640
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1046
6530/6530 [==============================] - 1s 90us/step - loss: 0.0641 - val_loss: 0.0773
Epoch 26/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0732
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0287
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1039
 624/6530 [=>............................] - ETA: 0s - loss: 0.0645
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0285
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1035
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0646
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0290
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1040
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0644
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0290
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0647
6530/6530 [==============================] - 1s 104us/step - loss: 0.1043 - val_loss: 0.1335
Epoch 23/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1477
2880/6530 [============>.................] - ETA: 0s - loss: 0.0291
2928/6530 [============>.................] - ETA: 0s - loss: 0.0648
 544/6530 [=>............................] - ETA: 0s - loss: 0.1035
3136/6530 [=============>................] - ETA: 0s - loss: 0.0291
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0642
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0980
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0294
4032/6530 [=================>............] - ETA: 0s - loss: 0.0644
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0993
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0294
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0644
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0982
3856/6530 [================>.............] - ETA: 0s - loss: 0.0294
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0641
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0988
4096/6530 [=================>............] - ETA: 0s - loss: 0.0292
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0642
3056/6530 [=============>................] - ETA: 0s - loss: 0.1000
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0298
6400/6530 [============================>.] - ETA: 0s - loss: 0.0639
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1007
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0297
6530/6530 [==============================] - 1s 91us/step - loss: 0.0638 - val_loss: 0.0688
Epoch 27/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0406
4032/6530 [=================>............] - ETA: 0s - loss: 0.1012
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0299
 624/6530 [=>............................] - ETA: 0s - loss: 0.0606
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1015
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0623
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0299
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1015
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0630
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0299
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1013
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0628
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0296
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1017
2960/6530 [============>.................] - ETA: 0s - loss: 0.0627
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0297
6530/6530 [==============================] - 1s 104us/step - loss: 0.1017 - val_loss: 0.1073
Epoch 24/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1028
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0630
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0295
 544/6530 [=>............................] - ETA: 0s - loss: 0.0991
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0633
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0295
1040/6530 [===>..........................] - ETA: 0s - loss: 0.1003
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0631
6448/6530 [============================>.] - ETA: 0s - loss: 0.0294
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0991
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0628
6530/6530 [==============================] - 1s 225us/step - loss: 0.0294 - val_loss: 0.0155
Epoch 10/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0490
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0995
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0629
 256/6530 [>.............................] - ETA: 1s - loss: 0.0283
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0997
6496/6530 [============================>.] - ETA: 0s - loss: 0.0629
 496/6530 [=>............................] - ETA: 1s - loss: 0.0277
6530/6530 [==============================] - 1s 90us/step - loss: 0.0629 - val_loss: 0.0694
Epoch 28/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0618
3072/6530 [=============>................] - ETA: 0s - loss: 0.0992
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0267
 608/6530 [=>............................] - ETA: 0s - loss: 0.0612
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0995
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0277
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0613
4032/6530 [=================>............] - ETA: 0s - loss: 0.1004
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0288
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0618
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0996
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0283
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0625
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0993
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0283
2960/6530 [============>.................] - ETA: 0s - loss: 0.0626
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0990
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0283
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0625
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0984
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0282
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0628
6530/6530 [==============================] - 1s 103us/step - loss: 0.0982 - val_loss: 0.1044
Epoch 25/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1140
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0285
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0625
 528/6530 [=>............................] - ETA: 0s - loss: 0.1001
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0286
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0624
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1001
2832/6530 [============>.................] - ETA: 0s - loss: 0.0288
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0625
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0990
3056/6530 [=============>................] - ETA: 0s - loss: 0.0289
6480/6530 [============================>.] - ETA: 0s - loss: 0.0632
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0999
6530/6530 [==============================] - 1s 91us/step - loss: 0.0631 - val_loss: 0.0696
Epoch 29/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0518
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0289
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0998
 608/6530 [=>............................] - ETA: 0s - loss: 0.0635
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0288
3056/6530 [=============>................] - ETA: 0s - loss: 0.0987
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0620
3792/6530 [================>.............] - ETA: 0s - loss: 0.0287
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0981
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0619
4048/6530 [=================>............] - ETA: 0s - loss: 0.0284
4096/6530 [=================>............] - ETA: 0s - loss: 0.0979
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0620
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0284
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0975
2944/6530 [============>.................] - ETA: 0s - loss: 0.0621
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0285
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0975
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0618
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0286
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0975
4064/6530 [=================>............] - ETA: 0s - loss: 0.0620
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0283
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0976
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0626
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0284
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0626
6530/6530 [==============================] - 1s 103us/step - loss: 0.0972 - val_loss: 0.0989
Epoch 26/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1002
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0285
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0631
 544/6530 [=>............................] - ETA: 0s - loss: 0.0944
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0285
6416/6530 [============================>.] - ETA: 0s - loss: 0.0628
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0955
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0284
6530/6530 [==============================] - 1s 91us/step - loss: 0.0628 - val_loss: 0.0877
Epoch 30/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0945
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0939
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0285
 592/6530 [=>............................] - ETA: 0s - loss: 0.0581
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0949
6448/6530 [============================>.] - ETA: 0s - loss: 0.0286
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0580
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0950
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0598
6530/6530 [==============================] - 1s 223us/step - loss: 0.0286 - val_loss: 0.0151
Epoch 11/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0225
3040/6530 [============>.................] - ETA: 0s - loss: 0.0948
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0616
 256/6530 [>.............................] - ETA: 1s - loss: 0.0289
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0948
2960/6530 [============>.................] - ETA: 0s - loss: 0.0621
 496/6530 [=>............................] - ETA: 1s - loss: 0.0281
4064/6530 [=================>............] - ETA: 0s - loss: 0.0944
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0624
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0276
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0943
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0624
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0281
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0944
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0622
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0278
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0943
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0620
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0276
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0945
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0624
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0271
6530/6530 [==============================] - 1s 106us/step - loss: 0.0945 - val_loss: 0.1037
Epoch 27/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0615
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0268
6530/6530 [==============================] - 1s 89us/step - loss: 0.0626 - val_loss: 0.0669
Epoch 31/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0459
 544/6530 [=>............................] - ETA: 0s - loss: 0.0901
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0270
 592/6530 [=>............................] - ETA: 0s - loss: 0.0601
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0947
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0269
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0602
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0944
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0273
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0608
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0937
2880/6530 [============>.................] - ETA: 0s - loss: 0.0274
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0607
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0945
3120/6530 [=============>................] - ETA: 0s - loss: 0.0276
2832/6530 [============>.................] - ETA: 0s - loss: 0.0599
3088/6530 [=============>................] - ETA: 0s - loss: 0.0941
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0603
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0275
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0941
3920/6530 [=================>............] - ETA: 0s - loss: 0.0603
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0273
4080/6530 [=================>............] - ETA: 0s - loss: 0.0939
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0611
3840/6530 [================>.............] - ETA: 0s - loss: 0.0272
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0938
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0610
4064/6530 [=================>............] - ETA: 0s - loss: 0.0271
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0926
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0616
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0274
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0927
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0617
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0274
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0923
6530/6530 [==============================] - 1s 93us/step - loss: 0.0619 - val_loss: 0.0792
Epoch 32/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0836
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0280
6530/6530 [==============================] - 1s 104us/step - loss: 0.0922 - val_loss: 0.0947
Epoch 28/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1051
 560/6530 [=>............................] - ETA: 0s - loss: 0.0637
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0279
 512/6530 [=>............................] - ETA: 0s - loss: 0.1004
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0628
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0280
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0966
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0614
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0280
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0614
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0933
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0278
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0922
2960/6530 [============>.................] - ETA: 0s - loss: 0.0613
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0277
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0922
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0615
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0277
3088/6530 [=============>................] - ETA: 0s - loss: 0.0912
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0611
6432/6530 [============================>.] - ETA: 0s - loss: 0.0279
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0906
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0610
6530/6530 [==============================] - 1s 224us/step - loss: 0.0279 - val_loss: 0.0146
Epoch 12/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0265
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0909
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0617
 256/6530 [>.............................] - ETA: 1s - loss: 0.0257
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0619
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0901
 496/6530 [=>............................] - ETA: 1s - loss: 0.0281
6512/6530 [============================>.] - ETA: 0s - loss: 0.0619
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0907
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0290
6530/6530 [==============================] - 1s 90us/step - loss: 0.0619 - val_loss: 0.0686
Epoch 33/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0909
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0904
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0300
 608/6530 [=>............................] - ETA: 0s - loss: 0.0621
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0906
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0292
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0601
6530/6530 [==============================] - 1s 104us/step - loss: 0.0907 - val_loss: 0.0923
Epoch 29/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0711
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0284
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0583
 528/6530 [=>............................] - ETA: 0s - loss: 0.0898
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0278
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0602
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0895
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0277
3024/6530 [============>.................] - ETA: 0s - loss: 0.0600
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0878
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0274
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0600
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0888
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0275
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0604
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0887
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0276
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0607
3072/6530 [=============>................] - ETA: 0s - loss: 0.0887
2880/6530 [============>.................] - ETA: 0s - loss: 0.0278
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0610
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0880
3120/6530 [=============>................] - ETA: 0s - loss: 0.0274
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0616
4032/6530 [=================>............] - ETA: 0s - loss: 0.0877
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0271
6530/6530 [==============================] - 1s 90us/step - loss: 0.0617 - val_loss: 0.0820
Epoch 34/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0767
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0877
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0269
 624/6530 [=>............................] - ETA: 0s - loss: 0.0620
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0885
3808/6530 [================>.............] - ETA: 0s - loss: 0.0270
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0618
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0886
4016/6530 [=================>............] - ETA: 0s - loss: 0.0267
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0612
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0894
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0271
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0607
6496/6530 [============================>.] - ETA: 0s - loss: 0.0888
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0273
6530/6530 [==============================] - 1s 106us/step - loss: 0.0888 - val_loss: 0.0930
Epoch 30/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0819
2960/6530 [============>.................] - ETA: 0s - loss: 0.0611
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0272
 512/6530 [=>............................] - ETA: 0s - loss: 0.0829
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0613
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0273
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0844
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0617
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0272
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0840
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0616
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0274
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0867
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0615
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0275
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0875
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0616
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0274
3088/6530 [=============>................] - ETA: 0s - loss: 0.0878
6496/6530 [============================>.] - ETA: 0s - loss: 0.0616
6530/6530 [==============================] - 1s 90us/step - loss: 0.0616 - val_loss: 0.0878
Epoch 35/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0449
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0274
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0879
 560/6530 [=>............................] - ETA: 0s - loss: 0.0605
6400/6530 [============================>.] - ETA: 0s - loss: 0.0274
4080/6530 [=================>............] - ETA: 0s - loss: 0.0879
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0612
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0878
6530/6530 [==============================] - 1s 226us/step - loss: 0.0275 - val_loss: 0.0157
Epoch 13/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0446
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0610
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0879
 240/6530 [>.............................] - ETA: 1s - loss: 0.0345
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0602
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0888
 480/6530 [=>............................] - ETA: 1s - loss: 0.0308
2864/6530 [============>.................] - ETA: 0s - loss: 0.0604
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0886
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0314
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0612
6530/6530 [==============================] - 1s 104us/step - loss: 0.0887 - val_loss: 0.0898
Epoch 31/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0659
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0300
4032/6530 [=================>............] - ETA: 0s - loss: 0.0618
 528/6530 [=>............................] - ETA: 0s - loss: 0.0794
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0295
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0621
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0846
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0293
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0621
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0845
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0290
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0621
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0845
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0279
6384/6530 [============================>.] - ETA: 0s - loss: 0.0618
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0845
6530/6530 [==============================] - 1s 91us/step - loss: 0.0617 - val_loss: 0.0798

2144/6530 [========>.....................] - ETA: 0s - loss: 0.0284
3104/6530 [=============>................] - ETA: 0s - loss: 0.0840
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0281
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0845
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0280
4128/6530 [=================>............] - ETA: 0s - loss: 0.0848
2880/6530 [============>.................] - ETA: 0s - loss: 0.0277
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0851
3120/6530 [=============>................] - ETA: 0s - loss: 0.0276
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0846
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0277
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0850
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0275
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0852
3856/6530 [================>.............] - ETA: 0s - loss: 0.0276
6530/6530 [==============================] - 1s 103us/step - loss: 0.0852 - val_loss: 0.0883
Epoch 32/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0853
4112/6530 [=================>............] - ETA: 0s - loss: 0.0276
 560/6530 [=>............................] - ETA: 0s - loss: 0.0852
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0276
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0821
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0275
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0823
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0275
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0831
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0272
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0831
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0271
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0837
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0271
3872/6530 [================>.............] - ETA: 0s - loss: 0.0833
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0272
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0828
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0270
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0832
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0268
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0835
6512/6530 [============================>.] - ETA: 0s - loss: 0.0268
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0833
6530/6530 [==============================] - 1s 221us/step - loss: 0.0268 - val_loss: 0.0143
Epoch 14/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0255
 272/6530 [>.............................] - ETA: 1s - loss: 0.0265
6530/6530 [==============================] - 1s 96us/step - loss: 0.0835 - val_loss: 0.0874
Epoch 33/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0982
 512/6530 [=>............................] - ETA: 1s - loss: 0.0274
 560/6530 [=>............................] - ETA: 0s - loss: 0.0829
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0270
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0827
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0263
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0838
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0268
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0829
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0265
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0842
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0265
3184/6530 [=============>................] - ETA: 0s - loss: 0.0849
3728/6530 [================>.............] - ETA: 0s - loss: 0.0833
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0269
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0836
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0276
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0841
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0279
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0844
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0275
# training | RMSE: 0.0901, MAE: 0.0706
worker 2  xfile  [2, 81, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27296712342622304}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 99, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.09014438947307063, 'rmse': 0.09014438947307063, 'mae': 0.070645004799532, 'early_stop': True}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  58 | activation: tanh    | extras: None 
layer 2 | size:  86 | activation: sigmoid | extras: None 
layer 3 | size:  96 | activation: sigmoid | extras: batchnorm 
layer 4 | size:   7 | activation: relu    | extras: batchnorm 
layer 5 | size:  24 | activation: tanh    | extras: dropout - rate: 11.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0494c9c240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 4:00 - loss: 1.3827
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0843
2912/6530 [============>.................] - ETA: 0s - loss: 0.0275
 224/6530 [>.............................] - ETA: 18s - loss: 0.7254 
6320/6530 [============================>.] - ETA: 0s - loss: 0.0842
3152/6530 [=============>................] - ETA: 0s - loss: 0.0276
 432/6530 [>.............................] - ETA: 9s - loss: 0.5629 
6530/6530 [==============================] - 1s 100us/step - loss: 0.0843 - val_loss: 0.0887
Epoch 34/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0636
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0272
 656/6530 [==>...........................] - ETA: 6s - loss: 0.4533
 528/6530 [=>............................] - ETA: 0s - loss: 0.0831
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0275
 880/6530 [===>..........................] - ETA: 5s - loss: 0.3800
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0816
3872/6530 [================>.............] - ETA: 0s - loss: 0.0275
1088/6530 [===>..........................] - ETA: 4s - loss: 0.3278
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0820
4096/6530 [=================>............] - ETA: 0s - loss: 0.0277
1296/6530 [====>.........................] - ETA: 3s - loss: 0.2926
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0814
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0275
1504/6530 [=====>........................] - ETA: 3s - loss: 0.2632
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0811
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0273
1728/6530 [======>.......................] - ETA: 2s - loss: 0.2400
3056/6530 [=============>................] - ETA: 0s - loss: 0.0811
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0274
1936/6530 [=======>......................] - ETA: 2s - loss: 0.2222
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0811
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0274
2160/6530 [========>.....................] - ETA: 2s - loss: 0.2065
4096/6530 [=================>............] - ETA: 0s - loss: 0.0810
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0274
2368/6530 [=========>....................] - ETA: 2s - loss: 0.1950
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0809
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0272
2592/6530 [==========>...................] - ETA: 1s - loss: 0.1844
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0811
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0271
2800/6530 [===========>..................] - ETA: 1s - loss: 0.1748
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0815
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0270
3024/6530 [============>.................] - ETA: 1s - loss: 0.1664
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0822
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0269
3248/6530 [=============>................] - ETA: 1s - loss: 0.1586
6530/6530 [==============================] - 1s 103us/step - loss: 0.0825 - val_loss: 0.0872
Epoch 35/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0676
6496/6530 [============================>.] - ETA: 0s - loss: 0.0269
3456/6530 [==============>...............] - ETA: 1s - loss: 0.1529
 464/6530 [=>............................] - ETA: 0s - loss: 0.0844
6530/6530 [==============================] - 1s 221us/step - loss: 0.0269 - val_loss: 0.0140
Epoch 15/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0249
3664/6530 [===============>..............] - ETA: 1s - loss: 0.1471
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0840
 240/6530 [>.............................] - ETA: 1s - loss: 0.0286
3888/6530 [================>.............] - ETA: 1s - loss: 0.1422
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0830
 480/6530 [=>............................] - ETA: 1s - loss: 0.0257
4112/6530 [=================>............] - ETA: 0s - loss: 0.1373
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0825
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0240
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1328
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0822
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0258
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1291
2976/6530 [============>.................] - ETA: 0s - loss: 0.0820
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0257
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1256
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0823
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0261
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1226
4000/6530 [=================>............] - ETA: 0s - loss: 0.0822
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0265
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1200
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0821
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0264
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1173
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0819
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0262
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1148
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0811
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0254
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1126
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0812
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0253
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1102
6530/6530 [==============================] - 1s 104us/step - loss: 0.0809 - val_loss: 0.0846
Epoch 36/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0824
2880/6530 [============>.................] - ETA: 0s - loss: 0.0260
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1082
 528/6530 [=>............................] - ETA: 0s - loss: 0.0827
3120/6530 [=============>................] - ETA: 0s - loss: 0.0259
6480/6530 [============================>.] - ETA: 0s - loss: 0.1062
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0819
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0257
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0804
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0255
6530/6530 [==============================] - 2s 344us/step - loss: 0.1058 - val_loss: 0.0764
Epoch 2/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0631
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0795
3872/6530 [================>.............] - ETA: 0s - loss: 0.0257
 240/6530 [>.............................] - ETA: 1s - loss: 0.0488
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0783
4096/6530 [=================>............] - ETA: 0s - loss: 0.0259
 464/6530 [=>............................] - ETA: 1s - loss: 0.0507
3088/6530 [=============>................] - ETA: 0s - loss: 0.0788
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0259
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0493
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0792
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0259
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0478
4096/6530 [=================>............] - ETA: 0s - loss: 0.0792
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0258
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0487
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0795
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0259
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0484
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0798
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0262
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0470
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0801
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0261
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0470
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0802
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0263
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0465
6530/6530 [==============================] - 1s 103us/step - loss: 0.0807 - val_loss: 0.0840
Epoch 37/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0850
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0262
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0465
 544/6530 [=>............................] - ETA: 0s - loss: 0.0854
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0262
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0466
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0827
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0471
6530/6530 [==============================] - 1s 221us/step - loss: 0.0261 - val_loss: 0.0146
Epoch 16/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0353
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0835
2832/6530 [============>.................] - ETA: 0s - loss: 0.0467
 256/6530 [>.............................] - ETA: 1s - loss: 0.0322
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0813
3056/6530 [=============>................] - ETA: 0s - loss: 0.0458
 496/6530 [=>............................] - ETA: 1s - loss: 0.0299
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0802
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0454
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0291
3168/6530 [=============>................] - ETA: 0s - loss: 0.0796
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0452
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0799
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0284
3728/6530 [================>.............] - ETA: 0s - loss: 0.0447
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0804
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0280
3952/6530 [=================>............] - ETA: 0s - loss: 0.0449
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0804
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0280
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0449
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0800
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0282
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0445
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0797
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0279
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0443
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0796
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0274
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0442
6530/6530 [==============================] - 1s 102us/step - loss: 0.0795 - val_loss: 0.0826

2416/6530 [==========>...................] - ETA: 0s - loss: 0.0268Epoch 38/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0965
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0440
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0266
 544/6530 [=>............................] - ETA: 0s - loss: 0.0816
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0439
2896/6530 [============>.................] - ETA: 0s - loss: 0.0268
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0804
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0436
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0786
3120/6530 [=============>................] - ETA: 0s - loss: 0.0268
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0434
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0792
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0265
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0435
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0791
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0264
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0433
3072/6530 [=============>................] - ETA: 0s - loss: 0.0783
3840/6530 [================>.............] - ETA: 0s - loss: 0.0267
6320/6530 [============================>.] - ETA: 0s - loss: 0.0434
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0799
4080/6530 [=================>............] - ETA: 0s - loss: 0.0263
4064/6530 [=================>............] - ETA: 0s - loss: 0.0792
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0263
6530/6530 [==============================] - 2s 247us/step - loss: 0.0433 - val_loss: 0.0482
Epoch 3/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0461
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0793
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0267
 240/6530 [>.............................] - ETA: 1s - loss: 0.0388
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0784
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0265
 464/6530 [=>............................] - ETA: 1s - loss: 0.0422
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0780
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0265
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0410
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0780
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0263
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0396
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0264
6530/6530 [==============================] - 1s 104us/step - loss: 0.0783 - val_loss: 0.0918
Epoch 39/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1062
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0402
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0262
 528/6530 [=>............................] - ETA: 0s - loss: 0.0810
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0401
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0262
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0802
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0391
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0261
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0804
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0392
6432/6530 [============================>.] - ETA: 0s - loss: 0.0261
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0814
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0393
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0803
6530/6530 [==============================] - 1s 225us/step - loss: 0.0261 - val_loss: 0.0142
Epoch 17/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0575
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0396
3088/6530 [=============>................] - ETA: 0s - loss: 0.0799
 256/6530 [>.............................] - ETA: 1s - loss: 0.0263
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0399
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0800
 496/6530 [=>............................] - ETA: 1s - loss: 0.0243
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0406
4096/6530 [=================>............] - ETA: 0s - loss: 0.0808
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0246
2848/6530 [============>.................] - ETA: 0s - loss: 0.0401
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0805
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0250
3072/6530 [=============>................] - ETA: 0s - loss: 0.0395
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0803
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0249
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0392
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0801
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0245
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0392
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0800
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0243
3712/6530 [================>.............] - ETA: 0s - loss: 0.0390
6530/6530 [==============================] - 1s 104us/step - loss: 0.0799 - val_loss: 0.0844
Epoch 40/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1013
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0244
3920/6530 [=================>............] - ETA: 0s - loss: 0.0393
 496/6530 [=>............................] - ETA: 0s - loss: 0.0839
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0249
4128/6530 [=================>............] - ETA: 0s - loss: 0.0394
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0796
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0248
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0391
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0795
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0254
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0391
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0784
2896/6530 [============>.................] - ETA: 0s - loss: 0.0253
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0390
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0786
3120/6530 [=============>................] - ETA: 0s - loss: 0.0251
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0387
2992/6530 [============>.................] - ETA: 0s - loss: 0.0780
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0251
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0385
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0776
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0252
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0385
4000/6530 [=================>............] - ETA: 0s - loss: 0.0780
3824/6530 [================>.............] - ETA: 0s - loss: 0.0250
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0382
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0779
4080/6530 [=================>............] - ETA: 0s - loss: 0.0249
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0384
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0785
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0249
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0382
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0785
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0251
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0384
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0784
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0252
6512/6530 [============================>.] - ETA: 0s - loss: 0.0382
6530/6530 [==============================] - 1s 104us/step - loss: 0.0779 - val_loss: 0.0922
Epoch 41/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1399
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0253
6530/6530 [==============================] - 2s 248us/step - loss: 0.0382 - val_loss: 0.0382
Epoch 4/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0369
 512/6530 [=>............................] - ETA: 0s - loss: 0.0745
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0253
 160/6530 [..............................] - ETA: 2s - loss: 0.0402
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0782
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0253
 384/6530 [>.............................] - ETA: 1s - loss: 0.0372
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0751
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0253
 608/6530 [=>............................] - ETA: 1s - loss: 0.0377
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0754
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0253
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0371
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0762
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0252
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0374
3072/6530 [=============>................] - ETA: 0s - loss: 0.0773
6480/6530 [============================>.] - ETA: 0s - loss: 0.0252
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0371
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0774
6530/6530 [==============================] - 1s 222us/step - loss: 0.0252 - val_loss: 0.0134
Epoch 18/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0401
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0368
4112/6530 [=================>............] - ETA: 0s - loss: 0.0778
 256/6530 [>.............................] - ETA: 1s - loss: 0.0241
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0367
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0775
 496/6530 [=>............................] - ETA: 1s - loss: 0.0255
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0366
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0770
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0242
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0368
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0774
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0257
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0373
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0775
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0259
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0378
6530/6530 [==============================] - 1s 103us/step - loss: 0.0774 - val_loss: 0.0811
Epoch 42/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0548
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0255
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0374
 560/6530 [=>............................] - ETA: 0s - loss: 0.0784
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0262
3008/6530 [============>.................] - ETA: 0s - loss: 0.0370
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0751
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0261
3232/6530 [=============>................] - ETA: 0s - loss: 0.0365
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0772
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0257
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0367
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0766
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0257
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0364
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0764
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0256
3904/6530 [================>.............] - ETA: 0s - loss: 0.0366
3136/6530 [=============>................] - ETA: 0s - loss: 0.0768
2896/6530 [============>.................] - ETA: 0s - loss: 0.0253
4112/6530 [=================>............] - ETA: 0s - loss: 0.0368
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0770
3104/6530 [=============>................] - ETA: 0s - loss: 0.0256
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0366
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0775
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0253
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0367
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0778
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0256
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0365
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0773
3776/6530 [================>.............] - ETA: 0s - loss: 0.0255
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0361
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0772
4016/6530 [=================>............] - ETA: 0s - loss: 0.0258
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0359
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0774
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0257
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0360
6530/6530 [==============================] - 1s 103us/step - loss: 0.0775 - val_loss: 0.0810

4480/6530 [===================>..........] - ETA: 0s - loss: 0.0257Epoch 43/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0595
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0358
 464/6530 [=>............................] - ETA: 0s - loss: 0.0777
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0255
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0358
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0791
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0256
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0356
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0786
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0256
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0358
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0783
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0257
6432/6530 [============================>.] - ETA: 0s - loss: 0.0357
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0785
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0256
3072/6530 [=============>................] - ETA: 0s - loss: 0.0782
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0257
6530/6530 [==============================] - 2s 250us/step - loss: 0.0357 - val_loss: 0.0335
Epoch 5/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0342
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0779
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0255
 240/6530 [>.............................] - ETA: 1s - loss: 0.0332
4112/6530 [=================>............] - ETA: 0s - loss: 0.0777
6400/6530 [============================>.] - ETA: 0s - loss: 0.0254
 464/6530 [=>............................] - ETA: 1s - loss: 0.0341
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0777
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0354
6530/6530 [==============================] - 1s 224us/step - loss: 0.0255 - val_loss: 0.0126
Epoch 19/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0315
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0774
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0342
 256/6530 [>.............................] - ETA: 1s - loss: 0.0284
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0776
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0344
 496/6530 [=>............................] - ETA: 1s - loss: 0.0276
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0777
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0342
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0264
6530/6530 [==============================] - 1s 102us/step - loss: 0.0776 - val_loss: 0.0850
Epoch 44/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0720
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0333
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0259
 528/6530 [=>............................] - ETA: 0s - loss: 0.0748
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0335
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0252
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0748
2016/6530 [========>.....................] - ETA: 1s - loss: 0.0337
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0252
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0751
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0339
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0251
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0758
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0249
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0340
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0755
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0253
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0344
3104/6530 [=============>................] - ETA: 0s - loss: 0.0751
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0252
2896/6530 [============>.................] - ETA: 0s - loss: 0.0341
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0759
3104/6530 [=============>................] - ETA: 0s - loss: 0.0336
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0254
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0764
2848/6530 [============>.................] - ETA: 0s - loss: 0.0254
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0335
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0763
3072/6530 [=============>................] - ETA: 0s - loss: 0.0253
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0334
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0757
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0254
3776/6530 [================>.............] - ETA: 0s - loss: 0.0332
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0758
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0255
3984/6530 [=================>............] - ETA: 0s - loss: 0.0334
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0759
3808/6530 [================>.............] - ETA: 0s - loss: 0.0255
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0335
6530/6530 [==============================] - 1s 101us/step - loss: 0.0761 - val_loss: 0.0915
Epoch 45/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0944
4048/6530 [=================>............] - ETA: 0s - loss: 0.0253
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0334
 560/6530 [=>............................] - ETA: 0s - loss: 0.0800
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0253
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0334
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0774
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0253
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0331
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0762
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0251
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0330
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0768
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0250
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0330
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0767
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0249
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0328
3120/6530 [=============>................] - ETA: 0s - loss: 0.0768
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0248
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0327
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0767
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0250
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0326
4096/6530 [=================>............] - ETA: 0s - loss: 0.0767
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0251
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0325
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0764
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0253
6384/6530 [============================>.] - ETA: 0s - loss: 0.0325
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0764
6416/6530 [============================>.] - ETA: 0s - loss: 0.0254
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0764
6530/6530 [==============================] - 2s 244us/step - loss: 0.0326 - val_loss: 0.0296
Epoch 6/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0275
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0763
6530/6530 [==============================] - 1s 224us/step - loss: 0.0254 - val_loss: 0.0128
Epoch 20/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0138
 240/6530 [>.............................] - ETA: 1s - loss: 0.0290
6530/6530 [==============================] - 1s 103us/step - loss: 0.0763 - val_loss: 0.0879

 240/6530 [>.............................] - ETA: 1s - loss: 0.0239Epoch 46/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0802
 448/6530 [=>............................] - ETA: 1s - loss: 0.0304
 480/6530 [=>............................] - ETA: 1s - loss: 0.0238
 544/6530 [=>............................] - ETA: 0s - loss: 0.0808
 656/6530 [==>...........................] - ETA: 1s - loss: 0.0324
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0231
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0767
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0316
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0773
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0227
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0320
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0772
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0238
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0317
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0780
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0252
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0310
2976/6530 [============>.................] - ETA: 0s - loss: 0.0785
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0253
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0309
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0778
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0255
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0311
4000/6530 [=================>............] - ETA: 0s - loss: 0.0773
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0257
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0311
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0774
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0256
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0313
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0770
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0252
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0317
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0766
2896/6530 [============>.................] - ETA: 0s - loss: 0.0251
2864/6530 [============>.................] - ETA: 0s - loss: 0.0315
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0766
3136/6530 [=============>................] - ETA: 0s - loss: 0.0252
3072/6530 [=============>................] - ETA: 0s - loss: 0.0312
6496/6530 [============================>.] - ETA: 0s - loss: 0.0765
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0252
6530/6530 [==============================] - 1s 105us/step - loss: 0.0765 - val_loss: 0.0819
Epoch 47/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0990
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0309
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0255
 544/6530 [=>............................] - ETA: 0s - loss: 0.0732
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0309
3840/6530 [================>.............] - ETA: 0s - loss: 0.0253
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0748
3728/6530 [================>.............] - ETA: 0s - loss: 0.0305
4080/6530 [=================>............] - ETA: 0s - loss: 0.0251
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0746
3936/6530 [=================>............] - ETA: 0s - loss: 0.0309
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0255
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0751
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0308
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0254
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0756
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0307
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0252
3104/6530 [=============>................] - ETA: 0s - loss: 0.0760
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0308
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0250
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0765
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0306
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0250
4128/6530 [=================>............] - ETA: 0s - loss: 0.0762
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0306
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0252
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0762
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0305
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0251
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0761
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0304
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0251
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0759
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0303
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0254
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0759
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0303
6400/6530 [============================>.] - ETA: 0s - loss: 0.0255
6530/6530 [==============================] - 1s 104us/step - loss: 0.0758 - val_loss: 0.0791
Epoch 48/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0820
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0302
6530/6530 [==============================] - 1s 224us/step - loss: 0.0254 - val_loss: 0.0126
Epoch 21/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0264
 544/6530 [=>............................] - ETA: 0s - loss: 0.0736
6368/6530 [============================>.] - ETA: 0s - loss: 0.0302
 272/6530 [>.............................] - ETA: 1s - loss: 0.0268
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0761
6530/6530 [==============================] - 2s 246us/step - loss: 0.0302 - val_loss: 0.0252

 496/6530 [=>............................] - ETA: 1s - loss: 0.0255Epoch 7/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0232
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0758
 224/6530 [>.............................] - ETA: 1s - loss: 0.0277
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0261
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0759
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0257
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0762
 448/6530 [=>............................] - ETA: 1s - loss: 0.0283
3120/6530 [=============>................] - ETA: 0s - loss: 0.0747
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0261
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0294
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0738
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0256
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0289
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0740
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0299
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0247
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0744
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0296
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0250
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0747
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0292
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0247
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0747
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0290
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0250
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0746
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0289
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0249
6530/6530 [==============================] - 1s 101us/step - loss: 0.0748 - val_loss: 0.0792

2160/6530 [========>.....................] - ETA: 1s - loss: 0.0287Epoch 49/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0755
2912/6530 [============>.................] - ETA: 0s - loss: 0.0245
 544/6530 [=>............................] - ETA: 0s - loss: 0.0695
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0287
3168/6530 [=============>................] - ETA: 0s - loss: 0.0246
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0721
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0293
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0244
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0747
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0293
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0245
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0756
3040/6530 [============>.................] - ETA: 0s - loss: 0.0289
3888/6530 [================>.............] - ETA: 0s - loss: 0.0244
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0749
4128/6530 [=================>............] - ETA: 0s - loss: 0.0245
3264/6530 [=============>................] - ETA: 0s - loss: 0.0287
3056/6530 [=============>................] - ETA: 0s - loss: 0.0750
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0245
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0288
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0751
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0244
3712/6530 [================>.............] - ETA: 0s - loss: 0.0283
4080/6530 [=================>............] - ETA: 0s - loss: 0.0749
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0245
3936/6530 [=================>............] - ETA: 0s - loss: 0.0285
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0748
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0244
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0284
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0754
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0244
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0283
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0753
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0243
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0285
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0752
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0242
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0283
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0243
6530/6530 [==============================] - 1s 104us/step - loss: 0.0751 - val_loss: 0.0831
Epoch 50/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0910
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0284
 544/6530 [=>............................] - ETA: 0s - loss: 0.0785
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0242
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0283
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0759
6480/6530 [============================>.] - ETA: 0s - loss: 0.0242
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0283
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0752
6530/6530 [==============================] - 1s 224us/step - loss: 0.0241 - val_loss: 0.0130

5648/6530 [========================>.....] - ETA: 0s - loss: 0.0281Epoch 22/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0247
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0755
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0281
 240/6530 [>.............................] - ETA: 1s - loss: 0.0276
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0747
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0281
 480/6530 [=>............................] - ETA: 1s - loss: 0.0266
3104/6530 [=============>................] - ETA: 0s - loss: 0.0757
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0281
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0258
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0749
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0251
6512/6530 [============================>.] - ETA: 0s - loss: 0.0279
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0753
6530/6530 [==============================] - 2s 248us/step - loss: 0.0280 - val_loss: 0.0230

1200/6530 [====>.........................] - ETA: 1s - loss: 0.0250Epoch 8/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0175
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0754
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0257
 240/6530 [>.............................] - ETA: 1s - loss: 0.0245
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0754
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0254
 464/6530 [=>............................] - ETA: 1s - loss: 0.0274
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0748
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0281
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0246
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0746
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0244
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0279
6530/6530 [==============================] - 1s 103us/step - loss: 0.0744 - val_loss: 0.0779
Epoch 51/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0684
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0248
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0283
 544/6530 [=>............................] - ETA: 0s - loss: 0.0713
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0246
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0279
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0758
2912/6530 [============>.................] - ETA: 0s - loss: 0.0244
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0272
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0761
3152/6530 [=============>................] - ETA: 0s - loss: 0.0241
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0274
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0759
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0243
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0271
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0743
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0244
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0270
3200/6530 [=============>................] - ETA: 0s - loss: 0.0741
3888/6530 [================>.............] - ETA: 0s - loss: 0.0243
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0269
3712/6530 [================>.............] - ETA: 0s - loss: 0.0748
4128/6530 [=================>............] - ETA: 0s - loss: 0.0240
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0275
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0750
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0243
2832/6530 [============>.................] - ETA: 0s - loss: 0.0275
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0745
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0243
3056/6530 [=============>................] - ETA: 0s - loss: 0.0272
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0747
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0241
3264/6530 [=============>................] - ETA: 0s - loss: 0.0269
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0746
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0240
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0271
6336/6530 [============================>.] - ETA: 0s - loss: 0.0747
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0239
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0267
6530/6530 [==============================] - 1s 100us/step - loss: 0.0747 - val_loss: 0.0798
Epoch 52/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0657
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0239
3920/6530 [=================>............] - ETA: 0s - loss: 0.0269
 528/6530 [=>............................] - ETA: 0s - loss: 0.0743
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0240
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0268
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0749
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0240
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0266
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0739
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0239
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0268
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0744
6496/6530 [============================>.] - ETA: 0s - loss: 0.0238
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0267
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0751
6530/6530 [==============================] - 1s 224us/step - loss: 0.0239 - val_loss: 0.0130

5040/6530 [======================>.......] - ETA: 0s - loss: 0.0268Epoch 23/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0339
3088/6530 [=============>................] - ETA: 0s - loss: 0.0744
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0268
 256/6530 [>.............................] - ETA: 1s - loss: 0.0253
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0744
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0267
 512/6530 [=>............................] - ETA: 1s - loss: 0.0252
4064/6530 [=================>............] - ETA: 0s - loss: 0.0737
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0268
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0737
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0248
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0267
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0733
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0249
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0730
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0267
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0246
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0731
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0266
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0247
6528/6530 [============================>.] - ETA: 0s - loss: 0.0266
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0242
6530/6530 [==============================] - 1s 105us/step - loss: 0.0733 - val_loss: 0.0802
Epoch 53/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0746
6530/6530 [==============================] - 2s 247us/step - loss: 0.0266 - val_loss: 0.0216
Epoch 9/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0160
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0236
 512/6530 [=>............................] - ETA: 0s - loss: 0.0727
 224/6530 [>.............................] - ETA: 1s - loss: 0.0232
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0238
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0754
 432/6530 [>.............................] - ETA: 1s - loss: 0.0252
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0241
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0744
 656/6530 [==>...........................] - ETA: 1s - loss: 0.0269
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0237
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0751
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0261
2928/6530 [============>.................] - ETA: 0s - loss: 0.0238
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0750
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0269
3168/6530 [=============>................] - ETA: 0s - loss: 0.0238
3120/6530 [=============>................] - ETA: 0s - loss: 0.0743
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0264
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0234
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0743
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0257
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0236
4032/6530 [=================>............] - ETA: 0s - loss: 0.0747
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0258
3888/6530 [================>.............] - ETA: 0s - loss: 0.0236
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0747
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0257
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0741
4128/6530 [=================>............] - ETA: 0s - loss: 0.0237
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0256
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0736
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0236
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0255
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0734
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0239
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0259
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0240
6530/6530 [==============================] - 1s 103us/step - loss: 0.0736 - val_loss: 0.0813
Epoch 54/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0774
2832/6530 [============>.................] - ETA: 0s - loss: 0.0259
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0239
 528/6530 [=>............................] - ETA: 0s - loss: 0.0695
3040/6530 [============>.................] - ETA: 0s - loss: 0.0256
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0241
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0680
3264/6530 [=============>................] - ETA: 0s - loss: 0.0255
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0243
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0700
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0256
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0243
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0714
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0253
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0242
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0726
3904/6530 [================>.............] - ETA: 0s - loss: 0.0254
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0242
3008/6530 [============>.................] - ETA: 0s - loss: 0.0726
4112/6530 [=================>............] - ETA: 0s - loss: 0.0255
6528/6530 [============================>.] - ETA: 0s - loss: 0.0241
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0727
6530/6530 [==============================] - 1s 223us/step - loss: 0.0241 - val_loss: 0.0124

4336/6530 [==================>...........] - ETA: 0s - loss: 0.0254Epoch 24/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0273
4016/6530 [=================>............] - ETA: 0s - loss: 0.0729
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0255
 256/6530 [>.............................] - ETA: 1s - loss: 0.0264
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0728
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0255
 496/6530 [=>............................] - ETA: 1s - loss: 0.0267
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0727
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0255
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0248
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0729
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0255
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0247
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0729
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0256
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0249
6530/6530 [==============================] - 1s 103us/step - loss: 0.0729 - val_loss: 0.1018
Epoch 55/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0678
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0255
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0246
 496/6530 [=>............................] - ETA: 0s - loss: 0.0814
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0255
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0246
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0761
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0255
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0249
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0756
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0255
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0254
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0768
6528/6530 [============================>.] - ETA: 0s - loss: 0.0254
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0252
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0749
6530/6530 [==============================] - 2s 246us/step - loss: 0.0254 - val_loss: 0.0210
Epoch 10/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0162
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0250
3088/6530 [=============>................] - ETA: 0s - loss: 0.0748
 224/6530 [>.............................] - ETA: 1s - loss: 0.0226
2912/6530 [============>.................] - ETA: 0s - loss: 0.0249
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0743
 448/6530 [=>............................] - ETA: 1s - loss: 0.0245
3152/6530 [=============>................] - ETA: 0s - loss: 0.0246
4128/6530 [=================>............] - ETA: 0s - loss: 0.0736
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0257
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0245
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0736
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0247
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0251
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0735
3872/6530 [================>.............] - ETA: 0s - loss: 0.0246
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0256
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0730
4112/6530 [=================>............] - ETA: 0s - loss: 0.0246
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0250
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0732
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0246
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0245
6530/6530 [==============================] - 1s 101us/step - loss: 0.0732 - val_loss: 0.0811

4592/6530 [====================>.........] - ETA: 0s - loss: 0.0245
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0246
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0242
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0244
# training | RMSE: 0.0978, MAE: 0.0736
worker 1  xfile  [1, 81, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 32, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20096170173138628}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.09777272165654949, 'rmse': 0.09777272165654949, 'mae': 0.07362662924793029, 'early_stop': True}
vggnet done  1

5072/6530 [======================>.......] - ETA: 0s - loss: 0.0243
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0245
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0245
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0245
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0246
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0247
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0246
2880/6530 [============>.................] - ETA: 0s - loss: 0.0247
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0247
3104/6530 [=============>................] - ETA: 0s - loss: 0.0245
6368/6530 [============================>.] - ETA: 0s - loss: 0.0247
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0244
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0246
6530/6530 [==============================] - 1s 216us/step - loss: 0.0246 - val_loss: 0.0123
Epoch 25/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0152
3824/6530 [================>.............] - ETA: 0s - loss: 0.0245
 272/6530 [>.............................] - ETA: 1s - loss: 0.0241
4064/6530 [=================>............] - ETA: 0s - loss: 0.0245
 528/6530 [=>............................] - ETA: 1s - loss: 0.0241
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0244
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0236
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0245
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0249
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0245
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0246
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0246
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0242
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0245
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0243
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0246
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0243
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0246
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0241
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0246
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0245
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0245
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0245
6368/6530 [============================>.] - ETA: 0s - loss: 0.0244
3056/6530 [=============>................] - ETA: 0s - loss: 0.0247
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0245
6530/6530 [==============================] - 2s 235us/step - loss: 0.0245 - val_loss: 0.0215
Epoch 11/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0145
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0243
 256/6530 [>.............................] - ETA: 1s - loss: 0.0234
3824/6530 [================>.............] - ETA: 0s - loss: 0.0243
 496/6530 [=>............................] - ETA: 1s - loss: 0.0243
4064/6530 [=================>............] - ETA: 0s - loss: 0.0242
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0252
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0239
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0245
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0240
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0247
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0242
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0242
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0241
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0239
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0241
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0238
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0243
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0236
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0241
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0238
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0241
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0239
6320/6530 [============================>.] - ETA: 0s - loss: 0.0238
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0239
2976/6530 [============>.................] - ETA: 0s - loss: 0.0238
6530/6530 [==============================] - 1s 212us/step - loss: 0.0237 - val_loss: 0.0130
Epoch 26/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0197
3200/6530 [=============>................] - ETA: 0s - loss: 0.0235
 272/6530 [>.............................] - ETA: 1s - loss: 0.0222
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0238
 528/6530 [=>............................] - ETA: 1s - loss: 0.0227
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0236
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0241
3920/6530 [=================>............] - ETA: 0s - loss: 0.0237
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0241
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0236
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0237
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0236
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0233
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0237
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0238
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0237
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0236
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0238
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0238
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0238
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0241
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0237
2880/6530 [============>.................] - ETA: 0s - loss: 0.0247
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0237
3136/6530 [=============>................] - ETA: 0s - loss: 0.0250
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0237
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0249
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0237
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0249
6432/6530 [============================>.] - ETA: 0s - loss: 0.0237
3888/6530 [================>.............] - ETA: 0s - loss: 0.0248
6530/6530 [==============================] - 2s 233us/step - loss: 0.0237 - val_loss: 0.0216
Epoch 12/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0148
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0244
 256/6530 [>.............................] - ETA: 1s - loss: 0.0229
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0242
 496/6530 [=>............................] - ETA: 1s - loss: 0.0234
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0241
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0242
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0240
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0236
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0242
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0240
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0241
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0235
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0239
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0231
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0239
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0228
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0239
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0228
6496/6530 [============================>.] - ETA: 0s - loss: 0.0239
6530/6530 [==============================] - 1s 205us/step - loss: 0.0239 - val_loss: 0.0121
Epoch 27/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0159
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0230
 288/6530 [>.............................] - ETA: 1s - loss: 0.0248
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0233
 528/6530 [=>............................] - ETA: 1s - loss: 0.0241
2848/6530 [============>.................] - ETA: 0s - loss: 0.0231
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0234
3088/6530 [=============>................] - ETA: 0s - loss: 0.0228
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0249
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0227
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0243
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0228
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0243
3776/6530 [================>.............] - ETA: 0s - loss: 0.0226
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0246
4000/6530 [=================>............] - ETA: 0s - loss: 0.0228
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0249
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0227
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0245
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0227
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0245
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0228
2832/6530 [============>.................] - ETA: 0s - loss: 0.0244
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0229
3088/6530 [=============>................] - ETA: 0s - loss: 0.0245
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0229
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0247
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0231
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0247
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0230
3856/6530 [================>.............] - ETA: 0s - loss: 0.0248
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0230
4112/6530 [=================>............] - ETA: 0s - loss: 0.0248
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0230
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0248
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0230
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0246
6512/6530 [============================>.] - ETA: 0s - loss: 0.0229
6530/6530 [==============================] - 1s 228us/step - loss: 0.0229 - val_loss: 0.0231

4880/6530 [=====================>........] - ETA: 0s - loss: 0.0245Epoch 13/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0148
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0245
 256/6530 [>.............................] - ETA: 1s - loss: 0.0223
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0244
 496/6530 [=>............................] - ETA: 1s - loss: 0.0228
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0245
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0235
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0244
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0231
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0243
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0234
6448/6530 [============================>.] - ETA: 0s - loss: 0.0244
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0229
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0225
6530/6530 [==============================] - 1s 207us/step - loss: 0.0244 - val_loss: 0.0140
Epoch 28/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0251
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0222
 272/6530 [>.............................] - ETA: 1s - loss: 0.0264
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0221
 528/6530 [=>............................] - ETA: 1s - loss: 0.0234
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0225
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0230
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0225
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0221
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0223
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0220
3008/6530 [============>.................] - ETA: 0s - loss: 0.0220
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0223
3232/6530 [=============>................] - ETA: 0s - loss: 0.0219
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0232
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0222
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0228
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0220
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0229
3920/6530 [=================>............] - ETA: 0s - loss: 0.0221
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0229
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0220
2848/6530 [============>.................] - ETA: 0s - loss: 0.0229
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0219
3088/6530 [=============>................] - ETA: 0s - loss: 0.0226
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0221
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0225
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0221
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0226
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0222
3840/6530 [================>.............] - ETA: 0s - loss: 0.0227
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0222
4096/6530 [=================>............] - ETA: 0s - loss: 0.0227
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0222
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0226
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0223
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0228
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0223
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0227
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0221
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0229
6400/6530 [============================>.] - ETA: 0s - loss: 0.0221
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0230
6530/6530 [==============================] - 2s 233us/step - loss: 0.0222 - val_loss: 0.0235
Epoch 14/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0150
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0231
 240/6530 [>.............................] - ETA: 1s - loss: 0.0203
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0232
 448/6530 [=>............................] - ETA: 1s - loss: 0.0214
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0232
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0221
6416/6530 [============================>.] - ETA: 0s - loss: 0.0232
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0220
6530/6530 [==============================] - 1s 211us/step - loss: 0.0232 - val_loss: 0.0119
Epoch 29/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0054
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0225
 272/6530 [>.............................] - ETA: 1s - loss: 0.0166
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0221
 528/6530 [=>............................] - ETA: 1s - loss: 0.0236
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0215
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0231
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0215
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0239
2016/6530 [========>.....................] - ETA: 1s - loss: 0.0213
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0234
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0214
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0229
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0216
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0233
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0215
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0236
2912/6530 [============>.................] - ETA: 0s - loss: 0.0212
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0240
3152/6530 [=============>................] - ETA: 0s - loss: 0.0210
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0240
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0211
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0239
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0212
3024/6530 [============>.................] - ETA: 0s - loss: 0.0238
3856/6530 [================>.............] - ETA: 0s - loss: 0.0213
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0239
4096/6530 [=================>............] - ETA: 0s - loss: 0.0213
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0237
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0211
3824/6530 [================>.............] - ETA: 0s - loss: 0.0236
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0212
4080/6530 [=================>............] - ETA: 0s - loss: 0.0237
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0213
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0235
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0214
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0236
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0214
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0232
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0214
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0233
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0214
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0233
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0214
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0231
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0213
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0232
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0213
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0233
6320/6530 [============================>.] - ETA: 0s - loss: 0.0232
6530/6530 [==============================] - 2s 236us/step - loss: 0.0213 - val_loss: 0.0195
Epoch 15/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0152
 224/6530 [>.............................] - ETA: 1s - loss: 0.0210
6530/6530 [==============================] - 1s 213us/step - loss: 0.0232 - val_loss: 0.0121
Epoch 30/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0203
 448/6530 [=>............................] - ETA: 1s - loss: 0.0218
 256/6530 [>.............................] - ETA: 1s - loss: 0.0229
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0223
 512/6530 [=>............................] - ETA: 1s - loss: 0.0231
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0221
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0244
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0224
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0231
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0219
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0234
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0214
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0235
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0212
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0232
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0209
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0230
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0209
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0227
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0211
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0226
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0210
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0228
2944/6530 [============>.................] - ETA: 0s - loss: 0.0207
3008/6530 [============>.................] - ETA: 0s - loss: 0.0227
3184/6530 [=============>................] - ETA: 0s - loss: 0.0204
3264/6530 [=============>................] - ETA: 0s - loss: 0.0226
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0206
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0226
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0207
3760/6530 [================>.............] - ETA: 0s - loss: 0.0226
3872/6530 [================>.............] - ETA: 0s - loss: 0.0206
4000/6530 [=================>............] - ETA: 0s - loss: 0.0225
4080/6530 [=================>............] - ETA: 0s - loss: 0.0207
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0227
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0205
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0227
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0206
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0227
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0207
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0228
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0207
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0230
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0208
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0230
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0208
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0232
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0208
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0232
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0208
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0230
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0207
6368/6530 [============================>.] - ETA: 0s - loss: 0.0206
6530/6530 [==============================] - 1s 211us/step - loss: 0.0230 - val_loss: 0.0117
Epoch 31/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0229
 272/6530 [>.............................] - ETA: 1s - loss: 0.0207
6530/6530 [==============================] - 2s 235us/step - loss: 0.0207 - val_loss: 0.0175
Epoch 16/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0131
 512/6530 [=>............................] - ETA: 1s - loss: 0.0218
 224/6530 [>.............................] - ETA: 1s - loss: 0.0193
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0229
 448/6530 [=>............................] - ETA: 1s - loss: 0.0198
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0225
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0203
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0222
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0213
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0231
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0213
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0230
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0209
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0230
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0206
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0229
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0204
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0227
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0201
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0225
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0201
3056/6530 [=============>................] - ETA: 0s - loss: 0.0223
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0203
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0225
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0200
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0229
2976/6530 [============>.................] - ETA: 0s - loss: 0.0197
3824/6530 [================>.............] - ETA: 0s - loss: 0.0229
3184/6530 [=============>................] - ETA: 0s - loss: 0.0195
4080/6530 [=================>............] - ETA: 0s - loss: 0.0231
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0196
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0229
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0197
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0227
3872/6530 [================>.............] - ETA: 0s - loss: 0.0197
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0230
4112/6530 [=================>............] - ETA: 0s - loss: 0.0197
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0231
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0196
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0231
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0196
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0231
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0196
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0229
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0196
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0228
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0197
6352/6530 [============================>.] - ETA: 0s - loss: 0.0228
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0197
6530/6530 [==============================] - 1s 210us/step - loss: 0.0229 - val_loss: 0.0126
Epoch 32/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0340
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0197
 272/6530 [>.............................] - ETA: 1s - loss: 0.0207
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0196
 528/6530 [=>............................] - ETA: 1s - loss: 0.0218
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0195
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0232
6384/6530 [============================>.] - ETA: 0s - loss: 0.0194
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0236
6530/6530 [==============================] - 2s 234us/step - loss: 0.0196 - val_loss: 0.0157
Epoch 17/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0130
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0235
 240/6530 [>.............................] - ETA: 1s - loss: 0.0197
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0236
 464/6530 [=>............................] - ETA: 1s - loss: 0.0221
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0231
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0226
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0233
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0224
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0229
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0224
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0228
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0218
2832/6530 [============>.................] - ETA: 0s - loss: 0.0230
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0212
3088/6530 [=============>................] - ETA: 0s - loss: 0.0229
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0209
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0228
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0207
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0228
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0206
3840/6530 [================>.............] - ETA: 0s - loss: 0.0230
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0210
4064/6530 [=================>............] - ETA: 0s - loss: 0.0229
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0208
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0229
2928/6530 [============>.................] - ETA: 0s - loss: 0.0205
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0229
3152/6530 [=============>................] - ETA: 0s - loss: 0.0202
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0229
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0203
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0229
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0206
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0230
3808/6530 [================>.............] - ETA: 0s - loss: 0.0205
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0229
4032/6530 [=================>............] - ETA: 0s - loss: 0.0207
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0231
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0203
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0231
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0203
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0230
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0202
6528/6530 [============================>.] - ETA: 0s - loss: 0.0230
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0201
6530/6530 [==============================] - 1s 212us/step - loss: 0.0230 - val_loss: 0.0120
Epoch 33/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0207
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0202
 272/6530 [>.............................] - ETA: 1s - loss: 0.0239
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0202
 528/6530 [=>............................] - ETA: 1s - loss: 0.0246
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0201
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0242
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0201
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0236
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0200
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0234
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0200
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0236
6512/6530 [============================>.] - ETA: 0s - loss: 0.0200
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0235
6530/6530 [==============================] - 2s 237us/step - loss: 0.0200 - val_loss: 0.0157
Epoch 18/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0116
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0232
 240/6530 [>.............................] - ETA: 1s - loss: 0.0207
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0232
 464/6530 [=>............................] - ETA: 1s - loss: 0.0218
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0230
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0214
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0230
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0218
3056/6530 [=============>................] - ETA: 0s - loss: 0.0231
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0214
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0232
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0211
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0230
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0205
3824/6530 [================>.............] - ETA: 0s - loss: 0.0231
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0202
4080/6530 [=================>............] - ETA: 0s - loss: 0.0229
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0199
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0227
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0198
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0226
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0199
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0225
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0196
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0225
2960/6530 [============>.................] - ETA: 0s - loss: 0.0192
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0225
3184/6530 [=============>................] - ETA: 0s - loss: 0.0190
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0225
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0191
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0224
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0192
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0224
3872/6530 [================>.............] - ETA: 0s - loss: 0.0191
6336/6530 [============================>.] - ETA: 0s - loss: 0.0225
4096/6530 [=================>............] - ETA: 0s - loss: 0.0192
6530/6530 [==============================] - 1s 211us/step - loss: 0.0224 - val_loss: 0.0118
Epoch 34/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0366
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0190
 256/6530 [>.............................] - ETA: 1s - loss: 0.0257
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0189
 512/6530 [=>............................] - ETA: 1s - loss: 0.0222
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0189
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0224
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0188
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0215
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0189
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0226
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0188
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0225
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0188
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0223
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0187
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0223
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0186
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0220
6368/6530 [============================>.] - ETA: 0s - loss: 0.0185
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0216
6530/6530 [==============================] - 2s 235us/step - loss: 0.0186 - val_loss: 0.0164
Epoch 19/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0104
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0220
 256/6530 [>.............................] - ETA: 1s - loss: 0.0181
3072/6530 [=============>................] - ETA: 0s - loss: 0.0219
 480/6530 [=>............................] - ETA: 1s - loss: 0.0176
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0221
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0184
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0224
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0181
3856/6530 [================>.............] - ETA: 0s - loss: 0.0224
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0186
4112/6530 [=================>............] - ETA: 0s - loss: 0.0227
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0183
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0228
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0181
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0229
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0177
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0230
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0176
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0229
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0177
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0231
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0231
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0177
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0231
2864/6530 [============>.................] - ETA: 0s - loss: 0.0174
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0230
3104/6530 [=============>................] - ETA: 0s - loss: 0.0172
6480/6530 [============================>.] - ETA: 0s - loss: 0.0229
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0172
6530/6530 [==============================] - 1s 206us/step - loss: 0.0229 - val_loss: 0.0130
Epoch 35/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0223
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0175
 272/6530 [>.............................] - ETA: 1s - loss: 0.0239
3808/6530 [================>.............] - ETA: 0s - loss: 0.0174
 528/6530 [=>............................] - ETA: 1s - loss: 0.0239
4032/6530 [=================>............] - ETA: 0s - loss: 0.0176
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0227
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0174
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0228
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0173
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0233
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0173
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0232
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0172
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0236
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0173
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0233
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0174
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0235
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0173
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0236
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0173
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0237
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0172
3088/6530 [=============>................] - ETA: 0s - loss: 0.0235
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0172
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0237
6530/6530 [==============================] - 2s 231us/step - loss: 0.0172 - val_loss: 0.0164
Epoch 20/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0098
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0236
 240/6530 [>.............................] - ETA: 1s - loss: 0.0157
3872/6530 [================>.............] - ETA: 0s - loss: 0.0232
 464/6530 [=>............................] - ETA: 1s - loss: 0.0169
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0233
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0174
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0232
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0173
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0230
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0178
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0232
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0175
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0232
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0172
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0230
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0228
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0169
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0228
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0168
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0169
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0228
6464/6530 [============================>.] - ETA: 0s - loss: 0.0229
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0169
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0167
6530/6530 [==============================] - 1s 207us/step - loss: 0.0230 - val_loss: 0.0113
Epoch 36/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0278
3056/6530 [=============>................] - ETA: 0s - loss: 0.0164
 272/6530 [>.............................] - ETA: 1s - loss: 0.0201
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0163
 528/6530 [=>............................] - ETA: 1s - loss: 0.0206
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0166
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0212
3728/6530 [================>.............] - ETA: 0s - loss: 0.0165
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0206
3952/6530 [=================>............] - ETA: 0s - loss: 0.0166
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0208
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0166
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0215
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0165
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0217
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0165
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0219
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0164
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0226
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0165
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0229
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0165
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0229
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0165
3056/6530 [=============>................] - ETA: 0s - loss: 0.0227
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0165
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0226
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0164
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0226
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0164
3808/6530 [================>.............] - ETA: 0s - loss: 0.0226
6464/6530 [============================>.] - ETA: 0s - loss: 0.0163
4064/6530 [=================>............] - ETA: 0s - loss: 0.0225
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0225
6530/6530 [==============================] - 2s 233us/step - loss: 0.0164 - val_loss: 0.0154
Epoch 21/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0089
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0226
 240/6530 [>.............................] - ETA: 1s - loss: 0.0149
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0226
 448/6530 [=>............................] - ETA: 1s - loss: 0.0162
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0226
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0164
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0224
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0168
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0223
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0173
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0222
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0170
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0224
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0167
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0224
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0166
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0164
6530/6530 [==============================] - 1s 213us/step - loss: 0.0224 - val_loss: 0.0123
Epoch 37/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0177
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0163
 240/6530 [>.............................] - ETA: 1s - loss: 0.0229
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0164
 512/6530 [=>............................] - ETA: 1s - loss: 0.0212
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0162
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0213
2976/6530 [============>.................] - ETA: 0s - loss: 0.0159
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0217
3216/6530 [=============>................] - ETA: 0s - loss: 0.0157
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0220
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0160
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0223
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0161
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0225
3888/6530 [================>.............] - ETA: 0s - loss: 0.0161
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0225
4128/6530 [=================>............] - ETA: 0s - loss: 0.0162
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0226
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0160
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0225
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0160
2832/6530 [============>.................] - ETA: 0s - loss: 0.0222
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0160
3104/6530 [=============>................] - ETA: 0s - loss: 0.0222
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0160
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0221
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0160
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0219
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0160
3856/6530 [================>.............] - ETA: 0s - loss: 0.0218
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0160
4096/6530 [=================>............] - ETA: 0s - loss: 0.0218
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0160
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0219
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0159
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0218
6448/6530 [============================>.] - ETA: 0s - loss: 0.0159
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0218
6530/6530 [==============================] - 2s 232us/step - loss: 0.0160 - val_loss: 0.0140
Epoch 22/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0088
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0219
 224/6530 [>.............................] - ETA: 1s - loss: 0.0149
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0217
 448/6530 [=>............................] - ETA: 1s - loss: 0.0157
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0218
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0159
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0219
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0163
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0220
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0169
6400/6530 [============================>.] - ETA: 0s - loss: 0.0221
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0166
6530/6530 [==============================] - 1s 207us/step - loss: 0.0223 - val_loss: 0.0115
Epoch 38/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0243
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0163
 272/6530 [>.............................] - ETA: 1s - loss: 0.0201
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0162
 512/6530 [=>............................] - ETA: 1s - loss: 0.0206
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0159
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0210
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0159
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0210
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0160
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0216
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0157
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0220
3008/6530 [============>.................] - ETA: 0s - loss: 0.0154
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0228
3232/6530 [=============>................] - ETA: 0s - loss: 0.0152
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0231
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0154
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0231
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0155
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0230
3920/6530 [=================>............] - ETA: 0s - loss: 0.0156
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0228
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0157
3072/6530 [=============>................] - ETA: 0s - loss: 0.0226
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0155
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0226
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0155
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0225
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0154
3840/6530 [================>.............] - ETA: 0s - loss: 0.0223
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0155
4096/6530 [=================>............] - ETA: 0s - loss: 0.0224
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0155
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0225
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0155
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0223
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0155
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0224
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0154
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0224
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0154
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0222
6464/6530 [============================>.] - ETA: 0s - loss: 0.0154
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0222
6530/6530 [==============================] - 2s 231us/step - loss: 0.0155 - val_loss: 0.0124
Epoch 23/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0086
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0223
 256/6530 [>.............................] - ETA: 1s - loss: 0.0156
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0224
 480/6530 [=>............................] - ETA: 1s - loss: 0.0154
6384/6530 [============================>.] - ETA: 0s - loss: 0.0225
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0160
6530/6530 [==============================] - 1s 207us/step - loss: 0.0224 - val_loss: 0.0118
Epoch 39/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0296
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0159
 272/6530 [>.............................] - ETA: 1s - loss: 0.0220
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0164
 528/6530 [=>............................] - ETA: 1s - loss: 0.0209
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0161
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0214
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0158
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0213
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0157
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0219
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0155
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0219
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0154
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0217
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0155
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0219
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0153
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0217
3040/6530 [============>.................] - ETA: 0s - loss: 0.0149
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0223
3264/6530 [=============>................] - ETA: 0s - loss: 0.0148
2864/6530 [============>.................] - ETA: 0s - loss: 0.0226
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0150
3120/6530 [=============>................] - ETA: 0s - loss: 0.0226
3744/6530 [================>.............] - ETA: 0s - loss: 0.0150
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0225
3984/6530 [=================>............] - ETA: 0s - loss: 0.0152
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0222
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0151
3904/6530 [================>.............] - ETA: 0s - loss: 0.0224
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0150
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0222
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0150
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0224
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0150
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0222
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0150
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0223
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0152
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0222
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0150
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0223
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0150
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0223
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0150
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0221
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0150
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0219
6464/6530 [============================>.] - ETA: 0s - loss: 0.0149
6448/6530 [============================>.] - ETA: 0s - loss: 0.0219
6530/6530 [==============================] - 2s 232us/step - loss: 0.0150 - val_loss: 0.0113
Epoch 24/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0081
6530/6530 [==============================] - 1s 208us/step - loss: 0.0218 - val_loss: 0.0113
Epoch 40/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0170
 240/6530 [>.............................] - ETA: 1s - loss: 0.0131
 272/6530 [>.............................] - ETA: 1s - loss: 0.0192
 480/6530 [=>............................] - ETA: 1s - loss: 0.0149
 528/6530 [=>............................] - ETA: 1s - loss: 0.0202
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0156
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0203
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0154
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0205
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0159
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0201
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0158
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0207
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0155
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0205
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0154
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0213
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0151
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0216
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0152
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0221
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0150
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0225
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0151
2960/6530 [============>.................] - ETA: 0s - loss: 0.0224
2864/6530 [============>.................] - ETA: 0s - loss: 0.0148
3216/6530 [=============>................] - ETA: 0s - loss: 0.0222
3072/6530 [=============>................] - ETA: 0s - loss: 0.0145
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0221
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0144
3728/6530 [================>.............] - ETA: 0s - loss: 0.0221
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0147
3984/6530 [=================>............] - ETA: 0s - loss: 0.0219
3760/6530 [================>.............] - ETA: 0s - loss: 0.0147
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0221
3984/6530 [=================>............] - ETA: 0s - loss: 0.0148
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0223
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0148
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0223
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0147
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0221
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0147
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0222
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0146
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0220
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0147
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0220
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0148
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0220
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0147
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0220
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0147
6400/6530 [============================>.] - ETA: 0s - loss: 0.0218
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0146
6530/6530 [==============================] - 1s 217us/step - loss: 0.0218 - val_loss: 0.0113
Epoch 41/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0272
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0145
 288/6530 [>.............................] - ETA: 1s - loss: 0.0263
6416/6530 [============================>.] - ETA: 0s - loss: 0.0145
 544/6530 [=>............................] - ETA: 1s - loss: 0.0258
6530/6530 [==============================] - 2s 240us/step - loss: 0.0146 - val_loss: 0.0114
Epoch 25/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0080
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0250
 256/6530 [>.............................] - ETA: 1s - loss: 0.0145
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0238
 480/6530 [=>............................] - ETA: 1s - loss: 0.0146
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0232
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0152
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0227
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0151
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0225
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0156
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0223
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0155
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0220
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0151
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0215
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0150
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0214
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0148
3040/6530 [============>.................] - ETA: 0s - loss: 0.0218
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0147
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0222
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0147
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0223
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0146
3808/6530 [================>.............] - ETA: 0s - loss: 0.0223
2944/6530 [============>.................] - ETA: 0s - loss: 0.0143
4064/6530 [=================>............] - ETA: 0s - loss: 0.0222
3184/6530 [=============>................] - ETA: 0s - loss: 0.0140
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0224
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0141
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0225
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0144
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0224
3840/6530 [================>.............] - ETA: 0s - loss: 0.0144
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0225
4064/6530 [=================>............] - ETA: 0s - loss: 0.0146
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0224
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0144
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0224
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0143
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0223
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0144
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0223
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0143
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0224
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0143
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0144
6530/6530 [==============================] - 1s 212us/step - loss: 0.0225 - val_loss: 0.0113
Epoch 42/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0230
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0144
 288/6530 [>.............................] - ETA: 1s - loss: 0.0218
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0143
 528/6530 [=>............................] - ETA: 1s - loss: 0.0232
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0142
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0227
6352/6530 [============================>.] - ETA: 0s - loss: 0.0142
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0226
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0222
6530/6530 [==============================] - 2s 236us/step - loss: 0.0143 - val_loss: 0.0111
Epoch 26/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0077
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0227
 240/6530 [>.............................] - ETA: 1s - loss: 0.0125
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0226
 464/6530 [=>............................] - ETA: 1s - loss: 0.0142
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0225
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0144
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0225
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0146
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0220
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0152
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0220
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0151
3024/6530 [============>.................] - ETA: 0s - loss: 0.0219
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0147
3264/6530 [=============>................] - ETA: 0s - loss: 0.0219
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0146
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0221
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0144
3760/6530 [================>.............] - ETA: 0s - loss: 0.0222
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0143
4000/6530 [=================>............] - ETA: 0s - loss: 0.0220
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0144
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0220
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0142
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0221
2960/6530 [============>.................] - ETA: 0s - loss: 0.0140
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0220
3184/6530 [=============>................] - ETA: 0s - loss: 0.0137
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0220
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0138
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0219
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0140
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0220
3856/6530 [================>.............] - ETA: 0s - loss: 0.0140
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0220
4064/6530 [=================>............] - ETA: 0s - loss: 0.0142
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0220
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0141
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0220
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0140
6512/6530 [============================>.] - ETA: 0s - loss: 0.0219
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0140
6530/6530 [==============================] - 1s 213us/step - loss: 0.0219 - val_loss: 0.0113
Epoch 43/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0231
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0140
 272/6530 [>.............................] - ETA: 1s - loss: 0.0202
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0140
 528/6530 [=>............................] - ETA: 1s - loss: 0.0195
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0141
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0195
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0140
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0206
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0140
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0204
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0139
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0208
6368/6530 [============================>.] - ETA: 0s - loss: 0.0139
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0213
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0214
6530/6530 [==============================] - 2s 234us/step - loss: 0.0140 - val_loss: 0.0100
Epoch 27/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0077
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0214
 240/6530 [>.............................] - ETA: 1s - loss: 0.0124
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0218
 464/6530 [=>............................] - ETA: 1s - loss: 0.0141
2848/6530 [============>.................] - ETA: 0s - loss: 0.0218
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0143
3088/6530 [=============>................] - ETA: 0s - loss: 0.0219
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0145
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0219
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0150
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0216
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0149
3824/6530 [================>.............] - ETA: 0s - loss: 0.0216
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0144
4064/6530 [=================>............] - ETA: 0s - loss: 0.0214
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0143
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0215
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0141
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0214
2224/6530 [=========>....................] - ETA: 1s - loss: 0.0140
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0213
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0140
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0212
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0139
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0213
2912/6530 [============>.................] - ETA: 0s - loss: 0.0138
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0215
3136/6530 [=============>................] - ETA: 0s - loss: 0.0135
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0216
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0135
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0214
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0138
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0214
3792/6530 [================>.............] - ETA: 0s - loss: 0.0136
6496/6530 [============================>.] - ETA: 0s - loss: 0.0214
4000/6530 [=================>............] - ETA: 0s - loss: 0.0139
6530/6530 [==============================] - 1s 213us/step - loss: 0.0214 - val_loss: 0.0125
Epoch 44/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0146
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0138
 272/6530 [>.............................] - ETA: 1s - loss: 0.0272
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0137
 528/6530 [=>............................] - ETA: 1s - loss: 0.0255
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0137
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0246
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0137
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0237
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0138
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0229
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0139
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0231
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0138
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0233
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0137
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0232
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0137
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0227
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0137
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0225
6496/6530 [============================>.] - ETA: 0s - loss: 0.0137
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0223
6530/6530 [==============================] - 2s 238us/step - loss: 0.0137 - val_loss: 0.0101
Epoch 28/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0076
2992/6530 [============>.................] - ETA: 0s - loss: 0.0226
 240/6530 [>.............................] - ETA: 1s - loss: 0.0123
3248/6530 [=============>................] - ETA: 0s - loss: 0.0225
 480/6530 [=>............................] - ETA: 1s - loss: 0.0139
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0222
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0145
3744/6530 [================>.............] - ETA: 0s - loss: 0.0223
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0142
4000/6530 [=================>............] - ETA: 0s - loss: 0.0219
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0147
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0217
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0146
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0219
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0142
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0218
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0141
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0218
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0138
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0219
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0138
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0219
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0138
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0218
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0137
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0216
2944/6530 [============>.................] - ETA: 0s - loss: 0.0135
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0216
3184/6530 [=============>................] - ETA: 0s - loss: 0.0132
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0134
6530/6530 [==============================] - 1s 211us/step - loss: 0.0214 - val_loss: 0.0112
Epoch 45/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0110
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0135
 272/6530 [>.............................] - ETA: 1s - loss: 0.0215
3872/6530 [================>.............] - ETA: 0s - loss: 0.0135
 528/6530 [=>............................] - ETA: 1s - loss: 0.0229
4096/6530 [=================>............] - ETA: 0s - loss: 0.0137
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0215
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0136
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0212
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0135
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0209
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0135
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0207
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0135
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0207
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0136
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0207
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0136
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0207
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0135
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0206
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0135
2832/6530 [============>.................] - ETA: 0s - loss: 0.0207
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0134
3104/6530 [=============>................] - ETA: 0s - loss: 0.0207
6400/6530 [============================>.] - ETA: 0s - loss: 0.0134
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0208
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0206
6530/6530 [==============================] - 2s 234us/step - loss: 0.0135 - val_loss: 0.0098
Epoch 29/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0074
3856/6530 [================>.............] - ETA: 0s - loss: 0.0208
 240/6530 [>.............................] - ETA: 1s - loss: 0.0119
4112/6530 [=================>............] - ETA: 0s - loss: 0.0208
 464/6530 [=>............................] - ETA: 1s - loss: 0.0136
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0211
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0139
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0210
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0140
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0209
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0145
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0209
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0144
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0210
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0140
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0209
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0139
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0211
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0136
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0211
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0135
6416/6530 [============================>.] - ETA: 0s - loss: 0.0212
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0136
6530/6530 [==============================] - 1s 207us/step - loss: 0.0212 - val_loss: 0.0110
Epoch 46/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0413
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0134
 272/6530 [>.............................] - ETA: 1s - loss: 0.0244
2960/6530 [============>.................] - ETA: 0s - loss: 0.0132
 512/6530 [=>............................] - ETA: 1s - loss: 0.0239
3184/6530 [=============>................] - ETA: 0s - loss: 0.0130
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0235
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0130
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0221
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0133
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0220
3856/6530 [================>.............] - ETA: 0s - loss: 0.0132
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0216
4096/6530 [=================>............] - ETA: 0s - loss: 0.0134
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0215
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0133
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0215
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0132
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0215
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0133
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0222
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0133
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0220
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0133
3072/6530 [=============>................] - ETA: 0s - loss: 0.0221
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0133
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0221
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0133
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0221
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0132
3808/6530 [================>.............] - ETA: 0s - loss: 0.0221
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0131
4048/6530 [=================>............] - ETA: 0s - loss: 0.0219
6416/6530 [============================>.] - ETA: 0s - loss: 0.0131
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0217
6530/6530 [==============================] - 2s 235us/step - loss: 0.0132 - val_loss: 0.0100
Epoch 30/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0078
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0216
 256/6530 [>.............................] - ETA: 1s - loss: 0.0135
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0217
 480/6530 [=>............................] - ETA: 1s - loss: 0.0134
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0217
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0140
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0217
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0137
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0217
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0142
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0216
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0141
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0215
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0138
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0214
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0136
6530/6530 [==============================] - 1s 210us/step - loss: 0.0214 - val_loss: 0.0112
Epoch 47/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0342
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0134
 256/6530 [>.............................] - ETA: 1s - loss: 0.0222
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0133
 480/6530 [=>............................] - ETA: 1s - loss: 0.0215
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0134
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0207
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0132
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0213
2976/6530 [============>.................] - ETA: 0s - loss: 0.0130
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0210
3200/6530 [=============>................] - ETA: 0s - loss: 0.0127
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0212
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0130
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0214
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0131
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0211
3904/6530 [================>.............] - ETA: 0s - loss: 0.0130
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0211
4128/6530 [=================>............] - ETA: 0s - loss: 0.0132
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0212
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0131
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0212
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0130
3056/6530 [=============>................] - ETA: 0s - loss: 0.0211
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0130
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0211
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0131
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0207
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0131
3824/6530 [================>.............] - ETA: 0s - loss: 0.0209
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0131
4080/6530 [=================>............] - ETA: 0s - loss: 0.0208
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0131
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0210
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0130
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0209
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0130
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0207
6448/6530 [============================>.] - ETA: 0s - loss: 0.0129
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0207
6530/6530 [==============================] - 2s 231us/step - loss: 0.0130 - val_loss: 0.0097
Epoch 31/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0074
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0209
 240/6530 [>.............................] - ETA: 1s - loss: 0.0114
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0209
 464/6530 [=>............................] - ETA: 1s - loss: 0.0132
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0211
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0134
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0211
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0135
6400/6530 [============================>.] - ETA: 0s - loss: 0.0211
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0140
6530/6530 [==============================] - 1s 208us/step - loss: 0.0211 - val_loss: 0.0110
Epoch 48/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0082
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0139
 272/6530 [>.............................] - ETA: 1s - loss: 0.0232
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0136
 528/6530 [=>............................] - ETA: 1s - loss: 0.0223
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0133
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0225
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0132
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0214
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0130
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0218
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0132
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0217
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0130
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0218
3008/6530 [============>.................] - ETA: 0s - loss: 0.0127
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0217
3232/6530 [=============>................] - ETA: 0s - loss: 0.0126
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0218
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0128
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0216
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0128
2832/6530 [============>.................] - ETA: 0s - loss: 0.0212
3920/6530 [=================>............] - ETA: 0s - loss: 0.0129
3104/6530 [=============>................] - ETA: 0s - loss: 0.0212
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0129
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0215
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0128
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0216
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0128
3888/6530 [================>.............] - ETA: 0s - loss: 0.0215
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0128
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0213
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0129
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0213
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0129
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0214
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0129
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0214
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0128
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0212
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0128
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0212
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0128
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0213
6480/6530 [============================>.] - ETA: 0s - loss: 0.0127
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0213
6530/6530 [==============================] - 2s 230us/step - loss: 0.0128 - val_loss: 0.0095
Epoch 32/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0074
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0212
 240/6530 [>.............................] - ETA: 1s - loss: 0.0113
6480/6530 [============================>.] - ETA: 0s - loss: 0.0212
 464/6530 [=>............................] - ETA: 1s - loss: 0.0131
6530/6530 [==============================] - 1s 205us/step - loss: 0.0213 - val_loss: 0.0115
Epoch 49/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0233
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0135
 272/6530 [>.............................] - ETA: 1s - loss: 0.0189
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0132
 528/6530 [=>............................] - ETA: 1s - loss: 0.0219
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0138
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0211
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0140
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0203
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0134
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0199
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0133
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0208
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0130
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0207
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0129
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0206
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0129
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0209
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0128
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0213
2896/6530 [============>.................] - ETA: 0s - loss: 0.0126
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0211
3120/6530 [=============>................] - ETA: 0s - loss: 0.0124
3056/6530 [=============>................] - ETA: 0s - loss: 0.0209
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0123
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0209
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0126
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0209
3824/6530 [================>.............] - ETA: 0s - loss: 0.0126
3856/6530 [================>.............] - ETA: 0s - loss: 0.0208
4048/6530 [=================>............] - ETA: 0s - loss: 0.0128
4112/6530 [=================>............] - ETA: 0s - loss: 0.0208
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0126
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0207
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0126
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0206
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0126
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0207
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0126
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0209
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0127
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0208
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0127
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0208
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0126
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0208
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0126
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0208
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0125
6368/6530 [============================>.] - ETA: 0s - loss: 0.0208
6336/6530 [============================>.] - ETA: 0s - loss: 0.0125
6530/6530 [==============================] - 1s 211us/step - loss: 0.0207 - val_loss: 0.0109
Epoch 50/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0156
6530/6530 [==============================] - 2s 236us/step - loss: 0.0126 - val_loss: 0.0094
Epoch 33/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0074
 272/6530 [>.............................] - ETA: 1s - loss: 0.0200
 240/6530 [>.............................] - ETA: 1s - loss: 0.0112
 528/6530 [=>............................] - ETA: 1s - loss: 0.0211
 464/6530 [=>............................] - ETA: 1s - loss: 0.0130
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0208
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0132
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0205
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0131
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0200
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0136
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0200
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0136
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0206
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0132
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0208
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0130
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0206
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0128
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0208
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0127
2832/6530 [============>.................] - ETA: 0s - loss: 0.0206
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0127
3072/6530 [=============>................] - ETA: 0s - loss: 0.0209
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0126
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0208
2928/6530 [============>.................] - ETA: 0s - loss: 0.0124
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0208
3168/6530 [=============>................] - ETA: 0s - loss: 0.0122
3824/6530 [================>.............] - ETA: 0s - loss: 0.0208
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0122
4080/6530 [=================>............] - ETA: 0s - loss: 0.0212
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0124
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0211
3872/6530 [================>.............] - ETA: 0s - loss: 0.0124
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0209
4096/6530 [=================>............] - ETA: 0s - loss: 0.0126
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0207
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0125
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0208
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0124
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0207
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0124
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0207
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0124
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0206
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0125
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0208
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0125
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0207
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0125
6496/6530 [============================>.] - ETA: 0s - loss: 0.0206
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0124
6530/6530 [==============================] - 1s 214us/step - loss: 0.0206 - val_loss: 0.0108
Epoch 51/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0216
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0124
 272/6530 [>.............................] - ETA: 1s - loss: 0.0188
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0123
 512/6530 [=>............................] - ETA: 1s - loss: 0.0198
6496/6530 [============================>.] - ETA: 0s - loss: 0.0123
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0209
6530/6530 [==============================] - 2s 238us/step - loss: 0.0124 - val_loss: 0.0091
Epoch 34/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0075
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0209
 240/6530 [>.............................] - ETA: 1s - loss: 0.0111
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0205
 464/6530 [=>............................] - ETA: 1s - loss: 0.0128
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0201
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0130
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0205
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0129
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0209
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0134
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0208
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0134
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0212
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0130
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0212
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0128
3024/6530 [============>.................] - ETA: 0s - loss: 0.0211
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0126
3264/6530 [=============>................] - ETA: 0s - loss: 0.0213
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0125
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0212
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0124
3776/6530 [================>.............] - ETA: 0s - loss: 0.0213
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0124
4016/6530 [=================>............] - ETA: 0s - loss: 0.0212
2928/6530 [============>.................] - ETA: 0s - loss: 0.0122
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0210
3152/6530 [=============>................] - ETA: 0s - loss: 0.0120
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0209
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0120
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0208
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0123
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0209
3840/6530 [================>.............] - ETA: 0s - loss: 0.0122
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0208
4064/6530 [=================>............] - ETA: 0s - loss: 0.0123
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0207
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0123
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0207
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0122
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0206
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0122
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0205
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0122
6496/6530 [============================>.] - ETA: 0s - loss: 0.0206
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0123
6530/6530 [==============================] - 1s 213us/step - loss: 0.0206 - val_loss: 0.0110
Epoch 52/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0184
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0123
 256/6530 [>.............................] - ETA: 1s - loss: 0.0202
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0123
 512/6530 [=>............................] - ETA: 1s - loss: 0.0209
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0122
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0208
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0122
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0202
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0121
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0204
6496/6530 [============================>.] - ETA: 0s - loss: 0.0121
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0203
6530/6530 [==============================] - 2s 236us/step - loss: 0.0122 - val_loss: 0.0090
Epoch 35/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0075
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0207
 240/6530 [>.............................] - ETA: 1s - loss: 0.0110
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0212
 464/6530 [=>............................] - ETA: 1s - loss: 0.0128
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0213
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0132
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0210
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0127
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0211
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0132
3056/6530 [=============>................] - ETA: 0s - loss: 0.0212
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0131
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0209
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0128
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0205
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0126
3792/6530 [================>.............] - ETA: 0s - loss: 0.0204
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0124
4048/6530 [=================>............] - ETA: 0s - loss: 0.0205
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0123
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0205
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0124
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0206
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0122
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0207
3024/6530 [============>.................] - ETA: 0s - loss: 0.0119
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0208
3232/6530 [=============>................] - ETA: 0s - loss: 0.0118
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0207
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0120
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0208
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0120
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0207
3904/6530 [================>.............] - ETA: 0s - loss: 0.0120
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0206
4128/6530 [=================>............] - ETA: 0s - loss: 0.0121
6352/6530 [============================>.] - ETA: 0s - loss: 0.0207
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0121
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0120
6530/6530 [==============================] - 1s 211us/step - loss: 0.0208 - val_loss: 0.0135
Epoch 53/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0160
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0120
 272/6530 [>.............................] - ETA: 1s - loss: 0.0210
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0121
 528/6530 [=>............................] - ETA: 1s - loss: 0.0200
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0121
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0192
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0121
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0204
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0121
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0204
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0120
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0206
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0119
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0205
6416/6530 [============================>.] - ETA: 0s - loss: 0.0119
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0207
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0208
6530/6530 [==============================] - 2s 233us/step - loss: 0.0120 - val_loss: 0.0091
Epoch 36/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0078
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0208
 240/6530 [>.............................] - ETA: 1s - loss: 0.0108
2848/6530 [============>.................] - ETA: 0s - loss: 0.0209
 480/6530 [=>............................] - ETA: 1s - loss: 0.0126
3120/6530 [=============>................] - ETA: 0s - loss: 0.0207
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0130
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0205
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0125
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0205
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0130
3888/6530 [================>.............] - ETA: 0s - loss: 0.0206
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0129
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0207
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0126
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0207
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0124
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0208
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0122
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0208
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0121
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0207
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0122
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0206
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0120
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0206
2976/6530 [============>.................] - ETA: 0s - loss: 0.0118
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0206
3184/6530 [=============>................] - ETA: 0s - loss: 0.0116
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0205
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0116
6352/6530 [============================>.] - ETA: 0s - loss: 0.0205
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0119
3840/6530 [================>.............] - ETA: 0s - loss: 0.0119
6530/6530 [==============================] - 1s 210us/step - loss: 0.0206 - val_loss: 0.0108
Epoch 54/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0244
4080/6530 [=================>............] - ETA: 0s - loss: 0.0120
 272/6530 [>.............................] - ETA: 1s - loss: 0.0191
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0119
 528/6530 [=>............................] - ETA: 1s - loss: 0.0193
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0119
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0202
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0118
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0216
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0119
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0215
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0119
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0216
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0120
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0217
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0119
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0220
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0119
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0215
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0118
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0213
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0118
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0211
6528/6530 [============================>.] - ETA: 0s - loss: 0.0118
2960/6530 [============>.................] - ETA: 0s - loss: 0.0209
6530/6530 [==============================] - 2s 237us/step - loss: 0.0118 - val_loss: 0.0091
Epoch 37/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0078
3184/6530 [=============>................] - ETA: 0s - loss: 0.0210
 224/6530 [>.............................] - ETA: 1s - loss: 0.0111
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0209
 448/6530 [=>............................] - ETA: 1s - loss: 0.0123
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0211
 640/6530 [=>............................] - ETA: 1s - loss: 0.0128
3920/6530 [=================>............] - ETA: 0s - loss: 0.0208
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0124
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0210
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0128
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0209
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0130
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0209
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0125
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0207
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0123
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0205
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0120
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0204
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0120
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0204
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0119
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0205
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0119
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0204
2880/6530 [============>.................] - ETA: 0s - loss: 0.0117
6400/6530 [============================>.] - ETA: 0s - loss: 0.0203
3104/6530 [=============>................] - ETA: 0s - loss: 0.0115
6530/6530 [==============================] - 1s 215us/step - loss: 0.0204 - val_loss: 0.0108
Epoch 55/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0223
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0115
 272/6530 [>.............................] - ETA: 1s - loss: 0.0219
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0116
 512/6530 [=>............................] - ETA: 1s - loss: 0.0221
3776/6530 [================>.............] - ETA: 0s - loss: 0.0116
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0218
4016/6530 [=================>............] - ETA: 0s - loss: 0.0118
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0214
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0117
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0207
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0117
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0209
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0117
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0208
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0117
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0208
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0118
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0207
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0118
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0208
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0118
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0206
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0117
3040/6530 [============>.................] - ETA: 0s - loss: 0.0204
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0117
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0205
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0117
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0204
6496/6530 [============================>.] - ETA: 0s - loss: 0.0116
3824/6530 [================>.............] - ETA: 0s - loss: 0.0203
6530/6530 [==============================] - 2s 239us/step - loss: 0.0117 - val_loss: 0.0091
Epoch 38/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0078
4064/6530 [=================>............] - ETA: 0s - loss: 0.0202
 240/6530 [>.............................] - ETA: 1s - loss: 0.0106
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0201
 464/6530 [=>............................] - ETA: 1s - loss: 0.0124
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0201
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0128
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0202
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0122
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0203
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0127
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0202
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0126
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0203
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0123
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0202
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0121
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0204
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0119
6352/6530 [============================>.] - ETA: 0s - loss: 0.0204
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0118
6530/6530 [==============================] - 1s 210us/step - loss: 0.0204 - val_loss: 0.0103
Epoch 56/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0180
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0118
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0117
 288/6530 [>.............................] - ETA: 1s - loss: 0.0187
 544/6530 [=>............................] - ETA: 1s - loss: 0.0196
2960/6530 [============>.................] - ETA: 0s - loss: 0.0115
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0190
3184/6530 [=============>................] - ETA: 0s - loss: 0.0113
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0195
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0113
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0201
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0116
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0206
3808/6530 [================>.............] - ETA: 0s - loss: 0.0115
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0208
4016/6530 [=================>............] - ETA: 0s - loss: 0.0116
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0211
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0115
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0215
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0115
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0218
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0115
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0217
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0115
2976/6530 [============>.................] - ETA: 0s - loss: 0.0221
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0116
3216/6530 [=============>................] - ETA: 0s - loss: 0.0220
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0116
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0220
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0116
3728/6530 [================>.............] - ETA: 0s - loss: 0.0221
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0116
3968/6530 [=================>............] - ETA: 0s - loss: 0.0219
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0115
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0216
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0115
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0215
6512/6530 [============================>.] - ETA: 0s - loss: 0.0115
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0215
6530/6530 [==============================] - 2s 238us/step - loss: 0.0115 - val_loss: 0.0092
Epoch 39/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0078
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0215
 240/6530 [>.............................] - ETA: 1s - loss: 0.0104
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0214
 464/6530 [=>............................] - ETA: 1s - loss: 0.0122
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0213
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0123
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0213
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0121
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0213
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0125
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0213
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0125
6432/6530 [============================>.] - ETA: 0s - loss: 0.0214
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0121
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0119
6530/6530 [==============================] - 1s 215us/step - loss: 0.0213 - val_loss: 0.0104
Epoch 57/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0207
2016/6530 [========>.....................] - ETA: 1s - loss: 0.0117
 272/6530 [>.............................] - ETA: 1s - loss: 0.0215
2224/6530 [=========>....................] - ETA: 1s - loss: 0.0117
 512/6530 [=>............................] - ETA: 1s - loss: 0.0198
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0116
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0221
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0116
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0215
2896/6530 [============>.................] - ETA: 0s - loss: 0.0114
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0212
3104/6530 [=============>................] - ETA: 0s - loss: 0.0112
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0214
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0112
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0217
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0113
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0213
3776/6530 [================>.............] - ETA: 0s - loss: 0.0113
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0210
4000/6530 [=================>............] - ETA: 0s - loss: 0.0114
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0209
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0114
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0208
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0114
2992/6530 [============>.................] - ETA: 0s - loss: 0.0210
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0113
3248/6530 [=============>................] - ETA: 0s - loss: 0.0207
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0113
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0208
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0114
3760/6530 [================>.............] - ETA: 0s - loss: 0.0207
4016/6530 [=================>............] - ETA: 0s - loss: 0.0206
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0115
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0114
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0206
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0114
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0205
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0113
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0203
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0113
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0202
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0202
6496/6530 [============================>.] - ETA: 0s - loss: 0.0113
6530/6530 [==============================] - 2s 239us/step - loss: 0.0113 - val_loss: 0.0091

5456/6530 [========================>.....] - ETA: 0s - loss: 0.0201
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0202
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0204
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0203
6416/6530 [============================>.] - ETA: 0s - loss: 0.0203
6530/6530 [==============================] - 1s 217us/step - loss: 0.0202 - val_loss: 0.0110
Epoch 58/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0292
# training | RMSE: 0.0893, MAE: 0.0701
worker 2  xfile  [4, 81, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11512374354045503}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.08928700668792097, 'rmse': 0.08928700668792097, 'mae': 0.07007112357472553, 'early_stop': True}
vggnet done  2

 256/6530 [>.............................] - ETA: 1s - loss: 0.0190
 512/6530 [=>............................] - ETA: 1s - loss: 0.0181
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0187
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0199
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0198
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0201
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0202
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0200
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0199
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0200
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0201
2896/6530 [============>.................] - ETA: 0s - loss: 0.0200
3088/6530 [=============>................] - ETA: 0s - loss: 0.0201
3264/6530 [=============>................] - ETA: 0s - loss: 0.0200
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0201
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0201
3760/6530 [================>.............] - ETA: 0s - loss: 0.0200
3904/6530 [================>.............] - ETA: 0s - loss: 0.0199
4032/6530 [=================>............] - ETA: 0s - loss: 0.0200
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0202
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0202
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0203
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0203
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0202
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0203
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0203
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0204
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0206
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0205
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0205
6432/6530 [============================>.] - ETA: 0s - loss: 0.0204
6530/6530 [==============================] - 2s 261us/step - loss: 0.0206 - val_loss: 0.0103
Epoch 59/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0179
 176/6530 [..............................] - ETA: 1s - loss: 0.0270
 368/6530 [>.............................] - ETA: 1s - loss: 0.0232
 560/6530 [=>............................] - ETA: 1s - loss: 0.0210
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0214
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0212
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0209
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0207
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0203
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0199
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0199
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0200
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0202
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0198
2672/6530 [===========>..................] - ETA: 1s - loss: 0.0200
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0199
2944/6530 [============>.................] - ETA: 1s - loss: 0.0199
3104/6530 [=============>................] - ETA: 0s - loss: 0.0202
3264/6530 [=============>................] - ETA: 0s - loss: 0.0203
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0204
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0202
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0203
3808/6530 [================>.............] - ETA: 0s - loss: 0.0203
3968/6530 [=================>............] - ETA: 0s - loss: 0.0204
4128/6530 [=================>............] - ETA: 0s - loss: 0.0205
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0206
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0206
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0205
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0205
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0205
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0204
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0204
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0205
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0205
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0203
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0203
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0202
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0201
6416/6530 [============================>.] - ETA: 0s - loss: 0.0201
6530/6530 [==============================] - 2s 325us/step - loss: 0.0201 - val_loss: 0.0109
Epoch 60/81

  16/6530 [..............................] - ETA: 3s - loss: 0.0163
 144/6530 [..............................] - ETA: 2s - loss: 0.0225
 304/6530 [>.............................] - ETA: 2s - loss: 0.0234
 432/6530 [>.............................] - ETA: 2s - loss: 0.0220
 544/6530 [=>............................] - ETA: 2s - loss: 0.0212
 672/6530 [==>...........................] - ETA: 2s - loss: 0.0204
 800/6530 [==>...........................] - ETA: 2s - loss: 0.0218
 944/6530 [===>..........................] - ETA: 2s - loss: 0.0218
1104/6530 [====>.........................] - ETA: 2s - loss: 0.0219
1264/6530 [====>.........................] - ETA: 2s - loss: 0.0216
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0219
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0217
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0214
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0215
2016/6530 [========>.....................] - ETA: 1s - loss: 0.0219
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0219
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0216
2512/6530 [==========>...................] - ETA: 1s - loss: 0.0218
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0215
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0217
2896/6530 [============>.................] - ETA: 1s - loss: 0.0215
3024/6530 [============>.................] - ETA: 1s - loss: 0.0213
3152/6530 [=============>................] - ETA: 1s - loss: 0.0216
3264/6530 [=============>................] - ETA: 1s - loss: 0.0216
3424/6530 [==============>...............] - ETA: 1s - loss: 0.0218
3568/6530 [===============>..............] - ETA: 1s - loss: 0.0218
3760/6530 [================>.............] - ETA: 1s - loss: 0.0215
3936/6530 [=================>............] - ETA: 0s - loss: 0.0214
4096/6530 [=================>............] - ETA: 0s - loss: 0.0212
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0212
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0211
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0212
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0212
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0211
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0210
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0209
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0208
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0207
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0206
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0205
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0205
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0204
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0203
6384/6530 [============================>.] - ETA: 0s - loss: 0.0203
6530/6530 [==============================] - 2s 367us/step - loss: 0.0202 - val_loss: 0.0107
Epoch 61/81

  16/6530 [..............................] - ETA: 3s - loss: 0.0139
 160/6530 [..............................] - ETA: 2s - loss: 0.0215
 288/6530 [>.............................] - ETA: 2s - loss: 0.0220
 432/6530 [>.............................] - ETA: 2s - loss: 0.0222
 560/6530 [=>............................] - ETA: 2s - loss: 0.0219
 704/6530 [==>...........................] - ETA: 2s - loss: 0.0223
 848/6530 [==>...........................] - ETA: 2s - loss: 0.0218
 992/6530 [===>..........................] - ETA: 2s - loss: 0.0225
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0233
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0233
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0228
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0223
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0224
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0224
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0223
2224/6530 [=========>....................] - ETA: 1s - loss: 0.0221
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0220
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0219
2672/6530 [===========>..................] - ETA: 1s - loss: 0.0217
2864/6530 [============>.................] - ETA: 1s - loss: 0.0214
3072/6530 [=============>................] - ETA: 1s - loss: 0.0213
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0209
3488/6530 [===============>..............] - ETA: 1s - loss: 0.0209
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0208
3776/6530 [================>.............] - ETA: 0s - loss: 0.0207
3904/6530 [================>.............] - ETA: 0s - loss: 0.0207
4048/6530 [=================>............] - ETA: 0s - loss: 0.0206
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0205
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0204
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0204
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0202
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0202
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0204
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0204
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0204
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0204
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0204
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0206
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0206
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0207
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0206
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0206
6384/6530 [============================>.] - ETA: 0s - loss: 0.0205
6528/6530 [============================>.] - ETA: 0s - loss: 0.0205
6530/6530 [==============================] - 2s 365us/step - loss: 0.0205 - val_loss: 0.0103
Epoch 62/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0203
 144/6530 [..............................] - ETA: 2s - loss: 0.0206
 288/6530 [>.............................] - ETA: 2s - loss: 0.0187
 416/6530 [>.............................] - ETA: 2s - loss: 0.0203
 560/6530 [=>............................] - ETA: 2s - loss: 0.0209
 704/6530 [==>...........................] - ETA: 2s - loss: 0.0209
 848/6530 [==>...........................] - ETA: 2s - loss: 0.0209
1008/6530 [===>..........................] - ETA: 2s - loss: 0.0203
1136/6530 [====>.........................] - ETA: 2s - loss: 0.0204
1280/6530 [====>.........................] - ETA: 2s - loss: 0.0207
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0207
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0203
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0204
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0204
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0202
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0204
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0203
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0204
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0206
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0206
2992/6530 [============>.................] - ETA: 1s - loss: 0.0204
3200/6530 [=============>................] - ETA: 1s - loss: 0.0203
3376/6530 [==============>...............] - ETA: 1s - loss: 0.0203
3552/6530 [===============>..............] - ETA: 1s - loss: 0.0203
3760/6530 [================>.............] - ETA: 0s - loss: 0.0200
3984/6530 [=================>............] - ETA: 0s - loss: 0.0199
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0199
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0199
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0199
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0198
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0199
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0198
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0199
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0201
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0202
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0202
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0201
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0201
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0201
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0201
6448/6530 [============================>.] - ETA: 0s - loss: 0.0202
6530/6530 [==============================] - 2s 337us/step - loss: 0.0202 - val_loss: 0.0104
Epoch 63/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0366
 176/6530 [..............................] - ETA: 2s - loss: 0.0189
 320/6530 [>.............................] - ETA: 2s - loss: 0.0196
 480/6530 [=>............................] - ETA: 2s - loss: 0.0187
 624/6530 [=>............................] - ETA: 2s - loss: 0.0181
 768/6530 [==>...........................] - ETA: 2s - loss: 0.0183
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0195
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0197
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0197
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0196
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0199
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0196
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0194
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0194
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0193
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0194
2816/6530 [===========>..................] - ETA: 1s - loss: 0.0195
2960/6530 [============>.................] - ETA: 1s - loss: 0.0194
3088/6530 [=============>................] - ETA: 1s - loss: 0.0195
3216/6530 [=============>................] - ETA: 1s - loss: 0.0196
3344/6530 [==============>...............] - ETA: 1s - loss: 0.0196
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0195
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0192
3728/6530 [================>.............] - ETA: 0s - loss: 0.0193
3856/6530 [================>.............] - ETA: 0s - loss: 0.0194
4016/6530 [=================>............] - ETA: 0s - loss: 0.0196
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0197
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0197
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0197
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0197
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0197
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0197
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0197
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0198
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0198
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0198
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0198
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0197
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0197
6320/6530 [============================>.] - ETA: 0s - loss: 0.0197
6448/6530 [============================>.] - ETA: 0s - loss: 0.0198
6530/6530 [==============================] - 2s 340us/step - loss: 0.0197 - val_loss: 0.0101
Epoch 64/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0313
 144/6530 [..............................] - ETA: 2s - loss: 0.0229
 304/6530 [>.............................] - ETA: 2s - loss: 0.0209
 464/6530 [=>............................] - ETA: 2s - loss: 0.0210
 624/6530 [=>............................] - ETA: 2s - loss: 0.0208
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0206
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0209
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0207
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0201
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0203
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0203
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0208
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0203
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0204
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0204
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0203
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0201
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0204
2912/6530 [============>.................] - ETA: 1s - loss: 0.0204
3040/6530 [============>.................] - ETA: 1s - loss: 0.0204
3168/6530 [=============>................] - ETA: 1s - loss: 0.0204
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0203
3408/6530 [==============>...............] - ETA: 1s - loss: 0.0203
3536/6530 [===============>..............] - ETA: 1s - loss: 0.0203
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0201
3840/6530 [================>.............] - ETA: 0s - loss: 0.0201
4032/6530 [=================>............] - ETA: 0s - loss: 0.0201
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0199
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0198
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0198
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0197
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0196
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0195
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0194
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0194
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0194
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0193
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0194
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0193
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0193
6448/6530 [============================>.] - ETA: 0s - loss: 0.0194
6530/6530 [==============================] - 2s 336us/step - loss: 0.0194 - val_loss: 0.0103
Epoch 65/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0221
 144/6530 [..............................] - ETA: 2s - loss: 0.0226
 320/6530 [>.............................] - ETA: 2s - loss: 0.0193
 496/6530 [=>............................] - ETA: 1s - loss: 0.0191
 656/6530 [==>...........................] - ETA: 1s - loss: 0.0179
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0186
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0183
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0190
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0194
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0195
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0195
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0194
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0195
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0192
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0194
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0197
2560/6530 [==========>...................] - ETA: 1s - loss: 0.0197
2720/6530 [===========>..................] - ETA: 1s - loss: 0.0198
2896/6530 [============>.................] - ETA: 1s - loss: 0.0198
3088/6530 [=============>................] - ETA: 1s - loss: 0.0199
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0198
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0199
3728/6530 [================>.............] - ETA: 0s - loss: 0.0197
3872/6530 [================>.............] - ETA: 0s - loss: 0.0197
4032/6530 [=================>............] - ETA: 0s - loss: 0.0197
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0196
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0196
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0196
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0195
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0197
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0197
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0199
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0199
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0199
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0198
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0198
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0198
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0200
6464/6530 [============================>.] - ETA: 0s - loss: 0.0199
6530/6530 [==============================] - 2s 317us/step - loss: 0.0200 - val_loss: 0.0110
Epoch 66/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0116
 256/6530 [>.............................] - ETA: 1s - loss: 0.0210
 448/6530 [=>............................] - ETA: 1s - loss: 0.0217
 592/6530 [=>............................] - ETA: 1s - loss: 0.0219
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0219
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0212
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0215
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0213
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0211
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0207
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0203
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0202
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0203
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0204
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0200
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0201
2784/6530 [===========>..................] - ETA: 1s - loss: 0.0201
2960/6530 [============>.................] - ETA: 1s - loss: 0.0199
3152/6530 [=============>................] - ETA: 1s - loss: 0.0203
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0202
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0201
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0202
3728/6530 [================>.............] - ETA: 0s - loss: 0.0201
3872/6530 [================>.............] - ETA: 0s - loss: 0.0200
4000/6530 [=================>............] - ETA: 0s - loss: 0.0202
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0200
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0201
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0201
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0203
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0202
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0202
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0201
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0200
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0200
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0200
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0200
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0200
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0200
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0200
6480/6530 [============================>.] - ETA: 0s - loss: 0.0200
6530/6530 [==============================] - 2s 326us/step - loss: 0.0200 - val_loss: 0.0101
Epoch 67/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0075
 192/6530 [..............................] - ETA: 1s - loss: 0.0147
 384/6530 [>.............................] - ETA: 1s - loss: 0.0150
 576/6530 [=>............................] - ETA: 1s - loss: 0.0177
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0180
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0175
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0182
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0186
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0183
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0185
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0187
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0187
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0184
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0184
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0187
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0184
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0184
2688/6530 [===========>..................] - ETA: 1s - loss: 0.0184
2832/6530 [============>.................] - ETA: 1s - loss: 0.0183
2976/6530 [============>.................] - ETA: 1s - loss: 0.0186
3104/6530 [=============>................] - ETA: 1s - loss: 0.0186
3232/6530 [=============>................] - ETA: 1s - loss: 0.0188
3376/6530 [==============>...............] - ETA: 1s - loss: 0.0190
3520/6530 [===============>..............] - ETA: 1s - loss: 0.0191
3648/6530 [===============>..............] - ETA: 1s - loss: 0.0192
3776/6530 [================>.............] - ETA: 0s - loss: 0.0192
3936/6530 [=================>............] - ETA: 0s - loss: 0.0193
4080/6530 [=================>............] - ETA: 0s - loss: 0.0196
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0198
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0197
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0197
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0197
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0198
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0197
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0198
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0198
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0199
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0197
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0197
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0196
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0196
6464/6530 [============================>.] - ETA: 0s - loss: 0.0196
6530/6530 [==============================] - 2s 345us/step - loss: 0.0196 - val_loss: 0.0102
Epoch 68/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0221
 224/6530 [>.............................] - ETA: 1s - loss: 0.0218
 448/6530 [=>............................] - ETA: 1s - loss: 0.0203
 624/6530 [=>............................] - ETA: 1s - loss: 0.0200
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0207
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0216
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0211
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0211
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0208
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0201
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0202
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0203
2256/6530 [=========>....................] - ETA: 1s - loss: 0.0201
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0202
2688/6530 [===========>..................] - ETA: 1s - loss: 0.0202
2912/6530 [============>.................] - ETA: 0s - loss: 0.0201
3104/6530 [=============>................] - ETA: 0s - loss: 0.0202
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0200
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0200
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0201
3840/6530 [================>.............] - ETA: 0s - loss: 0.0200
4000/6530 [=================>............] - ETA: 0s - loss: 0.0199
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0199
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0198
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0200
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0199
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0197
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0196
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0195
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0196
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0197
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0197
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0197
6448/6530 [============================>.] - ETA: 0s - loss: 0.0198
6530/6530 [==============================] - 2s 279us/step - loss: 0.0198 - val_loss: 0.0101
Epoch 69/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0159
 192/6530 [..............................] - ETA: 1s - loss: 0.0189
 384/6530 [>.............................] - ETA: 1s - loss: 0.0193
 576/6530 [=>............................] - ETA: 1s - loss: 0.0189
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0191
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0206
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0200
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0197
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0197
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0198
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0195
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0194
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0197
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0199
2560/6530 [==========>...................] - ETA: 1s - loss: 0.0197
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0198
2912/6530 [============>.................] - ETA: 1s - loss: 0.0199
3072/6530 [=============>................] - ETA: 1s - loss: 0.0201
3216/6530 [=============>................] - ETA: 0s - loss: 0.0200
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0199
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0199
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0199
3824/6530 [================>.............] - ETA: 0s - loss: 0.0197
3968/6530 [=================>............] - ETA: 0s - loss: 0.0196
4112/6530 [=================>............] - ETA: 0s - loss: 0.0196
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0198
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0198
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0199
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0201
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0200
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0201
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0201
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0201
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0201
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0201
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0201
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0200
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0199
6416/6530 [============================>.] - ETA: 0s - loss: 0.0199
6530/6530 [==============================] - 2s 325us/step - loss: 0.0199 - val_loss: 0.0101
Epoch 70/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0343
 176/6530 [..............................] - ETA: 2s - loss: 0.0192
 304/6530 [>.............................] - ETA: 2s - loss: 0.0183
 432/6530 [>.............................] - ETA: 2s - loss: 0.0185
 576/6530 [=>............................] - ETA: 2s - loss: 0.0188
 720/6530 [==>...........................] - ETA: 2s - loss: 0.0183
 880/6530 [===>..........................] - ETA: 2s - loss: 0.0191
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0192
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0195
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0194
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0196
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0194
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0195
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0195
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0193
2224/6530 [=========>....................] - ETA: 1s - loss: 0.0193
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0194
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0196
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0196
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0195
2928/6530 [============>.................] - ETA: 1s - loss: 0.0199
3072/6530 [=============>................] - ETA: 1s - loss: 0.0200
3200/6530 [=============>................] - ETA: 1s - loss: 0.0202
3312/6530 [==============>...............] - ETA: 1s - loss: 0.0202
3424/6530 [==============>...............] - ETA: 1s - loss: 0.0201
3536/6530 [===============>..............] - ETA: 1s - loss: 0.0201
3664/6530 [===============>..............] - ETA: 1s - loss: 0.0202
3792/6530 [================>.............] - ETA: 1s - loss: 0.0201
3936/6530 [=================>............] - ETA: 0s - loss: 0.0199
4096/6530 [=================>............] - ETA: 0s - loss: 0.0199
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0198
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0198
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0198
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0197
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0196
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0196
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0194
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0195
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0194
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0194
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0193
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0192
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0193
6432/6530 [============================>.] - ETA: 0s - loss: 0.0191
6530/6530 [==============================] - 2s 363us/step - loss: 0.0191 - val_loss: 0.0102
Epoch 71/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0127
 144/6530 [..............................] - ETA: 2s - loss: 0.0157
 288/6530 [>.............................] - ETA: 2s - loss: 0.0174
 416/6530 [>.............................] - ETA: 2s - loss: 0.0174
 560/6530 [=>............................] - ETA: 2s - loss: 0.0172
 688/6530 [==>...........................] - ETA: 2s - loss: 0.0176
 832/6530 [==>...........................] - ETA: 2s - loss: 0.0182
 976/6530 [===>..........................] - ETA: 2s - loss: 0.0191
1120/6530 [====>.........................] - ETA: 2s - loss: 0.0191
1264/6530 [====>.........................] - ETA: 2s - loss: 0.0191
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0188
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0188
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0188
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0188
2016/6530 [========>.....................] - ETA: 1s - loss: 0.0190
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0189
2288/6530 [=========>....................] - ETA: 1s - loss: 0.0191
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0193
2608/6530 [==========>...................] - ETA: 1s - loss: 0.0194
2784/6530 [===========>..................] - ETA: 1s - loss: 0.0195
2960/6530 [============>.................] - ETA: 1s - loss: 0.0194
3168/6530 [=============>................] - ETA: 1s - loss: 0.0197
3376/6530 [==============>...............] - ETA: 1s - loss: 0.0199
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0199
3808/6530 [================>.............] - ETA: 0s - loss: 0.0202
4032/6530 [=================>............] - ETA: 0s - loss: 0.0201
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0202
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0206
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0205
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0204
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0204
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0204
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0204
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0204
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0203
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0201
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0201
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0202
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0201
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0202
6336/6530 [============================>.] - ETA: 0s - loss: 0.0201
6480/6530 [============================>.] - ETA: 0s - loss: 0.0200
6530/6530 [==============================] - 2s 345us/step - loss: 0.0200 - val_loss: 0.0098
Epoch 72/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0201
 176/6530 [..............................] - ETA: 2s - loss: 0.0191
 336/6530 [>.............................] - ETA: 2s - loss: 0.0182
 496/6530 [=>............................] - ETA: 1s - loss: 0.0191
 640/6530 [=>............................] - ETA: 1s - loss: 0.0205
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0202
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0198
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0197
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0199
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0204
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0201
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0198
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0199
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0201
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0203
2224/6530 [=========>....................] - ETA: 1s - loss: 0.0205
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0204
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0204
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0205
2832/6530 [============>.................] - ETA: 1s - loss: 0.0207
2976/6530 [============>.................] - ETA: 1s - loss: 0.0208
3120/6530 [=============>................] - ETA: 1s - loss: 0.0206
3264/6530 [=============>................] - ETA: 1s - loss: 0.0205
3408/6530 [==============>...............] - ETA: 1s - loss: 0.0205
3552/6530 [===============>..............] - ETA: 1s - loss: 0.0204
3712/6530 [================>.............] - ETA: 1s - loss: 0.0203
3872/6530 [================>.............] - ETA: 0s - loss: 0.0202
4032/6530 [=================>............] - ETA: 0s - loss: 0.0202
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0203
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0203
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0204
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0203
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0205
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0204
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0202
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0203
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0202
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0203
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0205
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0204
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0204
6368/6530 [============================>.] - ETA: 0s - loss: 0.0204
6530/6530 [==============================] - 2s 343us/step - loss: 0.0203 - val_loss: 0.0102
Epoch 73/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0263
 224/6530 [>.............................] - ETA: 1s - loss: 0.0220
 448/6530 [=>............................] - ETA: 1s - loss: 0.0210
 656/6530 [==>...........................] - ETA: 1s - loss: 0.0211
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0222
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0223
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0224
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0219
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0209
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0207
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0205
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0206
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0203
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0202
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0204
2592/6530 [==========>...................] - ETA: 1s - loss: 0.0202
2816/6530 [===========>..................] - ETA: 1s - loss: 0.0201
3040/6530 [============>.................] - ETA: 1s - loss: 0.0201
3216/6530 [=============>................] - ETA: 0s - loss: 0.0201
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0200
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0200
3712/6530 [================>.............] - ETA: 0s - loss: 0.0200
3888/6530 [================>.............] - ETA: 0s - loss: 0.0201
4064/6530 [=================>............] - ETA: 0s - loss: 0.0201
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0202
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0201
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0201
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0200
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0201
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0202
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0202
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0202
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0202
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0203
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0202
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0203
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0205
6480/6530 [============================>.] - ETA: 0s - loss: 0.0204
6530/6530 [==============================] - 2s 308us/step - loss: 0.0204 - val_loss: 0.0101
Epoch 74/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0275
 192/6530 [..............................] - ETA: 1s - loss: 0.0215
 384/6530 [>.............................] - ETA: 1s - loss: 0.0189
 576/6530 [=>............................] - ETA: 1s - loss: 0.0188
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0184
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0192
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0196
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0196
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0196
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0196
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0197
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0197
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0198
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0200
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0200
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0200
2544/6530 [==========>...................] - ETA: 1s - loss: 0.0200
2704/6530 [===========>..................] - ETA: 1s - loss: 0.0200
2880/6530 [============>.................] - ETA: 1s - loss: 0.0199
3024/6530 [============>.................] - ETA: 1s - loss: 0.0197
3152/6530 [=============>................] - ETA: 1s - loss: 0.0197
3280/6530 [==============>...............] - ETA: 1s - loss: 0.0199
3440/6530 [==============>...............] - ETA: 1s - loss: 0.0196
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0198
3728/6530 [================>.............] - ETA: 0s - loss: 0.0198
3904/6530 [================>.............] - ETA: 0s - loss: 0.0196
4080/6530 [=================>............] - ETA: 0s - loss: 0.0196
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0197
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0196
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0198
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0199
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0198
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0199
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0198
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0198
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0197
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0198
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0198
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0198
6464/6530 [============================>.] - ETA: 0s - loss: 0.0199
6530/6530 [==============================] - 2s 326us/step - loss: 0.0199 - val_loss: 0.0098
Epoch 75/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0168
 160/6530 [..............................] - ETA: 2s - loss: 0.0221
 288/6530 [>.............................] - ETA: 2s - loss: 0.0219
 416/6530 [>.............................] - ETA: 2s - loss: 0.0206
 576/6530 [=>............................] - ETA: 2s - loss: 0.0204
 736/6530 [==>...........................] - ETA: 2s - loss: 0.0200
 848/6530 [==>...........................] - ETA: 2s - loss: 0.0200
 976/6530 [===>..........................] - ETA: 2s - loss: 0.0200
1104/6530 [====>.........................] - ETA: 2s - loss: 0.0204
1248/6530 [====>.........................] - ETA: 2s - loss: 0.0205
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0204
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0197
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0201
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0197
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0197
2512/6530 [==========>...................] - ETA: 1s - loss: 0.0194
2752/6530 [===========>..................] - ETA: 1s - loss: 0.0196
2944/6530 [============>.................] - ETA: 1s - loss: 0.0199
3136/6530 [=============>................] - ETA: 1s - loss: 0.0198
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0197
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0195
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0193
3872/6530 [================>.............] - ETA: 0s - loss: 0.0193
4032/6530 [=================>............] - ETA: 0s - loss: 0.0195
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0194
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0196
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0196
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0196
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0197
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0198
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0199
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0199
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0200
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0198
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0198
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0199
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0199
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0199
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0198
6352/6530 [============================>.] - ETA: 0s - loss: 0.0198
6496/6530 [============================>.] - ETA: 0s - loss: 0.0198
6530/6530 [==============================] - 2s 339us/step - loss: 0.0199 - val_loss: 0.0103
Epoch 76/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0248
 144/6530 [..............................] - ETA: 2s - loss: 0.0223
 288/6530 [>.............................] - ETA: 2s - loss: 0.0214
 448/6530 [=>............................] - ETA: 2s - loss: 0.0219
 608/6530 [=>............................] - ETA: 2s - loss: 0.0204
 752/6530 [==>...........................] - ETA: 2s - loss: 0.0197
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0187
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0198
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0195
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0197
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0195
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0198
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0197
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0197
2544/6530 [==========>...................] - ETA: 1s - loss: 0.0196
2720/6530 [===========>..................] - ETA: 1s - loss: 0.0195
2864/6530 [============>.................] - ETA: 1s - loss: 0.0193
3024/6530 [============>.................] - ETA: 1s - loss: 0.0193
3184/6530 [=============>................] - ETA: 0s - loss: 0.0191
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0191
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0191
3712/6530 [================>.............] - ETA: 0s - loss: 0.0192
3920/6530 [=================>............] - ETA: 0s - loss: 0.0193
4128/6530 [=================>............] - ETA: 0s - loss: 0.0193
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0195
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0194
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0194
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0193
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0192
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0191
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0191
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0191
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0192
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0192
6432/6530 [============================>.] - ETA: 0s - loss: 0.0192
6530/6530 [==============================] - 2s 286us/step - loss: 0.0193 - val_loss: 0.0107
Epoch 77/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0237
 160/6530 [..............................] - ETA: 2s - loss: 0.0221
 320/6530 [>.............................] - ETA: 2s - loss: 0.0227
 448/6530 [=>............................] - ETA: 2s - loss: 0.0215
 576/6530 [=>............................] - ETA: 2s - loss: 0.0210
 720/6530 [==>...........................] - ETA: 2s - loss: 0.0202
 864/6530 [==>...........................] - ETA: 2s - loss: 0.0196
 992/6530 [===>..........................] - ETA: 2s - loss: 0.0210
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0210
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0205
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0204
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0203
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0199
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0199
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0197
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0195
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0196
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0196
2784/6530 [===========>..................] - ETA: 1s - loss: 0.0198
2944/6530 [============>.................] - ETA: 1s - loss: 0.0197
3136/6530 [=============>................] - ETA: 1s - loss: 0.0199
3280/6530 [==============>...............] - ETA: 1s - loss: 0.0198
3424/6530 [==============>...............] - ETA: 1s - loss: 0.0197
3584/6530 [===============>..............] - ETA: 1s - loss: 0.0197
3744/6530 [================>.............] - ETA: 0s - loss: 0.0195
3888/6530 [================>.............] - ETA: 0s - loss: 0.0197
4048/6530 [=================>............] - ETA: 0s - loss: 0.0197
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0199
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0197
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0197
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0198
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0198
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0197
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0198
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0198
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0197
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0197
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0196
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0196
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0195
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0195
6448/6530 [============================>.] - ETA: 0s - loss: 0.0196
6530/6530 [==============================] - 2s 347us/step - loss: 0.0195 - val_loss: 0.0098
Epoch 78/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0123
 160/6530 [..............................] - ETA: 2s - loss: 0.0202
 304/6530 [>.............................] - ETA: 2s - loss: 0.0182
 496/6530 [=>............................] - ETA: 1s - loss: 0.0175
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0183
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0178
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0174
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0183
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0183
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0183
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0183
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0185
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0183
2224/6530 [=========>....................] - ETA: 1s - loss: 0.0187
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0186
2560/6530 [==========>...................] - ETA: 1s - loss: 0.0185
2720/6530 [===========>..................] - ETA: 1s - loss: 0.0184
2896/6530 [============>.................] - ETA: 1s - loss: 0.0184
3088/6530 [=============>................] - ETA: 1s - loss: 0.0184
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0184
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0184
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0184
3808/6530 [================>.............] - ETA: 0s - loss: 0.0184
3952/6530 [=================>............] - ETA: 0s - loss: 0.0186
4096/6530 [=================>............] - ETA: 0s - loss: 0.0186
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0186
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0186
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0185
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0184
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0184
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0184
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0186
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0186
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0185
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0185
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0184
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0185
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0187
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0187
6352/6530 [============================>.] - ETA: 0s - loss: 0.0186
6496/6530 [============================>.] - ETA: 0s - loss: 0.0186
6530/6530 [==============================] - 2s 333us/step - loss: 0.0187 - val_loss: 0.0107
Epoch 79/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0278
 208/6530 [..............................] - ETA: 1s - loss: 0.0202
 400/6530 [>.............................] - ETA: 1s - loss: 0.0200
 592/6530 [=>............................] - ETA: 1s - loss: 0.0193
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0189
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0193
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0192
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0188
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0191
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0192
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0191
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0194
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0193
2288/6530 [=========>....................] - ETA: 1s - loss: 0.0193
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0198
2608/6530 [==========>...................] - ETA: 1s - loss: 0.0198
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0197
2896/6530 [============>.................] - ETA: 1s - loss: 0.0195
3056/6530 [=============>................] - ETA: 1s - loss: 0.0197
3216/6530 [=============>................] - ETA: 1s - loss: 0.0197
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0196
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0197
3744/6530 [================>.............] - ETA: 0s - loss: 0.0197
3936/6530 [=================>............] - ETA: 0s - loss: 0.0195
4096/6530 [=================>............] - ETA: 0s - loss: 0.0194
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0194
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0194
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0194
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0195
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0195
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0193
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0193
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0193
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0193
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0194
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0194
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0193
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0194
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0193
6496/6530 [============================>.] - ETA: 0s - loss: 0.0193
6530/6530 [==============================] - 2s 325us/step - loss: 0.0193 - val_loss: 0.0096
Epoch 80/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0307
 224/6530 [>.............................] - ETA: 1s - loss: 0.0213
 416/6530 [>.............................] - ETA: 1s - loss: 0.0223
 624/6530 [=>............................] - ETA: 1s - loss: 0.0223
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0212
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0221
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0215
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0207
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0203
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0197
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0200
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0200
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0203
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0202
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0198
2832/6530 [============>.................] - ETA: 1s - loss: 0.0197
2976/6530 [============>.................] - ETA: 0s - loss: 0.0196
3152/6530 [=============>................] - ETA: 0s - loss: 0.0194
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0193
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0194
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0193
3808/6530 [================>.............] - ETA: 0s - loss: 0.0191
4016/6530 [=================>............] - ETA: 0s - loss: 0.0191
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0190
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0190
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0190
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0190
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0192
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0191
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0192
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0193
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0193
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0193
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0193
6320/6530 [============================>.] - ETA: 0s - loss: 0.0195
6480/6530 [============================>.] - ETA: 0s - loss: 0.0194
6530/6530 [==============================] - 2s 293us/step - loss: 0.0194 - val_loss: 0.0098
Epoch 81/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0351
 224/6530 [>.............................] - ETA: 1s - loss: 0.0213
 416/6530 [>.............................] - ETA: 1s - loss: 0.0212
 576/6530 [=>............................] - ETA: 1s - loss: 0.0202
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0197
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0190
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0191
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0190
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0193
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0192
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0193
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0198
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0202
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0203
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0202
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0199
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0201
2672/6530 [===========>..................] - ETA: 1s - loss: 0.0200
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0199
2960/6530 [============>.................] - ETA: 1s - loss: 0.0205
3120/6530 [=============>................] - ETA: 1s - loss: 0.0203
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0201
3504/6530 [===============>..............] - ETA: 1s - loss: 0.0201
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0202
3840/6530 [================>.............] - ETA: 0s - loss: 0.0201
3968/6530 [=================>............] - ETA: 0s - loss: 0.0201
4096/6530 [=================>............] - ETA: 0s - loss: 0.0200
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0199
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0198
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0199
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0199
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0200
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0199
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0201
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0199
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0199
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0198
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0198
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0198
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0199
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0200
6496/6530 [============================>.] - ETA: 0s - loss: 0.0198
6530/6530 [==============================] - 2s 344us/step - loss: 0.0198 - val_loss: 0.0104

# training | RMSE: 0.0917, MAE: 0.0711
worker 0  xfile  [3, 81, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13682127961566426}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4355574414439555}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.09170017267563997, 'rmse': 0.09170017267563997, 'mae': 0.07111755172700979, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=81 loss={'loss': 0.17395038013619699, 'rmse': 0.17395038013619699, 'mae': 0.14866187228985722, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 95, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2536073059624152}, 'layer_4_size': 79, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#2 epoch=81 loss={'loss': 0.09014438947307063, 'rmse': 0.09014438947307063, 'mae': 0.070645004799532, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27296712342622304}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 99, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#1 epoch=81 loss={'loss': 0.09777272165654949, 'rmse': 0.09777272165654949, 'mae': 0.07362662924793029, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 32, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20096170173138628}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#4 epoch=81 loss={'loss': 0.08928700668792097, 'rmse': 0.08928700668792097, 'mae': 0.07007112357472553, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11512374354045503}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#3 epoch=81 loss={'loss': 0.09170017267563997, 'rmse': 0.09170017267563997, 'mae': 0.07111755172700979, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13682127961566426}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4355574414439555}, 'layer_4_size': 84, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
get a list [results] of length 206
get a list [loss] of length 5
get a list [val_loss] of length 5
length of indices is (4, 2, 3, 1, 0)
length of indices is 5
length of T is 5
206 total, best:

loss: 7.05%  | #81.0th iterations | run 0 
("{'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', "
 "'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': "
 "'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, "
 "'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, "
 "'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': "
 "{'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', "
 "'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': "
 "'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': "
 "'StandardScaler', 'shuffle': False}")

loss: 8.38%  | #27.0th iterations | run 2 
("{'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', "
 "'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': "
 "'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, "
 "'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, "
 "'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': "
 "{'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'relu', "
 "'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': "
 "'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': "
 "'StandardScaler', 'shuffle': False}")

loss: 8.88%  | #27.0th iterations | run 1 
("{'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', "
 "'layer_1_extras': {'name': 'dropout', 'rate': 0.20166402653594076}, "
 "'layer_1_size': 96, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': "
 "'batchnorm'}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', "
 "'layer_3_extras': {'name': 'dropout', 'rate': 0.26587831508262944}, "
 "'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': "
 "{'name': 'dropout', 'rate': 0.1965905869310106}, 'layer_4_size': 86, "
 "'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, "
 "'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, "
 "'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}")

loss: 8.93%  | #81.0th iterations | run 0 
("{'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', "
 "'layer_1_extras': {'name': None}, 'layer_1_size': 42, 'layer_2_activation': "
 "'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, "
 "'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, "
 "'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': "
 "'batchnorm'}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', "
 "'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': "
 "'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': "
 "'MaxAbsScaler', 'shuffle': True}")

loss: 8.93%  | #81th iterations | run 4 
("{'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', "
 "'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': "
 "'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, "
 "'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, "
 "'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': "
 "'batchnorm'}, 'layer_4_size': 7, 'layer_5_activation': 'tanh', "
 "'layer_5_extras': {'name': 'dropout', 'rate': 0.11512374354045503}, "
 "'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 5, "
 "'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}")

saving...

567 seconds.
