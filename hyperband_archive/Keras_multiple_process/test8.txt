loading data...
None
[0, 1, 2]
s=4
T is of size 81
T=[{'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.31449329379998714}, 'layer_1_size': 59, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3979325003082128}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3012214949206562}, 'layer_3_size': 31, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2504247577292705}, 'layer_2_size': 56, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23735034266556831}, 'layer_3_size': 75, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.28190637044148625}, 'layer_4_size': 4, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 87, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14071525978619703}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1632186816763678}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.13349068180705492}, 'layer_4_size': 48, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 65, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 50, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23285260965765855}, 'layer_1_size': 88, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15396607186536132}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19976518586726605}, 'layer_1_size': 92, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.45757510847559735}, 'layer_5_size': 23, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27543604469585237}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3363880143625354}, 'layer_2_size': 72, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 9, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4164082518612683}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 2, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2568172258114946}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 36, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16264077375686514}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4080403007617508}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16454352642837314}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4288788424853015}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14505351726939095}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 50, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.161128861263999}, 'layer_4_size': 17, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 2, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15238026352041448}, 'layer_1_size': 14, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 83, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 8, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.13912763458249527}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3022629720858989}, 'layer_4_size': 74, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23374835004413852}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 31, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19883823963787806}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.32044028165442373}, 'layer_3_size': 2, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17273845308660446}, 'layer_4_size': 11, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 71, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 41, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27066805008790573}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1156475839256515}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2212949453718571}, 'layer_5_size': 87, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3414675344100685}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3564867865679183}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3700148745940326}, 'layer_3_size': 34, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3142325446127674}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2684385694928465}, 'layer_1_size': 26, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47712176301521403}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4122081315162861}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4922590499368914}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.40054943618782857}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2665061266155887}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22385916107638965}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4540864912159174}, 'layer_2_size': 57, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3773403287489404}, 'layer_5_size': 35, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.40563576142447133}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11801951126986109}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1031887790315289}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22642497672744197}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32553354455869177}, 'layer_4_size': 46, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.48502693835638644}, 'layer_1_size': 37, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16159727813614957}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1291148842886615}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 43, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41090246001998465}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4901363409208247}, 'layer_1_size': 18, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 72, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23683208726054494}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1539922143106104}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23265042009909262}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12192595655153445}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19774261829234205}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2797140258496348}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45844819637653644}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4187861969586798}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4047564904270293}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 94, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24594424623809785}, 'layer_1_size': 56, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41103794745290845}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11328582488940891}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 5, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 67, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11932348426737889}, 'layer_4_size': 93, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16597532291562125}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2703434644417436}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18309647526297057}, 'layer_2_size': 49, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.24184757063105722}, 'layer_3_size': 20, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 29, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3667571823118735}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4850000697615927}, 'layer_2_size': 75, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35238667661361356}, 'layer_5_size': 37, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4190781584026906}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 90, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1979973379913189}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16220811735884047}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.29275903749501153}, 'layer_2_size': 28, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4430161926599384}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22959196988663796}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 72, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10988727565631695}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40210907670547635}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19153313597417265}, 'layer_1_size': 44, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.17522488099171737}, 'layer_2_size': 26, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 19, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19065135093673313}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38430490289489483}, 'layer_3_size': 25, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 74, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21860967678915208}, 'layer_2_size': 70, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3106122218642763}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3955433867376831}, 'layer_3_size': 61, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3314818760746431}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3345520542427325}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17031695865320207}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12100320754320126}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 70, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3621584604959015}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.49532365596879}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 61, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4400657261566402}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20286776821312336}, 'layer_5_size': 9, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 66, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45371219109203254}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2517984823985751}, 'layer_3_size': 84, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23988635610248485}, 'layer_4_size': 39, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.490310731404365}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32673705199921127}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11609532697927981}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3839055503304921}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24531321904768233}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3494088453763823}, 'layer_2_size': 3, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 84, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3970450730321857}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3662141381855081}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 47, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19535362569725787}, 'layer_2_size': 56, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.467049592575547}, 'layer_3_size': 56, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 42, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.25524733212409445}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.31580884906435236}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20591799748748119}, 'layer_1_size': 20, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18154684042572067}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 92, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3825331912408373}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18016369794913892}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.266971287531688}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.28716433280713766}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4839684055418453}, 'layer_4_size': 57, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4516167757823034}, 'layer_5_size': 37, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30631658816098606}, 'layer_1_size': 95, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4022096769016439}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20978160463021867}, 'layer_5_size': 11, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37992630451798837}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.19268164550348207}, 'layer_4_size': 34, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4737581842737165}, 'layer_4_size': 55, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.17040957422716485}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4156317035728765}, 'layer_4_size': 16, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13739898324337785}, 'layer_5_size': 10, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.45830290314732824}, 'layer_2_size': 18, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4629906801783099}, 'layer_4_size': 12, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19453446764642846}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38296540910752286}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.49364251121821134}, 'layer_2_size': 60, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 30, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 82, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3758471250524331}, 'layer_1_size': 30, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2411727778235374}, 'layer_2_size': 31, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 8, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 29, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.27484937105018914}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 87, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 44, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 69, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17547866501321385}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32305615123930476}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4159975004466281}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 34, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 15, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 17, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]
newly formed T structure is:[[0, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.31449329379998714}, 'layer_1_size': 59, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3979325003082128}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3012214949206562}, 'layer_3_size': 31, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [1, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2504247577292705}, 'layer_2_size': 56, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23735034266556831}, 'layer_3_size': 75, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.28190637044148625}, 'layer_4_size': 4, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 87, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [2, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14071525978619703}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1632186816763678}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.13349068180705492}, 'layer_4_size': 48, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [3, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 65, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 50, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [4, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23285260965765855}, 'layer_1_size': 88, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15396607186536132}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [5, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19976518586726605}, 'layer_1_size': 92, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.45757510847559735}, 'layer_5_size': 23, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [6, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27543604469585237}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3363880143625354}, 'layer_2_size': 72, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 9, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4164082518612683}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [7, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 2, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2568172258114946}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 36, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16264077375686514}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [8, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4080403007617508}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16454352642837314}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4288788424853015}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [9, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14505351726939095}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 50, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.161128861263999}, 'layer_4_size': 17, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 2, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [10, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15238026352041448}, 'layer_1_size': 14, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 83, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [11, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 8, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.13912763458249527}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3022629720858989}, 'layer_4_size': 74, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [12, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23374835004413852}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [13, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 31, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [14, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19883823963787806}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.32044028165442373}, 'layer_3_size': 2, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17273845308660446}, 'layer_4_size': 11, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [15, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 71, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 41, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [16, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27066805008790573}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1156475839256515}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2212949453718571}, 'layer_5_size': 87, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [17, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3414675344100685}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3564867865679183}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3700148745940326}, 'layer_3_size': 34, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3142325446127674}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [18, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [19, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2684385694928465}, 'layer_1_size': 26, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47712176301521403}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4122081315162861}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [20, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4922590499368914}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.40054943618782857}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2665061266155887}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [21, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22385916107638965}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4540864912159174}, 'layer_2_size': 57, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3773403287489404}, 'layer_5_size': 35, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [22, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.40563576142447133}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11801951126986109}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1031887790315289}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22642497672744197}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [23, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32553354455869177}, 'layer_4_size': 46, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [24, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.48502693835638644}, 'layer_1_size': 37, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16159727813614957}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1291148842886615}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 43, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [25, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41090246001998465}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [26, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4901363409208247}, 'layer_1_size': 18, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 72, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23683208726054494}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [27, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1539922143106104}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [28, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [29, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23265042009909262}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12192595655153445}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [30, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19774261829234205}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2797140258496348}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45844819637653644}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [31, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4187861969586798}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4047564904270293}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 94, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [32, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24594424623809785}, 'layer_1_size': 56, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41103794745290845}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11328582488940891}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [33, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 5, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 67, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [34, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11932348426737889}, 'layer_4_size': 93, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [35, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16597532291562125}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2703434644417436}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [36, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18309647526297057}, 'layer_2_size': 49, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [37, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.24184757063105722}, 'layer_3_size': 20, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 29, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [38, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3667571823118735}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4850000697615927}, 'layer_2_size': 75, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35238667661361356}, 'layer_5_size': 37, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [39, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4190781584026906}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [40, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 90, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1979973379913189}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [41, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16220811735884047}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.29275903749501153}, 'layer_2_size': 28, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4430161926599384}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [42, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22959196988663796}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 72, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10988727565631695}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40210907670547635}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [43, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [44, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19153313597417265}, 'layer_1_size': 44, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [45, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.17522488099171737}, 'layer_2_size': 26, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 19, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19065135093673313}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [46, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38430490289489483}, 'layer_3_size': 25, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 74, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [47, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21860967678915208}, 'layer_2_size': 70, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3106122218642763}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [48, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [49, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3955433867376831}, 'layer_3_size': 61, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3314818760746431}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3345520542427325}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [50, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17031695865320207}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12100320754320126}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [51, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 70, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3621584604959015}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [52, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.49532365596879}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 61, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4400657261566402}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20286776821312336}, 'layer_5_size': 9, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [53, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 66, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45371219109203254}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [54, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2517984823985751}, 'layer_3_size': 84, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23988635610248485}, 'layer_4_size': 39, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.490310731404365}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [55, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32673705199921127}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [56, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [57, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11609532697927981}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [58, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3839055503304921}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24531321904768233}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [59, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3494088453763823}, 'layer_2_size': 3, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 84, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3970450730321857}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [60, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3662141381855081}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [61, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [62, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 47, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19535362569725787}, 'layer_2_size': 56, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.467049592575547}, 'layer_3_size': 56, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [63, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 42, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [64, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.25524733212409445}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.31580884906435236}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [65, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20591799748748119}, 'layer_1_size': 20, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18154684042572067}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 92, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3825331912408373}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18016369794913892}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [66, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.266971287531688}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.28716433280713766}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4839684055418453}, 'layer_4_size': 57, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4516167757823034}, 'layer_5_size': 37, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [67, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30631658816098606}, 'layer_1_size': 95, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4022096769016439}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20978160463021867}, 'layer_5_size': 11, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [68, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [69, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37992630451798837}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.19268164550348207}, 'layer_4_size': 34, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [70, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4737581842737165}, 'layer_4_size': 55, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [71, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.17040957422716485}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4156317035728765}, 'layer_4_size': 16, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13739898324337785}, 'layer_5_size': 10, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [72, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.45830290314732824}, 'layer_2_size': 18, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4629906801783099}, 'layer_4_size': 12, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [73, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19453446764642846}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38296540910752286}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [74, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.49364251121821134}, 'layer_2_size': 60, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 30, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [75, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 82, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [76, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3758471250524331}, 'layer_1_size': 30, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2411727778235374}, 'layer_2_size': 31, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 8, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 29, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.27484937105018914}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [77, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 87, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [78, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 44, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 69, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17547866501321385}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [79, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32305615123930476}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [80, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4159975004466281}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 34, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 15, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 17, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]] 

*** 81 configurations x 1.0 iterations each

1 | Thu Sep 27 23:04:45 2018 | lowest loss so far: inf (run -1)

{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  58 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8da860>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:17 - loss: 1.8380
2560/6530 [==========>...................] - ETA: 1s - loss: 1.0139  
5504/6530 [========================>.....] - ETA: 0s - loss: 0.6008
6530/6530 [==============================] - 1s 138us/step - loss: 0.5354 - val_loss: 0.1830
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  36 | activation: sigmoid | extras: dropout - rate: 14.1% 
layer 2 | size:  46 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:00 - loss: 0.5352{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  59 | activation: sigmoid | extras: dropout - rate: 31.4% 
layer 2 | size:   3 | activation: sigmoid | extras: dropout - rate: 39.8% 
layer 3 | size:  31 | activation: tanh    | extras: dropout - rate: 30.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8da7f0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 21s - loss: 0.5869
 864/6530 [==>...........................] - ETA: 6s - loss: 0.1898  
6144/6530 [===========================>..] - ETA: 0s - loss: 0.5071 
1632/6530 [======>.......................] - ETA: 2s - loss: 0.1316
6530/6530 [==============================] - 1s 147us/step - loss: 0.5002 - val_loss: 0.3831

2528/6530 [==========>...................] - ETA: 1s - loss: 0.1070
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0939
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0858
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0798
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0753
6530/6530 [==============================] - 1s 198us/step - loss: 0.0739 - val_loss: 0.0544

# training | RMSE: 0.2246, MAE: 0.1806
worker 1  xfile  [1, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2504247577292705}, 'layer_2_size': 56, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23735034266556831}, 'layer_3_size': 75, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.28190637044148625}, 'layer_4_size': 4, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 87, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.22456440851803255, 'rmse': 0.22456440851803255, 'mae': 0.18062943834514902, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  61 | activation: relu    | extras: batchnorm 
layer 2 | size:  65 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 16s - loss: 0.7297
3456/6530 [==============>...............] - ETA: 0s - loss: 0.3684 
6530/6530 [==============================] - 0s 71us/step - loss: 0.2646 - val_loss: 0.2776

# training | RMSE: 0.6134, MAE: 0.5542
worker 0  xfile  [0, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.31449329379998714}, 'layer_1_size': 59, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3979325003082128}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3012214949206562}, 'layer_3_size': 31, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.6134319363683851, 'rmse': 0.6134319363683851, 'mae': 0.5541655242799806, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  88 | activation: relu    | extras: dropout - rate: 23.3% 
layer 2 | size:  41 | activation: relu    | extras: dropout - rate: 15.4% 
layer 3 | size:   5 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 8s - loss: 0.5587
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1607
6530/6530 [==============================] - 0s 44us/step - loss: 0.1285 - val_loss: 0.0693

# training | RMSE: 0.2328, MAE: 0.1908
worker 2  xfile  [2, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14071525978619703}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1632186816763678}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.13349068180705492}, 'layer_4_size': 48, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2328448424783789, 'rmse': 0.2328448424783789, 'mae': 0.19075815102516294, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  92 | activation: relu    | extras: dropout - rate: 20.0% 
layer 2 | size:   2 | activation: tanh    | extras: None 
layer 3 | size:  32 | activation: relu    | extras: None 
layer 4 | size:  61 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f46042a0cc0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 19s - loss: 1.1008
1664/6530 [======>.......................] - ETA: 0s - loss: 0.3617 
3328/6530 [==============>...............] - ETA: 0s - loss: 0.2920
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2629
# training | RMSE: 0.2650, MAE: 0.2171
worker 0  xfile  [4, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23285260965765855}, 'layer_1_size': 88, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15396607186536132}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.26501945903594804, 'rmse': 0.26501945903594804, 'mae': 0.2170910695388972, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:   2 | activation: sigmoid | extras: None 
layer 2 | size:  91 | activation: relu    | extras: dropout - rate: 25.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f4604166e80>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 20s - loss: 0.7925
6530/6530 [==============================] - 0s 64us/step - loss: 0.2475 - val_loss: 0.1860

2176/6530 [========>.....................] - ETA: 0s - loss: 0.4696 
4288/6530 [==================>...........] - ETA: 0s - loss: 0.3523
6530/6530 [==============================] - 0s 57us/step - loss: 0.3093 - val_loss: 0.2280

# training | RMSE: 0.2745, MAE: 0.2250
worker 0  xfile  [7, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 2, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2568172258114946}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 36, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16264077375686514}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.27445490227145236, 'rmse': 0.27445490227145236, 'mae': 0.22499459619156972, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  29 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45df729ac8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 6s - loss: 1.2247
# training | RMSE: 0.3677, MAE: 0.2829
worker 1  xfile  [3, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 65, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 50, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.36771200014896366, 'rmse': 0.36771200014896366, 'mae': 0.28291883885405694, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  16 | activation: relu    | extras: dropout - rate: 27.5% 
layer 2 | size:  72 | activation: tanh    | extras: dropout - rate: 33.6% 
layer 3 | size:   9 | activation: tanh    | extras: batchnorm 
layer 4 | size:  21 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f460463ac18>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:31 - loss: 0.9119
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1997
6530/6530 [==============================] - 0s 33us/step - loss: 0.1863 - val_loss: 0.0597

 272/6530 [>.............................] - ETA: 13s - loss: 0.7961 
 528/6530 [=>............................] - ETA: 7s - loss: 0.7039 
 800/6530 [==>...........................] - ETA: 4s - loss: 0.6190
# training | RMSE: 0.2280, MAE: 0.1828
worker 2  xfile  [5, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19976518586726605}, 'layer_1_size': 92, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.45757510847559735}, 'layer_5_size': 23, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.22804689564111963, 'rmse': 0.22804689564111963, 'mae': 0.18283795323664223, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  30 | activation: tanh    | extras: dropout - rate: 40.8% 
layer 2 | size:  48 | activation: relu    | extras: None 
layer 3 | size:  79 | activation: relu    | extras: None 
layer 4 | size:  88 | activation: sigmoid | extras: dropout - rate: 16.5% 
layer 5 | size:  77 | activation: tanh    | extras: dropout - rate: 42.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f46042a0ba8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 13s - loss: 1.7049
1056/6530 [===>..........................] - ETA: 3s - loss: 0.5625
2944/6530 [============>.................] - ETA: 0s - loss: 0.5558 
1312/6530 [=====>........................] - ETA: 3s - loss: 0.5118
6016/6530 [==========================>...] - ETA: 0s - loss: 0.4194
1584/6530 [======>.......................] - ETA: 2s - loss: 0.4711
6530/6530 [==============================] - 0s 65us/step - loss: 0.4066 - val_loss: 0.1777

1824/6530 [=======>......................] - ETA: 2s - loss: 0.4398
2064/6530 [========>.....................] - ETA: 2s - loss: 0.4169
2336/6530 [=========>....................] - ETA: 1s - loss: 0.3954
# training | RMSE: 0.2385, MAE: 0.1941
worker 0  xfile  [9, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14505351726939095}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 50, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.161128861263999}, 'layer_4_size': 17, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 2, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.23854860447536622, 'rmse': 0.23854860447536622, 'mae': 0.1941251391258045, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  14 | activation: relu    | extras: dropout - rate: 15.2% 
layer 2 | size:  83 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45df729a58>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 1:13 - loss: 0.6396
2576/6530 [==========>...................] - ETA: 1s - loss: 0.3789
 624/6530 [=>............................] - ETA: 2s - loss: 0.0892  
2800/6530 [===========>..................] - ETA: 1s - loss: 0.3667
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0818
3040/6530 [============>.................] - ETA: 1s - loss: 0.3540
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0777
3296/6530 [==============>...............] - ETA: 1s - loss: 0.3441
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0753
3536/6530 [===============>..............] - ETA: 1s - loss: 0.3353
2928/6530 [============>.................] - ETA: 0s - loss: 0.0713
3760/6530 [================>.............] - ETA: 0s - loss: 0.3280
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0702
4016/6530 [=================>............] - ETA: 0s - loss: 0.3212
3968/6530 [=================>............] - ETA: 0s - loss: 0.0696
4272/6530 [==================>...........] - ETA: 0s - loss: 0.3145
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0685
4528/6530 [===================>..........] - ETA: 0s - loss: 0.3095
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0675
4736/6530 [====================>.........] - ETA: 0s - loss: 0.3047
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0666
4960/6530 [=====================>........] - ETA: 0s - loss: 0.3000
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0654
5216/6530 [======================>.......] - ETA: 0s - loss: 0.2953
6530/6530 [==============================] - 1s 126us/step - loss: 0.0648 - val_loss: 0.0555

5488/6530 [========================>.....] - ETA: 0s - loss: 0.2908
5744/6530 [=========================>....] - ETA: 0s - loss: 0.2873
6000/6530 [==========================>...] - ETA: 0s - loss: 0.2837
6240/6530 [===========================>..] - ETA: 0s - loss: 0.2807
6512/6530 [============================>.] - ETA: 0s - loss: 0.2774
6530/6530 [==============================] - 2s 304us/step - loss: 0.2772 - val_loss: 0.1972

# training | RMSE: 0.2233, MAE: 0.1811
worker 2  xfile  [8, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4080403007617508}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16454352642837314}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4288788424853015}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2233355475747338, 'rmse': 0.2233355475747338, 'mae': 0.18107603391676447, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  17 | activation: sigmoid | extras: None 
layer 2 | size:   8 | activation: relu    | extras: batchnorm 
layer 3 | size:  16 | activation: relu    | extras: dropout - rate: 13.9% 
layer 4 | size:  74 | activation: tanh    | extras: dropout - rate: 30.2% 
layer 5 | size:  32 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ef53f2b0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:41 - loss: 0.5748
 224/6530 [>.............................] - ETA: 16s - loss: 0.5265 
 464/6530 [=>............................] - ETA: 8s - loss: 0.5208 
 720/6530 [==>...........................] - ETA: 5s - loss: 0.4880
 992/6530 [===>..........................] - ETA: 4s - loss: 0.4572
1248/6530 [====>.........................] - ETA: 3s - loss: 0.4471
1488/6530 [=====>........................] - ETA: 2s - loss: 0.4330
1744/6530 [=======>......................] - ETA: 2s - loss: 0.4156
1984/6530 [========>.....................] - ETA: 2s - loss: 0.4048
2240/6530 [=========>....................] - ETA: 1s - loss: 0.3955
# training | RMSE: 0.2393, MAE: 0.1940
worker 1  xfile  [6, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27543604469585237}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3363880143625354}, 'layer_2_size': 72, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 9, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4164082518612683}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.23927037166623633, 'rmse': 0.23927037166623633, 'mae': 0.19395498889227125, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  33 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c8317b38>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 7s - loss: 0.6029
# training | RMSE: 0.2351, MAE: 0.1931
worker 0  xfile  [10, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15238026352041448}, 'layer_1_size': 14, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 83, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.23512839412235037, 'rmse': 0.23512839412235037, 'mae': 0.19312180101514703, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  33 | activation: tanh    | extras: None 
layer 2 | size:  34 | activation: sigmoid | extras: dropout - rate: 23.4% 
layer 3 | size:  98 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f45c06604e0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:30 - loss: 0.6859
2496/6530 [==========>...................] - ETA: 1s - loss: 0.3879
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0932
 432/6530 [>.............................] - ETA: 5s - loss: 0.1483  
2752/6530 [===========>..................] - ETA: 1s - loss: 0.3781
6530/6530 [==============================] - 0s 37us/step - loss: 0.0889 - val_loss: 0.0521

 896/6530 [===>..........................] - ETA: 2s - loss: 0.1089
3008/6530 [============>.................] - ETA: 1s - loss: 0.3698
1344/6530 [=====>........................] - ETA: 2s - loss: 0.0966
3280/6530 [==============>...............] - ETA: 1s - loss: 0.3630
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0869
3536/6530 [===============>..............] - ETA: 1s - loss: 0.3570
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0795
3776/6530 [================>.............] - ETA: 0s - loss: 0.3506
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0734
4048/6530 [=================>............] - ETA: 0s - loss: 0.3449
3104/6530 [=============>................] - ETA: 0s - loss: 0.0699
4304/6530 [==================>...........] - ETA: 0s - loss: 0.3400
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0674
4544/6530 [===================>..........] - ETA: 0s - loss: 0.3352
4000/6530 [=================>............] - ETA: 0s - loss: 0.0651
4816/6530 [=====================>........] - ETA: 0s - loss: 0.3310
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0628
5072/6530 [======================>.......] - ETA: 0s - loss: 0.3270
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0612
5328/6530 [=======================>......] - ETA: 0s - loss: 0.3232
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0601
5600/6530 [========================>.....] - ETA: 0s - loss: 0.3195
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0592
5872/6530 [=========================>....] - ETA: 0s - loss: 0.3159
# training | RMSE: 0.2260, MAE: 0.1826
worker 1  xfile  [13, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 31, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2260113786210233, 'rmse': 0.2260113786210233, 'mae': 0.18257290220016822, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  56 | activation: tanh    | extras: None 
layer 2 | size:  45 | activation: relu    | extras: dropout - rate: 19.9% 
layer 3 | size:   2 | activation: sigmoid | extras: dropout - rate: 32.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c83179e8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 26s - loss: 0.5592
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0578
6128/6530 [===========================>..] - ETA: 0s - loss: 0.3128
2048/6530 [========>.....................] - ETA: 0s - loss: 0.5079 
6384/6530 [============================>.] - ETA: 0s - loss: 0.3093
6530/6530 [==============================] - 1s 181us/step - loss: 0.0570 - val_loss: 0.0415

3904/6530 [================>.............] - ETA: 0s - loss: 0.4499
5888/6530 [==========================>...] - ETA: 0s - loss: 0.3782
6530/6530 [==============================] - 2s 297us/step - loss: 0.3075 - val_loss: 0.2019

6530/6530 [==============================] - 0s 72us/step - loss: 0.3561 - val_loss: 0.1384

# training | RMSE: 0.2014, MAE: 0.1600
worker 0  xfile  [12, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23374835004413852}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20142349042092042, 'rmse': 0.20142349042092042, 'mae': 0.15995877664440714, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  71 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b54ed7b8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 8s - loss: 0.7275
# training | RMSE: 0.3682, MAE: 0.3044
worker 1  xfile  [14, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19883823963787806}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.32044028165442373}, 'layer_3_size': 2, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17273845308660446}, 'layer_4_size': 11, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.3681748416854472, 'rmse': 0.3681748416854472, 'mae': 0.304436438277626, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  32 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c81f3f60>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 19s - loss: 0.8597
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2671
6530/6530 [==============================] - 0s 40us/step - loss: 0.2527 - val_loss: 0.1895

2688/6530 [===========>..................] - ETA: 0s - loss: 0.5962 
5312/6530 [=======================>......] - ETA: 0s - loss: 0.4447
6530/6530 [==============================] - 0s 53us/step - loss: 0.4034 - val_loss: 0.2120

# training | RMSE: 0.2518, MAE: 0.2013
worker 2  xfile  [11, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 8, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.13912763458249527}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3022629720858989}, 'layer_4_size': 74, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.25177857053905617, 'rmse': 0.25177857053905617, 'mae': 0.20130768914858813, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  13 | activation: sigmoid | extras: dropout - rate: 34.1% 
layer 2 | size:  66 | activation: relu    | extras: dropout - rate: 35.6% 
layer 3 | size:  34 | activation: tanh    | extras: dropout - rate: 37.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ec089ac8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 16s - loss: 1.2536
3584/6530 [===============>..............] - ETA: 0s - loss: 0.6807 
6530/6530 [==============================] - 0s 72us/step - loss: 0.5792 - val_loss: 0.2213

# training | RMSE: 0.2570, MAE: 0.2051
worker 1  xfile  [16, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27066805008790573}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1156475839256515}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2212949453718571}, 'layer_5_size': 87, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.25702240259247716, 'rmse': 0.25702240259247716, 'mae': 0.20510790361179787, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  26 | activation: tanh    | extras: dropout - rate: 26.8% 
layer 2 | size:  11 | activation: tanh    | extras: dropout - rate: 47.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b9eba2b0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 11s - loss: 0.7305
4608/6530 [====================>.........] - ETA: 0s - loss: 0.3387 
6530/6530 [==============================] - 0s 50us/step - loss: 0.3078 - val_loss: 0.1722

# training | RMSE: 0.2385, MAE: 0.1879
worker 0  xfile  [15, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 71, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 41, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.23849013769082872, 'rmse': 0.23849013769082872, 'mae': 0.1878721694143133, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  72 | activation: sigmoid | extras: None 
layer 2 | size:  30 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  91 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42d01d9630>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:22 - loss: 0.4752
 736/6530 [==>...........................] - ETA: 3s - loss: 0.2208  
1440/6530 [=====>........................] - ETA: 1s - loss: 0.1971
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1916
2880/6530 [============>.................] - ETA: 0s - loss: 0.1848
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1782
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1736
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1703
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1673
6336/6530 [============================>.] - ETA: 0s - loss: 0.1646
# training | RMSE: 0.2180, MAE: 0.1762
worker 1  xfile  [19, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2684385694928465}, 'layer_1_size': 26, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47712176301521403}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4122081315162861}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.21803750249453108, 'rmse': 0.21803750249453108, 'mae': 0.17615840615991504, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  68 | activation: sigmoid | extras: dropout - rate: 22.4% 
layer 2 | size:  57 | activation: sigmoid | extras: dropout - rate: 45.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b9eba048>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 31s - loss: 1.6280
6530/6530 [==============================] - 1s 144us/step - loss: 0.1641 - val_loss: 0.1468

2048/6530 [========>.....................] - ETA: 0s - loss: 0.3825 
4096/6530 [=================>............] - ETA: 0s - loss: 0.2309
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1794
# training | RMSE: 0.2753, MAE: 0.2259
worker 2  xfile  [17, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3414675344100685}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3564867865679183}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3700148745940326}, 'layer_3_size': 34, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3142325446127674}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2753316740486409, 'rmse': 0.2753316740486409, 'mae': 0.22594575370496486, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  71 | activation: sigmoid | extras: dropout - rate: 49.2% 
layer 2 | size:  35 | activation: relu    | extras: None 
layer 3 | size:  48 | activation: tanh    | extras: dropout - rate: 40.1% 
layer 4 | size:  64 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45d42cc2b0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 46s - loss: 0.6289
1600/6530 [======>.......................] - ETA: 1s - loss: 0.2245 
6530/6530 [==============================] - 1s 80us/step - loss: 0.1709 - val_loss: 0.0586

3200/6530 [=============>................] - ETA: 0s - loss: 0.1726
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1545
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1417
6530/6530 [==============================] - 1s 113us/step - loss: 0.1407 - val_loss: 0.1476

# training | RMSE: 0.2420, MAE: 0.1982
worker 1  xfile  [21, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22385916107638965}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4540864912159174}, 'layer_2_size': 57, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3773403287489404}, 'layer_5_size': 35, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.24202022387535696, 'rmse': 0.24202022387535696, 'mae': 0.19819934158037045, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  45 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b9c50550>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 24s - loss: 1.2977
1856/6530 [=======>......................] - ETA: 0s - loss: 0.6896 
3904/6530 [================>.............] - ETA: 0s - loss: 0.4197
5888/6530 [==========================>...] - ETA: 0s - loss: 0.3015
6530/6530 [==============================] - 0s 69us/step - loss: 0.2766 - val_loss: 0.0494

# training | RMSE: 0.3811, MAE: 0.3136
worker 2  xfile  [20, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4922590499368914}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.40054943618782857}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2665061266155887}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.3811312330759861, 'rmse': 0.3811312330759861, 'mae': 0.31357196868835563, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  37 | activation: sigmoid | extras: dropout - rate: 48.5% 
layer 2 | size:   6 | activation: sigmoid | extras: dropout - rate: 16.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45d47e0c18>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:17 - loss: 1.6525
 496/6530 [=>............................] - ETA: 4s - loss: 1.4049  
 976/6530 [===>..........................] - ETA: 2s - loss: 1.0981
1472/6530 [=====>........................] - ETA: 1s - loss: 0.8541
# training | RMSE: 0.1726, MAE: 0.1421
worker 0  xfile  [18, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.17261302245326615, 'rmse': 0.17261302245326615, 'mae': 0.14211839798658538, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  62 | activation: tanh    | extras: dropout - rate: 40.6% 
layer 2 | size:  66 | activation: sigmoid | extras: dropout - rate: 11.8% 
layer 3 | size:  46 | activation: relu    | extras: dropout - rate: 10.3% 
layer 4 | size:  79 | activation: tanh    | extras: batchnorm 
layer 5 | size:  29 | activation: relu    | extras: dropout - rate: 22.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b54ed668>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 27s - loss: 1.2916
1968/6530 [========>.....................] - ETA: 1s - loss: 0.7200
2416/6530 [==========>...................] - ETA: 1s - loss: 0.6402
2176/6530 [========>.....................] - ETA: 1s - loss: 0.5646 
2880/6530 [============>.................] - ETA: 0s - loss: 0.5823
4480/6530 [===================>..........] - ETA: 0s - loss: 0.4470
3328/6530 [==============>...............] - ETA: 0s - loss: 0.5407
3824/6530 [================>.............] - ETA: 0s - loss: 0.5043
6530/6530 [==============================] - 1s 120us/step - loss: 0.3959 - val_loss: 0.3464

4288/6530 [==================>...........] - ETA: 0s - loss: 0.4786
4784/6530 [====================>.........] - ETA: 0s - loss: 0.4569
5296/6530 [=======================>......] - ETA: 0s - loss: 0.4378
5776/6530 [=========================>....] - ETA: 0s - loss: 0.4224
6272/6530 [===========================>..] - ETA: 0s - loss: 0.4080
6530/6530 [==============================] - 1s 167us/step - loss: 0.4015 - val_loss: 0.1991

# training | RMSE: 0.2237, MAE: 0.1801
worker 1  xfile  [23, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32553354455869177}, 'layer_4_size': 46, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.22373016341579893, 'rmse': 0.22373016341579893, 'mae': 0.18009427874695247, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  41 | activation: relu    | extras: None 
layer 2 | size:  35 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  62 | activation: tanh    | extras: None 
layer 4 | size:  42 | activation: relu    | extras: dropout - rate: 41.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b9c50518>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:56 - loss: 0.4746
 352/6530 [>.............................] - ETA: 8s - loss: 0.1305  
 704/6530 [==>...........................] - ETA: 4s - loss: 0.0957
1040/6530 [===>..........................] - ETA: 3s - loss: 0.0849
1376/6530 [=====>........................] - ETA: 2s - loss: 0.0764
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0701
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0655
2416/6530 [==========>...................] - ETA: 1s - loss: 0.0618
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0582
3120/6530 [=============>................] - ETA: 0s - loss: 0.0558
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0541
3808/6530 [================>.............] - ETA: 0s - loss: 0.0526
4128/6530 [=================>............] - ETA: 0s - loss: 0.0508
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0495
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0484
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0472
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0459
# training | RMSE: 0.4112, MAE: 0.3409
worker 0  xfile  [22, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.40563576142447133}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11801951126986109}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1031887790315289}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22642497672744197}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.41121567455137753, 'rmse': 0.41121567455137753, 'mae': 0.3409453553815764, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  18 | activation: tanh    | extras: dropout - rate: 49.0% 
layer 2 | size:  72 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  67 | activation: relu    | extras: dropout - rate: 23.7% 
layer 4 | size:  84 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b4f51588>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:47 - loss: 0.4510
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0451
 640/6530 [=>............................] - ETA: 5s - loss: 0.2990  
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0441
1280/6530 [====>.........................] - ETA: 2s - loss: 0.2663
6528/6530 [============================>.] - ETA: 0s - loss: 0.0435
1856/6530 [=======>......................] - ETA: 1s - loss: 0.2574
2432/6530 [==========>...................] - ETA: 1s - loss: 0.2486
6530/6530 [==============================] - 1s 229us/step - loss: 0.0435 - val_loss: 0.0235

3008/6530 [============>.................] - ETA: 0s - loss: 0.2434
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2382
4192/6530 [==================>...........] - ETA: 0s - loss: 0.2346
4704/6530 [====================>.........] - ETA: 0s - loss: 0.2318
5344/6530 [=======================>......] - ETA: 0s - loss: 0.2284
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2253
6496/6530 [============================>.] - ETA: 0s - loss: 0.2237
6530/6530 [==============================] - 1s 181us/step - loss: 0.2235 - val_loss: 0.2153

# training | RMSE: 0.2466, MAE: 0.2008
worker 2  xfile  [24, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.48502693835638644}, 'layer_1_size': 37, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16159727813614957}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1291148842886615}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 43, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.24660581662310402, 'rmse': 0.24660581662310402, 'mae': 0.2008064512204931, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  89 | activation: tanh    | extras: batchnorm 
layer 2 | size:  73 | activation: relu    | extras: dropout - rate: 15.4% 
layer 3 | size:  53 | activation: relu    | extras: batchnorm 
layer 4 | size:  96 | activation: sigmoid | extras: None 
layer 5 | size:  22 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f45d47e0d68>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:15 - loss: 0.9219
1024/6530 [===>..........................] - ETA: 4s - loss: 0.2297  
1984/6530 [========>.....................] - ETA: 1s - loss: 0.1576
3008/6530 [============>.................] - ETA: 1s - loss: 0.1225
3904/6530 [================>.............] - ETA: 0s - loss: 0.1054
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0947
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0873
6400/6530 [============================>.] - ETA: 0s - loss: 0.0807
6530/6530 [==============================] - 1s 188us/step - loss: 0.0796 - val_loss: 0.0262

# training | RMSE: 0.1427, MAE: 0.1133
worker 1  xfile  [25, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41090246001998465}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.14273960183017992, 'rmse': 0.14273960183017992, 'mae': 0.11326856317934546, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  56 | activation: tanh    | extras: None 
layer 2 | size:  70 | activation: relu    | extras: batchnorm 
layer 3 | size:  98 | activation: relu    | extras: None 
layer 4 | size:  72 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b9743550>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 13s - loss: 0.5991
4352/6530 [==================>...........] - ETA: 0s - loss: 0.3051 
6530/6530 [==============================] - 1s 105us/step - loss: 0.2252 - val_loss: 0.0547

# training | RMSE: 0.2592, MAE: 0.2147
worker 0  xfile  [26, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4901363409208247}, 'layer_1_size': 18, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 72, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23683208726054494}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.25916821259054035, 'rmse': 0.25916821259054035, 'mae': 0.214671748427392, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  61 | activation: relu    | extras: batchnorm 
layer 2 | size:   3 | activation: tanh    | extras: None 
layer 3 | size:  16 | activation: tanh    | extras: dropout - rate: 23.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b4a7ae80>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:09 - loss: 2.0787
 336/6530 [>.............................] - ETA: 12s - loss: 1.3380 
 656/6530 [==>...........................] - ETA: 6s - loss: 0.9475 
 976/6530 [===>..........................] - ETA: 4s - loss: 0.7579
# training | RMSE: 0.1572, MAE: 0.1244
worker 2  xfile  [27, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1539922143106104}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.15720449975204717, 'rmse': 0.15720449975204717, 'mae': 0.1244371804481419, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  46 | activation: relu    | extras: dropout - rate: 19.8% 
layer 2 | size:  99 | activation: tanh    | extras: dropout - rate: 28.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45c1d88240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 19s - loss: 0.5591
1312/6530 [=====>........................] - ETA: 3s - loss: 0.6261
4096/6530 [=================>............] - ETA: 0s - loss: 0.1936 
1632/6530 [======>.......................] - ETA: 2s - loss: 0.5420
1968/6530 [========>.....................] - ETA: 2s - loss: 0.4761
6530/6530 [==============================] - 1s 80us/step - loss: 0.1395 - val_loss: 0.0444

2272/6530 [=========>....................] - ETA: 1s - loss: 0.4322
2592/6530 [==========>...................] - ETA: 1s - loss: 0.3920
2896/6530 [============>.................] - ETA: 1s - loss: 0.3617
3200/6530 [=============>................] - ETA: 1s - loss: 0.3365
3552/6530 [===============>..............] - ETA: 0s - loss: 0.3125
3904/6530 [================>.............] - ETA: 0s - loss: 0.2916
4240/6530 [==================>...........] - ETA: 0s - loss: 0.2749
4576/6530 [====================>.........] - ETA: 0s - loss: 0.2600
4928/6530 [=====================>........] - ETA: 0s - loss: 0.2466
5280/6530 [=======================>......] - ETA: 0s - loss: 0.2345
5584/6530 [========================>.....] - ETA: 0s - loss: 0.2257
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2174
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2095
6528/6530 [============================>.] - ETA: 0s - loss: 0.2024
6530/6530 [==============================] - 2s 267us/step - loss: 0.2023 - val_loss: 0.0587

# training | RMSE: 0.2067, MAE: 0.1668
worker 2  xfile  [30, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19774261829234205}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2797140258496348}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45844819637653644}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20666948892724032, 'rmse': 0.20666948892724032, 'mae': 0.1668326067401611, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  56 | activation: relu    | extras: dropout - rate: 24.6% 
layer 2 | size:  78 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45c16eb048>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 21s - loss: 0.6855
3840/6530 [================>.............] - ETA: 0s - loss: 0.4856 
# training | RMSE: 0.2322, MAE: 0.1924
worker 1  xfile  [28, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2321658549535544, 'rmse': 0.2321658549535544, 'mae': 0.1924489374703006, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  38 | activation: sigmoid | extras: dropout - rate: 41.9% 
layer 2 | size:  14 | activation: relu    | extras: dropout - rate: 40.5% 
layer 3 | size:  13 | activation: tanh    | extras: None 
layer 4 | size:  94 | activation: tanh    | extras: batchnorm 
layer 5 | size:   4 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b93d7400>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 34s - loss: 0.5234
6530/6530 [==============================] - 1s 87us/step - loss: 0.3739 - val_loss: 0.1811

2176/6530 [========>.....................] - ETA: 1s - loss: 0.3720 
4352/6530 [==================>...........] - ETA: 0s - loss: 0.3103
6528/6530 [============================>.] - ETA: 0s - loss: 0.2826
6530/6530 [==============================] - 1s 142us/step - loss: 0.2825 - val_loss: 0.2270

# training | RMSE: 0.2194, MAE: 0.1810
worker 2  xfile  [32, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24594424623809785}, 'layer_1_size': 56, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41103794745290845}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11328582488940891}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.21941897320950668, 'rmse': 0.21941897320950668, 'mae': 0.18099887114324645, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:   5 | activation: relu    | extras: None 
layer 2 | size:  67 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45c14e29b0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 35s - loss: 0.7695
1984/6530 [========>.....................] - ETA: 0s - loss: 0.4746 
4224/6530 [==================>...........] - ETA: 0s - loss: 0.4019
6272/6530 [===========================>..] - ETA: 0s - loss: 0.3595
6530/6530 [==============================] - 1s 88us/step - loss: 0.3552 - val_loss: 0.2449

# training | RMSE: 0.2471, MAE: 0.1993
worker 0  xfile  [29, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23265042009909262}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12192595655153445}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.24712990083208553, 'rmse': 0.24712990083208553, 'mae': 0.19930388036449936, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  63 | activation: tanh    | extras: batchnorm 
layer 2 | size:  61 | activation: relu    | extras: None 
layer 3 | size:  80 | activation: relu    | extras: None 
layer 4 | size:  93 | activation: tanh    | extras: dropout - rate: 11.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b45032b0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 34s - loss: 0.6998
# training | RMSE: 0.2842, MAE: 0.2336
worker 1  xfile  [31, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4187861969586798}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4047564904270293}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 94, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2841659709102846, 'rmse': 0.2841659709102846, 'mae': 0.23361202180895824, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  41 | activation: tanh    | extras: None 
layer 2 | size:  39 | activation: relu    | extras: batchnorm 
layer 3 | size:  30 | activation: relu    | extras: None 
layer 4 | size:  83 | activation: sigmoid | extras: dropout - rate: 16.6% 
layer 5 | size:  40 | activation: tanh    | extras: dropout - rate: 27.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b89ec0b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:09 - loss: 0.5998
1792/6530 [=======>......................] - ETA: 1s - loss: 0.3465 
 272/6530 [>.............................] - ETA: 15s - loss: 0.1040 
3840/6530 [================>.............] - ETA: 0s - loss: 0.2697
 528/6530 [=>............................] - ETA: 8s - loss: 0.0815 
 832/6530 [==>...........................] - ETA: 5s - loss: 0.0675
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2356
1136/6530 [====>.........................] - ETA: 3s - loss: 0.0609
6530/6530 [==============================] - 1s 144us/step - loss: 0.2278 - val_loss: 0.1533

1440/6530 [=====>........................] - ETA: 3s - loss: 0.0563
1744/6530 [=======>......................] - ETA: 2s - loss: 0.0527
2000/6530 [========>.....................] - ETA: 2s - loss: 0.0505
2288/6530 [=========>....................] - ETA: 1s - loss: 0.0483
2592/6530 [==========>...................] - ETA: 1s - loss: 0.0459
2880/6530 [============>.................] - ETA: 1s - loss: 0.0442
3200/6530 [=============>................] - ETA: 1s - loss: 0.0429
3488/6530 [===============>..............] - ETA: 1s - loss: 0.0415
3808/6530 [================>.............] - ETA: 0s - loss: 0.0401
4096/6530 [=================>............] - ETA: 0s - loss: 0.0392
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0387
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0378
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0373
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0368
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0359
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0355
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0350
# training | RMSE: 0.3089, MAE: 0.2444
worker 2  xfile  [33, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 5, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 67, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.30886039627547995, 'rmse': 0.30886039627547995, 'mae': 0.24441765952802932, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  31 | activation: relu    | extras: batchnorm 
layer 2 | size:  49 | activation: sigmoid | extras: dropout - rate: 18.3% 
layer 3 | size:   3 | activation: sigmoid | extras: None 
layer 4 | size:  20 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f45c14e2908>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 41s - loss: 2.9008
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0344
1920/6530 [=======>......................] - ETA: 2s - loss: 2.6500 
3712/6530 [================>.............] - ETA: 0s - loss: 2.4195
5376/6530 [=======================>......] - ETA: 0s - loss: 2.2057
6530/6530 [==============================] - 2s 295us/step - loss: 0.0340 - val_loss: 0.0255

6530/6530 [==============================] - 1s 170us/step - loss: 2.0599 - val_loss: 1.1984

# training | RMSE: 0.1853, MAE: 0.1457
worker 0  xfile  [34, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11932348426737889}, 'layer_4_size': 93, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.1853154305807106, 'rmse': 0.1853154305807106, 'mae': 0.14569521797754145, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  74 | activation: sigmoid | extras: None 
layer 2 | size:  70 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  20 | activation: sigmoid | extras: dropout - rate: 24.2% 
layer 4 | size:  29 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b4170f98>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:47 - loss: 0.7737
 272/6530 [>.............................] - ETA: 17s - loss: 0.4304 
 480/6530 [=>............................] - ETA: 10s - loss: 0.3333
 656/6530 [==>...........................] - ETA: 7s - loss: 0.2917 
 848/6530 [==>...........................] - ETA: 6s - loss: 0.2680
1008/6530 [===>..........................] - ETA: 5s - loss: 0.2541
1184/6530 [====>.........................] - ETA: 4s - loss: 0.2433
1376/6530 [=====>........................] - ETA: 4s - loss: 0.2368
1616/6530 [======>.......................] - ETA: 3s - loss: 0.2263
1840/6530 [=======>......................] - ETA: 3s - loss: 0.2220
2048/6530 [========>.....................] - ETA: 2s - loss: 0.2193
2272/6530 [=========>....................] - ETA: 2s - loss: 0.2157
# training | RMSE: 0.1565, MAE: 0.1222
worker 1  xfile  [35, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16597532291562125}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2703434644417436}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.15653635642649547, 'rmse': 0.15653635642649547, 'mae': 0.12216281975913237, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  68 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b89e7e80>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:23 - loss: 0.7065
2496/6530 [==========>...................] - ETA: 2s - loss: 0.2127
1152/6530 [====>.........................] - ETA: 2s - loss: 0.1166  
2704/6530 [===========>..................] - ETA: 1s - loss: 0.2096
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0777
2944/6530 [============>.................] - ETA: 1s - loss: 0.2069
3776/6530 [================>.............] - ETA: 0s - loss: 0.0642
3200/6530 [=============>................] - ETA: 1s - loss: 0.2047
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0587
3456/6530 [==============>...............] - ETA: 1s - loss: 0.2015
6400/6530 [============================>.] - ETA: 0s - loss: 0.0546
3696/6530 [===============>..............] - ETA: 1s - loss: 0.1996
3936/6530 [=================>............] - ETA: 1s - loss: 0.1977
6530/6530 [==============================] - 1s 115us/step - loss: 0.0543 - val_loss: 0.0416

4176/6530 [==================>...........] - ETA: 0s - loss: 0.1964
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1962
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1947
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1929
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1919
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1908
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1901
# training | RMSE: 1.2293, MAE: 1.1822
worker 2  xfile  [36, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18309647526297057}, 'layer_2_size': 49, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 1.2293268445192955, 'rmse': 1.2293268445192955, 'mae': 1.1822016765982466, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  43 | activation: tanh    | extras: dropout - rate: 36.7% 
layer 2 | size:  75 | activation: relu    | extras: dropout - rate: 48.5% 
layer 3 | size:  96 | activation: tanh    | extras: None 
layer 4 | size:  52 | activation: relu    | extras: batchnorm 
layer 5 | size:  37 | activation: tanh    | extras: dropout - rate: 35.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45c109b438>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:57 - loss: 1.6116
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1892
 512/6530 [=>............................] - ETA: 10s - loss: 0.8424 
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1888
 960/6530 [===>..........................] - ETA: 5s - loss: 0.6672 
6400/6530 [============================>.] - ETA: 0s - loss: 0.1877
1408/6530 [=====>........................] - ETA: 3s - loss: 0.5465
1856/6530 [=======>......................] - ETA: 2s - loss: 0.4777
2272/6530 [=========>....................] - ETA: 2s - loss: 0.4263
6530/6530 [==============================] - 2s 355us/step - loss: 0.1872 - val_loss: 0.1641

2752/6530 [===========>..................] - ETA: 1s - loss: 0.3783
3264/6530 [=============>................] - ETA: 1s - loss: 0.3396
3744/6530 [================>.............] - ETA: 0s - loss: 0.3124
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2930
4640/6530 [====================>.........] - ETA: 0s - loss: 0.2736
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2562
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2427
6016/6530 [==========================>...] - ETA: 0s - loss: 0.2315
6528/6530 [============================>.] - ETA: 0s - loss: 0.2192
6530/6530 [==============================] - 2s 261us/step - loss: 0.2192 - val_loss: 0.0510

# training | RMSE: 0.2019, MAE: 0.1625
worker 1  xfile  [39, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4190781584026906}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2018708882161571, 'rmse': 0.2018708882161571, 'mae': 0.16252438295386845, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  39 | activation: relu    | extras: None 
layer 2 | size:  50 | activation: sigmoid | extras: None 
layer 3 | size:  90 | activation: relu    | extras: batchnorm 
layer 4 | size:  67 | activation: relu    | extras: dropout - rate: 19.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b84c5a20>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 19s - loss: 1.1150
4096/6530 [=================>............] - ETA: 0s - loss: 0.3754 
6530/6530 [==============================] - 1s 145us/step - loss: 0.2768 - val_loss: 0.2272

# training | RMSE: 0.2008, MAE: 0.1629
worker 0  xfile  [37, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.24184757063105722}, 'layer_3_size': 20, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 29, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.20075642982326475, 'rmse': 0.20075642982326475, 'mae': 0.16293782793283815, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  54 | activation: sigmoid | extras: dropout - rate: 16.2% 
layer 2 | size:  28 | activation: sigmoid | extras: dropout - rate: 29.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f429b637cf8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 12s - loss: 0.9888
6530/6530 [==============================] - 1s 95us/step - loss: 0.3093 - val_loss: 0.2084

# training | RMSE: 0.2267, MAE: 0.1796
worker 2  xfile  [38, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3667571823118735}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4850000697615927}, 'layer_2_size': 75, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35238667661361356}, 'layer_5_size': 37, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.2266517940554529, 'rmse': 0.2266517940554529, 'mae': 0.1795985111278617, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  26 | activation: relu    | extras: dropout - rate: 23.0% 
layer 2 | size:  11 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45c1784240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 29s - loss: 0.5533
3072/6530 [=============>................] - ETA: 0s - loss: 0.4010 
6272/6530 [===========================>..] - ETA: 0s - loss: 0.3476
6530/6530 [==============================] - 1s 117us/step - loss: 0.3451 - val_loss: 0.2439

# training | RMSE: 0.4739, MAE: 0.3897
worker 1  xfile  [40, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 90, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1979973379913189}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.47392175691971244, 'rmse': 0.47392175691971244, 'mae': 0.3897131697868296, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  65 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b83517b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 51s - loss: 0.3889
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0770 
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0606
6530/6530 [==============================] - 1s 109us/step - loss: 0.0551 - val_loss: 0.0451

# training | RMSE: 0.2565, MAE: 0.2099
worker 0  xfile  [41, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16220811735884047}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.29275903749501153}, 'layer_2_size': 28, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4430161926599384}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2564519088725764, 'rmse': 0.2564519088725764, 'mae': 0.20994167237022315, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  44 | activation: sigmoid | extras: dropout - rate: 19.2% 
layer 2 | size:  24 | activation: tanh    | extras: None 
layer 3 | size:  66 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  96 | activation: tanh    | extras: None 
layer 5 | size:  76 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f429b637ac8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 18s - loss: 0.7486
3072/6530 [=============>................] - ETA: 0s - loss: 0.5437 
6530/6530 [==============================] - 1s 144us/step - loss: 0.4173 - val_loss: 0.2748

# training | RMSE: 0.2045, MAE: 0.1648
worker 1  xfile  [43, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.20452495542870655, 'rmse': 0.20452495542870655, 'mae': 0.16477802380839676, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  66 | activation: relu    | extras: None 
layer 2 | size:  67 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b8351748>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 29s - loss: 0.5278
3584/6530 [===============>..............] - ETA: 0s - loss: 0.3096 
# training | RMSE: 0.2927, MAE: 0.2373
worker 2  xfile  [42, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22959196988663796}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 72, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10988727565631695}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40210907670547635}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2927395370454333, 'rmse': 0.2927395370454333, 'mae': 0.23732246408136617, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  46 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  26 | activation: tanh    | extras: dropout - rate: 17.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45c0362780>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 17s - loss: 0.7332
4864/6530 [=====================>........] - ETA: 0s - loss: 0.6315 
6530/6530 [==============================] - 1s 122us/step - loss: 0.2701 - val_loss: 0.2119

6530/6530 [==============================] - 1s 134us/step - loss: 0.5890 - val_loss: 0.3015

# training | RMSE: 0.3275, MAE: 0.2669
worker 0  xfile  [44, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19153313597417265}, 'layer_1_size': 44, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.32749230910458166, 'rmse': 0.32749230910458166, 'mae': 0.2669432201886991, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  60 | activation: sigmoid | extras: None 
layer 2 | size:  70 | activation: tanh    | extras: dropout - rate: 21.9% 
layer 3 | size:  25 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f429b3d2470>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 15s - loss: 0.2665
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1374 
6530/6530 [==============================] - 1s 127us/step - loss: 0.1226 - val_loss: 0.0710

# training | RMSE: 0.3566, MAE: 0.3010
worker 2  xfile  [45, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.17522488099171737}, 'layer_2_size': 26, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 19, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19065135093673313}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.3565739209104217, 'rmse': 0.3565739209104217, 'mae': 0.3009715560165394, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  58 | activation: tanh    | extras: None 
layer 2 | size:  42 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f45c015f080>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 16s - loss: 0.7688
6530/6530 [==============================] - 1s 124us/step - loss: 0.3838 - val_loss: 0.2054

# training | RMSE: 0.2664, MAE: 0.2141
worker 1  xfile  [46, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38430490289489483}, 'layer_3_size': 25, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 74, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2663875497060918, 'rmse': 0.2663875497060918, 'mae': 0.21413688206621245, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  40 | activation: relu    | extras: None 
layer 2 | size:  13 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  49 | activation: relu    | extras: None 
layer 4 | size:  33 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f429fe684e0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 36s - loss: 0.4935
2048/6530 [========>.....................] - ETA: 1s - loss: 0.1978 
4096/6530 [=================>............] - ETA: 0s - loss: 0.1733
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1622
6530/6530 [==============================] - 1s 150us/step - loss: 0.1608 - val_loss: 0.1360

# training | RMSE: 0.2656, MAE: 0.2169
worker 0  xfile  [47, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21860967678915208}, 'layer_2_size': 70, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3106122218642763}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.2656003890323989, 'rmse': 0.2656003890323989, 'mae': 0.21690962004856457, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  39 | activation: sigmoid | extras: dropout - rate: 17.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f429ae9d358>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:24 - loss: 0.5229
1120/6530 [====>.........................] - ETA: 3s - loss: 0.2748  
2272/6530 [=========>....................] - ETA: 1s - loss: 0.2406
3488/6530 [===============>..............] - ETA: 0s - loss: 0.2253
4672/6530 [====================>.........] - ETA: 0s - loss: 0.2162
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2097
6530/6530 [==============================] - 1s 168us/step - loss: 0.2072 - val_loss: 0.1764

# training | RMSE: 0.1734, MAE: 0.1325
worker 1  xfile  [48, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.17338625968451782, 'rmse': 0.17338625968451782, 'mae': 0.13246553444119782, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  33 | activation: relu    | extras: dropout - rate: 49.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f429fccb860>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 14s - loss: 0.5883
# training | RMSE: 0.2531, MAE: 0.2014
worker 2  xfile  [49, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3955433867376831}, 'layer_3_size': 61, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3314818760746431}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3345520542427325}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.25313665585875234, 'rmse': 0.25313665585875234, 'mae': 0.20142687269335335, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  49 | activation: relu    | extras: None 
layer 2 | size:  13 | activation: tanh    | extras: None 
layer 3 | size:  70 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c74749b0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:12 - loss: 0.7083
1408/6530 [=====>........................] - ETA: 2s - loss: 0.6660  
2880/6530 [============>.................] - ETA: 1s - loss: 0.4855
6530/6530 [==============================] - 1s 106us/step - loss: 0.5013 - val_loss: 0.4301

4544/6530 [===================>..........] - ETA: 0s - loss: 0.3937
6208/6530 [===========================>..] - ETA: 0s - loss: 0.3477
6530/6530 [==============================] - 1s 156us/step - loss: 0.3408 - val_loss: 0.2112

# training | RMSE: 0.2187, MAE: 0.1778
worker 0  xfile  [50, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17031695865320207}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12100320754320126}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.21870003752459988, 'rmse': 0.21870003752459988, 'mae': 0.1777684074017919, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  66 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  63 | activation: tanh    | extras: None 
layer 3 | size:  35 | activation: relu    | extras: dropout - rate: 45.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f429ae97b38>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 20s - loss: 0.9310
2816/6530 [===========>..................] - ETA: 1s - loss: 0.6056 
5632/6530 [========================>.....] - ETA: 0s - loss: 0.4596
6530/6530 [==============================] - 1s 165us/step - loss: 0.4320 - val_loss: 0.2249

# training | RMSE: 0.6486, MAE: 0.5915
worker 1  xfile  [52, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.49532365596879}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 61, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4400657261566402}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20286776821312336}, 'layer_5_size': 9, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.648568605740579, 'rmse': 0.648568605740579, 'mae': 0.5915261896369804, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  53 | activation: sigmoid | extras: None 
layer 2 | size:  60 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  84 | activation: tanh    | extras: dropout - rate: 25.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f429f6051d0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:20 - loss: 0.8651
 960/6530 [===>..........................] - ETA: 4s - loss: 0.6701  
2048/6530 [========>.....................] - ETA: 1s - loss: 0.5306
3072/6530 [=============>................] - ETA: 1s - loss: 0.4313
4032/6530 [=================>............] - ETA: 0s - loss: 0.3755
5120/6530 [======================>.......] - ETA: 0s - loss: 0.3348
6144/6530 [===========================>..] - ETA: 0s - loss: 0.3066
6530/6530 [==============================] - 1s 190us/step - loss: 0.2974 - val_loss: 0.3535

# training | RMSE: 0.2605, MAE: 0.2125
worker 2  xfile  [51, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 70, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3621584604959015}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2605016588475522, 'rmse': 0.2605016588475522, 'mae': 0.2124648320942563, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  82 | activation: tanh    | extras: batchnorm 
layer 2 | size:  29 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  35 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c725b4e0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 51s - loss: 0.2400
1536/6530 [======>.......................] - ETA: 3s - loss: 0.1042 
2944/6530 [============>.................] - ETA: 1s - loss: 0.0775
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0663
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0596
# training | RMSE: 0.2974, MAE: 0.2347
worker 0  xfile  [53, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 66, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45371219109203254}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2974435334297008, 'rmse': 0.2974435334297008, 'mae': 0.23472882110879198, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  68 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f429ae97668>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:06 - loss: 0.6880
6530/6530 [==============================] - 1s 217us/step - loss: 0.0574 - val_loss: 0.0411

 832/6530 [==>...........................] - ETA: 4s - loss: 0.6170  
1696/6530 [======>.......................] - ETA: 2s - loss: 0.4273
2656/6530 [===========>..................] - ETA: 1s - loss: 0.3345
3680/6530 [===============>..............] - ETA: 0s - loss: 0.2868
4768/6530 [====================>.........] - ETA: 0s - loss: 0.2604
5824/6530 [=========================>....] - ETA: 0s - loss: 0.2430
6530/6530 [==============================] - 1s 162us/step - loss: 0.2345 - val_loss: 0.1804

# training | RMSE: 0.4113, MAE: 0.3560
worker 1  xfile  [54, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2517984823985751}, 'layer_3_size': 84, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23988635610248485}, 'layer_4_size': 39, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.490310731404365}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.411268671457726, 'rmse': 0.411268671457726, 'mae': 0.3559600818859608, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  30 | activation: relu    | extras: batchnorm 
layer 2 | size:  81 | activation: tanh    | extras: None 
layer 3 | size:   6 | activation: relu    | extras: None 
layer 4 | size:  57 | activation: relu    | extras: dropout - rate: 11.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f429f45b588>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 44s - loss: 0.5599
1536/6530 [======>.......................] - ETA: 3s - loss: 0.3760 
2944/6530 [============>.................] - ETA: 1s - loss: 0.2762
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2091
# training | RMSE: 0.2250, MAE: 0.1793
worker 0  xfile  [56, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.22502098983258487, 'rmse': 0.22502098983258487, 'mae': 0.17933388219501226, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  63 | activation: relu    | extras: None 
layer 2 | size:   3 | activation: sigmoid | extras: dropout - rate: 34.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f429ae97358>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 15s - loss: 0.6834
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1670
6530/6530 [==============================] - 1s 114us/step - loss: 0.5371 - val_loss: 0.4091

6530/6530 [==============================] - 1s 189us/step - loss: 0.1598 - val_loss: 0.0446

# training | RMSE: 0.2000, MAE: 0.1605
worker 2  xfile  [55, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32673705199921127}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2000017149188745, 'rmse': 0.2000017149188745, 'mae': 0.1604929579436235, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  67 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  34 | activation: tanh    | extras: dropout - rate: 38.4% 
layer 3 | size:  72 | activation: relu    | extras: None 
layer 4 | size:  37 | activation: sigmoid | extras: batchnorm 
layer 5 | size:  36 | activation: sigmoid | extras: dropout - rate: 24.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45e401d470>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 4:34 - loss: 0.3468
 288/6530 [>.............................] - ETA: 30s - loss: 0.1936 
 576/6530 [=>............................] - ETA: 15s - loss: 0.1373
 864/6530 [==>...........................] - ETA: 9s - loss: 0.1125 
1184/6530 [====>.........................] - ETA: 7s - loss: 0.0945
1472/6530 [=====>........................] - ETA: 5s - loss: 0.0847
1760/6530 [=======>......................] - ETA: 4s - loss: 0.0778
2048/6530 [========>.....................] - ETA: 3s - loss: 0.0735
2400/6530 [==========>...................] - ETA: 3s - loss: 0.0689
2720/6530 [===========>..................] - ETA: 2s - loss: 0.0664
3104/6530 [=============>................] - ETA: 2s - loss: 0.0626
3456/6530 [==============>...............] - ETA: 1s - loss: 0.0605
# training | RMSE: 0.4722, MAE: 0.4017
worker 0  xfile  [59, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3494088453763823}, 'layer_2_size': 3, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 84, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3970450730321857}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.4722454626223901, 'rmse': 0.4722454626223901, 'mae': 0.40172090640280966, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  30 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  34 | activation: tanh    | extras: None 
layer 3 | size:  55 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f429a656cf8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:38 - loss: 0.7566
3808/6530 [================>.............] - ETA: 1s - loss: 0.0584
 320/6530 [>.............................] - ETA: 17s - loss: 0.4951 
4192/6530 [==================>...........] - ETA: 1s - loss: 0.0563
 640/6530 [=>............................] - ETA: 8s - loss: 0.3579 
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0548
 960/6530 [===>..........................] - ETA: 5s - loss: 0.3041
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0536
1280/6530 [====>.........................] - ETA: 4s - loss: 0.2715
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0524
1600/6530 [======>.......................] - ETA: 3s - loss: 0.2519
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0511
1936/6530 [=======>......................] - ETA: 2s - loss: 0.2346
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0503
2240/6530 [=========>....................] - ETA: 2s - loss: 0.2263
6432/6530 [============================>.] - ETA: 0s - loss: 0.0492
2576/6530 [==========>...................] - ETA: 1s - loss: 0.2188
2880/6530 [============>.................] - ETA: 1s - loss: 0.2128
# training | RMSE: 0.2013, MAE: 0.1613
worker 1  xfile  [57, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11609532697927981}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.20132147593333424, 'rmse': 0.20132147593333424, 'mae': 0.16131781134543732, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  84 | activation: tanh    | extras: batchnorm 
layer 2 | size:  48 | activation: tanh    | extras: None 
layer 3 | size:  97 | activation: sigmoid | extras: None 
layer 4 | size:  82 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f429ea90278>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 50s - loss: 0.5521
3200/6530 [=============>................] - ETA: 1s - loss: 0.2081
1920/6530 [=======>......................] - ETA: 2s - loss: 0.1351 
6530/6530 [==============================] - 3s 385us/step - loss: 0.0488 - val_loss: 0.0270

3520/6530 [===============>..............] - ETA: 1s - loss: 0.2054
3840/6530 [================>.............] - ETA: 0s - loss: 0.1009
3840/6530 [================>.............] - ETA: 1s - loss: 0.2017
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0874
4144/6530 [==================>...........] - ETA: 0s - loss: 0.2004
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1974
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1958
6530/6530 [==============================] - 1s 201us/step - loss: 0.0823 - val_loss: 0.0479

4944/6530 [=====================>........] - ETA: 0s - loss: 0.1942
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1918
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1905
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1891
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1889
6530/6530 [==============================] - 2s 314us/step - loss: 0.1880 - val_loss: 0.1710

# training | RMSE: 0.2194, MAE: 0.1790
worker 1  xfile  [61, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.21936146599316989, 'rmse': 0.21936146599316989, 'mae': 0.17901452090241984, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  34 | activation: sigmoid | extras: None 
layer 2 | size:   4 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f429f082c50>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 20s - loss: 0.1616
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1009 
# training | RMSE: 0.1528, MAE: 0.1165
worker 2  xfile  [58, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3839055503304921}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24531321904768233}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.15280467853426655, 'rmse': 0.15280467853426655, 'mae': 0.11646173324041675, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  47 | activation: relu    | extras: batchnorm 
layer 2 | size:  56 | activation: sigmoid | extras: dropout - rate: 19.5% 
layer 3 | size:  56 | activation: relu    | extras: dropout - rate: 46.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c6a701d0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:30 - loss: 0.8133
1152/6530 [====>.........................] - ETA: 4s - loss: 0.2338  
2048/6530 [========>.....................] - ETA: 2s - loss: 0.2077
6530/6530 [==============================] - 1s 161us/step - loss: 0.0958 - val_loss: 0.0784

3072/6530 [=============>................] - ETA: 1s - loss: 0.1932
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1858
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1791
6464/6530 [============================>.] - ETA: 0s - loss: 0.1758
6530/6530 [==============================] - 1s 205us/step - loss: 0.1757 - val_loss: 0.2260

# training | RMSE: 0.2212, MAE: 0.1698
worker 0  xfile  [60, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3662141381855081}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.22122042877397805, 'rmse': 0.22122042877397805, 'mae': 0.16980579036947271, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  65 | activation: tanh    | extras: None 
layer 2 | size:  53 | activation: relu    | extras: dropout - rate: 25.5% 
layer 3 | size:  73 | activation: relu    | extras: dropout - rate: 31.6% 
layer 4 | size:  70 | activation: tanh    | extras: None 
layer 5 | size:  57 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f429a2f0c50>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 19s - loss: 0.5934
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1796 
6530/6530 [==============================] - 1s 153us/step - loss: 0.1504 - val_loss: 0.0727

# training | RMSE: 0.2808, MAE: 0.2303
worker 1  xfile  [63, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 42, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.28076261185215207, 'rmse': 0.28076261185215207, 'mae': 0.23029368161715277, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  20 | activation: relu    | extras: dropout - rate: 20.6% 
layer 2 | size:  10 | activation: tanh    | extras: dropout - rate: 18.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f429f03a080>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:47 - loss: 0.5842
 512/6530 [=>............................] - ETA: 8s - loss: 0.3876  
 992/6530 [===>..........................] - ETA: 4s - loss: 0.3609
1504/6530 [=====>........................] - ETA: 2s - loss: 0.3280
1984/6530 [========>.....................] - ETA: 2s - loss: 0.3093
2496/6530 [==========>...................] - ETA: 1s - loss: 0.2908
3024/6530 [============>.................] - ETA: 1s - loss: 0.2775
3536/6530 [===============>..............] - ETA: 0s - loss: 0.2684
3952/6530 [=================>............] - ETA: 0s - loss: 0.2609
4368/6530 [===================>..........] - ETA: 0s - loss: 0.2530
4752/6530 [====================>.........] - ETA: 0s - loss: 0.2489
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2446
5552/6530 [========================>.....] - ETA: 0s - loss: 0.2399
6000/6530 [==========================>...] - ETA: 0s - loss: 0.2362
6448/6530 [============================>.] - ETA: 0s - loss: 0.2330
# training | RMSE: 0.2655, MAE: 0.2227
worker 2  xfile  [62, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 47, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19535362569725787}, 'layer_2_size': 56, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.467049592575547}, 'layer_3_size': 56, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.26554971253457543, 'rmse': 0.26554971253457543, 'mae': 0.22267339289848412, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  91 | activation: tanh    | extras: dropout - rate: 26.7% 
layer 2 | size:  80 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  16 | activation: relu    | extras: dropout - rate: 28.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c5e3fb38>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 49s - loss: 2.8251
6530/6530 [==============================] - 2s 242us/step - loss: 0.2320 - val_loss: 0.1735

2048/6530 [========>.....................] - ETA: 2s - loss: 1.0656 
3968/6530 [=================>............] - ETA: 0s - loss: 0.7353
5632/6530 [========================>.....] - ETA: 0s - loss: 0.5954
6530/6530 [==============================] - 1s 199us/step - loss: 0.5437 - val_loss: 0.1006

# training | RMSE: 0.2686, MAE: 0.2190
worker 0  xfile  [64, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.25524733212409445}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.31580884906435236}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.26861092552375065, 'rmse': 0.26861092552375065, 'mae': 0.21904111295421394, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  95 | activation: relu    | extras: dropout - rate: 30.6% 
layer 2 | size:  79 | activation: relu    | extras: batchnorm 
layer 3 | size:  80 | activation: sigmoid | extras: dropout - rate: 40.2% 
layer 4 | size:  30 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f4299e19c18>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:35 - loss: 0.7707
 416/6530 [>.............................] - ETA: 16s - loss: 0.3145 
 832/6530 [==>...........................] - ETA: 8s - loss: 0.1978 
1216/6530 [====>.........................] - ETA: 5s - loss: 0.1579
1696/6530 [======>.......................] - ETA: 3s - loss: 0.1310
2176/6530 [========>.....................] - ETA: 2s - loss: 0.1164
2624/6530 [===========>..................] - ETA: 2s - loss: 0.1077
3072/6530 [=============>................] - ETA: 1s - loss: 0.1002
3552/6530 [===============>..............] - ETA: 1s - loss: 0.0942
3968/6530 [=================>............] - ETA: 0s - loss: 0.0903
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0869
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0842
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0814
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0792
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0773
6336/6530 [============================>.] - ETA: 0s - loss: 0.0752
6530/6530 [==============================] - 2s 310us/step - loss: 0.0741 - val_loss: 0.0311

# training | RMSE: 0.3050, MAE: 0.2388
worker 2  xfile  [66, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.266971287531688}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.28716433280713766}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4839684055418453}, 'layer_4_size': 57, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4516167757823034}, 'layer_5_size': 37, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.30495292396329654, 'rmse': 0.30495292396329654, 'mae': 0.23883123287570152, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  38 | activation: sigmoid | extras: None 
layer 2 | size:  42 | activation: relu    | extras: dropout - rate: 38.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c58f66d8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 42s - loss: 0.7221
3072/6530 [=============>................] - ETA: 1s - loss: 0.4350 
5888/6530 [==========================>...] - ETA: 0s - loss: 0.3289
6530/6530 [==============================] - 1s 165us/step - loss: 0.3163 - val_loss: 0.2067

# training | RMSE: 0.2186, MAE: 0.1777
worker 1  xfile  [65, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20591799748748119}, 'layer_1_size': 20, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18154684042572067}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 92, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3825331912408373}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18016369794913892}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.21855451559583908, 'rmse': 0.21855451559583908, 'mae': 0.1776601917911477, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  93 | activation: relu    | extras: None 
layer 2 | size:  84 | activation: relu    | extras: batchnorm 
layer 3 | size:  83 | activation: sigmoid | extras: dropout - rate: 18.6% 
layer 4 | size:  45 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f429e1f24a8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:44 - loss: 0.6867
 304/6530 [>.............................] - ETA: 21s - loss: 0.2196 
 624/6530 [=>............................] - ETA: 10s - loss: 0.2003
 928/6530 [===>..........................] - ETA: 6s - loss: 0.1890 
1216/6530 [====>.........................] - ETA: 5s - loss: 0.1797
1488/6530 [=====>........................] - ETA: 4s - loss: 0.1739
1760/6530 [=======>......................] - ETA: 3s - loss: 0.1682
2064/6530 [========>.....................] - ETA: 2s - loss: 0.1641
2400/6530 [==========>...................] - ETA: 2s - loss: 0.1607
2720/6530 [===========>..................] - ETA: 2s - loss: 0.1580
3040/6530 [============>.................] - ETA: 1s - loss: 0.1532
3360/6530 [==============>...............] - ETA: 1s - loss: 0.1510
3648/6530 [===============>..............] - ETA: 1s - loss: 0.1497
3920/6530 [=================>............] - ETA: 1s - loss: 0.1480
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1465
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1453
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1440
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1425
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1407
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1392
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1377
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1363
6432/6530 [============================>.] - ETA: 0s - loss: 0.1349
6530/6530 [==============================] - 2s 353us/step - loss: 0.1347 - val_loss: 0.0966

# training | RMSE: 0.2556, MAE: 0.2093
worker 2  xfile  [69, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37992630451798837}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.19268164550348207}, 'layer_4_size': 34, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2555641323945193, 'rmse': 0.2555641323945193, 'mae': 0.20928334237231772, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  99 | activation: relu    | extras: None 
layer 2 | size:  60 | activation: sigmoid | extras: dropout - rate: 17.0% 
layer 3 | size:  98 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c5694208>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:02 - loss: 0.4514
 768/6530 [==>...........................] - ETA: 7s - loss: 0.6033  
1568/6530 [======>.......................] - ETA: 3s - loss: 0.4533
2400/6530 [==========>...................] - ETA: 1s - loss: 0.3772
3232/6530 [=============>................] - ETA: 1s - loss: 0.3313
4064/6530 [=================>............] - ETA: 0s - loss: 0.3043
4896/6530 [=====================>........] - ETA: 0s - loss: 0.2867
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2702
# training | RMSE: 0.1725, MAE: 0.1351
worker 0  xfile  [67, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30631658816098606}, 'layer_1_size': 95, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4022096769016439}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20978160463021867}, 'layer_5_size': 11, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.1725288059040125, 'rmse': 0.1725288059040125, 'mae': 0.13511976526452352, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  31 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  93 | activation: relu    | extras: None 
layer 3 | size:  32 | activation: tanh    | extras: batchnorm 
layer 4 | size:  55 | activation: sigmoid | extras: dropout - rate: 47.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f4299afb748>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 8:19 - loss: 0.3069
 224/6530 [>.............................] - ETA: 36s - loss: 0.2624 
6530/6530 [==============================] - 1s 222us/step - loss: 0.2594 - val_loss: 0.5025

 432/6530 [>.............................] - ETA: 18s - loss: 0.2314
 640/6530 [=>............................] - ETA: 12s - loss: 0.2166
 864/6530 [==>...........................] - ETA: 9s - loss: 0.2090 
1072/6530 [===>..........................] - ETA: 7s - loss: 0.2045
1264/6530 [====>.........................] - ETA: 6s - loss: 0.2009
1472/6530 [=====>........................] - ETA: 5s - loss: 0.1972
1696/6530 [======>.......................] - ETA: 4s - loss: 0.1921
1920/6530 [=======>......................] - ETA: 4s - loss: 0.1879
2128/6530 [========>.....................] - ETA: 3s - loss: 0.1846
2352/6530 [=========>....................] - ETA: 3s - loss: 0.1816
2576/6530 [==========>...................] - ETA: 2s - loss: 0.1789
2800/6530 [===========>..................] - ETA: 2s - loss: 0.1766
2992/6530 [============>.................] - ETA: 2s - loss: 0.1748
3200/6530 [=============>................] - ETA: 2s - loss: 0.1737
3392/6530 [==============>...............] - ETA: 1s - loss: 0.1716
3584/6530 [===============>..............] - ETA: 1s - loss: 0.1698
3792/6530 [================>.............] - ETA: 1s - loss: 0.1677
4000/6530 [=================>............] - ETA: 1s - loss: 0.1670
4192/6530 [==================>...........] - ETA: 1s - loss: 0.1659
4384/6530 [===================>..........] - ETA: 1s - loss: 0.1649
4544/6530 [===================>..........] - ETA: 1s - loss: 0.1647
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1648
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1640
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1630
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1621
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1616
# training | RMSE: 0.1207, MAE: 0.0900
worker 1  xfile  [68, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.12070551303088048, 'rmse': 0.12070551303088048, 'mae': 0.09000886823924806, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  87 | activation: sigmoid | extras: None 
layer 2 | size:  18 | activation: relu    | extras: dropout - rate: 45.8% 
layer 3 | size:  91 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  12 | activation: relu    | extras: dropout - rate: 46.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42b8af8ac8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:39 - loss: 1.7713
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1603
1216/6530 [====>.........................] - ETA: 4s - loss: 0.2960  
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1588
2304/6530 [=========>....................] - ETA: 1s - loss: 0.2017
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1575
3456/6530 [==============>...............] - ETA: 1s - loss: 0.1609
6368/6530 [============================>.] - ETA: 0s - loss: 0.1571
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1404
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1273
6530/6530 [==============================] - 3s 473us/step - loss: 0.1564 - val_loss: 0.1176

6530/6530 [==============================] - 1s 218us/step - loss: 0.1201 - val_loss: 0.1429

# training | RMSE: 0.5343, MAE: 0.5022
worker 2  xfile  [71, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.17040957422716485}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4156317035728765}, 'layer_4_size': 16, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13739898324337785}, 'layer_5_size': 10, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.5343246641861108, 'rmse': 0.5343246641861108, 'mae': 0.5022330701195236, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  89 | activation: sigmoid | extras: dropout - rate: 19.5% 
layer 2 | size:  33 | activation: relu    | extras: batchnorm 
layer 3 | size:  33 | activation: tanh    | extras: batchnorm 
layer 4 | size:  87 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c56940f0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 2:33 - loss: 0.9290
 704/6530 [==>...........................] - ETA: 12s - loss: 0.4988 
1344/6530 [=====>........................] - ETA: 6s - loss: 0.3014 
1984/6530 [========>.....................] - ETA: 3s - loss: 0.2288
2688/6530 [===========>..................] - ETA: 2s - loss: 0.1871
3584/6530 [===============>..............] - ETA: 1s - loss: 0.1557
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1355
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1214
6336/6530 [============================>.] - ETA: 0s - loss: 0.1119
6530/6530 [==============================] - 2s 323us/step - loss: 0.1099 - val_loss: 0.0447

# training | RMSE: 0.1447, MAE: 0.1135
worker 0  xfile  [70, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4737581842737165}, 'layer_4_size': 55, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.14471332611093427, 'rmse': 0.14471332611093427, 'mae': 0.11345912045399492, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  67 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f42995f1780>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 21s - loss: 0.6002
6530/6530 [==============================] - 1s 157us/step - loss: 0.3647 - val_loss: 0.2079

# training | RMSE: 0.3816, MAE: 0.3167
worker 1  xfile  [72, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.45830290314732824}, 'layer_2_size': 18, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4629906801783099}, 'layer_4_size': 12, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.381620591879895, 'rmse': 0.381620591879895, 'mae': 0.3166895621327665, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  78 | activation: relu    | extras: batchnorm 
layer 2 | size:  60 | activation: relu    | extras: dropout - rate: 49.4% 
layer 3 | size:  30 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  68 | activation: tanh    | extras: batchnorm 
layer 5 | size:  61 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f429dd9cef0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 1:27 - loss: 1.5209
 896/6530 [===>..........................] - ETA: 11s - loss: 0.7404 
# training | RMSE: 0.2103, MAE: 0.1715
worker 2  xfile  [73, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19453446764642846}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38296540910752286}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2103175230611771, 'rmse': 0.2103175230611771, 'mae': 0.1714553326401845, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  30 | activation: sigmoid | extras: dropout - rate: 37.6% 
layer 2 | size:  31 | activation: sigmoid | extras: dropout - rate: 24.1% 
layer 3 | size:   8 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c536ce48>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 49s - loss: 0.6942
1792/6530 [=======>......................] - ETA: 4s - loss: 0.5174 
2816/6530 [===========>..................] - ETA: 1s - loss: 0.2872 
2816/6530 [===========>..................] - ETA: 2s - loss: 0.3988
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2571
3840/6530 [================>.............] - ETA: 1s - loss: 0.3386
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2990
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2691
6530/6530 [==============================] - 1s 192us/step - loss: 0.2504 - val_loss: 0.2013

6530/6530 [==============================] - 2s 349us/step - loss: 0.2554 - val_loss: 0.0735

# training | RMSE: 0.4551, MAE: 0.4137
worker 0  xfile  [75, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 82, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.45512121397596034, 'rmse': 0.45512121397596034, 'mae': 0.41374281033668653, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  17 | activation: sigmoid | extras: None 
layer 2 | size:  21 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42995f15c0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:07 - loss: 0.5035
 368/6530 [>.............................] - ETA: 15s - loss: 0.4627 
 704/6530 [==>...........................] - ETA: 8s - loss: 0.3236 
1104/6530 [====>.........................] - ETA: 5s - loss: 0.2377
1520/6530 [=====>........................] - ETA: 3s - loss: 0.1909
1952/6530 [=======>......................] - ETA: 2s - loss: 0.1652
2400/6530 [==========>...................] - ETA: 2s - loss: 0.1472
2832/6530 [============>.................] - ETA: 1s - loss: 0.1352
3264/6530 [=============>................] - ETA: 1s - loss: 0.1260
3696/6530 [===============>..............] - ETA: 1s - loss: 0.1198
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1138
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1085
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1046
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1015
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0988
6416/6530 [============================>.] - ETA: 0s - loss: 0.0964
6530/6530 [==============================] - 2s 282us/step - loss: 0.0956 - val_loss: 0.0638

# training | RMSE: 0.2654, MAE: 0.2138
worker 1  xfile  [74, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.49364251121821134}, 'layer_2_size': 60, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 30, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.26543327344996837, 'rmse': 0.26543327344996837, 'mae': 0.21381024668787496, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  86 | activation: relu    | extras: batchnorm 
layer 2 | size:  43 | activation: relu    | extras: None 
layer 3 | size:  49 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f429eb03198>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 26s - loss: 0.6005
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0998 
# training | RMSE: 0.2497, MAE: 0.2045
worker 2  xfile  [76, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3758471250524331}, 'layer_1_size': 30, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2411727778235374}, 'layer_2_size': 31, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 8, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 29, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.27484937105018914}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.24965276434760533, 'rmse': 0.24965276434760533, 'mae': 0.20447201118053512, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  44 | activation: relu    | extras: None 
layer 2 | size:  69 | activation: relu    | extras: None 
layer 3 | size:  85 | activation: tanh    | extras: batchnorm 
layer 4 | size:  14 | activation: relu    | extras: dropout - rate: 17.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c5668470>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 4:37 - loss: 0.7516
6530/6530 [==============================] - 1s 196us/step - loss: 0.0813 - val_loss: 0.0213

 384/6530 [>.............................] - ETA: 22s - loss: 0.4871 
 800/6530 [==>...........................] - ETA: 10s - loss: 0.4221
1248/6530 [====>.........................] - ETA: 6s - loss: 0.3785 
1728/6530 [======>.......................] - ETA: 4s - loss: 0.3471
2144/6530 [========>.....................] - ETA: 3s - loss: 0.3291
2528/6530 [==========>...................] - ETA: 2s - loss: 0.3170
2912/6530 [============>.................] - ETA: 2s - loss: 0.3051
3296/6530 [==============>...............] - ETA: 1s - loss: 0.2968
# training | RMSE: 0.1360, MAE: 0.1051
worker 1  xfile  [79, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32305615123930476}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.135952109566679, 'rmse': 0.135952109566679, 'mae': 0.10511401797226126, 'early_stop': False}
vggnet done  1

3712/6530 [================>.............] - ETA: 1s - loss: 0.2888
4096/6530 [=================>............] - ETA: 1s - loss: 0.2825
4608/6530 [====================>.........] - ETA: 0s - loss: 0.2726
# training | RMSE: 0.2537, MAE: 0.2073
worker 0  xfile  [77, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 87, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2537073411633414, 'rmse': 0.2537073411633414, 'mae': 0.20732787594903168, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  27 | activation: sigmoid | extras: dropout - rate: 41.6% 
layer 2 | size:  12 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f4298c35e80>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 44s - loss: 0.6815
5056/6530 [======================>.......] - ETA: 0s - loss: 0.2658
3584/6530 [===============>..............] - ETA: 0s - loss: 0.4237 
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2601
6016/6530 [==========================>...] - ETA: 0s - loss: 0.2550
6496/6530 [============================>.] - ETA: 0s - loss: 0.2488
6530/6530 [==============================] - 1s 168us/step - loss: 0.3639 - val_loss: 0.2028

6530/6530 [==============================] - 2s 355us/step - loss: 0.2487 - val_loss: 0.3696

# training | RMSE: 0.2548, MAE: 0.2051
worker 0  xfile  [80, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4159975004466281}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 34, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 15, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 17, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2548235091785397, 'rmse': 0.2548235091785397, 'mae': 0.20508887178445276, 'early_stop': False}
vggnet done  0

# training | RMSE: 0.4403, MAE: 0.3566
worker 2  xfile  [78, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 44, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 69, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17547866501321385}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.44027519216217215, 'rmse': 0.44027519216217215, 'mae': 0.3566256815872431, 'early_stop': False}
vggnet done  2
all of workers have been done
calculation finished
#1 epoch=1.0 loss={'loss': 0.22456440851803255, 'rmse': 0.22456440851803255, 'mae': 0.18062943834514902, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2504247577292705}, 'layer_2_size': 56, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23735034266556831}, 'layer_3_size': 75, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.28190637044148625}, 'layer_4_size': 4, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 87, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#0 epoch=1.0 loss={'loss': 0.6134319363683851, 'rmse': 0.6134319363683851, 'mae': 0.5541655242799806, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.31449329379998714}, 'layer_1_size': 59, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3979325003082128}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3012214949206562}, 'layer_3_size': 31, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#2 epoch=1.0 loss={'loss': 0.2328448424783789, 'rmse': 0.2328448424783789, 'mae': 0.19075815102516294, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14071525978619703}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1632186816763678}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.13349068180705492}, 'layer_4_size': 48, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#3 epoch=1.0 loss={'loss': 0.36771200014896366, 'rmse': 0.36771200014896366, 'mae': 0.28291883885405694, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 65, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 50, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#4 epoch=1.0 loss={'loss': 0.26501945903594804, 'rmse': 0.26501945903594804, 'mae': 0.2170910695388972, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23285260965765855}, 'layer_1_size': 88, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15396607186536132}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#5 epoch=1.0 loss={'loss': 0.22804689564111963, 'rmse': 0.22804689564111963, 'mae': 0.18283795323664223, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19976518586726605}, 'layer_1_size': 92, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.45757510847559735}, 'layer_5_size': 23, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#7 epoch=1.0 loss={'loss': 0.27445490227145236, 'rmse': 0.27445490227145236, 'mae': 0.22499459619156972, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 2, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2568172258114946}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 36, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16264077375686514}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#9 epoch=1.0 loss={'loss': 0.23854860447536622, 'rmse': 0.23854860447536622, 'mae': 0.1941251391258045, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14505351726939095}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 50, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.161128861263999}, 'layer_4_size': 17, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 2, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#8 epoch=1.0 loss={'loss': 0.2233355475747338, 'rmse': 0.2233355475747338, 'mae': 0.18107603391676447, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4080403007617508}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16454352642837314}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4288788424853015}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#10 epoch=1.0 loss={'loss': 0.23512839412235037, 'rmse': 0.23512839412235037, 'mae': 0.19312180101514703, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15238026352041448}, 'layer_1_size': 14, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 83, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#6 epoch=1.0 loss={'loss': 0.23927037166623633, 'rmse': 0.23927037166623633, 'mae': 0.19395498889227125, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27543604469585237}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3363880143625354}, 'layer_2_size': 72, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 9, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4164082518612683}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#13 epoch=1.0 loss={'loss': 0.2260113786210233, 'rmse': 0.2260113786210233, 'mae': 0.18257290220016822, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 31, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#12 epoch=1.0 loss={'loss': 0.20142349042092042, 'rmse': 0.20142349042092042, 'mae': 0.15995877664440714, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23374835004413852}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#14 epoch=1.0 loss={'loss': 0.3681748416854472, 'rmse': 0.3681748416854472, 'mae': 0.304436438277626, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19883823963787806}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.32044028165442373}, 'layer_3_size': 2, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17273845308660446}, 'layer_4_size': 11, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#11 epoch=1.0 loss={'loss': 0.25177857053905617, 'rmse': 0.25177857053905617, 'mae': 0.20130768914858813, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 8, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.13912763458249527}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3022629720858989}, 'layer_4_size': 74, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#15 epoch=1.0 loss={'loss': 0.23849013769082872, 'rmse': 0.23849013769082872, 'mae': 0.1878721694143133, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 71, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 41, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#16 epoch=1.0 loss={'loss': 0.25702240259247716, 'rmse': 0.25702240259247716, 'mae': 0.20510790361179787, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27066805008790573}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1156475839256515}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2212949453718571}, 'layer_5_size': 87, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#17 epoch=1.0 loss={'loss': 0.2753316740486409, 'rmse': 0.2753316740486409, 'mae': 0.22594575370496486, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3414675344100685}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3564867865679183}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3700148745940326}, 'layer_3_size': 34, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3142325446127674}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#19 epoch=1.0 loss={'loss': 0.21803750249453108, 'rmse': 0.21803750249453108, 'mae': 0.17615840615991504, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2684385694928465}, 'layer_1_size': 26, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47712176301521403}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4122081315162861}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#18 epoch=1.0 loss={'loss': 0.17261302245326615, 'rmse': 0.17261302245326615, 'mae': 0.14211839798658538, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#21 epoch=1.0 loss={'loss': 0.24202022387535696, 'rmse': 0.24202022387535696, 'mae': 0.19819934158037045, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22385916107638965}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4540864912159174}, 'layer_2_size': 57, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3773403287489404}, 'layer_5_size': 35, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#20 epoch=1.0 loss={'loss': 0.3811312330759861, 'rmse': 0.3811312330759861, 'mae': 0.31357196868835563, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4922590499368914}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.40054943618782857}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2665061266155887}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#23 epoch=1.0 loss={'loss': 0.22373016341579893, 'rmse': 0.22373016341579893, 'mae': 0.18009427874695247, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32553354455869177}, 'layer_4_size': 46, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#22 epoch=1.0 loss={'loss': 0.41121567455137753, 'rmse': 0.41121567455137753, 'mae': 0.3409453553815764, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.40563576142447133}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11801951126986109}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1031887790315289}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22642497672744197}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#24 epoch=1.0 loss={'loss': 0.24660581662310402, 'rmse': 0.24660581662310402, 'mae': 0.2008064512204931, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.48502693835638644}, 'layer_1_size': 37, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16159727813614957}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1291148842886615}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 43, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#25 epoch=1.0 loss={'loss': 0.14273960183017992, 'rmse': 0.14273960183017992, 'mae': 0.11326856317934546, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41090246001998465}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#26 epoch=1.0 loss={'loss': 0.25916821259054035, 'rmse': 0.25916821259054035, 'mae': 0.214671748427392, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4901363409208247}, 'layer_1_size': 18, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 72, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23683208726054494}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#27 epoch=1.0 loss={'loss': 0.15720449975204717, 'rmse': 0.15720449975204717, 'mae': 0.1244371804481419, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1539922143106104}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#28 epoch=1.0 loss={'loss': 0.2321658549535544, 'rmse': 0.2321658549535544, 'mae': 0.1924489374703006, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#30 epoch=1.0 loss={'loss': 0.20666948892724032, 'rmse': 0.20666948892724032, 'mae': 0.1668326067401611, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19774261829234205}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2797140258496348}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45844819637653644}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#32 epoch=1.0 loss={'loss': 0.21941897320950668, 'rmse': 0.21941897320950668, 'mae': 0.18099887114324645, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24594424623809785}, 'layer_1_size': 56, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41103794745290845}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11328582488940891}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#29 epoch=1.0 loss={'loss': 0.24712990083208553, 'rmse': 0.24712990083208553, 'mae': 0.19930388036449936, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23265042009909262}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12192595655153445}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#31 epoch=1.0 loss={'loss': 0.2841659709102846, 'rmse': 0.2841659709102846, 'mae': 0.23361202180895824, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4187861969586798}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4047564904270293}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 94, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#33 epoch=1.0 loss={'loss': 0.30886039627547995, 'rmse': 0.30886039627547995, 'mae': 0.24441765952802932, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 5, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 67, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#34 epoch=1.0 loss={'loss': 0.1853154305807106, 'rmse': 0.1853154305807106, 'mae': 0.14569521797754145, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11932348426737889}, 'layer_4_size': 93, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#36 epoch=1.0 loss={'loss': 1.2293268445192955, 'rmse': 1.2293268445192955, 'mae': 1.1822016765982466, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18309647526297057}, 'layer_2_size': 49, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#35 epoch=1.0 loss={'loss': 0.15653635642649547, 'rmse': 0.15653635642649547, 'mae': 0.12216281975913237, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16597532291562125}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2703434644417436}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#39 epoch=1.0 loss={'loss': 0.2018708882161571, 'rmse': 0.2018708882161571, 'mae': 0.16252438295386845, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4190781584026906}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#37 epoch=1.0 loss={'loss': 0.20075642982326475, 'rmse': 0.20075642982326475, 'mae': 0.16293782793283815, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.24184757063105722}, 'layer_3_size': 20, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 29, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#38 epoch=1.0 loss={'loss': 0.2266517940554529, 'rmse': 0.2266517940554529, 'mae': 0.1795985111278617, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3667571823118735}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4850000697615927}, 'layer_2_size': 75, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35238667661361356}, 'layer_5_size': 37, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#40 epoch=1.0 loss={'loss': 0.47392175691971244, 'rmse': 0.47392175691971244, 'mae': 0.3897131697868296, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 90, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1979973379913189}, 'layer_4_size': 67, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#41 epoch=1.0 loss={'loss': 0.2564519088725764, 'rmse': 0.2564519088725764, 'mae': 0.20994167237022315, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16220811735884047}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.29275903749501153}, 'layer_2_size': 28, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4430161926599384}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#42 epoch=1.0 loss={'loss': 0.2927395370454333, 'rmse': 0.2927395370454333, 'mae': 0.23732246408136617, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22959196988663796}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 72, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10988727565631695}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40210907670547635}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#43 epoch=1.0 loss={'loss': 0.20452495542870655, 'rmse': 0.20452495542870655, 'mae': 0.16477802380839676, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#44 epoch=1.0 loss={'loss': 0.32749230910458166, 'rmse': 0.32749230910458166, 'mae': 0.2669432201886991, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19153313597417265}, 'layer_1_size': 44, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#46 epoch=1.0 loss={'loss': 0.2663875497060918, 'rmse': 0.2663875497060918, 'mae': 0.21413688206621245, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38430490289489483}, 'layer_3_size': 25, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 74, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#45 epoch=1.0 loss={'loss': 0.3565739209104217, 'rmse': 0.3565739209104217, 'mae': 0.3009715560165394, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.17522488099171737}, 'layer_2_size': 26, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 19, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19065135093673313}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#47 epoch=1.0 loss={'loss': 0.2656003890323989, 'rmse': 0.2656003890323989, 'mae': 0.21690962004856457, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21860967678915208}, 'layer_2_size': 70, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3106122218642763}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#49 epoch=1.0 loss={'loss': 0.25313665585875234, 'rmse': 0.25313665585875234, 'mae': 0.20142687269335335, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3955433867376831}, 'layer_3_size': 61, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3314818760746431}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3345520542427325}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#48 epoch=1.0 loss={'loss': 0.17338625968451782, 'rmse': 0.17338625968451782, 'mae': 0.13246553444119782, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#50 epoch=1.0 loss={'loss': 0.21870003752459988, 'rmse': 0.21870003752459988, 'mae': 0.1777684074017919, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17031695865320207}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12100320754320126}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#52 epoch=1.0 loss={'loss': 0.648568605740579, 'rmse': 0.648568605740579, 'mae': 0.5915261896369804, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.49532365596879}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 61, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4400657261566402}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20286776821312336}, 'layer_5_size': 9, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#51 epoch=1.0 loss={'loss': 0.2605016588475522, 'rmse': 0.2605016588475522, 'mae': 0.2124648320942563, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 70, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3621584604959015}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#53 epoch=1.0 loss={'loss': 0.2974435334297008, 'rmse': 0.2974435334297008, 'mae': 0.23472882110879198, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 66, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45371219109203254}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#54 epoch=1.0 loss={'loss': 0.411268671457726, 'rmse': 0.411268671457726, 'mae': 0.3559600818859608, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2517984823985751}, 'layer_3_size': 84, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23988635610248485}, 'layer_4_size': 39, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.490310731404365}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#55 epoch=1.0 loss={'loss': 0.2000017149188745, 'rmse': 0.2000017149188745, 'mae': 0.1604929579436235, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32673705199921127}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#56 epoch=1.0 loss={'loss': 0.22502098983258487, 'rmse': 0.22502098983258487, 'mae': 0.17933388219501226, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#59 epoch=1.0 loss={'loss': 0.4722454626223901, 'rmse': 0.4722454626223901, 'mae': 0.40172090640280966, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3494088453763823}, 'layer_2_size': 3, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 84, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3970450730321857}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#57 epoch=1.0 loss={'loss': 0.20132147593333424, 'rmse': 0.20132147593333424, 'mae': 0.16131781134543732, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11609532697927981}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#58 epoch=1.0 loss={'loss': 0.15280467853426655, 'rmse': 0.15280467853426655, 'mae': 0.11646173324041675, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3839055503304921}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24531321904768233}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#61 epoch=1.0 loss={'loss': 0.21936146599316989, 'rmse': 0.21936146599316989, 'mae': 0.17901452090241984, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#60 epoch=1.0 loss={'loss': 0.22122042877397805, 'rmse': 0.22122042877397805, 'mae': 0.16980579036947271, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3662141381855081}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#63 epoch=1.0 loss={'loss': 0.28076261185215207, 'rmse': 0.28076261185215207, 'mae': 0.23029368161715277, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 42, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#62 epoch=1.0 loss={'loss': 0.26554971253457543, 'rmse': 0.26554971253457543, 'mae': 0.22267339289848412, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 47, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19535362569725787}, 'layer_2_size': 56, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.467049592575547}, 'layer_3_size': 56, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#64 epoch=1.0 loss={'loss': 0.26861092552375065, 'rmse': 0.26861092552375065, 'mae': 0.21904111295421394, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.25524733212409445}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.31580884906435236}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#65 epoch=1.0 loss={'loss': 0.21855451559583908, 'rmse': 0.21855451559583908, 'mae': 0.1776601917911477, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20591799748748119}, 'layer_1_size': 20, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18154684042572067}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 92, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3825331912408373}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18016369794913892}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#66 epoch=1.0 loss={'loss': 0.30495292396329654, 'rmse': 0.30495292396329654, 'mae': 0.23883123287570152, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.266971287531688}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.28716433280713766}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4839684055418453}, 'layer_4_size': 57, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4516167757823034}, 'layer_5_size': 37, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#67 epoch=1.0 loss={'loss': 0.1725288059040125, 'rmse': 0.1725288059040125, 'mae': 0.13511976526452352, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30631658816098606}, 'layer_1_size': 95, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4022096769016439}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20978160463021867}, 'layer_5_size': 11, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#69 epoch=1.0 loss={'loss': 0.2555641323945193, 'rmse': 0.2555641323945193, 'mae': 0.20928334237231772, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37992630451798837}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.19268164550348207}, 'layer_4_size': 34, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#68 epoch=1.0 loss={'loss': 0.12070551303088048, 'rmse': 0.12070551303088048, 'mae': 0.09000886823924806, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#71 epoch=1.0 loss={'loss': 0.5343246641861108, 'rmse': 0.5343246641861108, 'mae': 0.5022330701195236, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.17040957422716485}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4156317035728765}, 'layer_4_size': 16, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13739898324337785}, 'layer_5_size': 10, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#72 epoch=1.0 loss={'loss': 0.381620591879895, 'rmse': 0.381620591879895, 'mae': 0.3166895621327665, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.45830290314732824}, 'layer_2_size': 18, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4629906801783099}, 'layer_4_size': 12, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#70 epoch=1.0 loss={'loss': 0.14471332611093427, 'rmse': 0.14471332611093427, 'mae': 0.11345912045399492, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4737581842737165}, 'layer_4_size': 55, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#73 epoch=1.0 loss={'loss': 0.2103175230611771, 'rmse': 0.2103175230611771, 'mae': 0.1714553326401845, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19453446764642846}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38296540910752286}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#75 epoch=1.0 loss={'loss': 0.45512121397596034, 'rmse': 0.45512121397596034, 'mae': 0.41374281033668653, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 82, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 72, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#76 epoch=1.0 loss={'loss': 0.24965276434760533, 'rmse': 0.24965276434760533, 'mae': 0.20447201118053512, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3758471250524331}, 'layer_1_size': 30, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2411727778235374}, 'layer_2_size': 31, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 8, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 29, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.27484937105018914}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#74 epoch=1.0 loss={'loss': 0.26543327344996837, 'rmse': 0.26543327344996837, 'mae': 0.21381024668787496, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.49364251121821134}, 'layer_2_size': 60, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 30, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#77 epoch=1.0 loss={'loss': 0.2537073411633414, 'rmse': 0.2537073411633414, 'mae': 0.20732787594903168, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 87, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#79 epoch=1.0 loss={'loss': 0.135952109566679, 'rmse': 0.135952109566679, 'mae': 0.10511401797226126, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32305615123930476}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#80 epoch=1.0 loss={'loss': 0.2548235091785397, 'rmse': 0.2548235091785397, 'mae': 0.20508887178445276, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4159975004466281}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 34, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 15, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 17, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#78 epoch=1.0 loss={'loss': 0.44027519216217215, 'rmse': 0.44027519216217215, 'mae': 0.3566256815872431, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 44, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 69, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17547866501321385}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
get a list [results] of length 81
get a list [loss] of length 81
get a list [val_loss] of length 81
length of indices is (68, 79, 25, 70, 58, 35, 27, 67, 18, 48, 34, 55, 37, 57, 12, 39, 43, 30, 73, 19, 65, 50, 61, 32, 60, 8, 23, 1, 56, 13, 38, 5, 28, 2, 10, 15, 9, 6, 21, 24, 29, 76, 11, 49, 77, 80, 69, 41, 16, 26, 51, 4, 74, 62, 47, 46, 64, 7, 17, 63, 31, 42, 53, 66, 33, 44, 45, 3, 14, 20, 72, 22, 54, 78, 75, 59, 40, 71, 0, 52, 36)
length of indices is 81
length of T is 81
newly formed T structure is:[[0, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [1, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32305615123930476}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [2, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41090246001998465}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [3, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4737581842737165}, 'layer_4_size': 55, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [4, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3839055503304921}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24531321904768233}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [5, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16597532291562125}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2703434644417436}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [6, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1539922143106104}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [7, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30631658816098606}, 'layer_1_size': 95, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4022096769016439}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20978160463021867}, 'layer_5_size': 11, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [8, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [9, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [10, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11932348426737889}, 'layer_4_size': 93, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [11, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32673705199921127}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [12, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.24184757063105722}, 'layer_3_size': 20, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 29, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [13, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11609532697927981}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [14, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23374835004413852}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [15, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4190781584026906}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [16, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [17, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19774261829234205}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2797140258496348}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45844819637653644}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [18, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19453446764642846}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38296540910752286}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [19, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2684385694928465}, 'layer_1_size': 26, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47712176301521403}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4122081315162861}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [20, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20591799748748119}, 'layer_1_size': 20, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18154684042572067}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 92, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3825331912408373}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18016369794913892}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [21, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17031695865320207}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12100320754320126}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [22, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [23, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24594424623809785}, 'layer_1_size': 56, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41103794745290845}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11328582488940891}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [24, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3662141381855081}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [25, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4080403007617508}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16454352642837314}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4288788424853015}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [26, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32553354455869177}, 'layer_4_size': 46, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]] 

*** 27.0 configurations x 3.0 iterations each

79 | Thu Sep 27 23:05:34 2018 | lowest loss so far: 0.1207 (run 68)

{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  86 | activation: relu    | extras: batchnorm 
layer 2 | size:  43 | activation: relu    | extras: None 
layer 3 | size:  49 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 25s - loss: 0.6033{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  93 | activation: relu    | extras: None 
layer 2 | size:  84 | activation: relu    | extras: batchnorm 
layer 3 | size:  83 | activation: sigmoid | extras: dropout - rate: 18.6% 
layer 4 | size:  45 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 7:14 - loss: 0.6924
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1028 
 240/6530 [>.............................] - ETA: 29s - loss: 0.2279 {'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  41 | activation: relu    | extras: None 
layer 2 | size:  35 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  62 | activation: tanh    | extras: None 
layer 4 | size:  42 | activation: relu    | extras: dropout - rate: 41.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 7:47 - loss: 0.8776
6530/6530 [==============================] - 1s 176us/step - loss: 0.0806 - val_loss: 0.0177

 528/6530 [=>............................] - ETA: 13s - loss: 0.2011Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0158
 320/6530 [>.............................] - ETA: 23s - loss: 0.1700 
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0148
 848/6530 [==>...........................] - ETA: 8s - loss: 0.1883 
6530/6530 [==============================] - 0s 10us/step - loss: 0.0145 - val_loss: 0.0131
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0116
 608/6530 [=>............................] - ETA: 12s - loss: 0.1210
1184/6530 [====>.........................] - ETA: 5s - loss: 0.1782
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0117
6530/6530 [==============================] - 0s 10us/step - loss: 0.0116 - val_loss: 0.0115

 944/6530 [===>..........................] - ETA: 7s - loss: 0.1021 
1504/6530 [=====>........................] - ETA: 4s - loss: 0.1709
1296/6530 [====>.........................] - ETA: 5s - loss: 0.0896
1840/6530 [=======>......................] - ETA: 3s - loss: 0.1656
1648/6530 [======>.......................] - ETA: 4s - loss: 0.0801
2176/6530 [========>.....................] - ETA: 2s - loss: 0.1611
1968/6530 [========>.....................] - ETA: 3s - loss: 0.0731
2480/6530 [==========>...................] - ETA: 2s - loss: 0.1585
2336/6530 [=========>....................] - ETA: 2s - loss: 0.0685
2816/6530 [===========>..................] - ETA: 2s - loss: 0.1553
2688/6530 [===========>..................] - ETA: 2s - loss: 0.0644
3152/6530 [=============>................] - ETA: 1s - loss: 0.1514
3040/6530 [============>.................] - ETA: 1s - loss: 0.0614
3504/6530 [===============>..............] - ETA: 1s - loss: 0.1497
3408/6530 [==============>...............] - ETA: 1s - loss: 0.0590
3856/6530 [================>.............] - ETA: 1s - loss: 0.1477
3776/6530 [================>.............] - ETA: 1s - loss: 0.0564
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1456
4160/6530 [==================>...........] - ETA: 1s - loss: 0.0546
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1447
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0530
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1425
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0515
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1407
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0503
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1394
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0488
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1378
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0477
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1367
6336/6530 [============================>.] - ETA: 0s - loss: 0.0470
6530/6530 [==============================] - 2s 329us/step - loss: 0.1356 - val_loss: 0.1145
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0917
6530/6530 [==============================] - 2s 332us/step - loss: 0.0463 - val_loss: 0.0543
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0412
 352/6530 [>.............................] - ETA: 0s - loss: 0.1034
 352/6530 [>.............................] - ETA: 0s - loss: 0.0324
 688/6530 [==>...........................] - ETA: 0s - loss: 0.1106
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0321
1040/6530 [===>..........................] - ETA: 0s - loss: 0.1113
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0302
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1101
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0294
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1097
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0288
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1089
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0282
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1091
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0277
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1085
2912/6530 [============>.................] - ETA: 0s - loss: 0.0279
3104/6530 [=============>................] - ETA: 0s - loss: 0.1062
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0282
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1067
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0281
# training | RMSE: 0.0958, MAE: 0.0736
worker 1  xfile  [1, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32305615123930476}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.09577756827421148, 'rmse': 0.09577756827421148, 'mae': 0.07363248988402592, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  31 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  93 | activation: relu    | extras: None 
layer 3 | size:  32 | activation: tanh    | extras: batchnorm 
layer 4 | size:  55 | activation: sigmoid | extras: dropout - rate: 47.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8f44e0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 3:41 - loss: 1.8733
3792/6530 [================>.............] - ETA: 0s - loss: 0.1062
4016/6530 [=================>............] - ETA: 0s - loss: 0.0280
 256/6530 [>.............................] - ETA: 14s - loss: 1.6394 
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1067
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0278
 480/6530 [=>............................] - ETA: 8s - loss: 1.4180 
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1072
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0274
 720/6530 [==>...........................] - ETA: 5s - loss: 1.1784
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1065
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0273
 944/6530 [===>..........................] - ETA: 4s - loss: 0.9775
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1063
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0271
1168/6530 [====>.........................] - ETA: 3s - loss: 0.8297
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1063
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0269
1392/6530 [=====>........................] - ETA: 3s - loss: 0.7274
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1054
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0267
1632/6530 [======>.......................] - ETA: 2s - loss: 0.6440
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1051
6480/6530 [============================>.] - ETA: 0s - loss: 0.0266
1856/6530 [=======>......................] - ETA: 2s - loss: 0.5889
6480/6530 [============================>.] - ETA: 0s - loss: 0.1048
6530/6530 [==============================] - 1s 148us/step - loss: 0.0266 - val_loss: 0.0224
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0232
6530/6530 [==============================] - 1s 157us/step - loss: 0.1050 - val_loss: 0.0852
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0915
2080/6530 [========>.....................] - ETA: 2s - loss: 0.5419
 384/6530 [>.............................] - ETA: 0s - loss: 0.0235
 352/6530 [>.............................] - ETA: 0s - loss: 0.0935
2320/6530 [=========>....................] - ETA: 1s - loss: 0.5020
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0230
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1038
2544/6530 [==========>...................] - ETA: 1s - loss: 0.4713
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0230
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1027
2768/6530 [===========>..................] - ETA: 1s - loss: 0.4451
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0227
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1029
2976/6530 [============>.................] - ETA: 1s - loss: 0.4251
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0228
1680/6530 [======>.......................] - ETA: 0s - loss: 0.1008
3216/6530 [=============>................] - ETA: 1s - loss: 0.4055
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0229
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0990
3440/6530 [==============>...............] - ETA: 1s - loss: 0.3892
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0226
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0990
3680/6530 [===============>..............] - ETA: 1s - loss: 0.3740
2848/6530 [============>.................] - ETA: 0s - loss: 0.0227
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0991
3904/6530 [================>.............] - ETA: 0s - loss: 0.3609
3184/6530 [=============>................] - ETA: 0s - loss: 0.0230
3040/6530 [============>.................] - ETA: 0s - loss: 0.0967
4144/6530 [==================>...........] - ETA: 0s - loss: 0.3493
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0232
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0967
4368/6530 [===================>..........] - ETA: 0s - loss: 0.3391
3872/6530 [================>.............] - ETA: 0s - loss: 0.0233
3744/6530 [================>.............] - ETA: 0s - loss: 0.0967
4592/6530 [====================>.........] - ETA: 0s - loss: 0.3297
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0231
4096/6530 [=================>............] - ETA: 0s - loss: 0.0973
4832/6530 [=====================>........] - ETA: 0s - loss: 0.3197
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0228
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0976
5056/6530 [======================>.......] - ETA: 0s - loss: 0.3125
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0230
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0977
5296/6530 [=======================>......] - ETA: 0s - loss: 0.3039
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0228
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0975
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2967
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0229
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0978
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2909
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0231
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0971
6000/6530 [==========================>...] - ETA: 0s - loss: 0.2846
6336/6530 [============================>.] - ETA: 0s - loss: 0.0230
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0964
6224/6530 [===========================>..] - ETA: 0s - loss: 0.2790
6416/6530 [============================>.] - ETA: 0s - loss: 0.0963
6530/6530 [==============================] - 1s 153us/step - loss: 0.0229 - val_loss: 0.0174

6464/6530 [============================>.] - ETA: 0s - loss: 0.2733
6530/6530 [==============================] - 1s 157us/step - loss: 0.0963 - val_loss: 0.0774

6530/6530 [==============================] - 2s 322us/step - loss: 0.2719 - val_loss: 0.1233
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1204
 256/6530 [>.............................] - ETA: 1s - loss: 0.1396
 496/6530 [=>............................] - ETA: 1s - loss: 0.1329
 704/6530 [==>...........................] - ETA: 1s - loss: 0.1359
 944/6530 [===>..........................] - ETA: 1s - loss: 0.1326
1184/6530 [====>.........................] - ETA: 1s - loss: 0.1321
1424/6530 [=====>........................] - ETA: 1s - loss: 0.1306
1664/6530 [======>.......................] - ETA: 1s - loss: 0.1325
1904/6530 [=======>......................] - ETA: 1s - loss: 0.1335
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1341
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1335
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1331
2864/6530 [============>.................] - ETA: 0s - loss: 0.1324
3120/6530 [=============>................] - ETA: 0s - loss: 0.1312
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1313
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1298
3824/6530 [================>.............] - ETA: 0s - loss: 0.1300
4064/6530 [=================>............] - ETA: 0s - loss: 0.1295
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1299
# training | RMSE: 0.0936, MAE: 0.0716
worker 0  xfile  [0, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.093571068845624, 'rmse': 0.093571068845624, 'mae': 0.07155058140660678, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  41 | activation: tanh    | extras: None 
layer 2 | size:  39 | activation: relu    | extras: batchnorm 
layer 3 | size:  30 | activation: relu    | extras: None 
layer 4 | size:  83 | activation: sigmoid | extras: dropout - rate: 16.6% 
layer 5 | size:  40 | activation: tanh    | extras: dropout - rate: 27.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f4604098400>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:28 - loss: 0.8181
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1302
 352/6530 [>.............................] - ETA: 7s - loss: 0.0919  
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1303
 672/6530 [==>...........................] - ETA: 4s - loss: 0.0716
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1302
1008/6530 [===>..........................] - ETA: 2s - loss: 0.0626
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1298
1328/6530 [=====>........................] - ETA: 2s - loss: 0.0574
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1299
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0528
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1302
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0495
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1303
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0468
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1298
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0455
6528/6530 [============================>.] - ETA: 0s - loss: 0.1294
2992/6530 [============>.................] - ETA: 0s - loss: 0.0433
6530/6530 [==============================] - 1s 220us/step - loss: 0.1294 - val_loss: 0.1064
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1373
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0421
 256/6530 [>.............................] - ETA: 1s - loss: 0.1213
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0407
 480/6530 [=>............................] - ETA: 1s - loss: 0.1244
4016/6530 [=================>............] - ETA: 0s - loss: 0.0391
 720/6530 [==>...........................] - ETA: 1s - loss: 0.1228
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0379
 976/6530 [===>..........................] - ETA: 1s - loss: 0.1180
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0370
1216/6530 [====>.........................] - ETA: 1s - loss: 0.1196
# training | RMSE: 0.1204, MAE: 0.0944
worker 2  xfile  [2, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41090246001998465}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.12037732808109387, 'rmse': 0.12037732808109387, 'mae': 0.0944183056131048, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  67 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  34 | activation: tanh    | extras: dropout - rate: 38.4% 
layer 3 | size:  72 | activation: relu    | extras: None 
layer 4 | size:  37 | activation: sigmoid | extras: batchnorm 
layer 5 | size:  36 | activation: sigmoid | extras: dropout - rate: 24.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8f4550>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 2:18 - loss: 0.1201
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0362
1456/6530 [=====>........................] - ETA: 1s - loss: 0.1204
 480/6530 [=>............................] - ETA: 9s - loss: 0.0876  
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0353
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1207
 928/6530 [===>..........................] - ETA: 4s - loss: 0.0731
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0349
1936/6530 [=======>......................] - ETA: 0s - loss: 0.1215
1344/6530 [=====>........................] - ETA: 3s - loss: 0.0644
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0342
2160/6530 [========>.....................] - ETA: 0s - loss: 0.1239
1792/6530 [=======>......................] - ETA: 2s - loss: 0.0593
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0336
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1231
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0570
6512/6530 [============================>.] - ETA: 0s - loss: 0.0330
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1230
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0547
2864/6530 [============>.................] - ETA: 0s - loss: 0.1228
3072/6530 [=============>................] - ETA: 1s - loss: 0.0522
6530/6530 [==============================] - 1s 225us/step - loss: 0.0330 - val_loss: 0.0190
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0115
3104/6530 [=============>................] - ETA: 0s - loss: 0.1221
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0511
 336/6530 [>.............................] - ETA: 0s - loss: 0.0197
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1224
3936/6530 [=================>............] - ETA: 0s - loss: 0.0499
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0210
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1230
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0486
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0212
3824/6530 [================>.............] - ETA: 0s - loss: 0.1225
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0477
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0210
4064/6530 [=================>............] - ETA: 0s - loss: 0.1226
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0468
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0218
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1227
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0222
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0459
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1221
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0218
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0454
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1217
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0216
6528/6530 [============================>.] - ETA: 0s - loss: 0.0447
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1215
2880/6530 [============>.................] - ETA: 0s - loss: 0.0220
6530/6530 [==============================] - 2s 235us/step - loss: 0.0447 - val_loss: 0.0312
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0244
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1217
3216/6530 [=============>................] - ETA: 0s - loss: 0.0222
 448/6530 [=>............................] - ETA: 0s - loss: 0.0330
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1216
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0220
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0325
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1218
3824/6530 [================>.............] - ETA: 0s - loss: 0.0217
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0313
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1211
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0218
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0307
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1206
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0219
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0304
6432/6530 [============================>.] - ETA: 0s - loss: 0.1206
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0218
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0311
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0216
6530/6530 [==============================] - 1s 224us/step - loss: 0.1207 - val_loss: 0.0987

3040/6530 [============>.................] - ETA: 0s - loss: 0.0304
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0219
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0306
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0217
3872/6530 [================>.............] - ETA: 0s - loss: 0.0307
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0214
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0302
6416/6530 [============================>.] - ETA: 0s - loss: 0.0213
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0302
6530/6530 [==============================] - 1s 165us/step - loss: 0.0213 - val_loss: 0.0152
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0163
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0300
 336/6530 [>.............................] - ETA: 0s - loss: 0.0166
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0298
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0198
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0298
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0189
6530/6530 [==============================] - 1s 122us/step - loss: 0.0296 - val_loss: 0.0200

1280/6530 [====>.........................] - ETA: 0s - loss: 0.0182Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0272
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0182
 512/6530 [=>............................] - ETA: 0s - loss: 0.0252
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0186
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0248
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0184
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0250
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0180
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0253
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0183
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0253
3136/6530 [=============>................] - ETA: 0s - loss: 0.0183
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0253
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0181
3200/6530 [=============>................] - ETA: 0s - loss: 0.0250
3824/6530 [================>.............] - ETA: 0s - loss: 0.0182
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0256
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0183
4064/6530 [=================>............] - ETA: 0s - loss: 0.0256
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0183
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0256
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0181
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0256
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0183
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0259
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0182
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0256
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0181
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0256
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0181
6530/6530 [==============================] - 1s 122us/step - loss: 0.0255 - val_loss: 0.0159

6480/6530 [============================>.] - ETA: 0s - loss: 0.0182
6530/6530 [==============================] - 1s 164us/step - loss: 0.0182 - val_loss: 0.0162

# training | RMSE: 0.1216, MAE: 0.0932
worker 1  xfile  [3, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4737581842737165}, 'layer_4_size': 55, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.12157745331698268, 'rmse': 0.12157745331698268, 'mae': 0.09317467185427057, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  89 | activation: tanh    | extras: batchnorm 
layer 2 | size:  73 | activation: relu    | extras: dropout - rate: 15.4% 
layer 3 | size:  53 | activation: relu    | extras: batchnorm 
layer 4 | size:  96 | activation: sigmoid | extras: None 
layer 5 | size:  22 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f460413b0b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 1:03 - loss: 0.5766
 896/6530 [===>..........................] - ETA: 4s - loss: 0.1917  
1728/6530 [======>.......................] - ETA: 2s - loss: 0.1346
2432/6530 [==========>...................] - ETA: 1s - loss: 0.1132
3136/6530 [=============>................] - ETA: 0s - loss: 0.0988
3904/6530 [================>.............] - ETA: 0s - loss: 0.0891
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0815
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0755
6464/6530 [============================>.] - ETA: 0s - loss: 0.0707
6530/6530 [==============================] - 1s 172us/step - loss: 0.0704 - val_loss: 0.0333
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0424
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0399
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0385
# training | RMSE: 0.1195, MAE: 0.0942
worker 0  xfile  [5, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16597532291562125}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2703434644417436}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.1194959484889174, 'rmse': 0.1194959484889174, 'mae': 0.09416644218804907, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  72 | activation: sigmoid | extras: None 
layer 2 | size:  30 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  91 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f46079fc8d0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:12 - loss: 0.3572
2944/6530 [============>.................] - ETA: 0s - loss: 0.0377
 768/6530 [==>...........................] - ETA: 3s - loss: 0.1975  
3904/6530 [================>.............] - ETA: 0s - loss: 0.0375
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1824
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0365
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1770
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0363
2944/6530 [============>.................] - ETA: 0s - loss: 0.1699
6530/6530 [==============================] - 0s 56us/step - loss: 0.0358 - val_loss: 0.0275
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0276
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1665
# training | RMSE: 0.1173, MAE: 0.0905
worker 2  xfile  [4, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3839055503304921}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24531321904768233}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.11734819994865363, 'rmse': 0.11734819994865363, 'mae': 0.09052140570507014, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  95 | activation: relu    | extras: dropout - rate: 30.6% 
layer 2 | size:  79 | activation: relu    | extras: batchnorm 
layer 3 | size:  80 | activation: sigmoid | extras: dropout - rate: 40.2% 
layer 4 | size:  30 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f4604115da0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:47 - loss: 0.5924
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0329
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1625
 544/6530 [=>............................] - ETA: 6s - loss: 0.1780  
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0331
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1599
1056/6530 [===>..........................] - ETA: 3s - loss: 0.1346
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0329
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1569
1568/6530 [======>.......................] - ETA: 2s - loss: 0.1110
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0319
6432/6530 [============================>.] - ETA: 0s - loss: 0.1551
2048/6530 [========>.....................] - ETA: 1s - loss: 0.1002
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0316
6530/6530 [==============================] - 1s 135us/step - loss: 0.1550 - val_loss: 0.2943

2560/6530 [==========>...................] - ETA: 1s - loss: 0.0935Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1488
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0313
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1295
3072/6530 [=============>................] - ETA: 0s - loss: 0.0880
6400/6530 [============================>.] - ETA: 0s - loss: 0.0308
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1289
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0842
6530/6530 [==============================] - 0s 59us/step - loss: 0.0307 - val_loss: 0.0236

2208/6530 [=========>....................] - ETA: 0s - loss: 0.1252
4064/6530 [=================>............] - ETA: 0s - loss: 0.0811
2976/6530 [============>.................] - ETA: 0s - loss: 0.1254
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0780
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1251
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0751
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1244
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0728
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1240
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0708
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1252
6528/6530 [============================>.] - ETA: 0s - loss: 0.1248
6530/6530 [==============================] - 1s 194us/step - loss: 0.0688 - val_loss: 0.0296
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0408
6530/6530 [==============================] - 0s 74us/step - loss: 0.1248 - val_loss: 0.1899
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1275
 544/6530 [=>............................] - ETA: 0s - loss: 0.0455
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1192
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0450
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1174
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0419
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1177
2944/6530 [============>.................] - ETA: 0s - loss: 0.1161
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0423
3712/6530 [================>.............] - ETA: 0s - loss: 0.1168
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0430
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1168
3136/6530 [=============>................] - ETA: 0s - loss: 0.0418
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1162
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0413
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1151
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0408
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0406
6530/6530 [==============================] - 0s 73us/step - loss: 0.1152 - val_loss: 0.1357

5312/6530 [=======================>......] - ETA: 0s - loss: 0.0402
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0399
6400/6530 [============================>.] - ETA: 0s - loss: 0.0396
# training | RMSE: 0.1422, MAE: 0.1123
worker 1  xfile  [6, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1539922143106104}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.14216814216334972, 'rmse': 0.14216814216334972, 'mae': 0.11232051709558691, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  40 | activation: relu    | extras: None 
layer 2 | size:  13 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  49 | activation: relu    | extras: None 
layer 4 | size:  33 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45c02cb208>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 21s - loss: 0.6200
6530/6530 [==============================] - 1s 100us/step - loss: 0.0394 - val_loss: 0.0298
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0280
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1960 
 576/6530 [=>............................] - ETA: 0s - loss: 0.0341
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1708
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0329
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0328
6530/6530 [==============================] - 1s 95us/step - loss: 0.1662 - val_loss: 0.1575
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1610
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0327
2944/6530 [============>.................] - ETA: 0s - loss: 0.1292
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0333
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1286
6530/6530 [==============================] - 0s 20us/step - loss: 0.1273 - val_loss: 0.1758
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1270
3136/6530 [=============>................] - ETA: 0s - loss: 0.0328
2944/6530 [============>.................] - ETA: 0s - loss: 0.1205
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0329
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1172
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0323
6530/6530 [==============================] - 0s 20us/step - loss: 0.1160 - val_loss: 0.1192

4672/6530 [====================>.........] - ETA: 0s - loss: 0.0323
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0319
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0316
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0320
6530/6530 [==============================] - 1s 104us/step - loss: 0.0317 - val_loss: 0.0237

# training | RMSE: 0.1618, MAE: 0.1306
worker 0  xfile  [8, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.1617929120099789, 'rmse': 0.1617929120099789, 'mae': 0.1305709063065955, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  63 | activation: tanh    | extras: batchnorm 
layer 2 | size:  61 | activation: relu    | extras: None 
layer 3 | size:  80 | activation: relu    | extras: None 
layer 4 | size:  93 | activation: tanh    | extras: dropout - rate: 11.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f46079fc588>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 22s - loss: 0.5687
2304/6530 [=========>....................] - ETA: 0s - loss: 0.2675 
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2260
6530/6530 [==============================] - 1s 108us/step - loss: 0.2019 - val_loss: 0.1443
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1698
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1473
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1359
6528/6530 [============================>.] - ETA: 0s - loss: 0.1297
6530/6530 [==============================] - 0s 26us/step - loss: 0.1297 - val_loss: 0.1166
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1214
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1085
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1037
6530/6530 [==============================] - 0s 24us/step - loss: 0.1025 - val_loss: 0.1245

# training | RMSE: 0.1482, MAE: 0.1162
worker 1  xfile  [9, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.14817023982520097, 'rmse': 0.14817023982520097, 'mae': 0.11621674983612026, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  82 | activation: tanh    | extras: batchnorm 
layer 2 | size:  29 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  35 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45c02cb0f0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 33s - loss: 0.3568
1664/6530 [======>.......................] - ETA: 2s - loss: 0.1287 
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0879
# training | RMSE: 0.1473, MAE: 0.1131
worker 2  xfile  [7, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30631658816098606}, 'layer_1_size': 95, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4022096769016439}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20978160463021867}, 'layer_5_size': 11, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.14732265352430463, 'rmse': 0.14732265352430463, 'mae': 0.11308780520913578, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  74 | activation: sigmoid | extras: None 
layer 2 | size:  70 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  20 | activation: sigmoid | extras: dropout - rate: 24.2% 
layer 4 | size:  29 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45c42d7198>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 3:40 - loss: 1.6106
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0720
 256/6530 [>.............................] - ETA: 14s - loss: 1.1138 
 496/6530 [=>............................] - ETA: 7s - loss: 0.7914 
6530/6530 [==============================] - 1s 143us/step - loss: 0.0652 - val_loss: 0.0410
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0391
 720/6530 [==>...........................] - ETA: 5s - loss: 0.6198
 960/6530 [===>..........................] - ETA: 4s - loss: 0.5182
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0404
1200/6530 [====>.........................] - ETA: 3s - loss: 0.4550
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0391
1440/6530 [=====>........................] - ETA: 3s - loss: 0.4144
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0376
6530/6530 [==============================] - 0s 32us/step - loss: 0.0368 - val_loss: 0.0403

1696/6530 [======>.......................] - ETA: 2s - loss: 0.3807Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0320
1952/6530 [=======>......................] - ETA: 2s - loss: 0.3563
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0326
2208/6530 [=========>....................] - ETA: 1s - loss: 0.3375
3712/6530 [================>.............] - ETA: 0s - loss: 0.0321
2464/6530 [==========>...................] - ETA: 1s - loss: 0.3212
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0324
2720/6530 [===========>..................] - ETA: 1s - loss: 0.3083
6530/6530 [==============================] - 0s 32us/step - loss: 0.0318 - val_loss: 0.0342

2992/6530 [============>.................] - ETA: 1s - loss: 0.2981
3248/6530 [=============>................] - ETA: 1s - loss: 0.2896
3504/6530 [===============>..............] - ETA: 1s - loss: 0.2816
3744/6530 [================>.............] - ETA: 0s - loss: 0.2752
# training | RMSE: 0.1450, MAE: 0.1152
worker 0  xfile  [10, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11932348426737889}, 'layer_4_size': 93, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.14501942684135186, 'rmse': 0.14501942684135186, 'mae': 0.11517558942554211, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  30 | activation: relu    | extras: batchnorm 
layer 2 | size:  81 | activation: tanh    | extras: None 
layer 3 | size:   6 | activation: relu    | extras: None 
layer 4 | size:  57 | activation: relu    | extras: dropout - rate: 11.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45fc051470>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 30s - loss: 0.6352
3984/6530 [=================>............] - ETA: 0s - loss: 0.2690
2432/6530 [==========>...................] - ETA: 1s - loss: 0.3370 
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2638
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2190
4496/6530 [===================>..........] - ETA: 0s - loss: 0.2592
4752/6530 [====================>.........] - ETA: 0s - loss: 0.2553
6530/6530 [==============================] - 1s 123us/step - loss: 0.1790 - val_loss: 0.0543
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0480
5024/6530 [======================>.......] - ETA: 0s - loss: 0.2509
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0448
5280/6530 [=======================>......] - ETA: 0s - loss: 0.2470
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0408
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2438
6530/6530 [==============================] - 0s 23us/step - loss: 0.0389 - val_loss: 0.0326
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0271
5776/6530 [=========================>....] - ETA: 0s - loss: 0.2417
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0288
6032/6530 [==========================>...] - ETA: 0s - loss: 0.2393
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0282
6288/6530 [===========================>..] - ETA: 0s - loss: 0.2373
6530/6530 [==============================] - 0s 25us/step - loss: 0.0279 - val_loss: 0.0265

6530/6530 [==============================] - 2s 303us/step - loss: 0.2359 - val_loss: 0.1630
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1558
# training | RMSE: 0.1896, MAE: 0.1468
worker 1  xfile  [11, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32673705199921127}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.1895653037168338, 'rmse': 0.1895653037168338, 'mae': 0.14683810337575473, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  33 | activation: tanh    | extras: None 
layer 2 | size:  34 | activation: sigmoid | extras: dropout - rate: 23.4% 
layer 3 | size:  98 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c57ec2e8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:50 - loss: 0.5988
 240/6530 [>.............................] - ETA: 1s - loss: 0.1805
 416/6530 [>.............................] - ETA: 6s - loss: 0.1556  
 480/6530 [=>............................] - ETA: 1s - loss: 0.1766
 832/6530 [==>...........................] - ETA: 3s - loss: 0.1176
 720/6530 [==>...........................] - ETA: 1s - loss: 0.1754
1248/6530 [====>.........................] - ETA: 2s - loss: 0.0983
 976/6530 [===>..........................] - ETA: 1s - loss: 0.1735
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0874
1184/6530 [====>.........................] - ETA: 1s - loss: 0.1725
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0802
1440/6530 [=====>........................] - ETA: 1s - loss: 0.1720
2560/6530 [==========>...................] - ETA: 1s - loss: 0.0748
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1724
2976/6530 [============>.................] - ETA: 0s - loss: 0.0711
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1712
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0678
# training | RMSE: 0.1530, MAE: 0.1205
worker 0  xfile  [13, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11609532697927981}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1530180211793349, 'rmse': 0.1530180211793349, 'mae': 0.12045332194759586, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  68 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45c0739400>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 52s - loss: 0.6005
2224/6530 [=========>....................] - ETA: 0s - loss: 0.1715
3856/6530 [================>.............] - ETA: 0s - loss: 0.0652
1376/6530 [=====>........................] - ETA: 1s - loss: 0.1046 
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1734
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0636
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0733
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1723
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0616
4128/6530 [=================>............] - ETA: 0s - loss: 0.0627
2976/6530 [============>.................] - ETA: 0s - loss: 0.1707
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0599
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0574
3216/6530 [=============>................] - ETA: 0s - loss: 0.1710
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0591
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1710
6530/6530 [==============================] - 1s 85us/step - loss: 0.0545 - val_loss: 0.0423
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0425
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0580
3728/6530 [================>.............] - ETA: 0s - loss: 0.1710
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0390
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0572
3984/6530 [=================>............] - ETA: 0s - loss: 0.1709
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0398
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1707
3968/6530 [=================>............] - ETA: 0s - loss: 0.0408
6530/6530 [==============================] - 1s 200us/step - loss: 0.0566 - val_loss: 0.0418
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0315
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1707
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0413
 432/6530 [>.............................] - ETA: 0s - loss: 0.0424
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1707
6530/6530 [==============================] - 0s 41us/step - loss: 0.0407 - val_loss: 0.0422
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0342
 848/6530 [==>...........................] - ETA: 0s - loss: 0.0435
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1702
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0403
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0447
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1702
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0405
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0451
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1701
4096/6530 [=================>............] - ETA: 0s - loss: 0.0410
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0452
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1693
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0399
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0445
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1693
6530/6530 [==============================] - 0s 40us/step - loss: 0.0402 - val_loss: 0.0427

2880/6530 [============>.................] - ETA: 0s - loss: 0.0447
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1693
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0444
6530/6530 [==============================] - 1s 209us/step - loss: 0.1685 - val_loss: 0.2094
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1347
3712/6530 [================>.............] - ETA: 0s - loss: 0.0442
 272/6530 [>.............................] - ETA: 1s - loss: 0.1462
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0443
 544/6530 [=>............................] - ETA: 1s - loss: 0.1494
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0440
 800/6530 [==>...........................] - ETA: 1s - loss: 0.1581
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0441
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1582
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0441
1344/6530 [=====>........................] - ETA: 1s - loss: 0.1576
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0446
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1576
6352/6530 [============================>.] - ETA: 0s - loss: 0.0446
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1566
6530/6530 [==============================] - 1s 126us/step - loss: 0.0444 - val_loss: 0.0438
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0701
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1566
 464/6530 [=>............................] - ETA: 0s - loss: 0.0474
# training | RMSE: 0.2022, MAE: 0.1605
worker 0  xfile  [15, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4190781584026906}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2022104906215118, 'rmse': 0.2022104906215118, 'mae': 0.16046384618593104, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  65 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45c07393c8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 25s - loss: 1.1049
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1566
 880/6530 [===>..........................] - ETA: 0s - loss: 0.0460
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1453 
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1559
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0468
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0988
2880/6530 [============>.................] - ETA: 0s - loss: 0.1562
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0463
6530/6530 [==============================] - 0s 64us/step - loss: 0.0910 - val_loss: 0.0481
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0525
3136/6530 [=============>................] - ETA: 0s - loss: 0.1552
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0442
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0420
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1550
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0443
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0413
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1563
2848/6530 [============>.................] - ETA: 0s - loss: 0.0443
6530/6530 [==============================] - 0s 22us/step - loss: 0.0413 - val_loss: 0.0419
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0427
3904/6530 [================>.............] - ETA: 0s - loss: 0.1559
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0436
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0373
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1558
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0438
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0369
6530/6530 [==============================] - 0s 20us/step - loss: 0.0373 - val_loss: 0.0392

4096/6530 [=================>............] - ETA: 0s - loss: 0.0438
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1557
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0434
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1555
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0436
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1561
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0438
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1559
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0440
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1558
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0437
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1556
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1557
6530/6530 [==============================] - 1s 126us/step - loss: 0.0435 - val_loss: 0.0414

6224/6530 [===========================>..] - ETA: 0s - loss: 0.1562
6432/6530 [============================>.] - ETA: 0s - loss: 0.1561
6530/6530 [==============================] - 1s 209us/step - loss: 0.1559 - val_loss: 0.1487

# training | RMSE: 0.1901, MAE: 0.1511
worker 0  xfile  [16, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1900576699195534, 'rmse': 0.1900576699195534, 'mae': 0.15111478544081236, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  46 | activation: relu    | extras: dropout - rate: 19.8% 
layer 2 | size:  99 | activation: tanh    | extras: dropout - rate: 28.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c8b44940>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 16s - loss: 0.6022
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1851 
6530/6530 [==============================] - 0s 70us/step - loss: 0.1365 - val_loss: 0.0548
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0542
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0459
6530/6530 [==============================] - 0s 13us/step - loss: 0.0452 - val_loss: 0.0406
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0510
3840/6530 [================>.............] - ETA: 0s - loss: 0.0432
6530/6530 [==============================] - 0s 15us/step - loss: 0.0422 - val_loss: 0.0395

# training | RMSE: 0.1890, MAE: 0.1448
worker 2  xfile  [12, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.24184757063105722}, 'layer_3_size': 20, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 29, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.18900916200288045, 'rmse': 0.18900916200288045, 'mae': 0.14484790863979216, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  26 | activation: tanh    | extras: dropout - rate: 26.8% 
layer 2 | size:  11 | activation: tanh    | extras: dropout - rate: 47.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c96a7390>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 13s - loss: 0.8188
3840/6530 [================>.............] - ETA: 0s - loss: 0.3689 
6530/6530 [==============================] - 0s 63us/step - loss: 0.3077 - val_loss: 0.1631
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2315
4096/6530 [=================>............] - ETA: 0s - loss: 0.2029
6530/6530 [==============================] - 0s 14us/step - loss: 0.1965 - val_loss: 0.1573
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1864
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1808
6530/6530 [==============================] - 0s 12us/step - loss: 0.1781 - val_loss: 0.1481

# training | RMSE: 0.1959, MAE: 0.1570
worker 0  xfile  [17, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19774261829234205}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2797140258496348}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45844819637653644}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.1958957325486333, 'rmse': 0.1958957325486333, 'mae': 0.156957406667498, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  20 | activation: relu    | extras: dropout - rate: 20.6% 
layer 2 | size:  10 | activation: tanh    | extras: dropout - rate: 18.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c8a61cf8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:21 - loss: 0.6003
 544/6530 [=>............................] - ETA: 4s - loss: 0.3960  
1072/6530 [===>..........................] - ETA: 2s - loss: 0.3515
1632/6530 [======>.......................] - ETA: 1s - loss: 0.3195
2192/6530 [=========>....................] - ETA: 1s - loss: 0.2975
2720/6530 [===========>..................] - ETA: 0s - loss: 0.2822
3280/6530 [==============>...............] - ETA: 0s - loss: 0.2681
3824/6530 [================>.............] - ETA: 0s - loss: 0.2598
4368/6530 [===================>..........] - ETA: 0s - loss: 0.2511
# training | RMSE: 0.2015, MAE: 0.1624
worker 1  xfile  [14, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23374835004413852}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20150053140153754, 'rmse': 0.20150053140153754, 'mae': 0.16236479683829516, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  89 | activation: sigmoid | extras: dropout - rate: 19.5% 
layer 2 | size:  33 | activation: relu    | extras: batchnorm 
layer 3 | size:  33 | activation: tanh    | extras: batchnorm 
layer 4 | size:  87 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c534b198>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 1:16 - loss: 0.3870
4896/6530 [=====================>........] - ETA: 0s - loss: 0.2453
 960/6530 [===>..........................] - ETA: 4s - loss: 0.1559  
5392/6530 [=======================>......] - ETA: 0s - loss: 0.2400
# training | RMSE: 0.1882, MAE: 0.1476
worker 2  xfile  [19, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2684385694928465}, 'layer_1_size': 26, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47712176301521403}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4122081315162861}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.18821039060165115, 'rmse': 0.18821039060165115, 'mae': 0.14764712226771384, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  39 | activation: sigmoid | extras: dropout - rate: 17.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ac7326a0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 58s - loss: 0.5175
1920/6530 [=======>......................] - ETA: 2s - loss: 0.1173
5904/6530 [==========================>...] - ETA: 0s - loss: 0.2350
1248/6530 [====>.........................] - ETA: 1s - loss: 0.2771 
2816/6530 [===========>..................] - ETA: 1s - loss: 0.0997
6416/6530 [============================>.] - ETA: 0s - loss: 0.2320
2528/6530 [==========>...................] - ETA: 0s - loss: 0.2424
3776/6530 [================>.............] - ETA: 0s - loss: 0.0894
3840/6530 [================>.............] - ETA: 0s - loss: 0.2260
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0828
6530/6530 [==============================] - 1s 162us/step - loss: 0.2309 - val_loss: 0.1760
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1701
5088/6530 [======================>.......] - ETA: 0s - loss: 0.2181
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0781
 528/6530 [=>............................] - ETA: 0s - loss: 0.1845
6336/6530 [============================>.] - ETA: 0s - loss: 0.2110
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1879
6530/6530 [==============================] - 1s 92us/step - loss: 0.2100 - val_loss: 0.1760
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1794
1520/6530 [=====>........................] - ETA: 0s - loss: 0.1802
6530/6530 [==============================] - 1s 186us/step - loss: 0.0744 - val_loss: 0.0441
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0595
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1781
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1791
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0532
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1725
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1789
3808/6530 [================>.............] - ETA: 0s - loss: 0.1715
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0496
3008/6530 [============>.................] - ETA: 0s - loss: 0.1769
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1705
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0501
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1773
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1691
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0497
4064/6530 [=================>............] - ETA: 0s - loss: 0.1768
6530/6530 [==============================] - 0s 43us/step - loss: 0.1684 - val_loss: 0.1604
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1885
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0497
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1759
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1616
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0494
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1757
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1619
6464/6530 [============================>.] - ETA: 0s - loss: 0.0493
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1742
6530/6530 [==============================] - 0s 59us/step - loss: 0.0492 - val_loss: 0.0415

3648/6530 [===============>..............] - ETA: 0s - loss: 0.1612Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0621
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1741
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1619
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0497
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1610
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0472
6530/6530 [==============================] - 1s 104us/step - loss: 0.1733 - val_loss: 0.1681
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1490
6530/6530 [==============================] - 0s 45us/step - loss: 0.1607 - val_loss: 0.1607

2880/6530 [============>.................] - ETA: 0s - loss: 0.0468
 544/6530 [=>............................] - ETA: 0s - loss: 0.1707
3840/6530 [================>.............] - ETA: 0s - loss: 0.0466
1072/6530 [===>..........................] - ETA: 0s - loss: 0.1733
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0465
1616/6530 [======>.......................] - ETA: 0s - loss: 0.1659
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0464
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1654
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1662
6530/6530 [==============================] - 0s 56us/step - loss: 0.0464 - val_loss: 0.0433

3248/6530 [=============>................] - ETA: 0s - loss: 0.1645
3808/6530 [================>.............] - ETA: 0s - loss: 0.1653
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1643
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1646
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1643
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1642
6530/6530 [==============================] - 1s 97us/step - loss: 0.1637 - val_loss: 0.1539

# training | RMSE: 0.2060, MAE: 0.1618
worker 1  xfile  [18, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19453446764642846}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38296540910752286}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2059804581623919, 'rmse': 0.2059804581623919, 'mae': 0.1617590603169565, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  56 | activation: relu    | extras: dropout - rate: 24.6% 
layer 2 | size:  78 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c4f637f0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 21s - loss: 0.6885
3840/6530 [================>.............] - ETA: 0s - loss: 0.4725 
6530/6530 [==============================] - 1s 90us/step - loss: 0.3658 - val_loss: 0.1798
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1676
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1757
# training | RMSE: 0.2028, MAE: 0.1601
worker 2  xfile  [21, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17031695865320207}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12100320754320126}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2027627360703155, 'rmse': 0.2027627360703155, 'mae': 0.1600775883279155, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  84 | activation: tanh    | extras: batchnorm 
layer 2 | size:  48 | activation: tanh    | extras: None 
layer 3 | size:  97 | activation: sigmoid | extras: None 
layer 4 | size:  82 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ac5007f0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 33s - loss: 0.6405
6530/6530 [==============================] - 0s 17us/step - loss: 0.1736 - val_loss: 0.1654
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1817
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1503 
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1713
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1086
6530/6530 [==============================] - 0s 17us/step - loss: 0.1692 - val_loss: 0.1624

6272/6530 [===========================>..] - ETA: 0s - loss: 0.0909
6530/6530 [==============================] - 1s 137us/step - loss: 0.0890 - val_loss: 0.0468
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0537
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0447
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0440
6400/6530 [============================>.] - ETA: 0s - loss: 0.0435
6530/6530 [==============================] - 0s 26us/step - loss: 0.0433 - val_loss: 0.0423
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0429
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0445
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0441
# training | RMSE: 0.1908, MAE: 0.1526
worker 0  xfile  [20, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20591799748748119}, 'layer_1_size': 20, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18154684042572067}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 92, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3825331912408373}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18016369794913892}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.19084640693645868, 'rmse': 0.19084640693645868, 'mae': 0.15262702952323778, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  30 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  34 | activation: tanh    | extras: None 
layer 3 | size:  55 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c877a7f0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 3:42 - loss: 0.7766
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0436
 320/6530 [>.............................] - ETA: 11s - loss: 0.4538 
6530/6530 [==============================] - 0s 27us/step - loss: 0.0433 - val_loss: 0.0417

 624/6530 [=>............................] - ETA: 6s - loss: 0.3480 
 896/6530 [===>..........................] - ETA: 4s - loss: 0.3051
1184/6530 [====>.........................] - ETA: 3s - loss: 0.2804
1504/6530 [=====>........................] - ETA: 2s - loss: 0.2541
1824/6530 [=======>......................] - ETA: 2s - loss: 0.2418
2176/6530 [========>.....................] - ETA: 1s - loss: 0.2299
2512/6530 [==========>...................] - ETA: 1s - loss: 0.2212
2816/6530 [===========>..................] - ETA: 1s - loss: 0.2138
3168/6530 [=============>................] - ETA: 1s - loss: 0.2076
3488/6530 [===============>..............] - ETA: 0s - loss: 0.2047
3840/6530 [================>.............] - ETA: 0s - loss: 0.2012
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1994
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1972
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1959
# training | RMSE: 0.2013, MAE: 0.1617
worker 2  xfile  [22, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20127018526773174, 'rmse': 0.20127018526773174, 'mae': 0.16171767044821858, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  45 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c969e860>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 38s - loss: 0.8575
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1939
# training | RMSE: 0.2009, MAE: 0.1592
worker 1  xfile  [23, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24594424623809785}, 'layer_1_size': 56, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41103794745290845}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11328582488940891}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20089600583365988, 'rmse': 0.20089600583365988, 'mae': 0.15919749736715785, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  30 | activation: tanh    | extras: dropout - rate: 40.8% 
layer 2 | size:  48 | activation: relu    | extras: None 
layer 3 | size:  79 | activation: relu    | extras: None 
layer 4 | size:  88 | activation: sigmoid | extras: dropout - rate: 16.5% 
layer 5 | size:  77 | activation: tanh    | extras: dropout - rate: 42.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42c4803eb8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 28s - loss: 0.4608
1920/6530 [=======>......................] - ETA: 1s - loss: 0.4841 
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1916
2560/6530 [==========>...................] - ETA: 0s - loss: 0.3597 
3968/6530 [=================>............] - ETA: 0s - loss: 0.3103
5248/6530 [=======================>......] - ETA: 0s - loss: 0.3132
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1906
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2282
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1892
6530/6530 [==============================] - 1s 92us/step - loss: 0.2122 - val_loss: 0.0512
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0514
6530/6530 [==============================] - 1s 119us/step - loss: 0.2980 - val_loss: 0.1713

6416/6530 [============================>.] - ETA: 0s - loss: 0.1880Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2374
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0463
2944/6530 [============>.................] - ETA: 0s - loss: 0.2275
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0455
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2184
6530/6530 [==============================] - 2s 261us/step - loss: 0.1880 - val_loss: 0.1602

6530/6530 [==============================] - 0s 19us/step - loss: 0.2164 - val_loss: 0.1872
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1924Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2070
6530/6530 [==============================] - 0s 24us/step - loss: 0.0454 - val_loss: 0.0475
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0505
2944/6530 [============>.................] - ETA: 0s - loss: 0.2045
 352/6530 [>.............................] - ETA: 0s - loss: 0.1695
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0430
 688/6530 [==>...........................] - ETA: 0s - loss: 0.1683
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2009
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0428
6530/6530 [==============================] - 0s 19us/step - loss: 0.2004 - val_loss: 0.1606

1008/6530 [===>..........................] - ETA: 0s - loss: 0.1638
6530/6530 [==============================] - 0s 24us/step - loss: 0.0425 - val_loss: 0.0457

1360/6530 [=====>........................] - ETA: 0s - loss: 0.1631
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1650
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1657
# training | RMSE: 0.2062, MAE: 0.1617
worker 1  xfile  [25, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4080403007617508}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16454352642837314}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4288788424853015}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20618150087802, 'rmse': 0.20618150087802, 'mae': 0.16169536323780623, 'early_stop': False}
vggnet done  1

# training | RMSE: 0.2131, MAE: 0.1688
worker 2  xfile  [26, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32553354455869177}, 'layer_4_size': 46, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.21314322016830453, 'rmse': 0.21314322016830453, 'mae': 0.1688439409485328, 'early_stop': False}
vggnet done  2

2352/6530 [=========>....................] - ETA: 0s - loss: 0.1662
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1674
3072/6530 [=============>................] - ETA: 0s - loss: 0.1666
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1659
3856/6530 [================>.............] - ETA: 0s - loss: 0.1660
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1664
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1657
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1651
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1651
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1654
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1656
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1656
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1652
6368/6530 [============================>.] - ETA: 0s - loss: 0.1661
6530/6530 [==============================] - 1s 171us/step - loss: 0.1663 - val_loss: 0.1680
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1703
 224/6530 [>.............................] - ETA: 1s - loss: 0.1630
 432/6530 [>.............................] - ETA: 1s - loss: 0.1673
 672/6530 [==>...........................] - ETA: 1s - loss: 0.1661
 896/6530 [===>..........................] - ETA: 1s - loss: 0.1643
1136/6530 [====>.........................] - ETA: 1s - loss: 0.1615
1408/6530 [=====>........................] - ETA: 1s - loss: 0.1646
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1640
1968/6530 [========>.....................] - ETA: 0s - loss: 0.1658
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1658
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1665
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1657
2992/6530 [============>.................] - ETA: 0s - loss: 0.1655
3200/6530 [=============>................] - ETA: 0s - loss: 0.1662
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1657
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1651
3920/6530 [=================>............] - ETA: 0s - loss: 0.1654
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1656
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1661
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1654
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1656
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1653
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1649
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1653
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1651
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1651
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1654
6384/6530 [============================>.] - ETA: 0s - loss: 0.1656
6530/6530 [==============================] - 1s 226us/step - loss: 0.1658 - val_loss: 0.1619

# training | RMSE: 0.2015, MAE: 0.1604
worker 0  xfile  [24, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3662141381855081}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.20147707760804692, 'rmse': 0.20147707760804692, 'mae': 0.16038139103813925, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#1 epoch=3.0 loss={'loss': 0.09577756827421148, 'rmse': 0.09577756827421148, 'mae': 0.07363248988402592, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32305615123930476}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#2 epoch=3.0 loss={'loss': 0.12037732808109387, 'rmse': 0.12037732808109387, 'mae': 0.0944183056131048, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41090246001998465}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#0 epoch=3.0 loss={'loss': 0.093571068845624, 'rmse': 0.093571068845624, 'mae': 0.07155058140660678, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#3 epoch=3.0 loss={'loss': 0.12157745331698268, 'rmse': 0.12157745331698268, 'mae': 0.09317467185427057, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4737581842737165}, 'layer_4_size': 55, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#4 epoch=3.0 loss={'loss': 0.11734819994865363, 'rmse': 0.11734819994865363, 'mae': 0.09052140570507014, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3839055503304921}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24531321904768233}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#5 epoch=3.0 loss={'loss': 0.1194959484889174, 'rmse': 0.1194959484889174, 'mae': 0.09416644218804907, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16597532291562125}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2703434644417436}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#6 epoch=3.0 loss={'loss': 0.14216814216334972, 'rmse': 0.14216814216334972, 'mae': 0.11232051709558691, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1539922143106104}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#8 epoch=3.0 loss={'loss': 0.1617929120099789, 'rmse': 0.1617929120099789, 'mae': 0.1305709063065955, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 78, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#9 epoch=3.0 loss={'loss': 0.14817023982520097, 'rmse': 0.14817023982520097, 'mae': 0.11621674983612026, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#7 epoch=3.0 loss={'loss': 0.14732265352430463, 'rmse': 0.14732265352430463, 'mae': 0.11308780520913578, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30631658816098606}, 'layer_1_size': 95, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4022096769016439}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20978160463021867}, 'layer_5_size': 11, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#10 epoch=3.0 loss={'loss': 0.14501942684135186, 'rmse': 0.14501942684135186, 'mae': 0.11517558942554211, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11932348426737889}, 'layer_4_size': 93, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#11 epoch=3.0 loss={'loss': 0.1895653037168338, 'rmse': 0.1895653037168338, 'mae': 0.14683810337575473, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32673705199921127}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#13 epoch=3.0 loss={'loss': 0.1530180211793349, 'rmse': 0.1530180211793349, 'mae': 0.12045332194759586, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11609532697927981}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#15 epoch=3.0 loss={'loss': 0.2022104906215118, 'rmse': 0.2022104906215118, 'mae': 0.16046384618593104, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4190781584026906}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#16 epoch=3.0 loss={'loss': 0.1900576699195534, 'rmse': 0.1900576699195534, 'mae': 0.15111478544081236, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#14 epoch=3.0 loss={'loss': 0.20150053140153754, 'rmse': 0.20150053140153754, 'mae': 0.16236479683829516, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23374835004413852}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#12 epoch=3.0 loss={'loss': 0.18900916200288045, 'rmse': 0.18900916200288045, 'mae': 0.14484790863979216, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.24184757063105722}, 'layer_3_size': 20, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 29, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#17 epoch=3.0 loss={'loss': 0.1958957325486333, 'rmse': 0.1958957325486333, 'mae': 0.156957406667498, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19774261829234205}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2797140258496348}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45844819637653644}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#19 epoch=3.0 loss={'loss': 0.18821039060165115, 'rmse': 0.18821039060165115, 'mae': 0.14764712226771384, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2684385694928465}, 'layer_1_size': 26, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47712176301521403}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4122081315162861}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#21 epoch=3.0 loss={'loss': 0.2027627360703155, 'rmse': 0.2027627360703155, 'mae': 0.1600775883279155, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17031695865320207}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12100320754320126}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#18 epoch=3.0 loss={'loss': 0.2059804581623919, 'rmse': 0.2059804581623919, 'mae': 0.1617590603169565, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.19453446764642846}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38296540910752286}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#20 epoch=3.0 loss={'loss': 0.19084640693645868, 'rmse': 0.19084640693645868, 'mae': 0.15262702952323778, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20591799748748119}, 'layer_1_size': 20, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18154684042572067}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 92, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3825331912408373}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18016369794913892}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#23 epoch=3.0 loss={'loss': 0.20089600583365988, 'rmse': 0.20089600583365988, 'mae': 0.15919749736715785, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24594424623809785}, 'layer_1_size': 56, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41103794745290845}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11328582488940891}, 'layer_4_size': 95, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#22 epoch=3.0 loss={'loss': 0.20127018526773174, 'rmse': 0.20127018526773174, 'mae': 0.16171767044821858, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#25 epoch=3.0 loss={'loss': 0.20618150087802, 'rmse': 0.20618150087802, 'mae': 0.16169536323780623, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4080403007617508}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16454352642837314}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4288788424853015}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#26 epoch=3.0 loss={'loss': 0.21314322016830453, 'rmse': 0.21314322016830453, 'mae': 0.1688439409485328, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32553354455869177}, 'layer_4_size': 46, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#24 epoch=3.0 loss={'loss': 0.20147707760804692, 'rmse': 0.20147707760804692, 'mae': 0.16038139103813925, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3662141381855081}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
get a list [results] of length 108
get a list [loss] of length 27
get a list [val_loss] of length 27
length of indices is (0, 1, 4, 5, 2, 3, 6, 10, 7, 9, 13, 8, 19, 12, 11, 16, 20, 17, 23, 22, 24, 14, 15, 21, 18, 25, 26)
length of indices is 27
length of T is 27
newly formed T structure is:[[0, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [1, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32305615123930476}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [2, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3839055503304921}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24531321904768233}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [3, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16597532291562125}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2703434644417436}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [4, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41090246001998465}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [5, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4737581842737165}, 'layer_4_size': 55, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [6, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1539922143106104}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [7, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11932348426737889}, 'layer_4_size': 93, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [8, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30631658816098606}, 'layer_1_size': 95, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4022096769016439}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20978160463021867}, 'layer_5_size': 11, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]] 

*** 9.0 configurations x 9.0 iterations each

25 | Thu Sep 27 23:06:01 2018 | lowest loss so far: 0.0936 (run 0)

{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  86 | activation: relu    | extras: batchnorm 
layer 2 | size:  43 | activation: relu    | extras: None 
layer 3 | size:  49 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 20s - loss: 0.6033
4096/6530 [=================>............] - ETA: 0s - loss: 0.1123 
6530/6530 [==============================] - 1s 145us/step - loss: 0.0806 - val_loss: 0.0177
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0158
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0142
6530/6530 [==============================] - 0s 12us/step - loss: 0.0145 - val_loss: 0.0131
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0116
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0111
6530/6530 [==============================] - 0s 11us/step - loss: 0.0116 - val_loss: 0.0115
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0103{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  93 | activation: relu    | extras: None 
layer 2 | size:  84 | activation: relu    | extras: batchnorm 
layer 3 | size:  83 | activation: sigmoid | extras: dropout - rate: 18.6% 
layer 4 | size:  45 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 6:34 - loss: 0.6924
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0103
6530/6530 [==============================] - 0s 10us/step - loss: 0.0102 - val_loss: 0.0106
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0096
 336/6530 [>.............................] - ETA: 18s - loss: 0.2130 
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0094
6530/6530 [==============================] - 0s 9us/step - loss: 0.0094 - val_loss: 0.0100
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0091
 656/6530 [==>...........................] - ETA: 9s - loss: 0.1967 
6400/6530 [============================>.] - ETA: 0s - loss: 0.0088
6530/6530 [==============================] - 0s 9us/step - loss: 0.0088 - val_loss: 0.0096
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0087
 944/6530 [===>..........................] - ETA: 6s - loss: 0.1841
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 0s 10us/step - loss: 0.0083 - val_loss: 0.0092

1296/6530 [====>.........................] - ETA: 4s - loss: 0.1762Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0084
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0080
1664/6530 [======>.......................] - ETA: 3s - loss: 0.1669
6530/6530 [==============================] - 0s 9us/step - loss: 0.0079 - val_loss: 0.0089
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0081
2016/6530 [========>.....................] - ETA: 2s - loss: 0.1626
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0076
6530/6530 [==============================] - 0s 10us/step - loss: 0.0076 - val_loss: 0.0087

2368/6530 [=========>....................] - ETA: 2s - loss: 0.1591
2688/6530 [===========>..................] - ETA: 1s - loss: 0.1570
3056/6530 [=============>................] - ETA: 1s - loss: 0.1523
3408/6530 [==============>...............] - ETA: 1s - loss: 0.1501{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  67 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  34 | activation: tanh    | extras: dropout - rate: 38.4% 
layer 3 | size:  72 | activation: relu    | extras: None 
layer 4 | size:  37 | activation: sigmoid | extras: batchnorm 
layer 5 | size:  36 | activation: sigmoid | extras: dropout - rate: 24.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 3:51 - loss: 1.4358
3728/6530 [================>.............] - ETA: 1s - loss: 0.1483
 384/6530 [>.............................] - ETA: 19s - loss: 1.1267 
4096/6530 [=================>............] - ETA: 0s - loss: 0.1468
 832/6530 [==>...........................] - ETA: 8s - loss: 0.8378 
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1450
1248/6530 [====>.........................] - ETA: 5s - loss: 0.6466
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1432
1664/6530 [======>.......................] - ETA: 3s - loss: 0.5161
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1415
2112/6530 [========>.....................] - ETA: 2s - loss: 0.4216
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1398
2560/6530 [==========>...................] - ETA: 2s - loss: 0.3572
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1382
3008/6530 [============>.................] - ETA: 1s - loss: 0.3110
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1368
3456/6530 [==============>...............] - ETA: 1s - loss: 0.2764
3936/6530 [=================>............] - ETA: 1s - loss: 0.2482
6530/6530 [==============================] - 2s 308us/step - loss: 0.1356 - val_loss: 0.1144
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0940
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2282
 336/6530 [>.............................] - ETA: 0s - loss: 0.1039
4768/6530 [====================>.........] - ETA: 0s - loss: 0.2120
 688/6530 [==>...........................] - ETA: 0s - loss: 0.1109
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1981
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1109
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1855
1392/6530 [=====>........................] - ETA: 0s - loss: 0.1100
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1747
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1091
# training | RMSE: 0.0803, MAE: 0.0622
worker 1  xfile  [1, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32305615123930476}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.0803414916061439, 'rmse': 0.0803414916061439, 'mae': 0.06215190434143412, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  41 | activation: tanh    | extras: None 
layer 2 | size:  39 | activation: relu    | extras: batchnorm 
layer 3 | size:  30 | activation: relu    | extras: None 
layer 4 | size:  83 | activation: sigmoid | extras: dropout - rate: 16.6% 
layer 5 | size:  40 | activation: tanh    | extras: dropout - rate: 27.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f4604139438>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 2:31 - loss: 0.6707
6528/6530 [============================>.] - ETA: 0s - loss: 0.1653
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1083
 352/6530 [>.............................] - ETA: 7s - loss: 0.0912  
6530/6530 [==============================] - 2s 305us/step - loss: 0.1652 - val_loss: 0.0295

2448/6530 [==========>...................] - ETA: 0s - loss: 0.1084Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0469
 688/6530 [==>...........................] - ETA: 4s - loss: 0.0696
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1078
 384/6530 [>.............................] - ETA: 0s - loss: 0.0302
1008/6530 [===>..........................] - ETA: 2s - loss: 0.0618
3120/6530 [=============>................] - ETA: 0s - loss: 0.1052
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0319
1328/6530 [=====>........................] - ETA: 2s - loss: 0.0573
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1057
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0312
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0536
3824/6530 [================>.............] - ETA: 0s - loss: 0.1056
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0306
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0507
4128/6530 [=================>............] - ETA: 0s - loss: 0.1062
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0311
2288/6530 [=========>....................] - ETA: 1s - loss: 0.0487
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1066
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0321
2560/6530 [==========>...................] - ETA: 1s - loss: 0.0472
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1058
2976/6530 [============>.................] - ETA: 0s - loss: 0.0318
2864/6530 [============>.................] - ETA: 1s - loss: 0.0458
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1057
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0315
3184/6530 [=============>................] - ETA: 0s - loss: 0.0443
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1054
3872/6530 [================>.............] - ETA: 0s - loss: 0.0315
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0430
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1048
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0310
3824/6530 [================>.............] - ETA: 0s - loss: 0.0418
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1047
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0310
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0407
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0309
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0395
6530/6530 [==============================] - 1s 154us/step - loss: 0.1044 - val_loss: 0.0890
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0990
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0306
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0388
 368/6530 [>.............................] - ETA: 0s - loss: 0.0945
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0305
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0381
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1036
6432/6530 [============================>.] - ETA: 0s - loss: 0.0304
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0373
1040/6530 [===>..........................] - ETA: 0s - loss: 0.1022
6530/6530 [==============================] - 1s 124us/step - loss: 0.0304 - val_loss: 0.0220
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0267
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0366
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1029
 480/6530 [=>............................] - ETA: 0s - loss: 0.0284
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0359
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1004
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0279
6368/6530 [============================>.] - ETA: 0s - loss: 0.0354
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0990
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0269
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0990
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0269
6530/6530 [==============================] - 2s 231us/step - loss: 0.0351 - val_loss: 0.0259
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0194
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0987
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0272
 336/6530 [>.............................] - ETA: 1s - loss: 0.0248
3104/6530 [=============>................] - ETA: 0s - loss: 0.0963
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0274
 624/6530 [=>............................] - ETA: 1s - loss: 0.0228
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0966
3104/6530 [=============>................] - ETA: 0s - loss: 0.0267
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0226
3744/6530 [================>.............] - ETA: 0s - loss: 0.0963
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0268
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0242
4096/6530 [=================>............] - ETA: 0s - loss: 0.0969
3968/6530 [=================>............] - ETA: 0s - loss: 0.0267
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0243
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0972
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0266
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0238
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0971
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0264
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0237
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0970
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0265
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0233
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0970
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0264
2848/6530 [============>.................] - ETA: 0s - loss: 0.0232
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0964
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0264
3168/6530 [=============>................] - ETA: 0s - loss: 0.0230
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0960
6530/6530 [==============================] - 1s 123us/step - loss: 0.0264 - val_loss: 0.0175
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0342
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0230
6530/6530 [==============================] - 1s 154us/step - loss: 0.0958 - val_loss: 0.0786
Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0656
 480/6530 [=>............................] - ETA: 0s - loss: 0.0242
3792/6530 [================>.............] - ETA: 0s - loss: 0.0227
 368/6530 [>.............................] - ETA: 0s - loss: 0.0926
4112/6530 [=================>............] - ETA: 0s - loss: 0.0226
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0243
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0982
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0224
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0235
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0964
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0225
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0237
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0964
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0225
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0236
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0946
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0223
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0239
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0931
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0221
3136/6530 [=============>................] - ETA: 0s - loss: 0.0239
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0928
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0219
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0241
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0927
6352/6530 [============================>.] - ETA: 0s - loss: 0.0219
3968/6530 [=================>............] - ETA: 0s - loss: 0.0240
3056/6530 [=============>................] - ETA: 0s - loss: 0.0907
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0239
6530/6530 [==============================] - 1s 167us/step - loss: 0.0217 - val_loss: 0.0183
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0300
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0903
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0240
 336/6530 [>.............................] - ETA: 0s - loss: 0.0173
3728/6530 [================>.............] - ETA: 0s - loss: 0.0912
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0241
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0206
4048/6530 [=================>............] - ETA: 0s - loss: 0.0919
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0238
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0201
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0921
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0239
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0193
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0926
6528/6530 [============================>.] - ETA: 0s - loss: 0.0241
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0195
6530/6530 [==============================] - 1s 124us/step - loss: 0.0241 - val_loss: 0.0156
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0220
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0923
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0196
 416/6530 [>.............................] - ETA: 0s - loss: 0.0222
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0921
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0194
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0224
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0916
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0195
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0910
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0226
2896/6530 [============>.................] - ETA: 0s - loss: 0.0192
6416/6530 [============================>.] - ETA: 0s - loss: 0.0908
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0225
3216/6530 [=============>................] - ETA: 0s - loss: 0.0190
6530/6530 [==============================] - 1s 157us/step - loss: 0.0909 - val_loss: 0.0775
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0672
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0227
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0188
 368/6530 [>.............................] - ETA: 0s - loss: 0.0868
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0229
3840/6530 [================>.............] - ETA: 0s - loss: 0.0188
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0927
3040/6530 [============>.................] - ETA: 0s - loss: 0.0226
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0191
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0932
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0229
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0189
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0913
3904/6530 [================>.............] - ETA: 0s - loss: 0.0229
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0189
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0898
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0226
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0186
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0891
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0226
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0187
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0894
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0227
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0187
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0892
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0226
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0187
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0226
3152/6530 [=============>................] - ETA: 0s - loss: 0.0876
6368/6530 [============================>.] - ETA: 0s - loss: 0.0186
6432/6530 [============================>.] - ETA: 0s - loss: 0.0227
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0885
6530/6530 [==============================] - 1s 124us/step - loss: 0.0227 - val_loss: 0.0157
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0217
6530/6530 [==============================] - 1s 168us/step - loss: 0.0188 - val_loss: 0.0217
Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0241
3824/6530 [================>.............] - ETA: 0s - loss: 0.0882
 480/6530 [=>............................] - ETA: 0s - loss: 0.0211
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0889
 336/6530 [>.............................] - ETA: 1s - loss: 0.0164
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0208
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0898
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0170
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0207
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0893
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0171
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0209
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0892
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0167
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0214
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0891
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0163
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0216
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0887
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0167
3072/6530 [=============>................] - ETA: 0s - loss: 0.0213
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0887
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0165
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0216
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0170
6530/6530 [==============================] - 1s 155us/step - loss: 0.0884 - val_loss: 0.0756
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0667
3936/6530 [=================>............] - ETA: 0s - loss: 0.0216
2864/6530 [============>.................] - ETA: 0s - loss: 0.0172
 368/6530 [>.............................] - ETA: 0s - loss: 0.0858
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0215
3184/6530 [=============>................] - ETA: 0s - loss: 0.0171
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0920
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0216
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0170
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0913
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0216
3824/6530 [================>.............] - ETA: 0s - loss: 0.0167
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0901
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0215
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0165
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0893
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0216
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0168
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0883
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0166
6530/6530 [==============================] - 1s 123us/step - loss: 0.0216 - val_loss: 0.0143
Epoch 7/9

  32/6530 [..............................] - ETA: 1s - loss: 0.0229
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0884
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0169
 448/6530 [=>............................] - ETA: 0s - loss: 0.0214
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0881
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0170
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0211
3120/6530 [=============>................] - ETA: 0s - loss: 0.0859
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0169
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0212
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0867
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0171
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0210
3808/6530 [================>.............] - ETA: 0s - loss: 0.0867
6336/6530 [============================>.] - ETA: 0s - loss: 0.0172
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0206
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0876
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0209
6530/6530 [==============================] - 1s 170us/step - loss: 0.0172 - val_loss: 0.0147
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0114
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0885
3008/6530 [============>.................] - ETA: 0s - loss: 0.0208
 352/6530 [>.............................] - ETA: 0s - loss: 0.0185
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0880
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0212
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0168
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0880
3840/6530 [================>.............] - ETA: 0s - loss: 0.0213
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0160
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0880
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0210
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0155
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0875
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0209
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0159
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0874
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0210
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0162
6530/6530 [==============================] - 1s 154us/step - loss: 0.0873 - val_loss: 0.0732
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0654
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0209
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0161
 368/6530 [>.............................] - ETA: 0s - loss: 0.0842
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0210
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0163
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0896
6400/6530 [============================>.] - ETA: 0s - loss: 0.0209
2896/6530 [============>.................] - ETA: 0s - loss: 0.0160
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0893
6530/6530 [==============================] - 1s 126us/step - loss: 0.0210 - val_loss: 0.0142
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0231
3216/6530 [=============>................] - ETA: 0s - loss: 0.0158
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0882
 480/6530 [=>............................] - ETA: 0s - loss: 0.0206
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0159
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0862
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0207
3840/6530 [================>.............] - ETA: 0s - loss: 0.0161
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0850
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0200
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0160
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0853
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0200
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0159
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0848
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0201
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0158
3120/6530 [=============>................] - ETA: 0s - loss: 0.0831
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0203
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0158
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0839
3040/6530 [============>.................] - ETA: 0s - loss: 0.0203
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0158
3776/6530 [================>.............] - ETA: 0s - loss: 0.0841
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0207
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0157
4096/6530 [=================>............] - ETA: 0s - loss: 0.0851
3936/6530 [=================>............] - ETA: 0s - loss: 0.0208
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0156
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0858
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0205
6352/6530 [============================>.] - ETA: 0s - loss: 0.0158
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0857
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0205
6530/6530 [==============================] - 1s 167us/step - loss: 0.0158 - val_loss: 0.0131
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0206
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0855
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0206
 288/6530 [>.............................] - ETA: 1s - loss: 0.0132
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0855
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0205
 608/6530 [=>............................] - ETA: 1s - loss: 0.0142
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0849
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0206
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0150
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0846
6528/6530 [============================>.] - ETA: 0s - loss: 0.0207
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0141
6530/6530 [==============================] - 1s 124us/step - loss: 0.0207 - val_loss: 0.0127
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0241
6432/6530 [============================>.] - ETA: 0s - loss: 0.0846
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0141
 480/6530 [=>............................] - ETA: 0s - loss: 0.0205
6530/6530 [==============================] - 1s 158us/step - loss: 0.0847 - val_loss: 0.0696
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0636
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0143
 336/6530 [>.............................] - ETA: 0s - loss: 0.0818
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0198
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0148
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0878
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0192
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0146
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0871
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0193
2864/6530 [============>.................] - ETA: 0s - loss: 0.0147
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0871
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0196
3184/6530 [=============>................] - ETA: 0s - loss: 0.0149
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0846
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0201
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0150
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0836
3072/6530 [=============>................] - ETA: 0s - loss: 0.0202
3792/6530 [================>.............] - ETA: 0s - loss: 0.0149
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0836
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0204
4096/6530 [=================>............] - ETA: 0s - loss: 0.0150
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0834
3936/6530 [=================>............] - ETA: 0s - loss: 0.0204
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0150
3152/6530 [=============>................] - ETA: 0s - loss: 0.0819
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0202
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0150
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0824
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0204
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0150
3808/6530 [================>.............] - ETA: 0s - loss: 0.0824
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0203
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0151
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0831
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0201
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0153
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0839
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0201
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0152
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0833
6496/6530 [============================>.] - ETA: 0s - loss: 0.0201
6320/6530 [============================>.] - ETA: 0s - loss: 0.0152
6530/6530 [==============================] - 1s 124us/step - loss: 0.0201 - val_loss: 0.0133

5184/6530 [======================>.......] - ETA: 0s - loss: 0.0833
6530/6530 [==============================] - 1s 168us/step - loss: 0.0152 - val_loss: 0.0115
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0172
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0833
 336/6530 [>.............................] - ETA: 0s - loss: 0.0176
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0832
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0156
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0833
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0150
6530/6530 [==============================] - 1s 152us/step - loss: 0.0833 - val_loss: 0.0669
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0499
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0145
 352/6530 [>.............................] - ETA: 0s - loss: 0.0777
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0149
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0867
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0147
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0868
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0148
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0855
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0147
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0842
2976/6530 [============>.................] - ETA: 0s - loss: 0.0147
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0831
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0150
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0836
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0150
2832/6530 [============>.................] - ETA: 0s - loss: 0.0826
4000/6530 [=================>............] - ETA: 0s - loss: 0.0148
3168/6530 [=============>................] - ETA: 0s - loss: 0.0813
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0148
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0818
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0148
3888/6530 [================>.............] - ETA: 0s - loss: 0.0824
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0148
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0825
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0147
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0835
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0146
# training | RMSE: 0.1086, MAE: 0.0839
worker 2  xfile  [2, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3839055503304921}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24531321904768233}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.1086278040973701, 'rmse': 0.1086278040973701, 'mae': 0.0838510841338714, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  41 | activation: relu    | extras: None 
layer 2 | size:  35 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  62 | activation: tanh    | extras: None 
layer 4 | size:  42 | activation: relu    | extras: dropout - rate: 41.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8ef358>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 2:21 - loss: 2.2975
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0828
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0147
 352/6530 [>.............................] - ETA: 6s - loss: 0.2546  
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0831
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0146
 688/6530 [==>...........................] - ETA: 3s - loss: 0.1721
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0830
1024/6530 [===>..........................] - ETA: 2s - loss: 0.1391
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0826
6530/6530 [==============================] - 1s 163us/step - loss: 0.0146 - val_loss: 0.0111
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0146
1376/6530 [=====>........................] - ETA: 2s - loss: 0.1195
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0825
 320/6530 [>.............................] - ETA: 1s - loss: 0.0149
1728/6530 [======>.......................] - ETA: 1s - loss: 0.1071
 640/6530 [=>............................] - ETA: 0s - loss: 0.0155
6530/6530 [==============================] - 1s 153us/step - loss: 0.0825 - val_loss: 0.0690

2080/6530 [========>.....................] - ETA: 1s - loss: 0.0978
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0165
2400/6530 [==========>...................] - ETA: 1s - loss: 0.0914
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0160
2752/6530 [===========>..................] - ETA: 1s - loss: 0.0854
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0160
3120/6530 [=============>................] - ETA: 0s - loss: 0.0804
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0161
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0769
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0157
3808/6530 [================>.............] - ETA: 0s - loss: 0.0734
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0154
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0706
2864/6530 [============>.................] - ETA: 0s - loss: 0.0150
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0687
3168/6530 [=============>................] - ETA: 0s - loss: 0.0149
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0667
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0150
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0655
3744/6530 [================>.............] - ETA: 0s - loss: 0.0148
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0634
4048/6530 [=================>............] - ETA: 0s - loss: 0.0146
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0619
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0147
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0606
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0145
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0146
6530/6530 [==============================] - 1s 208us/step - loss: 0.0596 - val_loss: 0.0327
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0168
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0145
 400/6530 [>.............................] - ETA: 0s - loss: 0.0336
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0145
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0336
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0145
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0324
6368/6530 [============================>.] - ETA: 0s - loss: 0.0145
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0317
6530/6530 [==============================] - 1s 167us/step - loss: 0.0145 - val_loss: 0.0105
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0258
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0331
 320/6530 [>.............................] - ETA: 1s - loss: 0.0140
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0336
 624/6530 [=>............................] - ETA: 0s - loss: 0.0135
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0338
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0142
2992/6530 [============>.................] - ETA: 0s - loss: 0.0331
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0144
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0332
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0143
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0330
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0139
4032/6530 [=================>............] - ETA: 0s - loss: 0.0328
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0140
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0327
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0137
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0323
2912/6530 [============>.................] - ETA: 0s - loss: 0.0134
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0320
3248/6530 [=============>................] - ETA: 0s - loss: 0.0136
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0321
# training | RMSE: 0.0800, MAE: 0.0620
worker 0  xfile  [0, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.0800198033817519, 'rmse': 0.0800198033817519, 'mae': 0.06201183720707693, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  31 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  93 | activation: relu    | extras: None 
layer 3 | size:  32 | activation: tanh    | extras: batchnorm 
layer 4 | size:  55 | activation: sigmoid | extras: dropout - rate: 47.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8ef240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 3:59 - loss: 0.4945
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0137
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0321
 256/6530 [>.............................] - ETA: 15s - loss: 0.3174 
3888/6530 [================>.............] - ETA: 0s - loss: 0.0137
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0319
 496/6530 [=>............................] - ETA: 8s - loss: 0.2646 
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0137
6496/6530 [============================>.] - ETA: 0s - loss: 0.0317
 736/6530 [==>...........................] - ETA: 5s - loss: 0.2411
6530/6530 [==============================] - 1s 147us/step - loss: 0.0317 - val_loss: 0.0241
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0346
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0136
 976/6530 [===>..........................] - ETA: 4s - loss: 0.2253
 384/6530 [>.............................] - ETA: 0s - loss: 0.0313
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0136
1216/6530 [====>.........................] - ETA: 3s - loss: 0.2147
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0135
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0285
1440/6530 [=====>........................] - ETA: 3s - loss: 0.2065
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0136
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0288
1664/6530 [======>.......................] - ETA: 2s - loss: 0.2006
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0136
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0296
1904/6530 [=======>......................] - ETA: 2s - loss: 0.1959
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0136
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0288
2144/6530 [========>.....................] - ETA: 2s - loss: 0.1919
6432/6530 [============================>.] - ETA: 0s - loss: 0.0136
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0288
2384/6530 [=========>....................] - ETA: 1s - loss: 0.1897
6530/6530 [==============================] - 1s 165us/step - loss: 0.0136 - val_loss: 0.0118

2512/6530 [==========>...................] - ETA: 0s - loss: 0.0280
2608/6530 [==========>...................] - ETA: 1s - loss: 0.1863
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0285
2832/6530 [============>.................] - ETA: 1s - loss: 0.1844
3120/6530 [=============>................] - ETA: 0s - loss: 0.0281
3072/6530 [=============>................] - ETA: 1s - loss: 0.1818
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0279
3296/6530 [==============>...............] - ETA: 1s - loss: 0.1801
3824/6530 [================>.............] - ETA: 0s - loss: 0.0277
3520/6530 [===============>..............] - ETA: 1s - loss: 0.1778
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0271
3712/6530 [================>.............] - ETA: 1s - loss: 0.1759
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0271
3936/6530 [=================>............] - ETA: 0s - loss: 0.1740
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0269
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1728
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0267
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1710
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0266
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1697
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0265
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1675
6336/6530 [============================>.] - ETA: 0s - loss: 0.0263
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1663
6530/6530 [==============================] - 1s 151us/step - loss: 0.0263 - val_loss: 0.0215
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0150
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1648
 384/6530 [>.............................] - ETA: 0s - loss: 0.0245
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1636
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0248
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1624
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0247
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1613
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0250
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1603
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0252
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0252
6530/6530 [==============================] - 2s 324us/step - loss: 0.1590 - val_loss: 0.1189

2528/6530 [==========>...................] - ETA: 0s - loss: 0.0251Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0970
2896/6530 [============>.................] - ETA: 0s - loss: 0.0250
 256/6530 [>.............................] - ETA: 1s - loss: 0.1415
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0249
 480/6530 [=>............................] - ETA: 1s - loss: 0.1376
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0247
 704/6530 [==>...........................] - ETA: 1s - loss: 0.1369
4000/6530 [=================>............] - ETA: 0s - loss: 0.0245
 944/6530 [===>..........................] - ETA: 1s - loss: 0.1325
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0245
1184/6530 [====>.........................] - ETA: 1s - loss: 0.1337
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0243
1408/6530 [=====>........................] - ETA: 1s - loss: 0.1353
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0247
1648/6530 [======>.......................] - ETA: 1s - loss: 0.1364
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0244
1888/6530 [=======>......................] - ETA: 1s - loss: 0.1353
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0245
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1351
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0243
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1341
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1347
6530/6530 [==============================] - 1s 145us/step - loss: 0.0241 - val_loss: 0.0190
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0376
2848/6530 [============>.................] - ETA: 0s - loss: 0.1343
 368/6530 [>.............................] - ETA: 0s - loss: 0.0247
# training | RMSE: 0.1007, MAE: 0.0783
worker 1  xfile  [3, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16597532291562125}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2703434644417436}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.10073771109765839, 'rmse': 0.10073771109765839, 'mae': 0.0783159501251988, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  89 | activation: tanh    | extras: batchnorm 
layer 2 | size:  73 | activation: relu    | extras: dropout - rate: 15.4% 
layer 3 | size:  53 | activation: relu    | extras: batchnorm 
layer 4 | size:  96 | activation: sigmoid | extras: None 
layer 5 | size:  22 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f4604139320>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 1:02 - loss: 1.4622
3072/6530 [=============>................] - ETA: 0s - loss: 0.1340
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0232
 960/6530 [===>..........................] - ETA: 3s - loss: 0.3842  
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1339
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0230
1856/6530 [=======>......................] - ETA: 1s - loss: 0.2458
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1335
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0228
2816/6530 [===========>..................] - ETA: 1s - loss: 0.1822
3728/6530 [================>.............] - ETA: 0s - loss: 0.1329
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0222
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1520
3952/6530 [=================>............] - ETA: 0s - loss: 0.1325
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0218
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1306
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1320
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0216
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1157
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1319
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0217
6400/6530 [============================>.] - ETA: 0s - loss: 0.1040
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1321
3152/6530 [=============>................] - ETA: 0s - loss: 0.0217
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1318
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0218
6530/6530 [==============================] - 1s 163us/step - loss: 0.1027 - val_loss: 0.0251
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0343
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1305
3872/6530 [================>.............] - ETA: 0s - loss: 0.0222
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0362
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1299
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0223
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0355
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1299
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0221
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0351
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1293
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0225
3712/6530 [================>.............] - ETA: 0s - loss: 0.0347
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0224
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1297
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0345
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0223
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1296
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0338
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0222
6512/6530 [============================>.] - ETA: 0s - loss: 0.1297
6464/6530 [============================>.] - ETA: 0s - loss: 0.0333
6530/6530 [==============================] - 0s 59us/step - loss: 0.0333 - val_loss: 0.0192
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0257
6530/6530 [==============================] - 2s 230us/step - loss: 0.1297 - val_loss: 0.1097

6256/6530 [===========================>..] - ETA: 0s - loss: 0.0222Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1792
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0263
 240/6530 [>.............................] - ETA: 1s - loss: 0.1246
6530/6530 [==============================] - 1s 152us/step - loss: 0.0221 - val_loss: 0.0177
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0194
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0275
 464/6530 [=>............................] - ETA: 1s - loss: 0.1189
 384/6530 [>.............................] - ETA: 0s - loss: 0.0205
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0279
 704/6530 [==>...........................] - ETA: 1s - loss: 0.1185
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0201
3776/6530 [================>.............] - ETA: 0s - loss: 0.0274
 944/6530 [===>..........................] - ETA: 1s - loss: 0.1197
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0205
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0275
1184/6530 [====>.........................] - ETA: 1s - loss: 0.1193
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0210
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0272
1424/6530 [=====>........................] - ETA: 1s - loss: 0.1206
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0216
6530/6530 [==============================] - 0s 58us/step - loss: 0.0272 - val_loss: 0.0170
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0183
1664/6530 [======>.......................] - ETA: 1s - loss: 0.1213
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0219
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0237
1888/6530 [=======>......................] - ETA: 1s - loss: 0.1217
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0213
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1206
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0246
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0210
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1206
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0245
3136/6530 [=============>................] - ETA: 0s - loss: 0.0206
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1214
3712/6530 [================>.............] - ETA: 0s - loss: 0.0241
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0209
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1203
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0242
3856/6530 [================>.............] - ETA: 0s - loss: 0.0209
3024/6530 [============>.................] - ETA: 0s - loss: 0.1213
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0239
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0208
3248/6530 [=============>................] - ETA: 0s - loss: 0.1224
6464/6530 [============================>.] - ETA: 0s - loss: 0.0236
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0207
6530/6530 [==============================] - 0s 59us/step - loss: 0.0235 - val_loss: 0.0149
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0193
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1215
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0207
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0226
3728/6530 [================>.............] - ETA: 0s - loss: 0.1219
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0208
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0234
3968/6530 [=================>............] - ETA: 0s - loss: 0.1218
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0208
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0240
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1217
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0208
3712/6530 [================>.............] - ETA: 0s - loss: 0.0235
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1215
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0208
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0236
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1212
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0234
6530/6530 [==============================] - 1s 152us/step - loss: 0.0206 - val_loss: 0.0163
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0142
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1202
6336/6530 [============================>.] - ETA: 0s - loss: 0.0230
 336/6530 [>.............................] - ETA: 0s - loss: 0.0206
6530/6530 [==============================] - 0s 60us/step - loss: 0.0229 - val_loss: 0.0144
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0251
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1205
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0198
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0208
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1209
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0201
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0212
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1209
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0201
2880/6530 [============>.................] - ETA: 0s - loss: 0.0217
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1211
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0196
3776/6530 [================>.............] - ETA: 0s - loss: 0.0213
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1209
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0190
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0212
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1207
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0191
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0212
6512/6530 [============================>.] - ETA: 0s - loss: 0.1206
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0191
6530/6530 [==============================] - 2s 230us/step - loss: 0.1207 - val_loss: 0.1023

6464/6530 [============================>.] - ETA: 0s - loss: 0.0210Epoch 4/9

  16/6530 [..............................] - ETA: 2s - loss: 0.1199
6530/6530 [==============================] - 0s 59us/step - loss: 0.0210 - val_loss: 0.0130
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0158
3168/6530 [=============>................] - ETA: 0s - loss: 0.0194
 256/6530 [>.............................] - ETA: 1s - loss: 0.1144
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0195
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0195
 496/6530 [=>............................] - ETA: 1s - loss: 0.1100
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0202
3872/6530 [================>.............] - ETA: 0s - loss: 0.0195
 736/6530 [==>...........................] - ETA: 1s - loss: 0.1155
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0201
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0195
 976/6530 [===>..........................] - ETA: 1s - loss: 0.1177
3712/6530 [================>.............] - ETA: 0s - loss: 0.0199
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0195
1184/6530 [====>.........................] - ETA: 1s - loss: 0.1169
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0200
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0195
1408/6530 [=====>........................] - ETA: 1s - loss: 0.1153
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0201
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0195
1648/6530 [======>.......................] - ETA: 1s - loss: 0.1156
6400/6530 [============================>.] - ETA: 0s - loss: 0.0199
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0194
6530/6530 [==============================] - 0s 59us/step - loss: 0.0199 - val_loss: 0.0126
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0204
1888/6530 [=======>......................] - ETA: 1s - loss: 0.1153
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0195
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0195
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1155
6336/6530 [============================>.] - ETA: 0s - loss: 0.0194
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0191
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1152
6530/6530 [==============================] - 1s 153us/step - loss: 0.0195 - val_loss: 0.0180

2688/6530 [===========>..................] - ETA: 0s - loss: 0.0193Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0207
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1164
 352/6530 [>.............................] - ETA: 0s - loss: 0.0214
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0193
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1171
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0198
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0191
3040/6530 [============>.................] - ETA: 0s - loss: 0.1161
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0185
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0192
3248/6530 [=============>................] - ETA: 0s - loss: 0.1163
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0188
6336/6530 [============================>.] - ETA: 0s - loss: 0.0189
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1166
6530/6530 [==============================] - 0s 60us/step - loss: 0.0189 - val_loss: 0.0118
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0185
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0186
3712/6530 [================>.............] - ETA: 0s - loss: 0.1160
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0178
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0183
3952/6530 [=================>............] - ETA: 0s - loss: 0.1162
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0180
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0179
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1155
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0183
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0182
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1168
3776/6530 [================>.............] - ETA: 0s - loss: 0.0180
3104/6530 [=============>................] - ETA: 0s - loss: 0.0182
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1161
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0179
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0180
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1169
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0180
3824/6530 [================>.............] - ETA: 0s - loss: 0.0183
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1165
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0183
6530/6530 [==============================] - 0s 58us/step - loss: 0.0179 - val_loss: 0.0113

5344/6530 [=======================>......] - ETA: 0s - loss: 0.1169
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0184
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1169
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0185
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1166
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0186
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1173
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0187
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1173
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0189
6528/6530 [============================>.] - ETA: 0s - loss: 0.1173
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0190
6530/6530 [==============================] - 1s 228us/step - loss: 0.1174 - val_loss: 0.0917
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0794
6480/6530 [============================>.] - ETA: 0s - loss: 0.0189
 256/6530 [>.............................] - ETA: 1s - loss: 0.1139
6530/6530 [==============================] - 1s 154us/step - loss: 0.0189 - val_loss: 0.0173
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0138
 496/6530 [=>............................] - ETA: 1s - loss: 0.1143
 400/6530 [>.............................] - ETA: 0s - loss: 0.0181
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1133
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0177
1008/6530 [===>..........................] - ETA: 1s - loss: 0.1140
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0179
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1138
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0177
1488/6530 [=====>........................] - ETA: 1s - loss: 0.1119
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0180
1744/6530 [=======>......................] - ETA: 0s - loss: 0.1134
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0177
1968/6530 [========>.....................] - ETA: 0s - loss: 0.1130
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0175
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1140
2864/6530 [============>.................] - ETA: 0s - loss: 0.0175
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1147
3216/6530 [=============>................] - ETA: 0s - loss: 0.0173
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1155
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0172
2912/6530 [============>.................] - ETA: 0s - loss: 0.1147
3920/6530 [=================>............] - ETA: 0s - loss: 0.0175
3136/6530 [=============>................] - ETA: 0s - loss: 0.1141
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0176
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1150
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0176
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1159
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0177
3856/6530 [================>.............] - ETA: 0s - loss: 0.1155
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0177
4080/6530 [=================>............] - ETA: 0s - loss: 0.1148
# training | RMSE: 0.1009, MAE: 0.0780
worker 1  xfile  [6, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1539922143106104}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.10088924032636437, 'rmse': 0.10088924032636437, 'mae': 0.0780401808990467, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  63 | activation: tanh    | extras: batchnorm 
layer 2 | size:  61 | activation: relu    | extras: None 
layer 3 | size:  80 | activation: relu    | extras: None 
layer 4 | size:  93 | activation: tanh    | extras: dropout - rate: 11.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45f466cda0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 25s - loss: 0.3424
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0177
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1150
2432/6530 [==========>...................] - ETA: 0s - loss: 0.2091 
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0178
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1149
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1802
6400/6530 [============================>.] - ETA: 0s - loss: 0.0178
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1146
6530/6530 [==============================] - 1s 150us/step - loss: 0.0178 - val_loss: 0.0152

5008/6530 [======================>.......] - ETA: 0s - loss: 0.1145
6530/6530 [==============================] - 1s 108us/step - loss: 0.1671 - val_loss: 0.1314
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1428
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1148
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1318
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1147
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1219
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1146
6530/6530 [==============================] - 0s 25us/step - loss: 0.1170 - val_loss: 0.1095
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1027
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1141
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1001
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1143
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0962
6400/6530 [============================>.] - ETA: 0s - loss: 0.1148
6530/6530 [==============================] - 0s 24us/step - loss: 0.0947 - val_loss: 0.1120
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1188
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1082
6530/6530 [==============================] - 1s 224us/step - loss: 0.1147 - val_loss: 0.0849
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1140
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0985
 240/6530 [>.............................] - ETA: 1s - loss: 0.1131
6528/6530 [============================>.] - ETA: 0s - loss: 0.0941
6530/6530 [==============================] - 0s 25us/step - loss: 0.0941 - val_loss: 0.0847

 464/6530 [=>............................] - ETA: 1s - loss: 0.1093Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0788
 720/6530 [==>...........................] - ETA: 1s - loss: 0.1114
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0848
 976/6530 [===>..........................] - ETA: 1s - loss: 0.1121
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0822
1216/6530 [====>.........................] - ETA: 1s - loss: 0.1111
6530/6530 [==============================] - 0s 24us/step - loss: 0.0826 - val_loss: 0.0954
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0998
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1119
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1010
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1112
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0908
1920/6530 [=======>......................] - ETA: 1s - loss: 0.1125
6530/6530 [==============================] - 0s 24us/step - loss: 0.0875 - val_loss: 0.0923
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0803
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1130
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0818
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1121
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0780
6530/6530 [==============================] - 0s 23us/step - loss: 0.0775 - val_loss: 0.0927
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0987
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1115
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0941
2848/6530 [============>.................] - ETA: 0s - loss: 0.1121
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0845
3088/6530 [=============>................] - ETA: 0s - loss: 0.1120
6530/6530 [==============================] - 0s 23us/step - loss: 0.0823 - val_loss: 0.0770
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0703
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1119
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0741
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1118
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0716
3792/6530 [================>.............] - ETA: 0s - loss: 0.1115
6530/6530 [==============================] - 0s 23us/step - loss: 0.0719 - val_loss: 0.0885

4032/6530 [=================>............] - ETA: 0s - loss: 0.1114
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1114
# training | RMSE: 0.1090, MAE: 0.0844
worker 1  xfile  [7, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11932348426737889}, 'layer_4_size': 93, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.10901352610514008, 'rmse': 0.10901352610514008, 'mae': 0.0843558092119087, 'early_stop': False}
vggnet done  1

4528/6530 [===================>..........] - ETA: 0s - loss: 0.1114
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1118
# training | RMSE: 0.1128, MAE: 0.0882
worker 2  xfile  [4, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41090246001998465}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.11279895787289662, 'rmse': 0.11279895787289662, 'mae': 0.0882080508049943, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  95 | activation: relu    | extras: dropout - rate: 30.6% 
layer 2 | size:  79 | activation: relu    | extras: batchnorm 
layer 3 | size:  80 | activation: sigmoid | extras: dropout - rate: 40.2% 
layer 4 | size:  30 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f45dc7a3048>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 1:45 - loss: 0.1818
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1112
 576/6530 [=>............................] - ETA: 5s - loss: 0.1008  
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1120
1120/6530 [====>.........................] - ETA: 3s - loss: 0.0921
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1126
1632/6530 [======>.......................] - ETA: 2s - loss: 0.0831
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1133
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0784
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1127
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0754
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1125
3168/6530 [=============>................] - ETA: 0s - loss: 0.0708
6528/6530 [============================>.] - ETA: 0s - loss: 0.1126
3712/6530 [================>.............] - ETA: 0s - loss: 0.0684
6530/6530 [==============================] - 1s 220us/step - loss: 0.1126 - val_loss: 0.0882
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1300
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0659
 272/6530 [>.............................] - ETA: 1s - loss: 0.1037
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0639
 528/6530 [=>............................] - ETA: 1s - loss: 0.1177
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0618
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1122
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0608
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1124
6400/6530 [============================>.] - ETA: 0s - loss: 0.0593
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1119
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1124
6530/6530 [==============================] - 1s 187us/step - loss: 0.0589 - val_loss: 0.0330
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0454
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1124
 576/6530 [=>............................] - ETA: 0s - loss: 0.0416
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1115
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0414
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1121
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0399
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1125
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0406
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1125
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0408
3024/6530 [============>.................] - ETA: 0s - loss: 0.1134
3264/6530 [=============>................] - ETA: 0s - loss: 0.0396
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1140
3776/6530 [================>.............] - ETA: 0s - loss: 0.0394
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1150
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0394
3792/6530 [================>.............] - ETA: 0s - loss: 0.1154
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0390
4048/6530 [=================>............] - ETA: 0s - loss: 0.1154
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0387
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1153
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0387
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1153
6528/6530 [============================>.] - ETA: 0s - loss: 0.0384
6530/6530 [==============================] - 1s 99us/step - loss: 0.0383 - val_loss: 0.0247
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0370
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1148
 576/6530 [=>............................] - ETA: 0s - loss: 0.0326
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1142
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0325
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1151
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0319
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1146
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0328
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1147
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0334
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1142
3264/6530 [=============>................] - ETA: 0s - loss: 0.0327
6368/6530 [============================>.] - ETA: 0s - loss: 0.1139
3808/6530 [================>.............] - ETA: 0s - loss: 0.0326
6530/6530 [==============================] - 1s 210us/step - loss: 0.1139 - val_loss: 0.0913
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1079
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0327
 272/6530 [>.............................] - ETA: 1s - loss: 0.1061
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0327
 528/6530 [=>............................] - ETA: 1s - loss: 0.1148
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0327
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1149
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0323
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1171
6496/6530 [============================>.] - ETA: 0s - loss: 0.0323
6530/6530 [==============================] - 1s 99us/step - loss: 0.0323 - val_loss: 0.0209
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0241
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1152
 576/6530 [=>............................] - ETA: 0s - loss: 0.0290
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1149
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0283
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1137
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0280
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1146
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0285
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1147
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0293
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1149
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0289
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1147
3840/6530 [================>.............] - ETA: 0s - loss: 0.0287
3040/6530 [============>.................] - ETA: 0s - loss: 0.1138
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0288
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1143
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0290
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1145
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0290
3808/6530 [================>.............] - ETA: 0s - loss: 0.1140
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0290
4064/6530 [=================>............] - ETA: 0s - loss: 0.1130
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1133
6530/6530 [==============================] - 1s 98us/step - loss: 0.0289 - val_loss: 0.0192
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0183
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1128
 576/6530 [=>............................] - ETA: 0s - loss: 0.0259
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1131
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0253
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1129
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0251
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1128
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0254
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1128
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0258
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1124
3264/6530 [=============>................] - ETA: 0s - loss: 0.0254
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1130
3840/6530 [================>.............] - ETA: 0s - loss: 0.0256
6320/6530 [============================>.] - ETA: 0s - loss: 0.1134
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0258
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0260
6530/6530 [==============================] - 1s 210us/step - loss: 0.1133 - val_loss: 0.0908
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1250
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0261
 256/6530 [>.............................] - ETA: 1s - loss: 0.1260
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0261
 496/6530 [=>............................] - ETA: 1s - loss: 0.1218
6496/6530 [============================>.] - ETA: 0s - loss: 0.0261
 736/6530 [==>...........................] - ETA: 1s - loss: 0.1243
6530/6530 [==============================] - 1s 99us/step - loss: 0.0261 - val_loss: 0.0167
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0221
 976/6530 [===>..........................] - ETA: 1s - loss: 0.1216
 544/6530 [=>............................] - ETA: 0s - loss: 0.0229
1232/6530 [====>.........................] - ETA: 1s - loss: 0.1191
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0242
1488/6530 [=====>........................] - ETA: 1s - loss: 0.1182
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0240
1744/6530 [=======>......................] - ETA: 1s - loss: 0.1172
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0239
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1171
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0245
2256/6530 [=========>....................] - ETA: 0s - loss: 0.1157
3264/6530 [=============>................] - ETA: 0s - loss: 0.0240
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1143
3808/6530 [================>.............] - ETA: 0s - loss: 0.0242
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0242
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1127
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0244
3024/6530 [============>.................] - ETA: 0s - loss: 0.1122
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0244
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1125
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0244
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1126
6432/6530 [============================>.] - ETA: 0s - loss: 0.0247
3776/6530 [================>.............] - ETA: 0s - loss: 0.1127
6530/6530 [==============================] - 1s 99us/step - loss: 0.0246 - val_loss: 0.0154
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0198
4016/6530 [=================>............] - ETA: 0s - loss: 0.1119
 544/6530 [=>............................] - ETA: 0s - loss: 0.0216
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1114
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0224
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1107
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0223
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1110
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0220
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1111
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0230
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1111
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0227
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1112
3840/6530 [================>.............] - ETA: 0s - loss: 0.0227
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1118
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0229
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1119
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0231
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1116
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0230
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0230
6530/6530 [==============================] - 1s 213us/step - loss: 0.1116 - val_loss: 0.0934

6530/6530 [==============================] - 1s 99us/step - loss: 0.0229 - val_loss: 0.0155
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0248
 544/6530 [=>............................] - ETA: 0s - loss: 0.0218
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0207
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0207
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0215
# training | RMSE: 0.1112, MAE: 0.0860
worker 0  xfile  [5, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4737581842737165}, 'layer_4_size': 55, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.11117021220498793, 'rmse': 0.11117021220498793, 'mae': 0.08602898661329164, 'early_stop': False}
vggnet done  0

2560/6530 [==========>...................] - ETA: 0s - loss: 0.0219
3136/6530 [=============>................] - ETA: 0s - loss: 0.0213
3744/6530 [================>.............] - ETA: 0s - loss: 0.0211
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0211
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0212
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0213
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0212
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0213
6530/6530 [==============================] - 1s 106us/step - loss: 0.0213 - val_loss: 0.0127
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0218
 480/6530 [=>............................] - ETA: 0s - loss: 0.0190
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0194
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0194
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0194
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0195
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0196
3040/6530 [============>.................] - ETA: 0s - loss: 0.0196
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0194
3840/6530 [================>.............] - ETA: 0s - loss: 0.0195
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0197
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0199
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0198
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0199
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0200
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0202
6530/6530 [==============================] - 1s 130us/step - loss: 0.0201 - val_loss: 0.0129

# training | RMSE: 0.1048, MAE: 0.0804
worker 2  xfile  [8, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30631658816098606}, 'layer_1_size': 95, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4022096769016439}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20978160463021867}, 'layer_5_size': 11, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.10477345696559887, 'rmse': 0.10477345696559887, 'mae': 0.08040521502626133, 'early_stop': False}
vggnet done  2
all of workers have been done
calculation finished
#1 epoch=9.0 loss={'loss': 0.0803414916061439, 'rmse': 0.0803414916061439, 'mae': 0.06215190434143412, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32305615123930476}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#2 epoch=9.0 loss={'loss': 0.1086278040973701, 'rmse': 0.1086278040973701, 'mae': 0.0838510841338714, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3839055503304921}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24531321904768233}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#0 epoch=9.0 loss={'loss': 0.0800198033817519, 'rmse': 0.0800198033817519, 'mae': 0.06201183720707693, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#3 epoch=9.0 loss={'loss': 0.10073771109765839, 'rmse': 0.10073771109765839, 'mae': 0.0783159501251988, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16597532291562125}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2703434644417436}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#6 epoch=9.0 loss={'loss': 0.10088924032636437, 'rmse': 0.10088924032636437, 'mae': 0.0780401808990467, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1539922143106104}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#4 epoch=9.0 loss={'loss': 0.11279895787289662, 'rmse': 0.11279895787289662, 'mae': 0.0882080508049943, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41090246001998465}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#7 epoch=9.0 loss={'loss': 0.10901352610514008, 'rmse': 0.10901352610514008, 'mae': 0.0843558092119087, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11932348426737889}, 'layer_4_size': 93, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#5 epoch=9.0 loss={'loss': 0.11117021220498793, 'rmse': 0.11117021220498793, 'mae': 0.08602898661329164, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4737581842737165}, 'layer_4_size': 55, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#8 epoch=9.0 loss={'loss': 0.10477345696559887, 'rmse': 0.10477345696559887, 'mae': 0.08040521502626133, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30631658816098606}, 'layer_1_size': 95, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4022096769016439}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20978160463021867}, 'layer_5_size': 11, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
get a list [results] of length 117
get a list [loss] of length 9
get a list [val_loss] of length 9
length of indices is (0, 1, 3, 6, 8, 2, 7, 5, 4)
length of indices is 9
length of T is 9
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [1, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32305615123930476}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [2, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16597532291562125}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2703434644417436}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]] 

*** 3.0 configurations x 27.0 iterations each

9 | Thu Sep 27 23:06:28 2018 | lowest loss so far: 0.0800 (run 0)

{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  86 | activation: relu    | extras: batchnorm 
layer 2 | size:  43 | activation: relu    | extras: None 
layer 3 | size:  49 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 256/6530 [>.............................] - ETA: 21s - loss: 0.6033
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1028 
6530/6530 [==============================] - 1s 153us/step - loss: 0.0806 - val_loss: 0.0177
Epoch 2/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0158
4096/6530 [=================>............] - ETA: 0s - loss: 0.0144{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  93 | activation: relu    | extras: None 
layer 2 | size:  84 | activation: relu    | extras: batchnorm 
layer 3 | size:  83 | activation: sigmoid | extras: dropout - rate: 18.6% 
layer 4 | size:  45 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 6:49 - loss: 0.6924
6530/6530 [==============================] - 0s 14us/step - loss: 0.0145 - val_loss: 0.0131
Epoch 3/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0116
 320/6530 [>.............................] - ETA: 20s - loss: 0.2144 
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0114
 656/6530 [==>...........................] - ETA: 9s - loss: 0.1967 
6530/6530 [==============================] - 0s 12us/step - loss: 0.0116 - val_loss: 0.0115
Epoch 4/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0103
 960/6530 [===>..........................] - ETA: 6s - loss: 0.1828
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0103
6530/6530 [==============================] - 0s 10us/step - loss: 0.0102 - val_loss: 0.0106
Epoch 5/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0096{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  41 | activation: tanh    | extras: None 
layer 2 | size:  39 | activation: relu    | extras: batchnorm 
layer 3 | size:  30 | activation: relu    | extras: None 
layer 4 | size:  83 | activation: sigmoid | extras: dropout - rate: 16.6% 
layer 5 | size:  40 | activation: tanh    | extras: dropout - rate: 27.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 7:32 - loss: 0.5132
1280/6530 [====>.........................] - ETA: 4s - loss: 0.1760
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0094
6530/6530 [==============================] - 0s 10us/step - loss: 0.0094 - val_loss: 0.0100
Epoch 6/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0091
 336/6530 [>.............................] - ETA: 21s - loss: 0.0865 
1600/6530 [======>.......................] - ETA: 3s - loss: 0.1687
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0089
 608/6530 [=>............................] - ETA: 11s - loss: 0.0720
6530/6530 [==============================] - 0s 11us/step - loss: 0.0088 - val_loss: 0.0096

1888/6530 [=======>......................] - ETA: 3s - loss: 0.1647Epoch 7/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0087
 896/6530 [===>..........................] - ETA: 7s - loss: 0.0619 
2224/6530 [=========>....................] - ETA: 2s - loss: 0.1607
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0084
6530/6530 [==============================] - 0s 11us/step - loss: 0.0083 - val_loss: 0.0092
Epoch 8/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0084
1120/6530 [====>.........................] - ETA: 6s - loss: 0.0555
2544/6530 [==========>...................] - ETA: 2s - loss: 0.1586
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0079
6530/6530 [==============================] - 0s 10us/step - loss: 0.0079 - val_loss: 0.0089
Epoch 9/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0081
1440/6530 [=====>........................] - ETA: 4s - loss: 0.0508
2864/6530 [============>.................] - ETA: 1s - loss: 0.1547
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0076
1744/6530 [=======>......................] - ETA: 3s - loss: 0.0476
6530/6530 [==============================] - 0s 9us/step - loss: 0.0076 - val_loss: 0.0087
Epoch 10/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0078
3184/6530 [=============>................] - ETA: 1s - loss: 0.1511
2064/6530 [========>.....................] - ETA: 3s - loss: 0.0452
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0074
3520/6530 [===============>..............] - ETA: 1s - loss: 0.1493
6530/6530 [==============================] - 0s 10us/step - loss: 0.0073 - val_loss: 0.0084
Epoch 11/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0076
2368/6530 [=========>....................] - ETA: 2s - loss: 0.0438
3856/6530 [================>.............] - ETA: 1s - loss: 0.1473
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0071
6530/6530 [==============================] - 0s 9us/step - loss: 0.0071 - val_loss: 0.0082
Epoch 12/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0074
2688/6530 [===========>..................] - ETA: 2s - loss: 0.0419
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1456
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0069
6530/6530 [==============================] - 0s 10us/step - loss: 0.0069 - val_loss: 0.0081
Epoch 13/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0072
3008/6530 [============>.................] - ETA: 1s - loss: 0.0405
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1445
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0067
6530/6530 [==============================] - 0s 10us/step - loss: 0.0067 - val_loss: 0.0079
Epoch 14/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0070
3328/6530 [==============>...............] - ETA: 1s - loss: 0.0391
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1424
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0066
3632/6530 [===============>..............] - ETA: 1s - loss: 0.0377
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1405
6530/6530 [==============================] - 0s 9us/step - loss: 0.0066 - val_loss: 0.0078
Epoch 15/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0069
3952/6530 [=================>............] - ETA: 1s - loss: 0.0369
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1391
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0064
6530/6530 [==============================] - 0s 9us/step - loss: 0.0064 - val_loss: 0.0077
Epoch 16/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0068
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0363
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1376
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0063
6530/6530 [==============================] - 0s 9us/step - loss: 0.0063 - val_loss: 0.0076
Epoch 17/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0066
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0356
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1364
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0062
6530/6530 [==============================] - 0s 9us/step - loss: 0.0062 - val_loss: 0.0075
Epoch 18/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0066
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0348
6400/6530 [============================>.] - ETA: 0s - loss: 0.0061
6530/6530 [==============================] - 0s 9us/step - loss: 0.0060 - val_loss: 0.0074
Epoch 19/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0065
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0342
6530/6530 [==============================] - 2s 320us/step - loss: 0.1351 - val_loss: 0.1190
Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0968
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0334
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0060
6530/6530 [==============================] - 0s 11us/step - loss: 0.0059 - val_loss: 0.0073
Epoch 20/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0064
 320/6530 [>.............................] - ETA: 1s - loss: 0.1036
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0328
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1116
6400/6530 [============================>.] - ETA: 0s - loss: 0.0059
6530/6530 [==============================] - 0s 9us/step - loss: 0.0058 - val_loss: 0.0072
Epoch 21/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0063
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0324
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1104
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0058
6530/6530 [==============================] - 0s 10us/step - loss: 0.0057 - val_loss: 0.0072
Epoch 22/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0062
6464/6530 [============================>.] - ETA: 0s - loss: 0.0320
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1118
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0057
6530/6530 [==============================] - 0s 10us/step - loss: 0.0057 - val_loss: 0.0071
Epoch 23/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0061
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1097
6530/6530 [==============================] - 2s 349us/step - loss: 0.0318 - val_loss: 0.0184
Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0212
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0056
6530/6530 [==============================] - 0s 9us/step - loss: 0.0056 - val_loss: 0.0070
Epoch 24/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0060
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1085
 320/6530 [>.............................] - ETA: 1s - loss: 0.0251
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0055
6530/6530 [==============================] - 0s 9us/step - loss: 0.0055 - val_loss: 0.0070
Epoch 25/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0059
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1082
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0241
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0055
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1084
6530/6530 [==============================] - 0s 9us/step - loss: 0.0054 - val_loss: 0.0069
Epoch 26/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0058
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0229
3008/6530 [============>.................] - ETA: 0s - loss: 0.1059
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0054
6530/6530 [==============================] - 0s 9us/step - loss: 0.0054 - val_loss: 0.0069
Epoch 27/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0058
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0226
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1053
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0053
6530/6530 [==============================] - 0s 9us/step - loss: 0.0053 - val_loss: 0.0068

1632/6530 [======>.......................] - ETA: 0s - loss: 0.0233
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1057
# training | RMSE: 0.0686, MAE: 0.0536
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32305615123930476}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.06856299539712692, 'rmse': 0.06856299539712692, 'mae': 0.05357597171061813, 'early_stop': False}
vggnet done  1

1968/6530 [========>.....................] - ETA: 0s - loss: 0.0234
4016/6530 [=================>............] - ETA: 0s - loss: 0.1059
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0231
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1058
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0231
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1065
2960/6530 [============>.................] - ETA: 0s - loss: 0.0226
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1060
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0223
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1058
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0222
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1048
3952/6530 [=================>............] - ETA: 0s - loss: 0.0218
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1048
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0214
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0216
6530/6530 [==============================] - 1s 154us/step - loss: 0.1047 - val_loss: 0.0881
Epoch 3/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0873
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0214
 384/6530 [>.............................] - ETA: 0s - loss: 0.0941
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0213
 752/6530 [==>...........................] - ETA: 0s - loss: 0.1024
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0212
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1039
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0210
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1017
6320/6530 [============================>.] - ETA: 0s - loss: 0.0207
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0994
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0984
6530/6530 [==============================] - 1s 159us/step - loss: 0.0208 - val_loss: 0.0151
Epoch 3/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0360
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0991
 368/6530 [>.............................] - ETA: 0s - loss: 0.0173
2976/6530 [============>.................] - ETA: 0s - loss: 0.0967
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0182
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0961
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0186
3712/6530 [================>.............] - ETA: 0s - loss: 0.0963
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0195
4080/6530 [=================>............] - ETA: 0s - loss: 0.0967
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0189
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0971
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0186
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0971
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0189
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0970
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0189
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0972
3056/6530 [=============>................] - ETA: 0s - loss: 0.0188
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0964
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0185
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0962
3744/6530 [================>.............] - ETA: 0s - loss: 0.0185
4112/6530 [=================>............] - ETA: 0s - loss: 0.0184
6530/6530 [==============================] - 1s 144us/step - loss: 0.0959 - val_loss: 0.0789
Epoch 4/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0738
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0182
 384/6530 [>.............................] - ETA: 0s - loss: 0.0926
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0183
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0985
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0182
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0990
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0182
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0958
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0181
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0939
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0180
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0932
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0932
6530/6530 [==============================] - 1s 153us/step - loss: 0.0180 - val_loss: 0.0202
Epoch 4/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0309
3024/6530 [============>.................] - ETA: 0s - loss: 0.0911
 368/6530 [>.............................] - ETA: 0s - loss: 0.0186
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0910
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0175
3760/6530 [================>.............] - ETA: 0s - loss: 0.0913
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0176
4128/6530 [=================>............] - ETA: 0s - loss: 0.0919
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0169
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0928
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0178
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0923
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0179
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0922
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0177
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0918
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0173
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0914
3152/6530 [=============>................] - ETA: 0s - loss: 0.0171
6320/6530 [============================>.] - ETA: 0s - loss: 0.0913
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0172
6530/6530 [==============================] - 1s 142us/step - loss: 0.0910 - val_loss: 0.0848
Epoch 5/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0670
3856/6530 [================>.............] - ETA: 0s - loss: 0.0173
 384/6530 [>.............................] - ETA: 0s - loss: 0.0848
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0171
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0924
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0170
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0938
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0169
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0912
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0169
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0898
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0167
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0897
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0167
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0899
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0166
2960/6530 [============>.................] - ETA: 0s - loss: 0.0886
6530/6530 [==============================] - 1s 153us/step - loss: 0.0166 - val_loss: 0.0134
Epoch 5/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0118
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0881
 368/6530 [>.............................] - ETA: 0s - loss: 0.0154
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0888
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0153
4080/6530 [=================>............] - ETA: 0s - loss: 0.0894
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0149
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0900
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0156
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0895
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0155
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0895
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0155
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0894
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0154
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0889
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0155
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0890
3136/6530 [=============>................] - ETA: 0s - loss: 0.0156
6530/6530 [==============================] - 1s 143us/step - loss: 0.0888 - val_loss: 0.0733
Epoch 6/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0630
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0159
 384/6530 [>.............................] - ETA: 0s - loss: 0.0859
3808/6530 [================>.............] - ETA: 0s - loss: 0.0157
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0924
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0156
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0934
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0157
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0905
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0156
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0892
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0156
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0884
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0155
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0886
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0156
2976/6530 [============>.................] - ETA: 0s - loss: 0.0864
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0156
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0860
6530/6530 [==============================] - 1s 152us/step - loss: 0.0156 - val_loss: 0.0115
Epoch 6/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0279
3728/6530 [================>.............] - ETA: 0s - loss: 0.0867
 368/6530 [>.............................] - ETA: 0s - loss: 0.0163
4112/6530 [=================>............] - ETA: 0s - loss: 0.0877
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0168
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0884
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0154
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0880
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0157
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0880
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0154
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0877
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0151
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0874
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0153
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0873
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0151
6530/6530 [==============================] - 1s 144us/step - loss: 0.0871 - val_loss: 0.0721
Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0619
3072/6530 [=============>................] - ETA: 0s - loss: 0.0153
 368/6530 [>.............................] - ETA: 0s - loss: 0.0837
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0156
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0895
3760/6530 [================>.............] - ETA: 0s - loss: 0.0156
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0898
4112/6530 [=================>............] - ETA: 0s - loss: 0.0154
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0882
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0156
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0856
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0155
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0853
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0155
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0857
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0154
2992/6530 [============>.................] - ETA: 0s - loss: 0.0837
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0154
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0833
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0154
3712/6530 [================>.............] - ETA: 0s - loss: 0.0845
6528/6530 [============================>.] - ETA: 0s - loss: 0.0153
4080/6530 [=================>............] - ETA: 0s - loss: 0.0853
6530/6530 [==============================] - 1s 154us/step - loss: 0.0153 - val_loss: 0.0109
Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0155
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0859
 368/6530 [>.............................] - ETA: 0s - loss: 0.0140
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0854
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0144
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0854
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0146
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0850
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0159
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0848
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0156
6336/6530 [============================>.] - ETA: 0s - loss: 0.0847
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0156
6530/6530 [==============================] - 1s 142us/step - loss: 0.0847 - val_loss: 0.0715

2496/6530 [==========>...................] - ETA: 0s - loss: 0.0152Epoch 8/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0626
 352/6530 [>.............................] - ETA: 0s - loss: 0.0816
2832/6530 [============>.................] - ETA: 0s - loss: 0.0151
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0894
3168/6530 [=============>................] - ETA: 0s - loss: 0.0150
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0150
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0889
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0866
3872/6530 [================>.............] - ETA: 0s - loss: 0.0150
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0840
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0151
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0151
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0834
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0150
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0839
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0150
2960/6530 [============>.................] - ETA: 0s - loss: 0.0823
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0149
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0817
3712/6530 [================>.............] - ETA: 0s - loss: 0.0826
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0149
4080/6530 [=================>............] - ETA: 0s - loss: 0.0834
6336/6530 [============================>.] - ETA: 0s - loss: 0.0149
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0840
6530/6530 [==============================] - 1s 151us/step - loss: 0.0149 - val_loss: 0.0150
Epoch 8/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0124
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0835
 368/6530 [>.............................] - ETA: 0s - loss: 0.0119
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0835
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0128
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0836
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0133
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0833
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0128
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0836
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0138
6530/6530 [==============================] - 1s 143us/step - loss: 0.0835 - val_loss: 0.0662
Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0559
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0140
 368/6530 [>.............................] - ETA: 0s - loss: 0.0809
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0143
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0875
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0147
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0875
3120/6530 [=============>................] - ETA: 0s - loss: 0.0147
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0858
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0145
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0836
3760/6530 [================>.............] - ETA: 0s - loss: 0.0144
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0829
4096/6530 [=================>............] - ETA: 0s - loss: 0.0143
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0835
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0143
2944/6530 [============>.................] - ETA: 0s - loss: 0.0816
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0143
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0809
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0144
3712/6530 [================>.............] - ETA: 0s - loss: 0.0819
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0143
4080/6530 [=================>............] - ETA: 0s - loss: 0.0827
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0143
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0828
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0143
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0827
6512/6530 [============================>.] - ETA: 0s - loss: 0.0143
6530/6530 [==============================] - 1s 156us/step - loss: 0.0143 - val_loss: 0.0157
Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0366
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0826
 368/6530 [>.............................] - ETA: 0s - loss: 0.0138
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0828
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0131
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0822
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0138
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0823
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0138
6530/6530 [==============================] - 1s 145us/step - loss: 0.0823 - val_loss: 0.0700
Epoch 10/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0474
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0141
 400/6530 [>.............................] - ETA: 0s - loss: 0.0788
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0140
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0867
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0137
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0866
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0137
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0842
3136/6530 [=============>................] - ETA: 0s - loss: 0.0135
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0823
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0135
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0824
3792/6530 [================>.............] - ETA: 0s - loss: 0.0134
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0822
4128/6530 [=================>............] - ETA: 0s - loss: 0.0133
3008/6530 [============>.................] - ETA: 0s - loss: 0.0805
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0133
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0800
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0133
3728/6530 [================>.............] - ETA: 0s - loss: 0.0811
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0134
4064/6530 [=================>............] - ETA: 0s - loss: 0.0823
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0134
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0822
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0135
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0825
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0137
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0823
6336/6530 [============================>.] - ETA: 0s - loss: 0.0137
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0827
6530/6530 [==============================] - 1s 159us/step - loss: 0.0138 - val_loss: 0.0110
Epoch 10/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0143
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0820
 352/6530 [>.............................] - ETA: 0s - loss: 0.0113
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0814
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0123
6528/6530 [============================>.] - ETA: 0s - loss: 0.0817
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0129
6530/6530 [==============================] - 1s 147us/step - loss: 0.0817 - val_loss: 0.0645
Epoch 11/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0482
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0124
 368/6530 [>.............................] - ETA: 0s - loss: 0.0792
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0129
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0854
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0135
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0855
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0134
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0837
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0130
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0809
3008/6530 [============>.................] - ETA: 0s - loss: 0.0133
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0798
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0132
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0806
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0136
2832/6530 [============>.................] - ETA: 0s - loss: 0.0799
4016/6530 [=================>............] - ETA: 0s - loss: 0.0135
3200/6530 [=============>................] - ETA: 0s - loss: 0.0784
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0135
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0798
3952/6530 [=================>............] - ETA: 0s - loss: 0.0800
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0136
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0135
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0808
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0812
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0135
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0810
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0133
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0812
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0132
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0808
6416/6530 [============================>.] - ETA: 0s - loss: 0.0132
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0803
6530/6530 [==============================] - 1s 158us/step - loss: 0.0132 - val_loss: 0.0104
Epoch 11/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0182
6432/6530 [============================>.] - ETA: 0s - loss: 0.0802
 352/6530 [>.............................] - ETA: 0s - loss: 0.0125
6530/6530 [==============================] - 1s 147us/step - loss: 0.0803 - val_loss: 0.0684
Epoch 12/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0528
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0132
 384/6530 [>.............................] - ETA: 0s - loss: 0.0783
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0128
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0841
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0125
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0839
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0129
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0819
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0131
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0802
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0137
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0797
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0134
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0797
3104/6530 [=============>................] - ETA: 0s - loss: 0.0135
2928/6530 [============>.................] - ETA: 0s - loss: 0.0780
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0134
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0775
3776/6530 [================>.............] - ETA: 0s - loss: 0.0134
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0787
4112/6530 [=================>............] - ETA: 0s - loss: 0.0132
4016/6530 [=================>............] - ETA: 0s - loss: 0.0794
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0134
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0801
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0135
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0800
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0134
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0797
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0133
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0802
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0133
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0796
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0133
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0797
6512/6530 [============================>.] - ETA: 0s - loss: 0.0133
6530/6530 [==============================] - 1s 156us/step - loss: 0.0133 - val_loss: 0.0094
Epoch 12/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0137
6530/6530 [==============================] - 1s 144us/step - loss: 0.0796 - val_loss: 0.0680
Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0583
 336/6530 [>.............................] - ETA: 0s - loss: 0.0149
 368/6530 [>.............................] - ETA: 0s - loss: 0.0767
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0143
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0813
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0136
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0823
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0140
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0805
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0138
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0789
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0136
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0789
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0136
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0796
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0135
2944/6530 [============>.................] - ETA: 0s - loss: 0.0780
3072/6530 [=============>................] - ETA: 0s - loss: 0.0134
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0771
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0134
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0783
3776/6530 [================>.............] - ETA: 0s - loss: 0.0132
4048/6530 [=================>............] - ETA: 0s - loss: 0.0794
4128/6530 [=================>............] - ETA: 0s - loss: 0.0132
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0796
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0132
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0796
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0131
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0794
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0131
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0796
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0132
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0793
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0131
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0791
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0131
6530/6530 [==============================] - 1s 145us/step - loss: 0.0792 - val_loss: 0.0681
Epoch 14/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0568
6496/6530 [============================>.] - ETA: 0s - loss: 0.0132
6530/6530 [==============================] - 1s 156us/step - loss: 0.0131 - val_loss: 0.0108

 352/6530 [>.............................] - ETA: 0s - loss: 0.0784Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0206
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0864
 352/6530 [>.............................] - ETA: 0s - loss: 0.0134
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0853
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0125
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0134
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0821
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0133
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0798
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0793
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0132
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0797
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0132
2976/6530 [============>.................] - ETA: 0s - loss: 0.0779
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0133
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0777
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0133
3712/6530 [================>.............] - ETA: 0s - loss: 0.0786
3136/6530 [=============>................] - ETA: 0s - loss: 0.0132
4064/6530 [=================>............] - ETA: 0s - loss: 0.0796
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0131
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0801
3792/6530 [================>.............] - ETA: 0s - loss: 0.0130
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0798
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0130
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0799
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0130
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0800
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0130
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0796
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0127
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0128
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0798
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0126
6530/6530 [==============================] - 1s 143us/step - loss: 0.0796 - val_loss: 0.0638
Epoch 15/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0720
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0126
 400/6530 [>.............................] - ETA: 0s - loss: 0.0775
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0829
6530/6530 [==============================] - 1s 153us/step - loss: 0.0126 - val_loss: 0.0101
Epoch 14/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0111
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0819
 352/6530 [>.............................] - ETA: 0s - loss: 0.0133
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0802
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0126
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0788
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0125
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0793
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0121
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0794
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0119
3008/6530 [============>.................] - ETA: 0s - loss: 0.0776
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0119
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0772
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0119
3760/6530 [================>.............] - ETA: 0s - loss: 0.0779
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0117
4128/6530 [=================>............] - ETA: 0s - loss: 0.0790
3152/6530 [=============>................] - ETA: 0s - loss: 0.0115
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0801
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0114
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0791
3824/6530 [================>.............] - ETA: 0s - loss: 0.0116
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0792
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0117
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0794
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0117
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0793
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0116
6336/6530 [============================>.] - ETA: 0s - loss: 0.0793
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0118
6530/6530 [==============================] - 1s 143us/step - loss: 0.0792 - val_loss: 0.0666
Epoch 16/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0474
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0118
 384/6530 [>.............................] - ETA: 0s - loss: 0.0755
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0117
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0815
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0119
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0825
6528/6530 [============================>.] - ETA: 0s - loss: 0.0118
6530/6530 [==============================] - 1s 155us/step - loss: 0.0118 - val_loss: 0.0088
Epoch 15/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0082
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0794
 384/6530 [>.............................] - ETA: 0s - loss: 0.0104
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0776
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0121
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0772
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0125
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0781
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0131
2944/6530 [============>.................] - ETA: 0s - loss: 0.0767
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0128
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0759
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0126
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0768
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0125
4064/6530 [=================>............] - ETA: 0s - loss: 0.0780
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0126
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0787
3104/6530 [=============>................] - ETA: 0s - loss: 0.0129
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0784
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0127
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0783
3792/6530 [================>.............] - ETA: 0s - loss: 0.0129
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0786
4128/6530 [=================>............] - ETA: 0s - loss: 0.0128
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0783
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0126
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0783
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0124
6530/6530 [==============================] - 1s 145us/step - loss: 0.0783 - val_loss: 0.0628
Epoch 17/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0577
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0124
 400/6530 [>.............................] - ETA: 0s - loss: 0.0752
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0125
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0818
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0125
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0829
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0123
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0804
6530/6530 [==============================] - 1s 154us/step - loss: 0.0124 - val_loss: 0.0099
Epoch 16/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0176
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0785
 368/6530 [>.............................] - ETA: 0s - loss: 0.0119
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0778
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0125
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0780
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0121
2960/6530 [============>.................] - ETA: 0s - loss: 0.0765
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0124
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0758
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0124
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0768
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0124
4064/6530 [=================>............] - ETA: 0s - loss: 0.0778
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0124
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0782
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0123
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0783
3152/6530 [=============>................] - ETA: 0s - loss: 0.0124
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0784
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0124
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0785
3856/6530 [================>.............] - ETA: 0s - loss: 0.0123
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0782
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0124
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0784
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0123
6530/6530 [==============================] - 1s 143us/step - loss: 0.0784 - val_loss: 0.0681
Epoch 18/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0490
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0123
 384/6530 [>.............................] - ETA: 0s - loss: 0.0756
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0121
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0812
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0121
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0823
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0122
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0789
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0121
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0776
6530/6530 [==============================] - 1s 151us/step - loss: 0.0121 - val_loss: 0.0085
Epoch 17/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0057
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0775
 352/6530 [>.............................] - ETA: 0s - loss: 0.0114
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0772
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0118
3040/6530 [============>.................] - ETA: 0s - loss: 0.0756
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0121
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0765
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0121
3776/6530 [================>.............] - ETA: 0s - loss: 0.0768
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0123
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0778
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0125
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0788
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0123
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0781
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0123
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0782
3088/6530 [=============>................] - ETA: 0s - loss: 0.0123
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0783
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0123
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0778
3744/6530 [================>.............] - ETA: 0s - loss: 0.0124
6320/6530 [============================>.] - ETA: 0s - loss: 0.0778
4080/6530 [=================>............] - ETA: 0s - loss: 0.0123
6530/6530 [==============================] - 1s 143us/step - loss: 0.0776 - val_loss: 0.0658
Epoch 19/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0560
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0121
 384/6530 [>.............................] - ETA: 0s - loss: 0.0771
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0121
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0818
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0120
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0823
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0120
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0795
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0119
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0780
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0120
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0769
6512/6530 [============================>.] - ETA: 0s - loss: 0.0119
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0771
6530/6530 [==============================] - 1s 155us/step - loss: 0.0120 - val_loss: 0.0087
Epoch 18/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0072
2960/6530 [============>.................] - ETA: 0s - loss: 0.0761
 368/6530 [>.............................] - ETA: 0s - loss: 0.0110
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0754
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0122
3712/6530 [================>.............] - ETA: 0s - loss: 0.0764
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0117
4096/6530 [=================>............] - ETA: 0s - loss: 0.0773
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0117
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0784
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0114
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0775
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0114
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0780
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0111
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0783
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0112
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0779
3152/6530 [=============>................] - ETA: 0s - loss: 0.0112
6336/6530 [============================>.] - ETA: 0s - loss: 0.0778
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0113
6530/6530 [==============================] - 1s 143us/step - loss: 0.0777 - val_loss: 0.0653
Epoch 20/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0475
3856/6530 [================>.............] - ETA: 0s - loss: 0.0114
 400/6530 [>.............................] - ETA: 0s - loss: 0.0734
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0113
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0815
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0114
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0812
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0115
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0784
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0117
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0762
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0116
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0760
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0116
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0762
6336/6530 [============================>.] - ETA: 0s - loss: 0.0116
3040/6530 [============>.................] - ETA: 0s - loss: 0.0746
6530/6530 [==============================] - 1s 151us/step - loss: 0.0117 - val_loss: 0.0121
Epoch 19/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0154
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0750
 384/6530 [>.............................] - ETA: 0s - loss: 0.0113
3776/6530 [================>.............] - ETA: 0s - loss: 0.0751
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0118
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0761
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0124
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0772
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0125
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0766
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0123
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0769
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0123
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0771
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0126
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0767
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0123
6368/6530 [============================>.] - ETA: 0s - loss: 0.0767
3136/6530 [=============>................] - ETA: 0s - loss: 0.0124
6530/6530 [==============================] - 1s 141us/step - loss: 0.0767 - val_loss: 0.0643
Epoch 21/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0557
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0123
 400/6530 [>.............................] - ETA: 0s - loss: 0.0767
3824/6530 [================>.............] - ETA: 0s - loss: 0.0122
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0809
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0122
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0803
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0123
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0771
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0123
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0754
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0122
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0756
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0121
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0759
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0121
3040/6530 [============>.................] - ETA: 0s - loss: 0.0745
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0120
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0749
6530/6530 [==============================] - 1s 152us/step - loss: 0.0120 - val_loss: 0.0091
Epoch 20/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0128
3792/6530 [================>.............] - ETA: 0s - loss: 0.0751
 352/6530 [>.............................] - ETA: 0s - loss: 0.0103
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0763
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0112
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0772
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0107
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0765
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0113
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0767
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0112
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0767
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0112
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0763
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0112
6368/6530 [============================>.] - ETA: 0s - loss: 0.0763
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0113
6530/6530 [==============================] - 1s 142us/step - loss: 0.0763 - val_loss: 0.0649

3136/6530 [=============>................] - ETA: 0s - loss: 0.0115
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0112
3840/6530 [================>.............] - ETA: 0s - loss: 0.0113
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0115
# training | RMSE: 0.0751, MAE: 0.0579
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.07513275798346111, 'rmse': 0.07513275798346111, 'mae': 0.05792381804211454, 'early_stop': True}
vggnet done  0

4496/6530 [===================>..........] - ETA: 0s - loss: 0.0115
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0116
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0117
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0117
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0117
6368/6530 [============================>.] - ETA: 0s - loss: 0.0119
6530/6530 [==============================] - 1s 150us/step - loss: 0.0119 - val_loss: 0.0083
Epoch 21/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0053
 368/6530 [>.............................] - ETA: 0s - loss: 0.0123
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0127
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0125
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0119
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0124
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0120
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0117
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0117
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0117
3072/6530 [=============>................] - ETA: 0s - loss: 0.0117
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0116
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0115
4016/6530 [=================>............] - ETA: 0s - loss: 0.0115
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0116
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0114
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0116
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0116
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0116
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0116
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0116
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0115
6530/6530 [==============================] - 1s 178us/step - loss: 0.0116 - val_loss: 0.0093
Epoch 22/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0140
 288/6530 [>.............................] - ETA: 1s - loss: 0.0108
 528/6530 [=>............................] - ETA: 1s - loss: 0.0120
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0122
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0119
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0112
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0111
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0110
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0108
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0108
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0107
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0107
2848/6530 [============>.................] - ETA: 0s - loss: 0.0107
3056/6530 [=============>................] - ETA: 0s - loss: 0.0109
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0109
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0110
3936/6530 [=================>............] - ETA: 0s - loss: 0.0112
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0111
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0111
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0112
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0112
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0112
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0114
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0113
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0114
6352/6530 [============================>.] - ETA: 0s - loss: 0.0114
6530/6530 [==============================] - 1s 211us/step - loss: 0.0114 - val_loss: 0.0083
Epoch 23/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0109
 240/6530 [>.............................] - ETA: 1s - loss: 0.0103
 448/6530 [=>............................] - ETA: 1s - loss: 0.0117
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0117
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0115
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0112
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0110
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0110
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0113
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0112
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0114
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0114
2848/6530 [============>.................] - ETA: 0s - loss: 0.0117
3168/6530 [=============>................] - ETA: 0s - loss: 0.0118
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0118
3824/6530 [================>.............] - ETA: 0s - loss: 0.0121
4128/6530 [=================>............] - ETA: 0s - loss: 0.0119
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0118
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0117
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0116
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0115
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0115
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0115
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0115
6448/6530 [============================>.] - ETA: 0s - loss: 0.0114
6530/6530 [==============================] - 1s 200us/step - loss: 0.0114 - val_loss: 0.0082
Epoch 24/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0137
 272/6530 [>.............................] - ETA: 1s - loss: 0.0093
 576/6530 [=>............................] - ETA: 1s - loss: 0.0114
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0119
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0114
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0110
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0111
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0112
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0112
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0110
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0111
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0111
3024/6530 [============>.................] - ETA: 0s - loss: 0.0110
3248/6530 [=============>................] - ETA: 0s - loss: 0.0110
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0111
3712/6530 [================>.............] - ETA: 0s - loss: 0.0112
3920/6530 [=================>............] - ETA: 0s - loss: 0.0112
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0115
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0114
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0115
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0116
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0114
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0113
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0113
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0114
6464/6530 [============================>.] - ETA: 0s - loss: 0.0113
6530/6530 [==============================] - 1s 205us/step - loss: 0.0113 - val_loss: 0.0083
Epoch 25/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0055
 288/6530 [>.............................] - ETA: 1s - loss: 0.0118
 576/6530 [=>............................] - ETA: 1s - loss: 0.0128
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0123
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0116
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0117
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0116
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0120
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0124
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0125
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0126
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0126
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0124
2864/6530 [============>.................] - ETA: 0s - loss: 0.0121
3120/6530 [=============>................] - ETA: 0s - loss: 0.0119
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0119
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0118
3792/6530 [================>.............] - ETA: 0s - loss: 0.0117
3984/6530 [=================>............] - ETA: 0s - loss: 0.0116
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0116
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0117
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0118
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0117
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0116
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0117
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0116
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0116
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0115
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0115
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0116
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0116
6368/6530 [============================>.] - ETA: 0s - loss: 0.0116
6530/6530 [==============================] - 2s 263us/step - loss: 0.0116 - val_loss: 0.0081
Epoch 26/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0086
 176/6530 [..............................] - ETA: 2s - loss: 0.0099
 352/6530 [>.............................] - ETA: 1s - loss: 0.0101
 528/6530 [=>............................] - ETA: 1s - loss: 0.0103
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0104
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0101
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0102
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0100
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0098
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0097
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0100
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0100
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0101
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0101
2544/6530 [==========>...................] - ETA: 1s - loss: 0.0104
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0104
2944/6530 [============>.................] - ETA: 1s - loss: 0.0104
3136/6530 [=============>................] - ETA: 0s - loss: 0.0103
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0102
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0102
3872/6530 [================>.............] - ETA: 0s - loss: 0.0102
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0102
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0105
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0106
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0107
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0107
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0107
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0106
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0106
6432/6530 [============================>.] - ETA: 0s - loss: 0.0107
6530/6530 [==============================] - 2s 241us/step - loss: 0.0107 - val_loss: 0.0094
Epoch 27/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0079
 240/6530 [>.............................] - ETA: 1s - loss: 0.0108
 464/6530 [=>............................] - ETA: 1s - loss: 0.0110
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0102
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0099
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0106
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0116
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0113
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0113
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0111
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0111
2992/6530 [============>.................] - ETA: 0s - loss: 0.0113
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0113
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0112
3824/6530 [================>.............] - ETA: 0s - loss: 0.0113
4048/6530 [=================>............] - ETA: 0s - loss: 0.0112
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0112
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0113
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0112
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0113
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0113
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0112
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0111
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0111
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0110
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0112
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0113
6320/6530 [============================>.] - ETA: 0s - loss: 0.0113
6530/6530 [==============================] - 1s 228us/step - loss: 0.0112 - val_loss: 0.0089

# training | RMSE: 0.0871, MAE: 0.0681
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16597532291562125}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2703434644417436}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.08708573766499171, 'rmse': 0.08708573766499171, 'mae': 0.06808612358293353, 'early_stop': False}
vggnet done  2
all of workers have been done
calculation finished
#1 epoch=27.0 loss={'loss': 0.06856299539712692, 'rmse': 0.06856299539712692, 'mae': 0.05357597171061813, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32305615123930476}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#0 epoch=27.0 loss={'loss': 0.07513275798346111, 'rmse': 0.07513275798346111, 'mae': 0.05792381804211454, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#2 epoch=27.0 loss={'loss': 0.08708573766499171, 'rmse': 0.08708573766499171, 'mae': 0.06808612358293353, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 41, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16597532291562125}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2703434644417436}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
get a list [results] of length 120
get a list [loss] of length 3
get a list [val_loss] of length 3
length of indices is (1, 0, 2)
length of indices is 3
length of T is 3
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]] 

*** 1.0 configurations x 81.0 iterations each

3 | Thu Sep 27 23:07:01 2018 | lowest loss so far: 0.0686 (run 1)

vggnet done  1
vggnet done  2
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  93 | activation: relu    | extras: None 
layer 2 | size:  84 | activation: relu    | extras: batchnorm 
layer 3 | size:  83 | activation: sigmoid | extras: dropout - rate: 18.6% 
layer 4 | size:  45 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 5:52 - loss: 0.6924
 224/6530 [>.............................] - ETA: 25s - loss: 0.2378 
 432/6530 [>.............................] - ETA: 13s - loss: 0.2078
 656/6530 [==>...........................] - ETA: 9s - loss: 0.1967 
 864/6530 [==>...........................] - ETA: 7s - loss: 0.1874
1056/6530 [===>..........................] - ETA: 5s - loss: 0.1822
1216/6530 [====>.........................] - ETA: 5s - loss: 0.1769
1392/6530 [=====>........................] - ETA: 4s - loss: 0.1739
1584/6530 [======>.......................] - ETA: 4s - loss: 0.1680
1792/6530 [=======>......................] - ETA: 3s - loss: 0.1656
2016/6530 [========>.....................] - ETA: 3s - loss: 0.1624
2272/6530 [=========>....................] - ETA: 2s - loss: 0.1595
2544/6530 [==========>...................] - ETA: 2s - loss: 0.1584
2832/6530 [============>.................] - ETA: 2s - loss: 0.1552
3088/6530 [=============>................] - ETA: 1s - loss: 0.1523
3360/6530 [==============>...............] - ETA: 1s - loss: 0.1501
3600/6530 [===============>..............] - ETA: 1s - loss: 0.1489
3872/6530 [================>.............] - ETA: 1s - loss: 0.1476
4160/6530 [==================>...........] - ETA: 1s - loss: 0.1459
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1451
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1429
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1414
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1402
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1387
5968/6530 [==========================>...] - ETA: 0s - loss: 0.1377
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1367
6480/6530 [============================>.] - ETA: 0s - loss: 0.1355
6530/6530 [==============================] - 2s 356us/step - loss: 0.1355 - val_loss: 0.1005
Epoch 2/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1029
 224/6530 [>.............................] - ETA: 1s - loss: 0.1018
 432/6530 [>.............................] - ETA: 1s - loss: 0.1048
 656/6530 [==>...........................] - ETA: 1s - loss: 0.1109
 880/6530 [===>..........................] - ETA: 1s - loss: 0.1105
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1126
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1109
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1088
1712/6530 [======>.......................] - ETA: 1s - loss: 0.1093
1888/6530 [=======>......................] - ETA: 1s - loss: 0.1082
2096/6530 [========>.....................] - ETA: 1s - loss: 0.1081
2320/6530 [=========>....................] - ETA: 1s - loss: 0.1077
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1084
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1075
3040/6530 [============>.................] - ETA: 0s - loss: 0.1056
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1053
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1059
3712/6530 [================>.............] - ETA: 0s - loss: 0.1056
3952/6530 [=================>............] - ETA: 0s - loss: 0.1057
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1061
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1060
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1063
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1058
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1057
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1059
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1054
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1050
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1048
6352/6530 [============================>.] - ETA: 0s - loss: 0.1047
6530/6530 [==============================] - 2s 235us/step - loss: 0.1047 - val_loss: 0.0877
Epoch 3/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0949
 304/6530 [>.............................] - ETA: 1s - loss: 0.0910
 608/6530 [=>............................] - ETA: 1s - loss: 0.1000
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1017
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1036
1488/6530 [=====>........................] - ETA: 0s - loss: 0.1015
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1001
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0990
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0988
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0995
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0982
3072/6530 [=============>................] - ETA: 0s - loss: 0.0967
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0964
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0968
3920/6530 [=================>............] - ETA: 0s - loss: 0.0968
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0970
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0978
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0975
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0975
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0975
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0972
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0967
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0965
6530/6530 [==============================] - 1s 185us/step - loss: 0.0963 - val_loss: 0.0783
Epoch 4/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0661
 272/6530 [>.............................] - ETA: 1s - loss: 0.0893
 496/6530 [=>............................] - ETA: 1s - loss: 0.0965
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0981
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0970
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0985
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0969
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0954
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0949
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0937
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0937
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0933
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0935
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0935
2912/6530 [============>.................] - ETA: 0s - loss: 0.0918
3120/6530 [=============>................] - ETA: 0s - loss: 0.0908
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0908
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0921
3792/6530 [================>.............] - ETA: 0s - loss: 0.0915
4016/6530 [=================>............] - ETA: 0s - loss: 0.0922
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0919
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0928
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0929
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0925
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0926
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0923
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0924
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0919
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0916
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0916
6496/6530 [============================>.] - ETA: 0s - loss: 0.0912
6530/6530 [==============================] - 2s 247us/step - loss: 0.0913 - val_loss: 0.0766
Epoch 5/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0668
 288/6530 [>.............................] - ETA: 1s - loss: 0.0801
 528/6530 [=>............................] - ETA: 1s - loss: 0.0864
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0920
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0922
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0905
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0897
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0890
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0894
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0894
2976/6530 [============>.................] - ETA: 0s - loss: 0.0879
3248/6530 [=============>................] - ETA: 0s - loss: 0.0875
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0885
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0885
3888/6530 [================>.............] - ETA: 0s - loss: 0.0885
4080/6530 [=================>............] - ETA: 0s - loss: 0.0892
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0893
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0898
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0895
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0895
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0894
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0895
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0891
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0888
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0889
6530/6530 [==============================] - 1s 203us/step - loss: 0.0886 - val_loss: 0.0729
Epoch 6/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0561
 272/6530 [>.............................] - ETA: 1s - loss: 0.0830
 496/6530 [=>............................] - ETA: 1s - loss: 0.0891
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0907
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0908
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0903
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0910
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0897
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0893
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0891
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0878
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0880
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0879
2576/6530 [==========>...................] - ETA: 1s - loss: 0.0883
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0876
2992/6530 [============>.................] - ETA: 0s - loss: 0.0860
3200/6530 [=============>................] - ETA: 0s - loss: 0.0852
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0857
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0861
3792/6530 [================>.............] - ETA: 0s - loss: 0.0859
3952/6530 [=================>............] - ETA: 0s - loss: 0.0865
4128/6530 [=================>............] - ETA: 0s - loss: 0.0872
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0876
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0882
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0881
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0877
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0878
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0878
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0881
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0880
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0876
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0874
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0872
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0873
6432/6530 [============================>.] - ETA: 0s - loss: 0.0870
6530/6530 [==============================] - 2s 284us/step - loss: 0.0872 - val_loss: 0.0716
Epoch 7/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0659
 192/6530 [..............................] - ETA: 1s - loss: 0.0802
 384/6530 [>.............................] - ETA: 1s - loss: 0.0821
 592/6530 [=>............................] - ETA: 1s - loss: 0.0844
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0890
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0880
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0894
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0874
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0861
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0852
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0854
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0853
3008/6530 [============>.................] - ETA: 0s - loss: 0.0836
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0832
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0839
3744/6530 [================>.............] - ETA: 0s - loss: 0.0841
3968/6530 [=================>............] - ETA: 0s - loss: 0.0844
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0851
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0854
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0857
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0852
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0853
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0854
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0849
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0847
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0847
6528/6530 [============================>.] - ETA: 0s - loss: 0.0846
6530/6530 [==============================] - 1s 212us/step - loss: 0.0846 - val_loss: 0.0708
Epoch 8/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0618
 288/6530 [>.............................] - ETA: 1s - loss: 0.0822
 528/6530 [=>............................] - ETA: 1s - loss: 0.0858
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0887
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0870
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0882
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0860
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0843
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0835
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0834
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0836
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0837
3024/6530 [============>.................] - ETA: 0s - loss: 0.0822
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0815
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0823
3888/6530 [================>.............] - ETA: 0s - loss: 0.0824
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0830
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0838
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0835
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0835
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0836
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0838
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0836
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0833
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0833
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0830
6368/6530 [============================>.] - ETA: 0s - loss: 0.0833
6530/6530 [==============================] - 1s 222us/step - loss: 0.0833 - val_loss: 0.0663
Epoch 9/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0477
 240/6530 [>.............................] - ETA: 1s - loss: 0.0736
 464/6530 [=>............................] - ETA: 1s - loss: 0.0836
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0876
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0853
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0862
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0852
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0844
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0833
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0833
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0834
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0835
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0829
2960/6530 [============>.................] - ETA: 0s - loss: 0.0818
3136/6530 [=============>................] - ETA: 0s - loss: 0.0810
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0810
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0818
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0821
3904/6530 [================>.............] - ETA: 0s - loss: 0.0823
4128/6530 [=================>............] - ETA: 0s - loss: 0.0829
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0830
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0836
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0833
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0832
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0831
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0831
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0828
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0826
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0824
6336/6530 [============================>.] - ETA: 0s - loss: 0.0825
6512/6530 [============================>.] - ETA: 0s - loss: 0.0826
6530/6530 [==============================] - 2s 250us/step - loss: 0.0825 - val_loss: 0.0689
Epoch 10/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0519
 192/6530 [..............................] - ETA: 1s - loss: 0.0752
 384/6530 [>.............................] - ETA: 1s - loss: 0.0779
 576/6530 [=>............................] - ETA: 1s - loss: 0.0821
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0857
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0844
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0863
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0852
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0843
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0826
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0823
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0822
2832/6530 [============>.................] - ETA: 0s - loss: 0.0814
3168/6530 [=============>................] - ETA: 0s - loss: 0.0798
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0805
3808/6530 [================>.............] - ETA: 0s - loss: 0.0809
4128/6530 [=================>............] - ETA: 0s - loss: 0.0817
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0824
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0823
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0820
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0820
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0819
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0816
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0811
6464/6530 [============================>.] - ETA: 0s - loss: 0.0811
6530/6530 [==============================] - 1s 197us/step - loss: 0.0814 - val_loss: 0.0659
Epoch 11/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0534
 336/6530 [>.............................] - ETA: 1s - loss: 0.0792
 592/6530 [=>............................] - ETA: 1s - loss: 0.0805
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0838
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0837
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0848
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0829
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0822
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0810
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0801
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0804
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0804
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0805
2976/6530 [============>.................] - ETA: 0s - loss: 0.0787
3200/6530 [=============>................] - ETA: 0s - loss: 0.0781
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0787
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0794
3792/6530 [================>.............] - ETA: 0s - loss: 0.0790
3984/6530 [=================>............] - ETA: 0s - loss: 0.0797
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0802
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0804
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0811
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0809
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0806
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0807
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0806
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0804
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0803
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0800
6368/6530 [============================>.] - ETA: 0s - loss: 0.0800
6530/6530 [==============================] - 2s 242us/step - loss: 0.0801 - val_loss: 0.0669
Epoch 12/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0597
 240/6530 [>.............................] - ETA: 1s - loss: 0.0723
 432/6530 [>.............................] - ETA: 1s - loss: 0.0773
 624/6530 [=>............................] - ETA: 1s - loss: 0.0813
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0823
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0807
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0830
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0824
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0816
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0811
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0802
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0790
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0795
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0791
2608/6530 [==========>...................] - ETA: 1s - loss: 0.0791
2848/6530 [============>.................] - ETA: 1s - loss: 0.0785
3072/6530 [=============>................] - ETA: 0s - loss: 0.0775
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0773
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0781
3712/6530 [================>.............] - ETA: 0s - loss: 0.0782
3920/6530 [=================>............] - ETA: 0s - loss: 0.0787
4128/6530 [=================>............] - ETA: 0s - loss: 0.0792
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0795
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0799
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0795
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0796
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0795
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0800
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0799
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0794
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0793
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0794
6496/6530 [============================>.] - ETA: 0s - loss: 0.0793
6530/6530 [==============================] - 2s 262us/step - loss: 0.0794 - val_loss: 0.0663
Epoch 13/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0634
 304/6530 [>.............................] - ETA: 1s - loss: 0.0763
 608/6530 [=>............................] - ETA: 1s - loss: 0.0801
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0810
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0821
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0817
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0801
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0785
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0790
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0795
2912/6530 [============>.................] - ETA: 0s - loss: 0.0781
3200/6530 [=============>................] - ETA: 0s - loss: 0.0771
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0780
3712/6530 [================>.............] - ETA: 0s - loss: 0.0780
3952/6530 [=================>............] - ETA: 0s - loss: 0.0784
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0788
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0797
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0798
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0792
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0794
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0794
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0796
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0794
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0791
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0790
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0792
6464/6530 [============================>.] - ETA: 0s - loss: 0.0789
6530/6530 [==============================] - 1s 218us/step - loss: 0.0790 - val_loss: 0.0660
Epoch 14/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0587
 208/6530 [..............................] - ETA: 1s - loss: 0.0766
 400/6530 [>.............................] - ETA: 1s - loss: 0.0786
 608/6530 [=>............................] - ETA: 1s - loss: 0.0831
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0844
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0837
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0838
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0830
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0817
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0797
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0797
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0796
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0793
3120/6530 [=============>................] - ETA: 0s - loss: 0.0775
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0784
3728/6530 [================>.............] - ETA: 0s - loss: 0.0783
4032/6530 [=================>............] - ETA: 0s - loss: 0.0793
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0794
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0799
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0796
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0796
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0802
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0799
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0794
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0793
6336/6530 [============================>.] - ETA: 0s - loss: 0.0795
6530/6530 [==============================] - 1s 212us/step - loss: 0.0795 - val_loss: 0.0640
Epoch 15/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0780
 208/6530 [..............................] - ETA: 1s - loss: 0.0760
 416/6530 [>.............................] - ETA: 1s - loss: 0.0773
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0831
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0817
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0823
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0807
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0808
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0798
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0794
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0794
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0795
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0795
2896/6530 [============>.................] - ETA: 0s - loss: 0.0782
3072/6530 [=============>................] - ETA: 0s - loss: 0.0776
3248/6530 [=============>................] - ETA: 0s - loss: 0.0773
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0779
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0781
3856/6530 [================>.............] - ETA: 0s - loss: 0.0782
4080/6530 [=================>............] - ETA: 0s - loss: 0.0791
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0790
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0799
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0796
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0790
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0796
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0795
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0793
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0790
6416/6530 [============================>.] - ETA: 0s - loss: 0.0790
6530/6530 [==============================] - 2s 237us/step - loss: 0.0792 - val_loss: 0.0639
Epoch 16/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0515
 208/6530 [..............................] - ETA: 1s - loss: 0.0720
 384/6530 [>.............................] - ETA: 1s - loss: 0.0738
 576/6530 [=>............................] - ETA: 1s - loss: 0.0783
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0806
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0806
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0817
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0807
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0787
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0787
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0774
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0774
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0774
2560/6530 [==========>...................] - ETA: 1s - loss: 0.0782
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0777
3088/6530 [=============>................] - ETA: 0s - loss: 0.0765
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0763
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0771
3840/6530 [================>.............] - ETA: 0s - loss: 0.0772
4080/6530 [=================>............] - ETA: 0s - loss: 0.0780
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0785
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0790
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0786
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0787
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0788
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0787
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0782
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0782
6528/6530 [============================>.] - ETA: 0s - loss: 0.0782
6530/6530 [==============================] - 1s 228us/step - loss: 0.0782 - val_loss: 0.0628
Epoch 17/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0616
 288/6530 [>.............................] - ETA: 1s - loss: 0.0735
 608/6530 [=>............................] - ETA: 1s - loss: 0.0806
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0807
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0826
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0802
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0785
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0782
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0781
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0781
2896/6530 [============>.................] - ETA: 0s - loss: 0.0771
3120/6530 [=============>................] - ETA: 0s - loss: 0.0762
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0761
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0772
3824/6530 [================>.............] - ETA: 0s - loss: 0.0770
4080/6530 [=================>............] - ETA: 0s - loss: 0.0779
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0780
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0786
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0781
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0782
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0788
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0784
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0782
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0782
6480/6530 [============================>.] - ETA: 0s - loss: 0.0780
6530/6530 [==============================] - 1s 199us/step - loss: 0.0783 - val_loss: 0.0659
Epoch 18/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0526
 272/6530 [>.............................] - ETA: 1s - loss: 0.0754
 480/6530 [=>............................] - ETA: 1s - loss: 0.0793
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0819
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0807
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0803
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0813
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0788
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0788
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0779
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0773
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0772
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0770
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0772
2720/6530 [===========>..................] - ETA: 1s - loss: 0.0773
2992/6530 [============>.................] - ETA: 0s - loss: 0.0758
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0756
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0768
3888/6530 [================>.............] - ETA: 0s - loss: 0.0770
4112/6530 [=================>............] - ETA: 0s - loss: 0.0776
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0779
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0786
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0783
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0779
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0780
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0785
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0781
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0776
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0774
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0775
6432/6530 [============================>.] - ETA: 0s - loss: 0.0773
6530/6530 [==============================] - 2s 251us/step - loss: 0.0774 - val_loss: 0.0627
Epoch 19/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0620
 224/6530 [>.............................] - ETA: 1s - loss: 0.0700
 448/6530 [=>............................] - ETA: 1s - loss: 0.0783
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0826
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0805
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0814
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0795
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0789
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0769
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0770
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0776
2832/6530 [============>.................] - ETA: 0s - loss: 0.0769
3104/6530 [=============>................] - ETA: 0s - loss: 0.0758
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0753
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0764
3920/6530 [=================>............] - ETA: 0s - loss: 0.0767
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0771
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0781
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0780
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0777
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0778
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0779
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0776
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0775
6530/6530 [==============================] - 1s 195us/step - loss: 0.0775 - val_loss: 0.0636
Epoch 20/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0551
 320/6530 [>.............................] - ETA: 1s - loss: 0.0725
 608/6530 [=>............................] - ETA: 1s - loss: 0.0796
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0792
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0806
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0781
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0771
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0763
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0760
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0769
2912/6530 [============>.................] - ETA: 0s - loss: 0.0751
3184/6530 [=============>................] - ETA: 0s - loss: 0.0744
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0751
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0755
3904/6530 [================>.............] - ETA: 0s - loss: 0.0754
4128/6530 [=================>............] - ETA: 0s - loss: 0.0760
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0763
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0769
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0764
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0766
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0771
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0769
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0765
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0763
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0766
6480/6530 [============================>.] - ETA: 0s - loss: 0.0763
6530/6530 [==============================] - 1s 210us/step - loss: 0.0765 - val_loss: 0.0637
Epoch 21/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0639
 208/6530 [..............................] - ETA: 1s - loss: 0.0748
 416/6530 [>.............................] - ETA: 1s - loss: 0.0761
 608/6530 [=>............................] - ETA: 1s - loss: 0.0807
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0808
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0797
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0798
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0777
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0766
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0760
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0760
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0761
3136/6530 [=============>................] - ETA: 0s - loss: 0.0746
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0749
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0757
3904/6530 [================>.............] - ETA: 0s - loss: 0.0757
4128/6530 [=================>............] - ETA: 0s - loss: 0.0763
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0767
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0773
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0766
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0767
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0772
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0770
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0764
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0762
6320/6530 [============================>.] - ETA: 0s - loss: 0.0764
6512/6530 [============================>.] - ETA: 0s - loss: 0.0764
6530/6530 [==============================] - 1s 216us/step - loss: 0.0764 - val_loss: 0.0620
Epoch 22/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0616
 240/6530 [>.............................] - ETA: 1s - loss: 0.0679
 496/6530 [=>............................] - ETA: 1s - loss: 0.0787
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0784
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0789
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0793
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0774
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0769
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0752
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0751
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0761
2912/6530 [============>.................] - ETA: 0s - loss: 0.0745
3232/6530 [=============>................] - ETA: 0s - loss: 0.0737
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0745
3824/6530 [================>.............] - ETA: 0s - loss: 0.0748
4128/6530 [=================>............] - ETA: 0s - loss: 0.0754
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0763
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0761
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0761
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0762
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0765
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0762
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0757
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0758
6530/6530 [==============================] - 1s 195us/step - loss: 0.0758 - val_loss: 0.0636
Epoch 23/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0594
 224/6530 [>.............................] - ETA: 1s - loss: 0.0713
 464/6530 [=>............................] - ETA: 1s - loss: 0.0789
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0818
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0792
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0808
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0801
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0787
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0767
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0760
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0762
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0764
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0760
2864/6530 [============>.................] - ETA: 0s - loss: 0.0755
3024/6530 [============>.................] - ETA: 0s - loss: 0.0748
3216/6530 [=============>................] - ETA: 0s - loss: 0.0744
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0749
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0753
3824/6530 [================>.............] - ETA: 0s - loss: 0.0752
4000/6530 [=================>............] - ETA: 0s - loss: 0.0759
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0761
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0765
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0768
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0763
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0764
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0764
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0763
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0760
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0759
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0759
6400/6530 [============================>.] - ETA: 0s - loss: 0.0758
6530/6530 [==============================] - 2s 253us/step - loss: 0.0759 - val_loss: 0.0631
Epoch 24/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0555
 224/6530 [>.............................] - ETA: 1s - loss: 0.0685
 416/6530 [>.............................] - ETA: 1s - loss: 0.0722
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0791
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0779
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0794
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0770
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0760
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0756
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0760
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0764
2912/6530 [============>.................] - ETA: 0s - loss: 0.0752
3168/6530 [=============>................] - ETA: 0s - loss: 0.0746
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0749
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0751
3808/6530 [================>.............] - ETA: 0s - loss: 0.0752
3968/6530 [=================>............] - ETA: 0s - loss: 0.0753
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0758
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0762
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0769
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0768
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0763
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0764
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0763
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0767
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0763
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0759
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0757
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0757
6448/6530 [============================>.] - ETA: 0s - loss: 0.0756
6530/6530 [==============================] - 2s 243us/step - loss: 0.0757 - val_loss: 0.0617
Epoch 25/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0695
 192/6530 [..............................] - ETA: 1s - loss: 0.0716
 368/6530 [>.............................] - ETA: 1s - loss: 0.0751
 544/6530 [=>............................] - ETA: 1s - loss: 0.0767
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0809
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0788
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0807
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0795
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0775
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0771
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0759
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0755
2416/6530 [==========>...................] - ETA: 1s - loss: 0.0760
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0763
3024/6530 [============>.................] - ETA: 0s - loss: 0.0745
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0739
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0750
3856/6530 [================>.............] - ETA: 0s - loss: 0.0750
4080/6530 [=================>............] - ETA: 0s - loss: 0.0756
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0758
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0763
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0758
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0759
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0764
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0761
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0756
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0757
6496/6530 [============================>.] - ETA: 0s - loss: 0.0756
6530/6530 [==============================] - 1s 221us/step - loss: 0.0757 - val_loss: 0.0630
Epoch 26/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0487
 272/6530 [>.............................] - ETA: 1s - loss: 0.0739
 544/6530 [=>............................] - ETA: 1s - loss: 0.0754
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0784
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0791
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0774
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0771
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0763
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0750
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0750
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0754
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0754
2960/6530 [============>.................] - ETA: 0s - loss: 0.0739
3152/6530 [=============>................] - ETA: 0s - loss: 0.0733
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0732
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0739
3760/6530 [================>.............] - ETA: 0s - loss: 0.0739
3968/6530 [=================>............] - ETA: 0s - loss: 0.0743
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0748
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0752
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0759
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0754
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0754
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0754
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0753
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0750
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0747
6352/6530 [============================>.] - ETA: 0s - loss: 0.0749
6530/6530 [==============================] - 1s 228us/step - loss: 0.0749 - val_loss: 0.0622
Epoch 27/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0542
 256/6530 [>.............................] - ETA: 1s - loss: 0.0736
 544/6530 [=>............................] - ETA: 1s - loss: 0.0748
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0787
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0798
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0791
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0774
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0760
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0748
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0747
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0753
2848/6530 [============>.................] - ETA: 0s - loss: 0.0741
3072/6530 [=============>................] - ETA: 0s - loss: 0.0733
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0729
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0738
3712/6530 [================>.............] - ETA: 0s - loss: 0.0739
3936/6530 [=================>............] - ETA: 0s - loss: 0.0744
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0750
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0755
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0758
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0753
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0755
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0754
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0757
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0755
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0750
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0749
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0750
6480/6530 [============================>.] - ETA: 0s - loss: 0.0747
6530/6530 [==============================] - 2s 234us/step - loss: 0.0749 - val_loss: 0.0657
Epoch 28/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0571
 208/6530 [..............................] - ETA: 1s - loss: 0.0703
 448/6530 [=>............................] - ETA: 1s - loss: 0.0763
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0790
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0768
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0787
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0772
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0760
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0754
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0741
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0739
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0741
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0748
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0744
2960/6530 [============>.................] - ETA: 0s - loss: 0.0733
3184/6530 [=============>................] - ETA: 0s - loss: 0.0724
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0731
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0735
3840/6530 [================>.............] - ETA: 0s - loss: 0.0734
4048/6530 [=================>............] - ETA: 0s - loss: 0.0740
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0743
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0750
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0747
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0741
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0744
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0746
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0749
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0747
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0743
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0742
6384/6530 [============================>.] - ETA: 0s - loss: 0.0744
6530/6530 [==============================] - 2s 257us/step - loss: 0.0746 - val_loss: 0.0624
Epoch 29/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0598
 208/6530 [..............................] - ETA: 1s - loss: 0.0716
 448/6530 [=>............................] - ETA: 1s - loss: 0.0750
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0786
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0777
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0789
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0784
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0767
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0759
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0742
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0744
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0744
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0746
2832/6530 [============>.................] - ETA: 0s - loss: 0.0743
3072/6530 [=============>................] - ETA: 0s - loss: 0.0734
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0732
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0741
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0742
3856/6530 [================>.............] - ETA: 0s - loss: 0.0742
4032/6530 [=================>............] - ETA: 0s - loss: 0.0749
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0747
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0751
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0754
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0748
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0750
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0748
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0755
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0750
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0748
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0744
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0742
6320/6530 [============================>.] - ETA: 0s - loss: 0.0743
6530/6530 [==============================] - 2s 264us/step - loss: 0.0744 - val_loss: 0.0616
Epoch 30/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0374
 240/6530 [>.............................] - ETA: 1s - loss: 0.0677
 432/6530 [>.............................] - ETA: 1s - loss: 0.0743
 624/6530 [=>............................] - ETA: 1s - loss: 0.0772
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0768
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0771
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0773
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0761
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0755
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0751
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0741
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0745
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0742
2544/6530 [==========>...................] - ETA: 1s - loss: 0.0750
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0744
3008/6530 [============>.................] - ETA: 0s - loss: 0.0731
3248/6530 [=============>................] - ETA: 0s - loss: 0.0728
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0735
3760/6530 [================>.............] - ETA: 0s - loss: 0.0735
4016/6530 [=================>............] - ETA: 0s - loss: 0.0743
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0741
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0750
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0750
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0745
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0746
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0745
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0749
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0746
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0743
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0741
6336/6530 [============================>.] - ETA: 0s - loss: 0.0742
6530/6530 [==============================] - 2s 254us/step - loss: 0.0743 - val_loss: 0.0630
Epoch 31/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0552
 240/6530 [>.............................] - ETA: 1s - loss: 0.0704
 448/6530 [=>............................] - ETA: 1s - loss: 0.0758
 656/6530 [==>...........................] - ETA: 1s - loss: 0.0787
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0779
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0775
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0773
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0764
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0759
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0749
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0733
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0736
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0736
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0740
2944/6530 [============>.................] - ETA: 0s - loss: 0.0729
3152/6530 [=============>................] - ETA: 0s - loss: 0.0723
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0727
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0732
3968/6530 [=================>............] - ETA: 0s - loss: 0.0733
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0736
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0747
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0742
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0744
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0749
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0747
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0743
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0741
6336/6530 [============================>.] - ETA: 0s - loss: 0.0742
6528/6530 [============================>.] - ETA: 0s - loss: 0.0743
6530/6530 [==============================] - 2s 232us/step - loss: 0.0743 - val_loss: 0.0617
Epoch 32/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0652
 240/6530 [>.............................] - ETA: 1s - loss: 0.0696
 448/6530 [=>............................] - ETA: 1s - loss: 0.0747
 640/6530 [=>............................] - ETA: 1s - loss: 0.0778
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0788
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0785
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0787
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0770
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0761
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0747
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0744
2288/6530 [=========>....................] - ETA: 1s - loss: 0.0745
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0742
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0742
2896/6530 [============>.................] - ETA: 0s - loss: 0.0734
3072/6530 [=============>................] - ETA: 0s - loss: 0.0727
3264/6530 [=============>................] - ETA: 0s - loss: 0.0723
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0730
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0731
3776/6530 [================>.............] - ETA: 0s - loss: 0.0727
3968/6530 [=================>............] - ETA: 0s - loss: 0.0733
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0737
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0741
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0748
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0745
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0741
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0743
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0742
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0747
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0742
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0740
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0737
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0736
6416/6530 [============================>.] - ETA: 0s - loss: 0.0734
6530/6530 [==============================] - 2s 276us/step - loss: 0.0737 - val_loss: 0.0608
Epoch 33/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0546
 240/6530 [>.............................] - ETA: 1s - loss: 0.0704
 480/6530 [=>............................] - ETA: 1s - loss: 0.0769
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0798
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0764
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0789
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0778
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0761
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0760
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0746
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0737
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0736
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0739
2576/6530 [==========>...................] - ETA: 1s - loss: 0.0743
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0735
3008/6530 [============>.................] - ETA: 0s - loss: 0.0724
3200/6530 [=============>................] - ETA: 0s - loss: 0.0719
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0725
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0732
3760/6530 [================>.............] - ETA: 0s - loss: 0.0729
3936/6530 [=================>............] - ETA: 0s - loss: 0.0736
4112/6530 [=================>............] - ETA: 0s - loss: 0.0741
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0743
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0750
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0746
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0744
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0747
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0748
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0750
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0747
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0747
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0743
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0742
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0742
6496/6530 [============================>.] - ETA: 0s - loss: 0.0743
6530/6530 [==============================] - 2s 287us/step - loss: 0.0744 - val_loss: 0.0645
Epoch 34/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0498
 208/6530 [..............................] - ETA: 1s - loss: 0.0683
 400/6530 [>.............................] - ETA: 1s - loss: 0.0720
 592/6530 [=>............................] - ETA: 1s - loss: 0.0742
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0769
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0756
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0780
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0769
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0758
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0751
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0740
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0740
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0739
2544/6530 [==========>...................] - ETA: 1s - loss: 0.0745
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0739
2912/6530 [============>.................] - ETA: 0s - loss: 0.0726
3120/6530 [=============>................] - ETA: 0s - loss: 0.0720
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0718
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0728
3712/6530 [================>.............] - ETA: 0s - loss: 0.0728
3904/6530 [================>.............] - ETA: 0s - loss: 0.0729
4096/6530 [=================>............] - ETA: 0s - loss: 0.0735
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0739
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0743
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0742
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0738
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0740
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0740
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0745
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0741
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0737
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0734
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0734
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0737
6496/6530 [============================>.] - ETA: 0s - loss: 0.0735
6530/6530 [==============================] - 2s 284us/step - loss: 0.0737 - val_loss: 0.0608
Epoch 35/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0601
 272/6530 [>.............................] - ETA: 1s - loss: 0.0710
 560/6530 [=>............................] - ETA: 1s - loss: 0.0745
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0777
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0785
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0762
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0755
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0734
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0735
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0736
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0738
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0734
2992/6530 [============>.................] - ETA: 0s - loss: 0.0722
3184/6530 [=============>................] - ETA: 0s - loss: 0.0713
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0716
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0720
3744/6530 [================>.............] - ETA: 0s - loss: 0.0720
3936/6530 [=================>............] - ETA: 0s - loss: 0.0724
4096/6530 [=================>............] - ETA: 0s - loss: 0.0729
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0729
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0736
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0737
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0732
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0737
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0737
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0739
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0736
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0732
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0731
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0733
6528/6530 [============================>.] - ETA: 0s - loss: 0.0733
6530/6530 [==============================] - 2s 249us/step - loss: 0.0733 - val_loss: 0.0606
Epoch 36/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0473
 240/6530 [>.............................] - ETA: 1s - loss: 0.0648
 448/6530 [=>............................] - ETA: 1s - loss: 0.0724
 640/6530 [=>............................] - ETA: 1s - loss: 0.0756
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0761
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0758
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0759
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0742
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0738
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0719
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0721
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0722
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0728
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0725
2864/6530 [============>.................] - ETA: 0s - loss: 0.0720
3040/6530 [============>.................] - ETA: 0s - loss: 0.0711
3216/6530 [=============>................] - ETA: 0s - loss: 0.0711
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0715
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0722
3744/6530 [================>.............] - ETA: 0s - loss: 0.0717
3920/6530 [=================>............] - ETA: 0s - loss: 0.0722
4112/6530 [=================>............] - ETA: 0s - loss: 0.0726
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0731
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0736
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0733
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0730
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0734
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0733
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0738
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0733
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0731
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0729
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0728
6352/6530 [============================>.] - ETA: 0s - loss: 0.0731
6528/6530 [============================>.] - ETA: 0s - loss: 0.0732
6530/6530 [==============================] - 2s 284us/step - loss: 0.0732 - val_loss: 0.0601
Epoch 37/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0649
 192/6530 [..............................] - ETA: 1s - loss: 0.0698
 352/6530 [>.............................] - ETA: 1s - loss: 0.0719
 528/6530 [=>............................] - ETA: 1s - loss: 0.0753
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0780
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0778
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0780
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0782
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0766
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0758
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0747
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0734
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0736
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0739
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0736
2992/6530 [============>.................] - ETA: 0s - loss: 0.0721
3200/6530 [=============>................] - ETA: 0s - loss: 0.0714
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0721
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0722
3760/6530 [================>.............] - ETA: 0s - loss: 0.0723
3936/6530 [=================>............] - ETA: 0s - loss: 0.0726
4128/6530 [=================>............] - ETA: 0s - loss: 0.0730
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0734
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0742
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0739
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0737
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0740
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0739
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0744
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0741
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0735
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0734
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0735
6448/6530 [============================>.] - ETA: 0s - loss: 0.0734
6530/6530 [==============================] - 2s 276us/step - loss: 0.0736 - val_loss: 0.0617
Epoch 38/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0503
 224/6530 [>.............................] - ETA: 1s - loss: 0.0687
 448/6530 [=>............................] - ETA: 1s - loss: 0.0739
 656/6530 [==>...........................] - ETA: 1s - loss: 0.0780
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0761
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0770
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0757
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0742
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0733
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0726
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0718
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0719
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0721
2512/6530 [==========>...................] - ETA: 1s - loss: 0.0727
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0725
3056/6530 [=============>................] - ETA: 0s - loss: 0.0710
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0705
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0715
3856/6530 [================>.............] - ETA: 0s - loss: 0.0715
4096/6530 [=================>............] - ETA: 0s - loss: 0.0721
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0725
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0730
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0726
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0730
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0730
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0731
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0728
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0725
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0726
6530/6530 [==============================] - 2s 238us/step - loss: 0.0727 - val_loss: 0.0614
Epoch 39/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0475
 288/6530 [>.............................] - ETA: 1s - loss: 0.0748
 528/6530 [=>............................] - ETA: 1s - loss: 0.0749
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0780
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0760
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0776
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0758
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0752
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0734
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0729
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0726
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0728
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0730
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0727
2912/6530 [============>.................] - ETA: 0s - loss: 0.0717
3072/6530 [=============>................] - ETA: 0s - loss: 0.0711
3248/6530 [=============>................] - ETA: 0s - loss: 0.0709
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0718
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0721
3856/6530 [================>.............] - ETA: 0s - loss: 0.0719
4032/6530 [=================>............] - ETA: 0s - loss: 0.0726
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0721
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0731
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0731
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0725
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0728
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0729
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0731
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0729
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0726
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0724
6336/6530 [============================>.] - ETA: 0s - loss: 0.0727
6512/6530 [============================>.] - ETA: 0s - loss: 0.0728
6530/6530 [==============================] - 2s 266us/step - loss: 0.0728 - val_loss: 0.0602
Epoch 40/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0624
 192/6530 [..............................] - ETA: 1s - loss: 0.0706
 368/6530 [>.............................] - ETA: 1s - loss: 0.0731
 544/6530 [=>............................] - ETA: 1s - loss: 0.0744
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0773
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0753
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0774
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0763
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0745
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0740
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0727
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0725
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0728
2512/6530 [==========>...................] - ETA: 1s - loss: 0.0732
2704/6530 [===========>..................] - ETA: 1s - loss: 0.0731
2928/6530 [============>.................] - ETA: 0s - loss: 0.0718
3152/6530 [=============>................] - ETA: 0s - loss: 0.0714
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0718
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0723
3856/6530 [================>.............] - ETA: 0s - loss: 0.0725
4048/6530 [=================>............] - ETA: 0s - loss: 0.0730
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0725
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0735
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0735
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0729
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0730
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0729
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0734
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0732
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0728
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0726
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0724
6384/6530 [============================>.] - ETA: 0s - loss: 0.0725
6530/6530 [==============================] - 2s 275us/step - loss: 0.0726 - val_loss: 0.0609
Epoch 41/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0471
 192/6530 [..............................] - ETA: 1s - loss: 0.0654
 368/6530 [>.............................] - ETA: 1s - loss: 0.0698
 592/6530 [=>............................] - ETA: 1s - loss: 0.0722
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0752
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0751
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0755
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0737
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0730
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0722
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0713
2256/6530 [=========>....................] - ETA: 1s - loss: 0.0713
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0715
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0722
2960/6530 [============>.................] - ETA: 0s - loss: 0.0709
3232/6530 [=============>................] - ETA: 0s - loss: 0.0705
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0715
3712/6530 [================>.............] - ETA: 0s - loss: 0.0715
3920/6530 [=================>............] - ETA: 0s - loss: 0.0718
4112/6530 [=================>............] - ETA: 0s - loss: 0.0723
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0728
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0734
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0730
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0724
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0727
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0729
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0729
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0727
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0723
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0722
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0724
6496/6530 [============================>.] - ETA: 0s - loss: 0.0723
6530/6530 [==============================] - 2s 256us/step - loss: 0.0724 - val_loss: 0.0609

# training | RMSE: 0.0675, MAE: 0.0512
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.06745876153375946, 'rmse': 0.06745876153375946, 'mae': 0.051248630246028194, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=81.0 loss={'loss': 0.06745876153375946, 'rmse': 0.06745876153375946, 'mae': 0.051248630246028194, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
get a list [results] of length 121
get a list [loss] of length 1
get a list [val_loss] of length 1
length of indices is (0,)
length of indices is 1
length of T is 1
s=3
T is of size 34
T=[{'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3216388030084181}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2416967782581718}, 'layer_2_size': 5, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45351807633941055}, 'layer_3_size': 6, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.37809688818276255}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 23, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.40018692179574666}, 'layer_1_size': 68, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23890692753588347}, 'layer_2_size': 83, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 23, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2488149671850049}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 2, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 36, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 18, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2762703984959979}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27427562013650353}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4140653142464831}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 90, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 24, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42732504899124657}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20019437851553495}, 'layer_2_size': 65, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.28808774646849467}, 'layer_4_size': 8, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4062271963680081}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.32262807597466253}, 'layer_3_size': 79, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.18828675491346827}, 'layer_4_size': 72, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10607566151554111}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26857746513450387}, 'layer_4_size': 25, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.48790055374956387}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3000335360122979}, 'layer_5_size': 67, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2806300923523829}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48890225339762805}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.40274950378963015}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4464367148407562}, 'layer_1_size': 25, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 40, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13004315739512695}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36676658685766184}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2069230848746947}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.29629521052495933}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 100, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.16854481038475316}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1461065261872081}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3046963572135841}, 'layer_5_size': 88, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47285795476269665}, 'layer_2_size': 42, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2094503952911757}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3724373873602572}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.41014862887132686}, 'layer_1_size': 89, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 10, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38625217326660877}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11698107846478517}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4994624948580816}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43759601245045443}, 'layer_1_size': 94, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 31, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 56, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23742521546737436}, 'layer_3_size': 70, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 68, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4230964493371582}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20779557935052267}, 'layer_1_size': 20, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1919884355305059}, 'layer_3_size': 19, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48470744723662407}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 51, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 90, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1260034089426204}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 5, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18897806035083972}, 'layer_3_size': 19, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 29, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2060226759034579}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 94, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3202676902718764}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.26678947117970675}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.49082008909040964}, 'layer_2_size': 64, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4982940259391383}, 'layer_3_size': 2, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 79, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4383437949009624}, 'layer_4_size': 60, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]
newly formed T structure is:[[0, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3216388030084181}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [1, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2416967782581718}, 'layer_2_size': 5, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45351807633941055}, 'layer_3_size': 6, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.37809688818276255}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 23, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [2, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.40018692179574666}, 'layer_1_size': 68, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23890692753588347}, 'layer_2_size': 83, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 23, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [3, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2488149671850049}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 2, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 36, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [4, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 18, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [5, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2762703984959979}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27427562013650353}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [6, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4140653142464831}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 90, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 24, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [7, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42732504899124657}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20019437851553495}, 'layer_2_size': 65, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.28808774646849467}, 'layer_4_size': 8, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [8, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4062271963680081}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [9, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.32262807597466253}, 'layer_3_size': 79, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.18828675491346827}, 'layer_4_size': 72, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [10, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10607566151554111}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [11, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26857746513450387}, 'layer_4_size': 25, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.48790055374956387}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [12, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3000335360122979}, 'layer_5_size': 67, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [13, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2806300923523829}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [14, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48890225339762805}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [15, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.40274950378963015}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [16, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4464367148407562}, 'layer_1_size': 25, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 40, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13004315739512695}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [17, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [18, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36676658685766184}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2069230848746947}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [19, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.29629521052495933}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 100, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [20, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.16854481038475316}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1461065261872081}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3046963572135841}, 'layer_5_size': 88, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [21, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47285795476269665}, 'layer_2_size': 42, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2094503952911757}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3724373873602572}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [22, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.41014862887132686}, 'layer_1_size': 89, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 10, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38625217326660877}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [23, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11698107846478517}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4994624948580816}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [24, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [25, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43759601245045443}, 'layer_1_size': 94, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 31, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 56, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [26, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23742521546737436}, 'layer_3_size': 70, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 68, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4230964493371582}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [27, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20779557935052267}, 'layer_1_size': 20, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1919884355305059}, 'layer_3_size': 19, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48470744723662407}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [28, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 51, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 90, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1260034089426204}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [29, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 5, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [30, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18897806035083972}, 'layer_3_size': 19, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 29, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [31, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2060226759034579}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 94, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3202676902718764}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [32, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.26678947117970675}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.49082008909040964}, 'layer_2_size': 64, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4982940259391383}, 'layer_3_size': 2, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [33, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 79, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4383437949009624}, 'layer_4_size': 60, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]] 

*** 34 configurations x 3.0 iterations each

1 | Thu Sep 27 23:08:07 2018 | lowest loss so far: 0.0675 (run 0)

{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  68 | activation: tanh    | extras: dropout - rate: 40.0% 
layer 2 | size:  83 | activation: relu    | extras: dropout - rate: 23.9% 
layer 3 | size:  25 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 2:54 - loss: 0.7090
 736/6530 [==>...........................] - ETA: 7s - loss: 0.2919  
1600/6530 [======>.......................] - ETA: 2s - loss: 0.2299
2496/6530 [==========>...................] - ETA: 1s - loss: 0.2098
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1969
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1915
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1869
6530/6530 [==============================] - 1s 190us/step - loss: 0.1840 - val_loss: 0.1793
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size: 100 | activation: sigmoid | extras: None 
layer 2 | size:   5 | activation: relu    | extras: dropout - rate: 24.2% 
layer 3 | size:   6 | activation: relu    | extras: dropout - rate: 45.4% 
layer 4 | size:  57 | activation: sigmoid | extras: dropout - rate: 37.8% 
layer 5 | size:  23 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8efe80>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 51s - loss: 0.2368Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1671{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  11 | activation: sigmoid | extras: None 
layer 2 | size:  63 | activation: relu    | extras: None 
layer 3 | size:  38 | activation: sigmoid | extras: None 
layer 4 | size:  63 | activation: relu    | extras: dropout - rate: 32.2% 
layer 5 | size:  30 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 54s - loss: 1.4838
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1654
2688/6530 [===========>..................] - ETA: 1s - loss: 0.2474 
3072/6530 [=============>................] - ETA: 1s - loss: 0.5835 
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1678
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2381
5888/6530 [==========================>...] - ETA: 0s - loss: 0.4622
2880/6530 [============>.................] - ETA: 0s - loss: 0.1678
6530/6530 [==============================] - 1s 189us/step - loss: 0.4459 - val_loss: 0.2144
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2855
6530/6530 [==============================] - 1s 185us/step - loss: 0.2370 - val_loss: 0.2115
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2280
3936/6530 [=================>............] - ETA: 0s - loss: 0.1658
3072/6530 [=============>................] - ETA: 0s - loss: 0.2737
3072/6530 [=============>................] - ETA: 0s - loss: 0.2295
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1667
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2589
6144/6530 [===========================>..] - ETA: 0s - loss: 0.2231
6530/6530 [==============================] - 0s 17us/step - loss: 0.2579 - val_loss: 0.2032
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2350
6530/6530 [==============================] - 0s 18us/step - loss: 0.2225 - val_loss: 0.1892
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2318
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1661
3200/6530 [=============>................] - ETA: 0s - loss: 0.2288
3072/6530 [=============>................] - ETA: 0s - loss: 0.2114
6530/6530 [==============================] - 0s 56us/step - loss: 0.1651 - val_loss: 0.1594
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1556
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2211
6530/6530 [==============================] - 0s 18us/step - loss: 0.2205 - val_loss: 0.1940

6016/6530 [==========================>...] - ETA: 0s - loss: 0.2086
6530/6530 [==============================] - 0s 18us/step - loss: 0.2081 - val_loss: 0.1935

1056/6530 [===>..........................] - ETA: 0s - loss: 0.1725
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1655
3136/6530 [=============>................] - ETA: 0s - loss: 0.1649
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1640
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1643
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1635
6530/6530 [==============================] - 0s 52us/step - loss: 0.1635 - val_loss: 0.1675

# training | RMSE: 0.2501, MAE: 0.1979
worker 0  xfile  [0, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3216388030084181}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2500969479590981, 'rmse': 0.2500969479590981, 'mae': 0.1979225746937054, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  36 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8a1048>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 24s - loss: 0.7150
1408/6530 [=====>........................] - ETA: 0s - loss: 0.3934 
2784/6530 [===========>..................] - ETA: 0s - loss: 0.3200
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2939
# training | RMSE: 0.2356, MAE: 0.1933
worker 1  xfile  [1, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2416967782581718}, 'layer_2_size': 5, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45351807633941055}, 'layer_3_size': 6, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.37809688818276255}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 23, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.23556898768046652, 'rmse': 0.23556898768046652, 'mae': 0.19332082317542376, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  48 | activation: sigmoid | extras: None 
layer 2 | size:   7 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 35s - loss: 0.1363
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2785
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0916 
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0778
6530/6530 [==============================] - 0s 60us/step - loss: 0.2713 - val_loss: 0.2212
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1998
3264/6530 [=============>................] - ETA: 0s - loss: 0.0700
1408/6530 [=====>........................] - ETA: 0s - loss: 0.2134
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0647
2720/6530 [===========>..................] - ETA: 0s - loss: 0.2115
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0610
4064/6530 [=================>............] - ETA: 0s - loss: 0.2098
# training | RMSE: 0.2186, MAE: 0.1643
worker 2  xfile  [2, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.40018692179574666}, 'layer_1_size': 68, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23890692753588347}, 'layer_2_size': 83, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 23, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.21859010613333232, 'rmse': 0.21859010613333232, 'mae': 0.16433084023186068, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  38 | activation: tanh    | extras: dropout - rate: 27.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f46042905c0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 49s - loss: 0.7259
6496/6530 [============================>.] - ETA: 0s - loss: 0.0583
5344/6530 [=======================>......] - ETA: 0s - loss: 0.2046
 672/6530 [==>...........................] - ETA: 1s - loss: 0.5421 
6530/6530 [==============================] - 1s 78us/step - loss: 0.0581 - val_loss: 0.0426
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0533
1328/6530 [=====>........................] - ETA: 0s - loss: 0.3816
6530/6530 [==============================] - 0s 40us/step - loss: 0.2028 - val_loss: 0.1854
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2063
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0439
1968/6530 [========>.....................] - ETA: 0s - loss: 0.3104
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1798
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0413
2640/6530 [===========>..................] - ETA: 0s - loss: 0.2735
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1801
2976/6530 [============>.................] - ETA: 0s - loss: 0.0410
3296/6530 [==============>...............] - ETA: 0s - loss: 0.2500
3904/6530 [================>.............] - ETA: 0s - loss: 0.1783
4032/6530 [=================>............] - ETA: 0s - loss: 0.0412
3984/6530 [=================>............] - ETA: 0s - loss: 0.2357
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1791
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0415
4672/6530 [====================>.........] - ETA: 0s - loss: 0.2248
6530/6530 [==============================] - 0s 40us/step - loss: 0.1777 - val_loss: 0.1689

6144/6530 [===========================>..] - ETA: 0s - loss: 0.0417
5360/6530 [=======================>......] - ETA: 0s - loss: 0.2173
6530/6530 [==============================] - 0s 52us/step - loss: 0.0414 - val_loss: 0.0409
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0489
6048/6530 [==========================>...] - ETA: 0s - loss: 0.2111
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0423
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0399
6530/6530 [==============================] - 1s 99us/step - loss: 0.2071 - val_loss: 0.1624
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1811
3072/6530 [=============>................] - ETA: 0s - loss: 0.0397
 752/6530 [==>...........................] - ETA: 0s - loss: 0.1664
4128/6530 [=================>............] - ETA: 0s - loss: 0.0406
1456/6530 [=====>........................] - ETA: 0s - loss: 0.1614
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0409
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1604
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0412
2896/6530 [============>.................] - ETA: 0s - loss: 0.1607
6530/6530 [==============================] - 0s 50us/step - loss: 0.0409 - val_loss: 0.0407

3584/6530 [===============>..............] - ETA: 0s - loss: 0.1606
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1603
# training | RMSE: 0.2163, MAE: 0.1698
worker 0  xfile  [3, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2488149671850049}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 2, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 36, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.21626794786944162, 'rmse': 0.21626794786944162, 'mae': 0.1697984864829356, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  15 | activation: sigmoid | extras: None 
layer 2 | size:  18 | activation: relu    | extras: dropout - rate: 41.4% 
layer 3 | size:  90 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f46041a3da0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:12 - loss: 0.1015
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1609
 560/6530 [=>............................] - ETA: 2s - loss: 0.1370  
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1607
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1113
6368/6530 [============================>.] - ETA: 0s - loss: 0.1610
6530/6530 [==============================] - 0s 75us/step - loss: 0.1606 - val_loss: 0.1606
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1768
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0943
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1651
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0869
1456/6530 [=====>........................] - ETA: 0s - loss: 0.1594
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0834
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1581
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0797
2896/6530 [============>.................] - ETA: 0s - loss: 0.1587
3888/6530 [================>.............] - ETA: 0s - loss: 0.0767
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1588
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0743
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1584
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0725
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1591
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0711
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1585
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0694
6464/6530 [============================>.] - ETA: 0s - loss: 0.1588
6530/6530 [==============================] - 0s 73us/step - loss: 0.1586 - val_loss: 0.1585

6530/6530 [==============================] - 1s 127us/step - loss: 0.0686 - val_loss: 0.0466
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0610
 544/6530 [=>............................] - ETA: 0s - loss: 0.0531
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0559
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0528
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0526
2848/6530 [============>.................] - ETA: 0s - loss: 0.0521
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0523
# training | RMSE: 0.2010, MAE: 0.1635
worker 1  xfile  [4, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 18, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.20101348217515796, 'rmse': 0.20101348217515796, 'mae': 0.163454166034713, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  85 | activation: sigmoid | extras: dropout - rate: 42.7% 
layer 2 | size:  65 | activation: tanh    | extras: dropout - rate: 20.0% 
layer 3 | size:  57 | activation: sigmoid | extras: batchnorm 
layer 4 | size:   8 | activation: sigmoid | extras: dropout - rate: 28.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f4604101160>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 46s - loss: 0.5626
3952/6530 [=================>............] - ETA: 0s - loss: 0.0521
1152/6530 [====>.........................] - ETA: 2s - loss: 0.4824 
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0521
2240/6530 [=========>....................] - ETA: 1s - loss: 0.4237
# training | RMSE: 0.1967, MAE: 0.1579
worker 2  xfile  [5, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2762703984959979}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27427562013650353}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.1967168130900827, 'rmse': 0.1967168130900827, 'mae': 0.15788449282148734, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  79 | activation: sigmoid | extras: None 
layer 2 | size:  15 | activation: tanh    | extras: dropout - rate: 40.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f46042a5f98>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:00 - loss: 0.8550
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0520
 640/6530 [=>............................] - ETA: 1s - loss: 0.3012  
3328/6530 [==============>...............] - ETA: 0s - loss: 0.3646
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0517
1232/6530 [====>.........................] - ETA: 1s - loss: 0.2484
4416/6530 [===================>..........] - ETA: 0s - loss: 0.3152
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0516
1840/6530 [=======>......................] - ETA: 0s - loss: 0.2229
5440/6530 [=======================>......] - ETA: 0s - loss: 0.2772
2432/6530 [==========>...................] - ETA: 0s - loss: 0.2064
6530/6530 [==============================] - 1s 95us/step - loss: 0.0514 - val_loss: 0.0439

6464/6530 [============================>.] - ETA: 0s - loss: 0.2466Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0603
3024/6530 [============>.................] - ETA: 0s - loss: 0.1981
 576/6530 [=>............................] - ETA: 0s - loss: 0.0521
6530/6530 [==============================] - 1s 128us/step - loss: 0.2448 - val_loss: 0.0799
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0897
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1946
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0502
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0810
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1904
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0475
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0768
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1864
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0480
3136/6530 [=============>................] - ETA: 0s - loss: 0.0737
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1830
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0476
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0731
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1811
3200/6530 [=============>................] - ETA: 0s - loss: 0.0470
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0717
6528/6530 [============================>.] - ETA: 0s - loss: 0.1804
3728/6530 [================>.............] - ETA: 0s - loss: 0.0474
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0716
6530/6530 [==============================] - 1s 114us/step - loss: 0.1804 - val_loss: 0.1699
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1729
6530/6530 [==============================] - 0s 52us/step - loss: 0.0713 - val_loss: 0.0695
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0841
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0474
 592/6530 [=>............................] - ETA: 0s - loss: 0.1655
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0774
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0477
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1574
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0739
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0477
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1531
3200/6530 [=============>................] - ETA: 0s - loss: 0.0719
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0478
2384/6530 [=========>....................] - ETA: 0s - loss: 0.1531
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0714
6384/6530 [============================>.] - ETA: 0s - loss: 0.0477
2928/6530 [============>.................] - ETA: 0s - loss: 0.1529
6530/6530 [==============================] - 1s 100us/step - loss: 0.0475 - val_loss: 0.0401

5248/6530 [=======================>......] - ETA: 0s - loss: 0.0705
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1524
6336/6530 [============================>.] - ETA: 0s - loss: 0.0705
6530/6530 [==============================] - 0s 51us/step - loss: 0.0702 - val_loss: 0.0750

4112/6530 [=================>............] - ETA: 0s - loss: 0.1512
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1500
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1497
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1501
6528/6530 [============================>.] - ETA: 0s - loss: 0.1494
6530/6530 [==============================] - 1s 89us/step - loss: 0.1494 - val_loss: 0.1450
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1593
 608/6530 [=>............................] - ETA: 0s - loss: 0.1397
1104/6530 [====>.........................] - ETA: 0s - loss: 0.1392
1680/6530 [======>.......................] - ETA: 0s - loss: 0.1374
2256/6530 [=========>....................] - ETA: 0s - loss: 0.1358
2864/6530 [============>.................] - ETA: 0s - loss: 0.1352
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1345
# training | RMSE: 0.1974, MAE: 0.1583
worker 0  xfile  [6, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4140653142464831}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 90, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 24, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.19736924470129383, 'rmse': 0.19736924470129383, 'mae': 0.15826095346210023, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  69 | activation: sigmoid | extras: None 
layer 2 | size:  32 | activation: tanh    | extras: None 
layer 3 | size:  79 | activation: tanh    | extras: dropout - rate: 32.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f4607d01898>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 21s - loss: 0.1898
4064/6530 [=================>............] - ETA: 0s - loss: 0.1363
1856/6530 [=======>......................] - ETA: 0s - loss: 0.2872 
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1357
# training | RMSE: 0.2739, MAE: 0.2226
worker 1  xfile  [7, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42732504899124657}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20019437851553495}, 'layer_2_size': 65, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.28808774646849467}, 'layer_4_size': 8, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2738646836604897, 'rmse': 0.2738646836604897, 'mae': 0.2226255450048799, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  96 | activation: tanh    | extras: dropout - rate: 10.6% 
layer 2 | size:  78 | activation: tanh    | extras: None 
layer 3 | size:  76 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45fc7439e8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 46s - loss: 0.8181
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2536
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1347
1088/6530 [===>..........................] - ETA: 1s - loss: 0.4009 
5312/6530 [=======================>......] - ETA: 0s - loss: 0.2489
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1340
2176/6530 [========>.....................] - ETA: 0s - loss: 0.3113
6464/6530 [============================>.] - ETA: 0s - loss: 0.1344
6530/6530 [==============================] - 0s 67us/step - loss: 0.2413 - val_loss: 0.2152
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2505
3264/6530 [=============>................] - ETA: 0s - loss: 0.2690
6530/6530 [==============================] - 1s 90us/step - loss: 0.1344 - val_loss: 0.1292

1856/6530 [=======>......................] - ETA: 0s - loss: 0.1859
4320/6530 [==================>...........] - ETA: 0s - loss: 0.2454
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1833
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2268
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1835
6400/6530 [============================>.] - ETA: 0s - loss: 0.2147
6530/6530 [==============================] - 0s 30us/step - loss: 0.1852 - val_loss: 0.2405
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2584
6530/6530 [==============================] - 1s 90us/step - loss: 0.2131 - val_loss: 0.2406
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2604
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1795
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1478
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1792
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1422
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1776
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1385
6530/6530 [==============================] - 0s 30us/step - loss: 0.1761 - val_loss: 0.4799

4544/6530 [===================>..........] - ETA: 0s - loss: 0.1359
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1336
6530/6530 [==============================] - 0s 47us/step - loss: 0.1324 - val_loss: 0.1441
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1211
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1192
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1202
3104/6530 [=============>................] - ETA: 0s - loss: 0.1201
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1209
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1198
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1186
6530/6530 [==============================] - 0s 53us/step - loss: 0.1178 - val_loss: 0.1245

# training | RMSE: 0.1643, MAE: 0.1258
worker 2  xfile  [8, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4062271963680081}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.16433017487956403, 'rmse': 0.16433017487956403, 'mae': 0.12580216675043382, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  15 | activation: tanh    | extras: batchnorm 
layer 2 | size:  27 | activation: relu    | extras: batchnorm 
layer 3 | size:  23 | activation: sigmoid | extras: None 
layer 4 | size:  25 | activation: tanh    | extras: dropout - rate: 26.9% 
layer 5 | size:  24 | activation: sigmoid | extras: dropout - rate: 48.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f46040b0748>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 3:36 - loss: 0.2861
 192/6530 [..............................] - ETA: 19s - loss: 0.2501 
# training | RMSE: 0.5185, MAE: 0.4784
worker 0  xfile  [9, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.32262807597466253}, 'layer_3_size': 79, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.18828675491346827}, 'layer_4_size': 72, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.5184734551897187, 'rmse': 0.5184734551897187, 'mae': 0.47839814348815396, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  96 | activation: tanh    | extras: None 
layer 2 | size:  30 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  99 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45cd57fc88>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 39s - loss: 0.8572
 368/6530 [>.............................] - ETA: 10s - loss: 0.2191
1088/6530 [===>..........................] - ETA: 2s - loss: 0.2613 
 592/6530 [=>............................] - ETA: 6s - loss: 0.2144 
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1796
 832/6530 [==>...........................] - ETA: 5s - loss: 0.2065
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1469
1072/6530 [===>..........................] - ETA: 4s - loss: 0.2002
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1300
1312/6530 [=====>........................] - ETA: 3s - loss: 0.1920
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1175
1552/6530 [======>.......................] - ETA: 2s - loss: 0.1865
1792/6530 [=======>......................] - ETA: 2s - loss: 0.1838
6530/6530 [==============================] - 1s 111us/step - loss: 0.1123 - val_loss: 0.1204
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0801
2032/6530 [========>.....................] - ETA: 2s - loss: 0.1819
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0562
2288/6530 [=========>....................] - ETA: 1s - loss: 0.1791
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0567
2544/6530 [==========>...................] - ETA: 1s - loss: 0.1785
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0549
2816/6530 [===========>..................] - ETA: 1s - loss: 0.1756
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0546
3088/6530 [=============>................] - ETA: 1s - loss: 0.1726
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0531
3344/6530 [==============>...............] - ETA: 1s - loss: 0.1710
6530/6530 [==============================] - 0s 45us/step - loss: 0.0525 - val_loss: 0.0505
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0634
3600/6530 [===============>..............] - ETA: 1s - loss: 0.1700
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0413
3840/6530 [================>.............] - ETA: 0s - loss: 0.1692
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0426
4096/6530 [=================>............] - ETA: 0s - loss: 0.1689
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0418
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1679
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0418
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1678
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0407
# training | RMSE: 0.1520, MAE: 0.1203
worker 1  xfile  [10, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10607566151554111}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.15199125209535203, 'rmse': 0.15199125209535203, 'mae': 0.1202789432087981, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  90 | activation: relu    | extras: batchnorm 
layer 2 | size:  64 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  57 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f45e0225a58>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 29s - loss: 1.5843
6530/6530 [==============================] - 0s 46us/step - loss: 0.0408 - val_loss: 0.0358

4864/6530 [=====================>........] - ETA: 0s - loss: 0.1665
2176/6530 [========>.....................] - ETA: 1s - loss: 1.2824 
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1653
4224/6530 [==================>...........] - ETA: 0s - loss: 0.9815
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1645
6272/6530 [===========================>..] - ETA: 0s - loss: 0.7347
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1634
6530/6530 [==============================] - 1s 125us/step - loss: 0.7121 - val_loss: 0.1689

5856/6530 [=========================>....] - ETA: 0s - loss: 0.1631Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1593
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1625
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1527
6336/6530 [============================>.] - ETA: 0s - loss: 0.1617
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1428
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1333
6530/6530 [==============================] - 0s 31us/step - loss: 0.1284 - val_loss: 0.1116
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1094
6530/6530 [==============================] - 2s 309us/step - loss: 0.1613 - val_loss: 0.1270
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1504
# training | RMSE: 0.1766, MAE: 0.1404
worker 0  xfile  [12, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3000335360122979}, 'layer_5_size': 67, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.1765883508942092, 'rmse': 0.1765883508942092, 'mae': 0.14042368167120617, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  57 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45b4121320>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:20 - loss: 0.9623
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1122
 272/6530 [>.............................] - ETA: 1s - loss: 0.1407
 704/6530 [==>...........................] - ETA: 2s - loss: 0.4566  
3712/6530 [================>.............] - ETA: 0s - loss: 0.1079
 528/6530 [=>............................] - ETA: 1s - loss: 0.1416
1344/6530 [=====>........................] - ETA: 1s - loss: 0.2766
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1035
6530/6530 [==============================] - 0s 29us/step - loss: 0.1017 - val_loss: 0.1006

2016/6530 [========>.....................] - ETA: 0s - loss: 0.1995
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1412
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1587
1040/6530 [===>..........................] - ETA: 1s - loss: 0.1451
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1351
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1439
4096/6530 [=================>............] - ETA: 0s - loss: 0.1205
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1421
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1095
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1426
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1006
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1416
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0962
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1416
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1438
6530/6530 [==============================] - 1s 114us/step - loss: 0.0908 - val_loss: 0.0425
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0467
2896/6530 [============>.................] - ETA: 0s - loss: 0.1422
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0406
3152/6530 [=============>................] - ETA: 0s - loss: 0.1417
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0393
3408/6530 [==============>...............] - ETA: 0s - loss: 0.1421
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0392
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1422
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0391
3904/6530 [================>.............] - ETA: 0s - loss: 0.1425
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0385
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1426
4032/6530 [=================>............] - ETA: 0s - loss: 0.0385
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1426
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0383
# training | RMSE: 0.1230, MAE: 0.0965
worker 1  xfile  [13, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2806300923523829}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.12301220137446423, 'rmse': 0.12301220137446423, 'mae': 0.0965322964336906, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  27 | activation: tanh    | extras: dropout - rate: 40.3% 
layer 2 | size:  37 | activation: tanh    | extras: None 
layer 3 | size:  47 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f45e0113748>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 15s - loss: 0.7267
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1428
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0381
3968/6530 [=================>............] - ETA: 0s - loss: 0.4072 
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1424
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0376
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1424
6530/6530 [==============================] - 0s 66us/step - loss: 0.3175 - val_loss: 0.2005
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1997
6530/6530 [==============================] - 1s 78us/step - loss: 0.0374 - val_loss: 0.0365
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0418
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1425
3712/6530 [================>.............] - ETA: 0s - loss: 0.1740
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0343
6530/6530 [==============================] - 0s 14us/step - loss: 0.1734 - val_loss: 0.1662

5760/6530 [=========================>....] - ETA: 0s - loss: 0.1421Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1722
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0331
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1424
3840/6530 [================>.............] - ETA: 0s - loss: 0.1663
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0330
6530/6530 [==============================] - 0s 14us/step - loss: 0.1676 - val_loss: 0.1647

6272/6530 [===========================>..] - ETA: 0s - loss: 0.1421
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0330
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0326
6530/6530 [==============================] - 1s 203us/step - loss: 0.1418 - val_loss: 0.1217
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1336
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0323
 288/6530 [>.............................] - ETA: 1s - loss: 0.1354
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0321
 560/6530 [=>............................] - ETA: 1s - loss: 0.1366
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0316
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1375
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0315
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1415
6530/6530 [==============================] - 1s 77us/step - loss: 0.0313 - val_loss: 0.0303

1360/6530 [=====>........................] - ETA: 0s - loss: 0.1381
1616/6530 [======>.......................] - ETA: 0s - loss: 0.1386
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1379
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1370
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1384
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1398
# training | RMSE: 0.2064, MAE: 0.1612
worker 1  xfile  [15, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.40274950378963015}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.20641934432327075, 'rmse': 0.20641934432327075, 'mae': 0.16115313239987944, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  25 | activation: tanh    | extras: dropout - rate: 44.6% 
layer 2 | size:  79 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f45c4178b38>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 7s - loss: 1.0459
2896/6530 [============>.................] - ETA: 0s - loss: 0.1379
3152/6530 [=============>................] - ETA: 0s - loss: 0.1371
6530/6530 [==============================] - 0s 63us/step - loss: 0.4679 - val_loss: 0.2228
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2753
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1376
6530/6530 [==============================] - 0s 7us/step - loss: 0.2125 - val_loss: 0.1622
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1882
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1378
6530/6530 [==============================] - 0s 7us/step - loss: 0.1865 - val_loss: 0.1617

3936/6530 [=================>............] - ETA: 0s - loss: 0.1381
# training | RMSE: 0.1719, MAE: 0.1366
worker 0  xfile  [14, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48890225339762805}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1718570450195309, 'rmse': 0.1718570450195309, 'mae': 0.13656836187199206, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:   8 | activation: tanh    | extras: None 
layer 2 | size:  39 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ac124908>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 22s - loss: 0.6938
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1379
2432/6530 [==========>...................] - ETA: 0s - loss: 0.4168 
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1385
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2502
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1386
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1383
6530/6530 [==============================] - 0s 61us/step - loss: 0.1977 - val_loss: 0.0418
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0459
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1382
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0418
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1383
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0414
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1382
6530/6530 [==============================] - 0s 23us/step - loss: 0.0409 - val_loss: 0.0412
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0395
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1385
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0406
6320/6530 [============================>.] - ETA: 0s - loss: 0.1382
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0412
6530/6530 [==============================] - 0s 23us/step - loss: 0.0408 - val_loss: 0.0412

6530/6530 [==============================] - 1s 202us/step - loss: 0.1381 - val_loss: 0.1186

# training | RMSE: 0.2040, MAE: 0.1626
worker 1  xfile  [16, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4464367148407562}, 'layer_1_size': 25, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 40, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13004315739512695}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.20395737361506647, 'rmse': 0.20395737361506647, 'mae': 0.16257758439045134, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  41 | activation: sigmoid | extras: dropout - rate: 36.7% 
layer 2 | size:  93 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  25 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f460418d6a0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 13s - loss: 0.8922
4096/6530 [=================>............] - ETA: 0s - loss: 0.7428 
# training | RMSE: 0.2017, MAE: 0.1620
worker 0  xfile  [17, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.20167615785340534, 'rmse': 0.20167615785340534, 'mae': 0.16198793655166957, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  26 | activation: tanh    | extras: dropout - rate: 29.6% 
layer 2 | size:  28 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ac17f5f8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 13s - loss: 0.7274
6530/6530 [==============================] - 1s 106us/step - loss: 0.6568 - val_loss: 0.3055
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.3912
3712/6530 [================>.............] - ETA: 0s - loss: 0.6930 
4608/6530 [====================>.........] - ETA: 0s - loss: 0.2909
6530/6530 [==============================] - 0s 13us/step - loss: 0.2656 - val_loss: 0.2923
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1969
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1987
6530/6530 [==============================] - 0s 64us/step - loss: 0.6497 - val_loss: 0.5044
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.5082
# training | RMSE: 0.1508, MAE: 0.1139
worker 2  xfile  [11, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26857746513450387}, 'layer_4_size': 25, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.48790055374956387}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.15076981533726688, 'rmse': 0.15076981533726688, 'mae': 0.11386248600960594, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  15 | activation: relu    | extras: None 
layer 2 | size:  21 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ff4601d0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 47s - loss: 1.2516
6530/6530 [==============================] - 0s 12us/step - loss: 0.1991 - val_loss: 0.2089

3712/6530 [================>.............] - ETA: 0s - loss: 0.3229
 992/6530 [===>..........................] - ETA: 1s - loss: 0.6597 
6530/6530 [==============================] - 0s 15us/step - loss: 0.2609 - val_loss: 0.1633
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1713
2048/6530 [========>.....................] - ETA: 0s - loss: 0.4185
3840/6530 [================>.............] - ETA: 0s - loss: 0.1729
3072/6530 [=============>................] - ETA: 0s - loss: 0.3141
6530/6530 [==============================] - 0s 14us/step - loss: 0.1722 - val_loss: 0.1619

4160/6530 [==================>...........] - ETA: 0s - loss: 0.2536
5312/6530 [=======================>......] - ETA: 0s - loss: 0.2140
6400/6530 [============================>.] - ETA: 0s - loss: 0.1881
6530/6530 [==============================] - 1s 89us/step - loss: 0.1855 - val_loss: 0.0595
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0767
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0609
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0549
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0523
4128/6530 [=================>............] - ETA: 0s - loss: 0.0513
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0500
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0493
6530/6530 [==============================] - 0s 51us/step - loss: 0.0492 - val_loss: 0.0439
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0321
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0431
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0417
3200/6530 [=============>................] - ETA: 0s - loss: 0.0421
# training | RMSE: 0.2628, MAE: 0.2091
worker 1  xfile  [18, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36676658685766184}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2069230848746947}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.26280417708237547, 'rmse': 0.26280417708237547, 'mae': 0.20910040620342715, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  93 | activation: relu    | extras: None 
layer 2 | size:  42 | activation: sigmoid | extras: dropout - rate: 47.3% 
layer 3 | size:  28 | activation: relu    | extras: dropout - rate: 20.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45addef710>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 9s - loss: 1.9215
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0415
# training | RMSE: 0.2028, MAE: 0.1621
worker 0  xfile  [19, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.29629521052495933}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 100, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20275761825921346, 'rmse': 0.20275761825921346, 'mae': 0.16205472489478928, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  89 | activation: sigmoid | extras: dropout - rate: 41.0% 
layer 2 | size:  40 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42cd2003c8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 30s - loss: 1.9840
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0411
2240/6530 [=========>....................] - ETA: 0s - loss: 0.4602 
6530/6530 [==============================] - 0s 75us/step - loss: 0.5311 - val_loss: 0.1619
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2775
6336/6530 [============================>.] - ETA: 0s - loss: 0.0405
6530/6530 [==============================] - 0s 51us/step - loss: 0.0403 - val_loss: 0.0402

4224/6530 [==================>...........] - ETA: 0s - loss: 0.3546
6530/6530 [==============================] - 0s 8us/step - loss: 0.2146 - val_loss: 0.0948
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2024
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2980
6530/6530 [==============================] - 0s 7us/step - loss: 0.1673 - val_loss: 0.1126

6530/6530 [==============================] - 1s 78us/step - loss: 0.2914 - val_loss: 0.0490
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1626
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1442
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1370
6528/6530 [============================>.] - ETA: 0s - loss: 0.1292
6530/6530 [==============================] - 0s 25us/step - loss: 0.1292 - val_loss: 0.0478
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1338
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0932
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0908
6530/6530 [==============================] - 0s 24us/step - loss: 0.0893 - val_loss: 0.0475

# training | RMSE: 0.3329, MAE: 0.2780
worker 1  xfile  [21, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47285795476269665}, 'layer_2_size': 42, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2094503952911757}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3724373873602572}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.3328971154995626, 'rmse': 0.3328971154995626, 'mae': 0.2780119343303215, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  89 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ad81eeb8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 29s - loss: 0.6340
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1028 
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0745
6530/6530 [==============================] - 0s 72us/step - loss: 0.0677 - val_loss: 0.0400
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0358
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0373
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0368
6530/6530 [==============================] - 0s 21us/step - loss: 0.0362 - val_loss: 0.0352
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0214
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0324
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0322
6530/6530 [==============================] - 0s 20us/step - loss: 0.0319 - val_loss: 0.0333

# training | RMSE: 0.1951, MAE: 0.1563
worker 2  xfile  [20, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.16854481038475316}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1461065261872081}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3046963572135841}, 'layer_5_size': 88, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.195080421379282, 'rmse': 0.195080421379282, 'mae': 0.1562934420899685, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  63 | activation: relu    | extras: batchnorm 
layer 2 | size:  66 | activation: sigmoid | extras: dropout - rate: 11.7% 
layer 3 | size:  16 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ec2d20b8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:32 - loss: 0.5477
 640/6530 [=>............................] - ETA: 4s - loss: 0.2591  
1216/6530 [====>.........................] - ETA: 2s - loss: 0.1649
1792/6530 [=======>......................] - ETA: 1s - loss: 0.1252
2368/6530 [=========>....................] - ETA: 1s - loss: 0.1050
2912/6530 [============>.................] - ETA: 0s - loss: 0.0928
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0842
3872/6530 [================>.............] - ETA: 0s - loss: 0.0790
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0739
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0707
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0673
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0642
6400/6530 [============================>.] - ETA: 0s - loss: 0.0613
6530/6530 [==============================] - 1s 176us/step - loss: 0.0606 - val_loss: 0.0324
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0317
 480/6530 [=>............................] - ETA: 0s - loss: 0.0318
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0299
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0291
# training | RMSE: 0.2185, MAE: 0.1774
worker 0  xfile  [22, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.41014862887132686}, 'layer_1_size': 89, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 10, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38625217326660877}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.21847061895846207, 'rmse': 0.21847061895846207, 'mae': 0.177359013166112, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  94 | activation: tanh    | extras: dropout - rate: 43.8% 
layer 2 | size:  42 | activation: relu    | extras: batchnorm 
layer 3 | size:  31 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  56 | activation: tanh    | extras: None 
layer 5 | size:  81 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42cced5898>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 2:17 - loss: 0.6651
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0295
 512/6530 [=>............................] - ETA: 8s - loss: 0.2819  
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0304
1024/6530 [===>..........................] - ETA: 4s - loss: 0.2261
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0296
1504/6530 [=====>........................] - ETA: 2s - loss: 0.1965
3872/6530 [================>.............] - ETA: 0s - loss: 0.0298
2016/6530 [========>.....................] - ETA: 1s - loss: 0.1797
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0295
2528/6530 [==========>...................] - ETA: 1s - loss: 0.1655
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0292
3008/6530 [============>.................] - ETA: 1s - loss: 0.1536
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0288
# training | RMSE: 0.1757, MAE: 0.1395
worker 1  xfile  [24, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.17574987448305954, 'rmse': 0.17574987448305954, 'mae': 0.13946526249119665, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  13 | activation: tanh    | extras: None 
layer 2 | size:  62 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  70 | activation: tanh    | extras: dropout - rate: 23.7% 
layer 4 | size:  68 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ad81d6a0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 29s - loss: 0.8957
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1446
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0289
2688/6530 [===========>..................] - ETA: 0s - loss: 0.3838 
3936/6530 [=================>............] - ETA: 0s - loss: 0.1370
6530/6530 [==============================] - 1s 94us/step - loss: 0.0286 - val_loss: 0.0238
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0189
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2619
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1311
 640/6530 [=>............................] - ETA: 0s - loss: 0.0235
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1252
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0231
6530/6530 [==============================] - 1s 123us/step - loss: 0.2287 - val_loss: 0.1579

5344/6530 [=======================>......] - ETA: 0s - loss: 0.1210Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1982
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0236
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1168
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0713
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0235
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1132
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0623
2848/6530 [============>.................] - ETA: 0s - loss: 0.0239
6530/6530 [==============================] - 0s 22us/step - loss: 0.0579 - val_loss: 0.0394
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0468
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0237
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0413
6530/6530 [==============================] - 1s 223us/step - loss: 0.1116 - val_loss: 0.0576
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0792
4032/6530 [=================>............] - ETA: 0s - loss: 0.0234
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0378
 480/6530 [=>............................] - ETA: 0s - loss: 0.0613
6530/6530 [==============================] - 0s 22us/step - loss: 0.0366 - val_loss: 0.0485

4608/6530 [====================>.........] - ETA: 0s - loss: 0.0234
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0621
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0231
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0599
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0231
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0616
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0234
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0615
6530/6530 [==============================] - 1s 93us/step - loss: 0.0231 - val_loss: 0.0193

2944/6530 [============>.................] - ETA: 0s - loss: 0.0602
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0588
3968/6530 [=================>............] - ETA: 0s - loss: 0.0578
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0574
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0569
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0556
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0549
6400/6530 [============================>.] - ETA: 0s - loss: 0.0545
6530/6530 [==============================] - 1s 110us/step - loss: 0.0542 - val_loss: 0.0419
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0498
 480/6530 [=>............................] - ETA: 0s - loss: 0.0487
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0471
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0473
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0461
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0462
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0451
3072/6530 [=============>................] - ETA: 0s - loss: 0.0449
# training | RMSE: 0.1309, MAE: 0.1005
worker 2  xfile  [23, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11698107846478517}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4994624948580816}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.13085953116774451, 'rmse': 0.13085953116774451, 'mae': 0.10049947906823552, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  51 | activation: sigmoid | extras: None 
layer 2 | size:  90 | activation: tanh    | extras: None 
layer 3 | size:  42 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ec1f7dd8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:07 - loss: 0.6713
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0437
 928/6530 [===>..........................] - ETA: 2s - loss: 0.3282  
4032/6530 [=================>............] - ETA: 0s - loss: 0.0435
1888/6530 [=======>......................] - ETA: 1s - loss: 0.2697
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0431
2848/6530 [============>.................] - ETA: 0s - loss: 0.2494
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0433
3712/6530 [================>.............] - ETA: 0s - loss: 0.2381
# training | RMSE: 0.2151, MAE: 0.1751
worker 1  xfile  [26, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23742521546737436}, 'layer_3_size': 70, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 68, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4230964493371582}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.2151097881221919, 'rmse': 0.2151097881221919, 'mae': 0.17511215789995044, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  20 | activation: sigmoid | extras: dropout - rate: 20.8% 
layer 2 | size:  68 | activation: sigmoid | extras: None 
layer 3 | size:  19 | activation: relu    | extras: dropout - rate: 19.2% 
layer 4 | size:  69 | activation: relu    | extras: dropout - rate: 48.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45acfc11d0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 33s - loss: 0.7187
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0430
4608/6530 [====================>.........] - ETA: 0s - loss: 0.2297
2944/6530 [============>.................] - ETA: 0s - loss: 0.6083 
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0429
5344/6530 [=======================>......] - ETA: 0s - loss: 0.2244
5632/6530 [========================>.....] - ETA: 0s - loss: 0.4407
6336/6530 [============================>.] - ETA: 0s - loss: 0.0428
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2184
6530/6530 [==============================] - 1s 119us/step - loss: 0.0428 - val_loss: 0.0327

6530/6530 [==============================] - 1s 134us/step - loss: 0.4130 - val_loss: 0.2218
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2447
6530/6530 [==============================] - 1s 115us/step - loss: 0.2162 - val_loss: 0.1747
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2159
2944/6530 [============>.................] - ETA: 0s - loss: 0.2331
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1764
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2318
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1691
6530/6530 [==============================] - 0s 19us/step - loss: 0.2320 - val_loss: 0.2133
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2326
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1678
2944/6530 [============>.................] - ETA: 0s - loss: 0.2254
3744/6530 [================>.............] - ETA: 0s - loss: 0.1659
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2268
6530/6530 [==============================] - 0s 19us/step - loss: 0.2272 - val_loss: 0.2167

4672/6530 [====================>.........] - ETA: 0s - loss: 0.1655
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1661
6530/6530 [==============================] - 0s 56us/step - loss: 0.1656 - val_loss: 0.1614
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1817
 864/6530 [==>...........................] - ETA: 0s - loss: 0.1697
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1659
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1663
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1639
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1641
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1649
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1649
6530/6530 [==============================] - 0s 59us/step - loss: 0.1646 - val_loss: 0.1611

# training | RMSE: 0.2663, MAE: 0.2171
worker 1  xfile  [27, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20779557935052267}, 'layer_1_size': 20, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1919884355305059}, 'layer_3_size': 19, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48470744723662407}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.26628991159620174, 'rmse': 0.26628991159620174, 'mae': 0.21706561734778812, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  40 | activation: sigmoid | extras: batchnorm 
layer 2 | size:   4 | activation: relu    | extras: batchnorm 
layer 3 | size:  19 | activation: relu    | extras: dropout - rate: 18.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45acfc0fd0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 4:44 - loss: 1.3951
 272/6530 [>.............................] - ETA: 17s - loss: 0.4662 
 544/6530 [=>............................] - ETA: 8s - loss: 0.3116 
 816/6530 [==>...........................] - ETA: 5s - loss: 0.2444
# training | RMSE: 0.1806, MAE: 0.1431
worker 0  xfile  [25, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43759601245045443}, 'layer_1_size': 94, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 31, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 56, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.1806480660094588, 'rmse': 0.1806480660094588, 'mae': 0.14312775102474296, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  84 | activation: relu    | extras: batchnorm 
layer 2 | size:   5 | activation: relu    | extras: batchnorm 
layer 3 | size:  94 | activation: sigmoid | extras: None 
layer 4 | size:  77 | activation: relu    | extras: None 
layer 5 | size:  43 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f42ccdae908>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 2:38 - loss: 0.7738
1056/6530 [===>..........................] - ETA: 4s - loss: 0.2093
 416/6530 [>.............................] - ETA: 12s - loss: 0.1521 
1296/6530 [====>.........................] - ETA: 3s - loss: 0.1890
 768/6530 [==>...........................] - ETA: 6s - loss: 0.1197 
1520/6530 [=====>........................] - ETA: 3s - loss: 0.1747
1120/6530 [====>.........................] - ETA: 4s - loss: 0.1048
1760/6530 [=======>......................] - ETA: 2s - loss: 0.1614
1504/6530 [=====>........................] - ETA: 3s - loss: 0.0979
2000/6530 [========>.....................] - ETA: 2s - loss: 0.1530
# training | RMSE: 0.2036, MAE: 0.1605
worker 2  xfile  [28, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 51, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 90, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1260034089426204}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.20359751058911463, 'rmse': 0.20359751058911463, 'mae': 0.1605168359050304, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  27 | activation: tanh    | extras: None 
layer 2 | size:  47 | activation: tanh    | extras: dropout - rate: 20.6% 
layer 3 | size:  91 | activation: relu    | extras: batchnorm 
layer 4 | size:  94 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45e43d1128>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 31s - loss: 6.5770
1856/6530 [=======>......................] - ETA: 2s - loss: 0.0923
2256/6530 [=========>....................] - ETA: 2s - loss: 0.1451
1920/6530 [=======>......................] - ETA: 1s - loss: 2.1302 
2240/6530 [=========>....................] - ETA: 2s - loss: 0.0874
2528/6530 [==========>...................] - ETA: 1s - loss: 0.1378
3712/6530 [================>.............] - ETA: 0s - loss: 1.3266
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0876
2800/6530 [===========>..................] - ETA: 1s - loss: 0.1313
5632/6530 [========================>.....] - ETA: 0s - loss: 0.9917
3088/6530 [=============>................] - ETA: 1s - loss: 0.1268
3072/6530 [=============>................] - ETA: 1s - loss: 0.0831
3376/6530 [==============>...............] - ETA: 1s - loss: 0.1224
6530/6530 [==============================] - 1s 134us/step - loss: 0.8959 - val_loss: 0.2304

3488/6530 [===============>..............] - ETA: 1s - loss: 0.0806Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.3535
3648/6530 [===============>..............] - ETA: 1s - loss: 0.1180
3872/6530 [================>.............] - ETA: 0s - loss: 0.0781
2176/6530 [========>.....................] - ETA: 0s - loss: 0.2580
3936/6530 [=================>............] - ETA: 0s - loss: 0.1149
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2348
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0762
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1121
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2226
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0746
6530/6530 [==============================] - 0s 26us/step - loss: 0.2220 - val_loss: 0.1747
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1367
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1097
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0724
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1754
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1076
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0700
3968/6530 [=================>............] - ETA: 0s - loss: 0.1648
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1063
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0685
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1591
6530/6530 [==============================] - 0s 28us/step - loss: 0.1575 - val_loss: 0.1346

5296/6530 [=======================>......] - ETA: 0s - loss: 0.1046
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0675
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1028
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1010
6530/6530 [==============================] - 2s 268us/step - loss: 0.0674 - val_loss: 0.0492
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0527
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0992
 448/6530 [=>............................] - ETA: 0s - loss: 0.0436
6416/6530 [============================>.] - ETA: 0s - loss: 0.0976
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0484
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0446
6530/6530 [==============================] - 2s 320us/step - loss: 0.0970 - val_loss: 0.0536
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0714
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0434
 304/6530 [>.............................] - ETA: 1s - loss: 0.0690
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0456
 576/6530 [=>............................] - ETA: 1s - loss: 0.0674
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0449
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0665
2848/6530 [============>.................] - ETA: 0s - loss: 0.0440
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0650
3264/6530 [=============>................] - ETA: 0s - loss: 0.0466
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0658
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0452
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0660
4032/6530 [=================>............] - ETA: 0s - loss: 0.0460
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0664
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0452
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0664
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0450
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0658
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0444
2848/6530 [============>.................] - ETA: 0s - loss: 0.0656
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0439
3152/6530 [=============>................] - ETA: 0s - loss: 0.0660
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0431
# training | RMSE: 0.3490, MAE: 0.2830
worker 2  xfile  [31, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2060226759034579}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 94, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3202676902718764}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.34904216093267826, 'rmse': 0.34904216093267826, 'mae': 0.28300808655393983, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  29 | activation: tanh    | extras: dropout - rate: 26.7% 
layer 2 | size:  64 | activation: tanh    | extras: dropout - rate: 49.1% 
layer 3 | size:   2 | activation: relu    | extras: dropout - rate: 49.8% 
layer 4 | size:  77 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f45e4117f98>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:24 - loss: 0.3556
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0652
6432/6530 [============================>.] - ETA: 0s - loss: 0.0424
 832/6530 [==>...........................] - ETA: 3s - loss: 0.2610  
6530/6530 [==============================] - 1s 136us/step - loss: 0.0429 - val_loss: 0.0654
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0688
3744/6530 [================>.............] - ETA: 0s - loss: 0.0648
1568/6530 [======>.......................] - ETA: 1s - loss: 0.2140
 416/6530 [>.............................] - ETA: 0s - loss: 0.0418
4048/6530 [=================>............] - ETA: 0s - loss: 0.0641
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1818
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0382
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0636
3168/6530 [=============>................] - ETA: 0s - loss: 0.1560
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0357
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0630
3968/6530 [=================>............] - ETA: 0s - loss: 0.1391
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0342
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0631
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1273
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0332
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0632
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1181
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0629
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0326
6368/6530 [============================>.] - ETA: 0s - loss: 0.1111
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0631
2912/6530 [============>.................] - ETA: 0s - loss: 0.0333
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0626
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0328
6530/6530 [==============================] - 1s 139us/step - loss: 0.1100 - val_loss: 0.0705
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0673
3712/6530 [================>.............] - ETA: 0s - loss: 0.0329
6368/6530 [============================>.] - ETA: 0s - loss: 0.0622
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0607
4096/6530 [=================>............] - ETA: 0s - loss: 0.0338
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0609
6530/6530 [==============================] - 1s 186us/step - loss: 0.0623 - val_loss: 0.0486
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0368
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0337
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0620
 304/6530 [>.............................] - ETA: 1s - loss: 0.0498
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0336
3200/6530 [=============>................] - ETA: 0s - loss: 0.0614
 592/6530 [=>............................] - ETA: 1s - loss: 0.0528
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0336
4000/6530 [=================>............] - ETA: 0s - loss: 0.0607
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0524
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0336
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0596
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0532
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0333
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0597
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0541
6496/6530 [============================>.] - ETA: 0s - loss: 0.0332
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0595
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0545
6530/6530 [==============================] - 1s 132us/step - loss: 0.0332 - val_loss: 0.0229

6530/6530 [==============================] - 0s 68us/step - loss: 0.0596 - val_loss: 0.0500
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0748
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0541
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0557
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0545
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0545
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0548
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0576
2864/6530 [============>.................] - ETA: 0s - loss: 0.0546
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0585
3152/6530 [=============>................] - ETA: 0s - loss: 0.0550
4096/6530 [=================>............] - ETA: 0s - loss: 0.0579
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0554
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0576
3744/6530 [================>.............] - ETA: 0s - loss: 0.0559
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0579
4048/6530 [=================>............] - ETA: 0s - loss: 0.0564
6530/6530 [==============================] - 0s 64us/step - loss: 0.0580 - val_loss: 0.0596

4320/6530 [==================>...........] - ETA: 0s - loss: 0.0566
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0566
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0559
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0560
# training | RMSE: 0.2439, MAE: 0.2008
worker 2  xfile  [32, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.26678947117970675}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.49082008909040964}, 'layer_2_size': 64, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4982940259391383}, 'layer_3_size': 2, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.243934020647421, 'rmse': 0.243934020647421, 'mae': 0.20080872206752465, 'early_stop': False}
vggnet done  2

5456/6530 [========================>.....] - ETA: 0s - loss: 0.0562
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0564
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0564
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0562
6530/6530 [==============================] - 1s 187us/step - loss: 0.0557 - val_loss: 0.0443

# training | RMSE: 0.2103, MAE: 0.1688
worker 1  xfile  [30, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18897806035083972}, 'layer_3_size': 19, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 29, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.21025824649849437, 'rmse': 0.21025824649849437, 'mae': 0.16876566172537763, 'early_stop': False}
vggnet done  1

# training | RMSE: 0.1463, MAE: 0.1145
worker 0  xfile  [29, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 5, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.14631854762706523, 'rmse': 0.14631854762706523, 'mae': 0.11452097304784052, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  89 | activation: tanh    | extras: None 
layer 2 | size:  97 | activation: relu    | extras: batchnorm 
layer 3 | size:  79 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  60 | activation: relu    | extras: dropout - rate: 43.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f42ccdae898>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 16s - loss: 2.7376
3584/6530 [===============>..............] - ETA: 0s - loss: 0.4683 
6530/6530 [==============================] - 1s 131us/step - loss: 0.3265 - val_loss: 0.1675
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1294
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1148
6530/6530 [==============================] - 0s 17us/step - loss: 0.1069 - val_loss: 0.1013
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0613
3072/6530 [=============>................] - ETA: 0s - loss: 0.0818
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0787
6530/6530 [==============================] - 0s 21us/step - loss: 0.0806 - val_loss: 0.1476

# training | RMSE: 0.3751, MAE: 0.2979
worker 0  xfile  [33, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 79, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4383437949009624}, 'layer_4_size': 60, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.3751112652864579, 'rmse': 0.3751112652864579, 'mae': 0.29792970988762996, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=3.0 loss={'loss': 0.2500969479590981, 'rmse': 0.2500969479590981, 'mae': 0.1979225746937054, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3216388030084181}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#1 epoch=3.0 loss={'loss': 0.23556898768046652, 'rmse': 0.23556898768046652, 'mae': 0.19332082317542376, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2416967782581718}, 'layer_2_size': 5, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.45351807633941055}, 'layer_3_size': 6, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.37809688818276255}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 23, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#2 epoch=3.0 loss={'loss': 0.21859010613333232, 'rmse': 0.21859010613333232, 'mae': 0.16433084023186068, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.40018692179574666}, 'layer_1_size': 68, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23890692753588347}, 'layer_2_size': 83, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 23, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#3 epoch=3.0 loss={'loss': 0.21626794786944162, 'rmse': 0.21626794786944162, 'mae': 0.1697984864829356, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2488149671850049}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 2, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 84, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 36, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#4 epoch=3.0 loss={'loss': 0.20101348217515796, 'rmse': 0.20101348217515796, 'mae': 0.163454166034713, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 18, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#5 epoch=3.0 loss={'loss': 0.1967168130900827, 'rmse': 0.1967168130900827, 'mae': 0.15788449282148734, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2762703984959979}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27427562013650353}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#6 epoch=3.0 loss={'loss': 0.19736924470129383, 'rmse': 0.19736924470129383, 'mae': 0.15826095346210023, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4140653142464831}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 90, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 24, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#7 epoch=3.0 loss={'loss': 0.2738646836604897, 'rmse': 0.2738646836604897, 'mae': 0.2226255450048799, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42732504899124657}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20019437851553495}, 'layer_2_size': 65, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.28808774646849467}, 'layer_4_size': 8, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#8 epoch=3.0 loss={'loss': 0.16433017487956403, 'rmse': 0.16433017487956403, 'mae': 0.12580216675043382, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4062271963680081}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#9 epoch=3.0 loss={'loss': 0.5184734551897187, 'rmse': 0.5184734551897187, 'mae': 0.47839814348815396, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.32262807597466253}, 'layer_3_size': 79, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.18828675491346827}, 'layer_4_size': 72, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#10 epoch=3.0 loss={'loss': 0.15199125209535203, 'rmse': 0.15199125209535203, 'mae': 0.1202789432087981, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10607566151554111}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#12 epoch=3.0 loss={'loss': 0.1765883508942092, 'rmse': 0.1765883508942092, 'mae': 0.14042368167120617, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3000335360122979}, 'layer_5_size': 67, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#13 epoch=3.0 loss={'loss': 0.12301220137446423, 'rmse': 0.12301220137446423, 'mae': 0.0965322964336906, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2806300923523829}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#15 epoch=3.0 loss={'loss': 0.20641934432327075, 'rmse': 0.20641934432327075, 'mae': 0.16115313239987944, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.40274950378963015}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#14 epoch=3.0 loss={'loss': 0.1718570450195309, 'rmse': 0.1718570450195309, 'mae': 0.13656836187199206, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48890225339762805}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#16 epoch=3.0 loss={'loss': 0.20395737361506647, 'rmse': 0.20395737361506647, 'mae': 0.16257758439045134, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4464367148407562}, 'layer_1_size': 25, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 40, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13004315739512695}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#17 epoch=3.0 loss={'loss': 0.20167615785340534, 'rmse': 0.20167615785340534, 'mae': 0.16198793655166957, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 39, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#11 epoch=3.0 loss={'loss': 0.15076981533726688, 'rmse': 0.15076981533726688, 'mae': 0.11386248600960594, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26857746513450387}, 'layer_4_size': 25, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.48790055374956387}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#18 epoch=3.0 loss={'loss': 0.26280417708237547, 'rmse': 0.26280417708237547, 'mae': 0.20910040620342715, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36676658685766184}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2069230848746947}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
#19 epoch=3.0 loss={'loss': 0.20275761825921346, 'rmse': 0.20275761825921346, 'mae': 0.16205472489478928, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.29629521052495933}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 100, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#20 epoch=3.0 loss={'loss': 0.195080421379282, 'rmse': 0.195080421379282, 'mae': 0.1562934420899685, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.16854481038475316}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1461065261872081}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3046963572135841}, 'layer_5_size': 88, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#21 epoch=3.0 loss={'loss': 0.3328971154995626, 'rmse': 0.3328971154995626, 'mae': 0.2780119343303215, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47285795476269665}, 'layer_2_size': 42, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2094503952911757}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3724373873602572}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#22 epoch=3.0 loss={'loss': 0.21847061895846207, 'rmse': 0.21847061895846207, 'mae': 0.177359013166112, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.41014862887132686}, 'layer_1_size': 89, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 40, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 10, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38625217326660877}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#24 epoch=3.0 loss={'loss': 0.17574987448305954, 'rmse': 0.17574987448305954, 'mae': 0.13946526249119665, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#26 epoch=3.0 loss={'loss': 0.2151097881221919, 'rmse': 0.2151097881221919, 'mae': 0.17511215789995044, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23742521546737436}, 'layer_3_size': 70, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 68, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4230964493371582}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#23 epoch=3.0 loss={'loss': 0.13085953116774451, 'rmse': 0.13085953116774451, 'mae': 0.10049947906823552, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11698107846478517}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4994624948580816}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#25 epoch=3.0 loss={'loss': 0.1806480660094588, 'rmse': 0.1806480660094588, 'mae': 0.14312775102474296, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43759601245045443}, 'layer_1_size': 94, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 31, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 56, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#27 epoch=3.0 loss={'loss': 0.26628991159620174, 'rmse': 0.26628991159620174, 'mae': 0.21706561734778812, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20779557935052267}, 'layer_1_size': 20, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1919884355305059}, 'layer_3_size': 19, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48470744723662407}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#28 epoch=3.0 loss={'loss': 0.20359751058911463, 'rmse': 0.20359751058911463, 'mae': 0.1605168359050304, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 51, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 90, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1260034089426204}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#31 epoch=3.0 loss={'loss': 0.34904216093267826, 'rmse': 0.34904216093267826, 'mae': 0.28300808655393983, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2060226759034579}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 94, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3202676902718764}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#29 epoch=3.0 loss={'loss': 0.14631854762706523, 'rmse': 0.14631854762706523, 'mae': 0.11452097304784052, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 5, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#32 epoch=3.0 loss={'loss': 0.243934020647421, 'rmse': 0.243934020647421, 'mae': 0.20080872206752465, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.26678947117970675}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.49082008909040964}, 'layer_2_size': 64, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4982940259391383}, 'layer_3_size': 2, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#30 epoch=3.0 loss={'loss': 0.21025824649849437, 'rmse': 0.21025824649849437, 'mae': 0.16876566172537763, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18897806035083972}, 'layer_3_size': 19, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 29, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#33 epoch=3.0 loss={'loss': 0.3751112652864579, 'rmse': 0.3751112652864579, 'mae': 0.29792970988762996, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 79, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4383437949009624}, 'layer_4_size': 60, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
get a list [results] of length 155
get a list [loss] of length 34
get a list [val_loss] of length 34
length of indices is (13, 23, 29, 11, 10, 8, 14, 24, 12, 25, 20, 5, 6, 4, 17, 19, 28, 16, 15, 30, 26, 3, 22, 2, 1, 32, 0, 18, 27, 7, 21, 31, 33, 9)
length of indices is 34
length of T is 34
newly formed T structure is:[[0, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2806300923523829}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [1, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11698107846478517}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4994624948580816}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [2, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 5, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [3, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26857746513450387}, 'layer_4_size': 25, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.48790055374956387}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [4, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10607566151554111}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [5, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4062271963680081}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [6, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48890225339762805}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [7, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [8, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3000335360122979}, 'layer_5_size': 67, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [9, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43759601245045443}, 'layer_1_size': 94, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 31, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 56, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [10, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.16854481038475316}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1461065261872081}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3046963572135841}, 'layer_5_size': 88, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]] 

*** 11.333333333333332 configurations x 9.0 iterations each

34 | Thu Sep 27 23:08:32 2018 | lowest loss so far: 0.0675 (run 0)

{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  63 | activation: relu    | extras: batchnorm 
layer 2 | size:  66 | activation: sigmoid | extras: dropout - rate: 11.7% 
layer 3 | size:  16 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 3:31 - loss: 0.5578
 416/6530 [>.............................] - ETA: 16s - loss: 0.3686 
 800/6530 [==>...........................] - ETA: 8s - loss: 0.2332 {'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  90 | activation: relu    | extras: batchnorm 
layer 2 | size:  64 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  57 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8abba8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 50s - loss: 1.2484
1184/6530 [====>.........................] - ETA: 5s - loss: 0.1777
1664/6530 [======>.......................] - ETA: 3s - loss: 1.0219 
1696/6530 [======>.......................] - ETA: 3s - loss: 0.1374
3328/6530 [==============>...............] - ETA: 1s - loss: 0.7848
2240/6530 [=========>....................] - ETA: 2s - loss: 0.1142
5248/6530 [=======================>......] - ETA: 0s - loss: 0.5709
2848/6530 [============>.................] - ETA: 1s - loss: 0.0985
3424/6530 [==============>...............] - ETA: 1s - loss: 0.0883
6530/6530 [==============================] - 1s 192us/step - loss: 0.4909 - val_loss: 0.1477
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1489
4032/6530 [=================>............] - ETA: 0s - loss: 0.0807{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  84 | activation: relu    | extras: batchnorm 
layer 2 | size:   5 | activation: relu    | extras: batchnorm 
layer 3 | size:  94 | activation: sigmoid | extras: None 
layer 4 | size:  77 | activation: relu    | extras: None 
layer 5 | size:  43 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 3:54 - loss: 0.4046
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1325
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0750
 448/6530 [=>............................] - ETA: 16s - loss: 0.1105 
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1263
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0704
 864/6530 [==>...........................] - ETA: 8s - loss: 0.0950 
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1222
6530/6530 [==============================] - 0s 27us/step - loss: 0.1213 - val_loss: 0.1074
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1126
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0667
1280/6530 [====>.........................] - ETA: 5s - loss: 0.0850
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1059
6464/6530 [============================>.] - ETA: 0s - loss: 0.0635
1664/6530 [======>.......................] - ETA: 3s - loss: 0.0844
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1024
2048/6530 [========>.....................] - ETA: 3s - loss: 0.0839
6530/6530 [==============================] - 2s 264us/step - loss: 0.0631 - val_loss: 0.0320
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0328
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1014
2464/6530 [==========>...................] - ETA: 2s - loss: 0.0812
 608/6530 [=>............................] - ETA: 0s - loss: 0.0315
6530/6530 [==============================] - 0s 27us/step - loss: 0.1011 - val_loss: 0.0961
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0980
2880/6530 [============>.................] - ETA: 1s - loss: 0.0780
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0306
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0963
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0775
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0299
4096/6530 [=================>............] - ETA: 0s - loss: 0.0931
3712/6530 [================>.............] - ETA: 1s - loss: 0.0743
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0303
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0916
6530/6530 [==============================] - 0s 27us/step - loss: 0.0909 - val_loss: 0.0923
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0864
4128/6530 [=================>............] - ETA: 0s - loss: 0.0713
3008/6530 [============>.................] - ETA: 0s - loss: 0.0302
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0901
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0685
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0301
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0878
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0670
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0299
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0864
6530/6530 [==============================] - 0s 26us/step - loss: 0.0861 - val_loss: 0.0883

4800/6530 [=====================>........] - ETA: 0s - loss: 0.0298Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0764
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0655
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0296
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0824
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0651
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0292
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0805
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0635
6528/6530 [============================>.] - ETA: 0s - loss: 0.0289
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0798
6530/6530 [==============================] - 0s 26us/step - loss: 0.0796 - val_loss: 0.0859

6530/6530 [==============================] - 1s 89us/step - loss: 0.0289 - val_loss: 0.0245
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0743Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0206
6530/6530 [==============================] - 2s 312us/step - loss: 0.0622 - val_loss: 0.0976
Epoch 2/9

  32/6530 [..............................] - ETA: 1s - loss: 0.0852
 640/6530 [=>............................] - ETA: 0s - loss: 0.0240
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0801
 480/6530 [=>............................] - ETA: 0s - loss: 0.0428
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0237
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0784
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0446
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0239
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0777
6530/6530 [==============================] - 0s 27us/step - loss: 0.0776 - val_loss: 0.0863
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0776
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0433
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0246
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0814
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0427
3040/6530 [============>.................] - ETA: 0s - loss: 0.0241
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0786
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0437
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0239
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0770
6530/6530 [==============================] - 0s 27us/step - loss: 0.0768 - val_loss: 0.0772
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0703
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0436
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0237
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0739
2976/6530 [============>.................] - ETA: 0s - loss: 0.0432
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0236
4096/6530 [=================>............] - ETA: 0s - loss: 0.0737
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0424
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0235
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0728
3776/6530 [================>.............] - ETA: 0s - loss: 0.0434
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0233
6530/6530 [==============================] - 0s 27us/step - loss: 0.0727 - val_loss: 0.0780

4224/6530 [==================>...........] - ETA: 0s - loss: 0.0427
6530/6530 [==============================] - 1s 88us/step - loss: 0.0231 - val_loss: 0.0208
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0181
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0427
 640/6530 [=>............................] - ETA: 0s - loss: 0.0195
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0417
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0190
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0416
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0196
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0411
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0202
6400/6530 [============================>.] - ETA: 0s - loss: 0.0408
3040/6530 [============>.................] - ETA: 0s - loss: 0.0198
6530/6530 [==============================] - 1s 125us/step - loss: 0.0407 - val_loss: 0.0377
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0474
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0197
 480/6530 [=>............................] - ETA: 0s - loss: 0.0393
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0196
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0362
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0197
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0388
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0196
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0382
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0194
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0369
6530/6530 [==============================] - 1s 89us/step - loss: 0.0194 - val_loss: 0.0162
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0164
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0356
 608/6530 [=>............................] - ETA: 0s - loss: 0.0164
3072/6530 [=============>................] - ETA: 0s - loss: 0.0353
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0166
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0349
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0167
3936/6530 [=================>............] - ETA: 0s - loss: 0.0351
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0170
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0348
3008/6530 [============>.................] - ETA: 0s - loss: 0.0169
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0347
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0168
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0346
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0167
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0343
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0168
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0345
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0167
6336/6530 [============================>.] - ETA: 0s - loss: 0.0342
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0165
6530/6530 [==============================] - 1s 127us/step - loss: 0.0343 - val_loss: 0.1755
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1939
6528/6530 [============================>.] - ETA: 0s - loss: 0.0165
 448/6530 [=>............................] - ETA: 0s - loss: 0.0420
6530/6530 [==============================] - 1s 90us/step - loss: 0.0165 - val_loss: 0.0143
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0123
# training | RMSE: 0.0921, MAE: 0.0698
worker 0  xfile  [0, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2806300923523829}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.09208078884797326, 'rmse': 0.09208078884797326, 'mae': 0.06978214485301047, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  15 | activation: tanh    | extras: batchnorm 
layer 2 | size:  27 | activation: relu    | extras: batchnorm 
layer 3 | size:  23 | activation: sigmoid | extras: None 
layer 4 | size:  25 | activation: tanh    | extras: dropout - rate: 26.9% 
layer 5 | size:  24 | activation: sigmoid | extras: dropout - rate: 48.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45cd5a57f0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 3:34 - loss: 1.4905
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0383
 640/6530 [=>............................] - ETA: 0s - loss: 0.0149
 288/6530 [>.............................] - ETA: 12s - loss: 0.4549 
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0353
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0145
 560/6530 [=>............................] - ETA: 6s - loss: 0.3511 
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0348
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0151
 832/6530 [==>...........................] - ETA: 4s - loss: 0.3117
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0332
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0151
1104/6530 [====>.........................] - ETA: 3s - loss: 0.2859
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0319
3008/6530 [============>.................] - ETA: 0s - loss: 0.0149
2912/6530 [============>.................] - ETA: 0s - loss: 0.0316
1360/6530 [=====>........................] - ETA: 2s - loss: 0.2672
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0149
1616/6530 [======>.......................] - ETA: 2s - loss: 0.2552
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0308
4128/6530 [=================>............] - ETA: 0s - loss: 0.0149
1872/6530 [=======>......................] - ETA: 2s - loss: 0.2456
3744/6530 [================>.............] - ETA: 0s - loss: 0.0310
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0151
2128/6530 [========>.....................] - ETA: 1s - loss: 0.2374
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0150
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0304
2384/6530 [=========>....................] - ETA: 1s - loss: 0.2317
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0149
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0300
2640/6530 [===========>..................] - ETA: 1s - loss: 0.2286
6528/6530 [============================>.] - ETA: 0s - loss: 0.0150
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0298
6530/6530 [==============================] - 1s 89us/step - loss: 0.0150 - val_loss: 0.0129
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0115
2896/6530 [============>.................] - ETA: 1s - loss: 0.2230
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0300
 640/6530 [=>............................] - ETA: 0s - loss: 0.0142
3168/6530 [=============>................] - ETA: 1s - loss: 0.2182
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0302
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0140
3424/6530 [==============>...............] - ETA: 1s - loss: 0.2154
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0303
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0143
3680/6530 [===============>..............] - ETA: 0s - loss: 0.2126
6530/6530 [==============================] - 1s 128us/step - loss: 0.0300 - val_loss: 0.0321
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0355
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0145
3936/6530 [=================>............] - ETA: 0s - loss: 0.2107
 448/6530 [=>............................] - ETA: 0s - loss: 0.0257
3040/6530 [============>.................] - ETA: 0s - loss: 0.0139
4192/6530 [==================>...........] - ETA: 0s - loss: 0.2080
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0251
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0139
4448/6530 [===================>..........] - ETA: 0s - loss: 0.2063
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0248
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0138
4704/6530 [====================>.........] - ETA: 0s - loss: 0.2046
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0279
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0139
4976/6530 [=====================>........] - ETA: 0s - loss: 0.2029
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0272
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0138
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2008
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0277
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0138
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1989
2944/6530 [============>.................] - ETA: 0s - loss: 0.0273
6530/6530 [==============================] - 1s 88us/step - loss: 0.0138 - val_loss: 0.0120
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0123
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1974
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0284
 640/6530 [=>............................] - ETA: 0s - loss: 0.0134
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1968
3776/6530 [================>.............] - ETA: 0s - loss: 0.0278
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0130
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1951
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0278
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0135
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0278
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0136
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0273
3040/6530 [============>.................] - ETA: 0s - loss: 0.0131
6530/6530 [==============================] - 2s 291us/step - loss: 0.1939 - val_loss: 0.1454
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1407
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0273
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0132
 272/6530 [>.............................] - ETA: 1s - loss: 0.1705
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0270
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0132
 528/6530 [=>............................] - ETA: 1s - loss: 0.1761
6432/6530 [============================>.] - ETA: 0s - loss: 0.0270
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0134
 800/6530 [==>...........................] - ETA: 1s - loss: 0.1700
6530/6530 [==============================] - 1s 126us/step - loss: 0.0270 - val_loss: 0.0518
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1019
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0134
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1687
 448/6530 [=>............................] - ETA: 0s - loss: 0.0296
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0132
1344/6530 [=====>........................] - ETA: 1s - loss: 0.1650
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0279
6496/6530 [============================>.] - ETA: 0s - loss: 0.0133
1616/6530 [======>.......................] - ETA: 0s - loss: 0.1625
6530/6530 [==============================] - 1s 91us/step - loss: 0.0133 - val_loss: 0.0128
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0111
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0267
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1628
 608/6530 [=>............................] - ETA: 0s - loss: 0.0130
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0259
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1613
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0127
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0254
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1624
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0129
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0252
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1634
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0128
2944/6530 [============>.................] - ETA: 0s - loss: 0.0249
2944/6530 [============>.................] - ETA: 0s - loss: 0.1624
2976/6530 [============>.................] - ETA: 0s - loss: 0.0128
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0246
3200/6530 [=============>................] - ETA: 0s - loss: 0.1609
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0128
3776/6530 [================>.............] - ETA: 0s - loss: 0.0247
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1616
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0128
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0247
3744/6530 [================>.............] - ETA: 0s - loss: 0.1607
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0128
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0254
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0128
4016/6530 [=================>............] - ETA: 0s - loss: 0.1610
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0254
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1608
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0127
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0253
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1610
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0249
6530/6530 [==============================] - 1s 88us/step - loss: 0.0127 - val_loss: 0.0103

4816/6530 [=====================>........] - ETA: 0s - loss: 0.1603
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0250
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1603
6530/6530 [==============================] - 1s 127us/step - loss: 0.0249 - val_loss: 0.0409
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0434
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1605
 448/6530 [=>............................] - ETA: 0s - loss: 0.0268
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1598
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0240
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1602
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0243
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1604
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0242
6368/6530 [============================>.] - ETA: 0s - loss: 0.1604
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0246
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0239
6530/6530 [==============================] - 1s 203us/step - loss: 0.1603 - val_loss: 0.1368
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1614
2976/6530 [============>.................] - ETA: 0s - loss: 0.0242
 288/6530 [>.............................] - ETA: 1s - loss: 0.1528
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0238
 560/6530 [=>............................] - ETA: 1s - loss: 0.1589
# training | RMSE: 0.0934, MAE: 0.0721
worker 1  xfile  [1, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11698107846478517}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4994624948580816}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.09341991209338138, 'rmse': 0.09341991209338138, 'mae': 0.0721426525760442, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  96 | activation: tanh    | extras: dropout - rate: 10.6% 
layer 2 | size:  78 | activation: tanh    | extras: None 
layer 3 | size:  76 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8abef0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 35s - loss: 0.8595
3872/6530 [================>.............] - ETA: 0s - loss: 0.0235
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1570
1120/6530 [====>.........................] - ETA: 1s - loss: 0.3935 
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0234
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1568
2176/6530 [========>.....................] - ETA: 0s - loss: 0.3056
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0227
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1549
3264/6530 [=============>................] - ETA: 0s - loss: 0.2675
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0229
1616/6530 [======>.......................] - ETA: 0s - loss: 0.1516
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2421
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0226
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1507
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2250
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1508
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0226
6368/6530 [============================>.] - ETA: 0s - loss: 0.2133
2384/6530 [=========>....................] - ETA: 0s - loss: 0.1521
6368/6530 [============================>.] - ETA: 0s - loss: 0.0226
6530/6530 [==============================] - 1s 80us/step - loss: 0.2116 - val_loss: 0.2102
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1426
6530/6530 [==============================] - 1s 126us/step - loss: 0.0227 - val_loss: 0.0944
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0886
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1535
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1393
 448/6530 [=>............................] - ETA: 0s - loss: 0.0264
2896/6530 [============>.................] - ETA: 0s - loss: 0.1518
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1399
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0238
3168/6530 [=============>................] - ETA: 0s - loss: 0.1515
3232/6530 [=============>................] - ETA: 0s - loss: 0.1351
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1521
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0218
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1346
3696/6530 [===============>..............] - ETA: 0s - loss: 0.1525
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1326
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0218
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1315
3968/6530 [=================>............] - ETA: 0s - loss: 0.1529
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0232
6530/6530 [==============================] - 0s 52us/step - loss: 0.1307 - val_loss: 0.1711
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1924
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1524
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0236
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1304
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1531
2976/6530 [============>.................] - ETA: 0s - loss: 0.0229
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1280
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1531
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0225
3104/6530 [=============>................] - ETA: 0s - loss: 0.1244
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1529
3808/6530 [================>.............] - ETA: 0s - loss: 0.0224
4064/6530 [=================>............] - ETA: 0s - loss: 0.1236
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1521
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0219
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1218
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1519
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0221
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1203
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0219
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1516
6530/6530 [==============================] - 0s 52us/step - loss: 0.1201 - val_loss: 0.1668
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1829
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1522
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0217
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1188
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1519
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0217
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1153
6512/6530 [============================>.] - ETA: 0s - loss: 0.1517
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0214
3264/6530 [=============>................] - ETA: 0s - loss: 0.1153
6530/6530 [==============================] - 1s 204us/step - loss: 0.1517 - val_loss: 0.1336
Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1245
6530/6530 [==============================] - 1s 127us/step - loss: 0.0215 - val_loss: 0.1224
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0954
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1152
 272/6530 [>.............................] - ETA: 1s - loss: 0.1497
 448/6530 [=>............................] - ETA: 0s - loss: 0.0227
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1143
 544/6530 [=>............................] - ETA: 1s - loss: 0.1501
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0197
6464/6530 [============================>.] - ETA: 0s - loss: 0.1137
6530/6530 [==============================] - 0s 50us/step - loss: 0.1136 - val_loss: 0.1544
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1489
 800/6530 [==>...........................] - ETA: 1s - loss: 0.1505
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0199
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1094
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1509
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0202
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1078
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1502
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0199
3104/6530 [=============>................] - ETA: 0s - loss: 0.1079
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1459
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0196
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1068
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1457
2912/6530 [============>.................] - ETA: 0s - loss: 0.0193
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1064
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1456
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0188
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1066
6530/6530 [==============================] - 0s 51us/step - loss: 0.1067 - val_loss: 0.1129

2368/6530 [=========>....................] - ETA: 0s - loss: 0.1464Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1091
3744/6530 [================>.............] - ETA: 0s - loss: 0.0185
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1483
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1068
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0185
2896/6530 [============>.................] - ETA: 0s - loss: 0.1477
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1049
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0183
3168/6530 [=============>................] - ETA: 0s - loss: 0.1468
3264/6530 [=============>................] - ETA: 0s - loss: 0.1050
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0184
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1476
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1051
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0185
3696/6530 [===============>..............] - ETA: 0s - loss: 0.1476
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1044
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0190
3952/6530 [=================>............] - ETA: 0s - loss: 0.1482
6496/6530 [============================>.] - ETA: 0s - loss: 0.1037
6368/6530 [============================>.] - ETA: 0s - loss: 0.0193
6530/6530 [==============================] - 0s 51us/step - loss: 0.1037 - val_loss: 0.1161
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1079
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1483
6530/6530 [==============================] - 1s 127us/step - loss: 0.0195 - val_loss: 0.1196

1120/6530 [====>.........................] - ETA: 0s - loss: 0.1012
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1489
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1006
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1486
2944/6530 [============>.................] - ETA: 0s - loss: 0.1011
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1483
4000/6530 [=================>............] - ETA: 0s - loss: 0.1019
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1474
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1016
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1472
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1015
6530/6530 [==============================] - 0s 51us/step - loss: 0.1011 - val_loss: 0.1019
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1106
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1478
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0994
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1476
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0996
6352/6530 [============================>.] - ETA: 0s - loss: 0.1476
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0990
6530/6530 [==============================] - 1s 202us/step - loss: 0.1477 - val_loss: 0.1308
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1378
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0986
 272/6530 [>.............................] - ETA: 1s - loss: 0.1452
# training | RMSE: 0.3419, MAE: 0.3227
worker 2  xfile  [2, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 5, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.3418541171732119, 'rmse': 0.3418541171732119, 'mae': 0.3226677289743417, 'early_stop': True}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  79 | activation: sigmoid | extras: None 
layer 2 | size:  15 | activation: tanh    | extras: dropout - rate: 40.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8abef0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 1:14 - loss: 0.5806
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0989
 544/6530 [=>............................] - ETA: 1s - loss: 0.1431
 624/6530 [=>............................] - ETA: 2s - loss: 0.2808  
6530/6530 [==============================] - 0s 48us/step - loss: 0.0988 - val_loss: 0.0915
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0923
 800/6530 [==>...........................] - ETA: 1s - loss: 0.1447
1216/6530 [====>.........................] - ETA: 1s - loss: 0.2325
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0963
1040/6530 [===>..........................] - ETA: 1s - loss: 0.1463
1808/6530 [=======>......................] - ETA: 0s - loss: 0.2117
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0967
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1446
2416/6530 [==========>...................] - ETA: 0s - loss: 0.2006
3168/6530 [=============>................] - ETA: 0s - loss: 0.0980
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1412
2992/6530 [============>.................] - ETA: 0s - loss: 0.1927
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0973
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1422
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1888
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0970
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1423
4096/6530 [=================>............] - ETA: 0s - loss: 0.1858
6336/6530 [============================>.] - ETA: 0s - loss: 0.0967
6530/6530 [==============================] - 0s 50us/step - loss: 0.0966 - val_loss: 0.0954

2304/6530 [=========>....................] - ETA: 0s - loss: 0.1429
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1826
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1445
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1799
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1442
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1773
3072/6530 [=============>................] - ETA: 0s - loss: 0.1437
6496/6530 [============================>.] - ETA: 0s - loss: 0.1742
6530/6530 [==============================] - 1s 120us/step - loss: 0.1742 - val_loss: 0.1887
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1899
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1438
 656/6530 [==>...........................] - ETA: 0s - loss: 0.1497
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1444
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1478
3824/6530 [================>.............] - ETA: 0s - loss: 0.1443
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1482
4096/6530 [=================>............] - ETA: 0s - loss: 0.1450
# training | RMSE: 0.1125, MAE: 0.0864
worker 1  xfile  [4, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10607566151554111}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.11252647421811493, 'rmse': 0.11252647421811493, 'mae': 0.08643188075155847, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  57 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45db5accc0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 56s - loss: 0.7344
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1462
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1443
 720/6530 [==>...........................] - ETA: 1s - loss: 0.3093 
3056/6530 [=============>................] - ETA: 0s - loss: 0.1476
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1449
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1859
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1461
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1446
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1378
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1458
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1445
2848/6530 [============>.................] - ETA: 0s - loss: 0.1135
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1452
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1446
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0992
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1445
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1443
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0910
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1439
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1446
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0840
6432/6530 [============================>.] - ETA: 0s - loss: 0.1422
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1444
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0784
6530/6530 [==============================] - 1s 91us/step - loss: 0.1422 - val_loss: 0.1364
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1663
6432/6530 [============================>.] - ETA: 0s - loss: 0.1444
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0746
 592/6530 [=>............................] - ETA: 0s - loss: 0.1318
6530/6530 [==============================] - 1s 206us/step - loss: 0.1444 - val_loss: 0.1251
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1010
6530/6530 [==============================] - 1s 100us/step - loss: 0.0731 - val_loss: 0.0411

1168/6530 [====>.........................] - ETA: 0s - loss: 0.1324Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0539
 272/6530 [>.............................] - ETA: 1s - loss: 0.1365
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1310
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0423
 544/6530 [=>............................] - ETA: 1s - loss: 0.1404
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1318
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0394
 816/6530 [==>...........................] - ETA: 1s - loss: 0.1402
2912/6530 [============>.................] - ETA: 0s - loss: 0.1327
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0388
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1421
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1321
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0384
1328/6530 [=====>........................] - ETA: 1s - loss: 0.1414
4016/6530 [=================>............] - ETA: 0s - loss: 0.1310
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0380
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1388
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1293
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0377
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0375
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1394
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1292
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0371
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1288
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1387
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0371
6336/6530 [============================>.] - ETA: 0s - loss: 0.1285
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1394
6530/6530 [==============================] - 1s 92us/step - loss: 0.1281 - val_loss: 0.1275

6530/6530 [==============================] - 0s 76us/step - loss: 0.0368 - val_loss: 0.0347
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1279Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0475
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1410
 608/6530 [=>............................] - ETA: 0s - loss: 0.1204
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0345
2896/6530 [============>.................] - ETA: 0s - loss: 0.1405
1168/6530 [====>.........................] - ETA: 0s - loss: 0.1214
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0328
3152/6530 [=============>................] - ETA: 0s - loss: 0.1399
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1205
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0327
3408/6530 [==============>...............] - ETA: 0s - loss: 0.1408
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1190
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0320
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1412
2896/6530 [============>.................] - ETA: 0s - loss: 0.1194
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0317
3920/6530 [=================>............] - ETA: 0s - loss: 0.1414
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1199
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0312
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1415
4048/6530 [=================>............] - ETA: 0s - loss: 0.1199
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0310
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1417
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1189
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0306
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1422
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1189
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0305
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1420
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1177
6530/6530 [==============================] - 1s 77us/step - loss: 0.0303 - val_loss: 0.0279
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0393
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1419
6336/6530 [============================>.] - ETA: 0s - loss: 0.1175
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0275
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1419
6530/6530 [==============================] - 1s 92us/step - loss: 0.1174 - val_loss: 0.1222
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1272
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0261
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1419
 608/6530 [=>............................] - ETA: 0s - loss: 0.1133
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0262
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1419
1168/6530 [====>.........................] - ETA: 0s - loss: 0.1136
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0260
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1420
1744/6530 [=======>......................] - ETA: 0s - loss: 0.1133
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0256
6464/6530 [============================>.] - ETA: 0s - loss: 0.1417
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1115
4096/6530 [=================>............] - ETA: 0s - loss: 0.0253
6530/6530 [==============================] - 1s 207us/step - loss: 0.1417 - val_loss: 0.1220
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1477
2912/6530 [============>.................] - ETA: 0s - loss: 0.1147
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0252
 272/6530 [>.............................] - ETA: 1s - loss: 0.1355
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1149
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0248
 544/6530 [=>............................] - ETA: 1s - loss: 0.1376
4096/6530 [=================>............] - ETA: 0s - loss: 0.1138
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0248
 800/6530 [==>...........................] - ETA: 1s - loss: 0.1402
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1131
6530/6530 [==============================] - 1s 77us/step - loss: 0.0246 - val_loss: 0.0235
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0314
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1413
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1117
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0229
1328/6530 [=====>........................] - ETA: 1s - loss: 0.1400
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1118
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0220
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1369
6448/6530 [============================>.] - ETA: 0s - loss: 0.1109
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0222
6530/6530 [==============================] - 1s 90us/step - loss: 0.1110 - val_loss: 0.1124
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1162
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1379
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0223
 544/6530 [=>............................] - ETA: 0s - loss: 0.1142
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1361
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0220
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1091
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1374
4112/6530 [=================>............] - ETA: 0s - loss: 0.0217
1712/6530 [======>.......................] - ETA: 0s - loss: 0.1065
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1391
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0218
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1072
2848/6530 [============>.................] - ETA: 0s - loss: 0.1381
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0215
2864/6530 [============>.................] - ETA: 0s - loss: 0.1073
3104/6530 [=============>................] - ETA: 0s - loss: 0.1378
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0215
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1066
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1380
6530/6530 [==============================] - 1s 78us/step - loss: 0.0214 - val_loss: 0.0215
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0259
4016/6530 [=================>............] - ETA: 0s - loss: 0.1062
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1387
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0205
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1060
3888/6530 [================>.............] - ETA: 0s - loss: 0.1391
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0200
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1063
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1392
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0203
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1056
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1393
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0203
6336/6530 [============================>.] - ETA: 0s - loss: 0.1056
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1395
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0199
6530/6530 [==============================] - 1s 92us/step - loss: 0.1057 - val_loss: 0.1060
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1062
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1397
4064/6530 [=================>............] - ETA: 0s - loss: 0.0200
 592/6530 [=>............................] - ETA: 0s - loss: 0.1037
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1396
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0201
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1029
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1396
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0198
1712/6530 [======>.......................] - ETA: 0s - loss: 0.1029
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0198
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1389
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1030
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1393
6530/6530 [==============================] - 1s 79us/step - loss: 0.0197 - val_loss: 0.0203
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0222
2864/6530 [============>.................] - ETA: 0s - loss: 0.1028
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1392
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0188
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1027
6432/6530 [============================>.] - ETA: 0s - loss: 0.1393
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0190
3984/6530 [=================>............] - ETA: 0s - loss: 0.1029
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0191
6530/6530 [==============================] - 1s 207us/step - loss: 0.1391 - val_loss: 0.1201

4544/6530 [===================>..........] - ETA: 0s - loss: 0.1035Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1049
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0193
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1033
 272/6530 [>.............................] - ETA: 1s - loss: 0.1329
3264/6530 [=============>................] - ETA: 0s - loss: 0.0188
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1021
 528/6530 [=>............................] - ETA: 1s - loss: 0.1375
3904/6530 [================>.............] - ETA: 0s - loss: 0.0188
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1012
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1358
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0188
6530/6530 [==============================] - 1s 94us/step - loss: 0.1009 - val_loss: 0.1017
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0768
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1359
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0186
 544/6530 [=>............................] - ETA: 0s - loss: 0.0968
1264/6530 [====>.........................] - ETA: 1s - loss: 0.1357
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0186
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1341
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0980
6496/6530 [============================>.] - ETA: 0s - loss: 0.0185
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1333
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0985
6530/6530 [==============================] - 1s 82us/step - loss: 0.0185 - val_loss: 0.0194
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0196
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1326
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0987
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0179
2256/6530 [=========>....................] - ETA: 0s - loss: 0.1342
2848/6530 [============>.................] - ETA: 0s - loss: 0.0980
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0181
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1358
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0974
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0181
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1354
3952/6530 [=================>............] - ETA: 0s - loss: 0.0964
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0180
3024/6530 [============>.................] - ETA: 0s - loss: 0.1345
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0965
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0179
3264/6530 [=============>................] - ETA: 0s - loss: 0.1342
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0965
4096/6530 [=================>............] - ETA: 0s - loss: 0.0178
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1353
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0960
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0179
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0961
3776/6530 [================>.............] - ETA: 0s - loss: 0.1354
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0177
4032/6530 [=================>............] - ETA: 0s - loss: 0.1366
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0177
6530/6530 [==============================] - 1s 96us/step - loss: 0.0962 - val_loss: 0.0984
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1040
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1364
 480/6530 [=>............................] - ETA: 0s - loss: 0.0982
6530/6530 [==============================] - 1s 80us/step - loss: 0.0176 - val_loss: 0.0187
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0176
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1367
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0971
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0167
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1366
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0966
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0174
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1368
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0968
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0174
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1358
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0964
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0172
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1357
3216/6530 [=============>................] - ETA: 0s - loss: 0.0961
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0170
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1362
3792/6530 [================>.............] - ETA: 0s - loss: 0.0966
4128/6530 [=================>............] - ETA: 0s - loss: 0.0169
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0963
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1364
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0171
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0955
6320/6530 [============================>.] - ETA: 0s - loss: 0.1365
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0169
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0947
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0169
6530/6530 [==============================] - 1s 211us/step - loss: 0.1364 - val_loss: 0.1172
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1146
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0948
6530/6530 [==============================] - 1s 78us/step - loss: 0.0168 - val_loss: 0.0180

 272/6530 [>.............................] - ETA: 1s - loss: 0.1291
6530/6530 [==============================] - 1s 96us/step - loss: 0.0947 - val_loss: 0.1045

 528/6530 [=>............................] - ETA: 1s - loss: 0.1339
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1343
1040/6530 [===>..........................] - ETA: 1s - loss: 0.1358
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1347
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1328
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1339
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1326
# training | RMSE: 0.1298, MAE: 0.0986
worker 1  xfile  [6, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48890225339762805}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1297972280525287, 'rmse': 0.1297972280525287, 'mae': 0.09858513556635924, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  89 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45d86ddeb8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 13s - loss: 0.6288
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1338
3008/6530 [============>.................] - ETA: 0s - loss: 0.0933 
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1349
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0679
2848/6530 [============>.................] - ETA: 0s - loss: 0.1342
6530/6530 [==============================] - 0s 42us/step - loss: 0.0650 - val_loss: 0.0398
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0403
3104/6530 [=============>................] - ETA: 0s - loss: 0.1337
3072/6530 [=============>................] - ETA: 0s - loss: 0.0384
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1339
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0364
6530/6530 [==============================] - 0s 18us/step - loss: 0.0362 - val_loss: 0.0355
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0207
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1348
2944/6530 [============>.................] - ETA: 0s - loss: 0.0327
3776/6530 [================>.............] - ETA: 0s - loss: 0.1341
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0322
6530/6530 [==============================] - 0s 20us/step - loss: 0.0317 - val_loss: 0.0330
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0297
4048/6530 [=================>............] - ETA: 0s - loss: 0.1352
2944/6530 [============>.................] - ETA: 0s - loss: 0.0292
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1347
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0281
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1355
6530/6530 [==============================] - 0s 20us/step - loss: 0.0284 - val_loss: 0.0289
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0310
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1357
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0264
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1359
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0256
6530/6530 [==============================] - 0s 19us/step - loss: 0.0255 - val_loss: 0.0272
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0262
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1353
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0230
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1348
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0229
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1352
6530/6530 [==============================] - 0s 20us/step - loss: 0.0229 - val_loss: 0.0243
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0248
# training | RMSE: 0.1268, MAE: 0.0963
worker 2  xfile  [5, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4062271963680081}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.1268035918949659, 'rmse': 0.1268035918949659, 'mae': 0.09626842418696946, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  96 | activation: tanh    | extras: None 
layer 2 | size:  30 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  99 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45cd49ecc0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 44s - loss: 0.9696
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1350
2880/6530 [============>.................] - ETA: 0s - loss: 0.0206
1216/6530 [====>.........................] - ETA: 2s - loss: 0.2727 
6352/6530 [============================>.] - ETA: 0s - loss: 0.1351
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0208
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1936
6530/6530 [==============================] - 0s 19us/step - loss: 0.0208 - val_loss: 0.0223
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0174
6530/6530 [==============================] - 1s 211us/step - loss: 0.1351 - val_loss: 0.1146

3584/6530 [===============>..............] - ETA: 0s - loss: 0.1591
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0191
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1396
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0195
6530/6530 [==============================] - 0s 19us/step - loss: 0.0193 - val_loss: 0.0211
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0203
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1260
2944/6530 [============>.................] - ETA: 0s - loss: 0.0184
6530/6530 [==============================] - 1s 117us/step - loss: 0.1222 - val_loss: 0.0970

5952/6530 [==========================>...] - ETA: 0s - loss: 0.0181Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1088
6530/6530 [==============================] - 0s 18us/step - loss: 0.0180 - val_loss: 0.0217

1280/6530 [====>.........................] - ETA: 0s - loss: 0.0668
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0619
3712/6530 [================>.............] - ETA: 0s - loss: 0.0577
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0560
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0548
6530/6530 [==============================] - 0s 43us/step - loss: 0.0549 - val_loss: 0.0449
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0636
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0465
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0446
3776/6530 [================>.............] - ETA: 0s - loss: 0.0420
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0421
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0417
6530/6530 [==============================] - 0s 44us/step - loss: 0.0422 - val_loss: 0.0454
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0573
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0371
# training | RMSE: 0.1426, MAE: 0.1087
worker 0  xfile  [3, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26857746513450387}, 'layer_4_size': 25, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.48790055374956387}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.14262074136702063, 'rmse': 0.14262074136702063, 'mae': 0.10873218895944155, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  15 | activation: relu    | extras: None 
layer 2 | size:  21 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45cd59b3c8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 46s - loss: 2.6074
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0362
1152/6530 [====>.........................] - ETA: 1s - loss: 1.7331 
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0343
2240/6530 [=========>....................] - ETA: 0s - loss: 1.1946
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0348
3328/6530 [==============>...............] - ETA: 0s - loss: 0.8805
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0342
4416/6530 [===================>..........] - ETA: 0s - loss: 0.6908
6530/6530 [==============================] - 0s 45us/step - loss: 0.0349 - val_loss: 0.0358
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0359
5472/6530 [========================>.....] - ETA: 0s - loss: 0.5740
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0303
6496/6530 [============================>.] - ETA: 0s - loss: 0.4954
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0305
6530/6530 [==============================] - 1s 88us/step - loss: 0.4931 - val_loss: 0.0672
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0512
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0292
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0643
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0301
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0644
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0292
3072/6530 [=============>................] - ETA: 0s - loss: 0.0609
6530/6530 [==============================] - 0s 49us/step - loss: 0.0298 - val_loss: 0.0419
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0397
4032/6530 [=================>............] - ETA: 0s - loss: 0.0588
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0270
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0566
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0271
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0559
3776/6530 [================>.............] - ETA: 0s - loss: 0.0259
6530/6530 [==============================] - 0s 53us/step - loss: 0.0554 - val_loss: 0.0481
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0380
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0265
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0469
# training | RMSE: 0.1376, MAE: 0.1074
worker 1  xfile  [7, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.13756803746049917, 'rmse': 0.13756803746049917, 'mae': 0.10743541403089522, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  94 | activation: tanh    | extras: dropout - rate: 43.8% 
layer 2 | size:  42 | activation: relu    | extras: batchnorm 
layer 3 | size:  31 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  56 | activation: tanh    | extras: None 
layer 5 | size:  81 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f460405c2b0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 2:00 - loss: 3.4340
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0260
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0452
 512/6530 [=>............................] - ETA: 7s - loss: 1.0205  
6530/6530 [==============================] - 0s 44us/step - loss: 0.0265 - val_loss: 0.0412
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0295
3008/6530 [============>.................] - ETA: 0s - loss: 0.0452
 992/6530 [===>..........................] - ETA: 3s - loss: 0.6452
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0246
3968/6530 [=================>............] - ETA: 0s - loss: 0.0446
1440/6530 [=====>........................] - ETA: 2s - loss: 0.5086
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0246
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0444
1920/6530 [=======>......................] - ETA: 1s - loss: 0.4238
3712/6530 [================>.............] - ETA: 0s - loss: 0.0234
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0440
2400/6530 [==========>...................] - ETA: 1s - loss: 0.3677
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0241
6530/6530 [==============================] - 0s 53us/step - loss: 0.0438 - val_loss: 0.0425
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0386
2880/6530 [============>.................] - ETA: 1s - loss: 0.3273
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0234
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0402
3392/6530 [==============>...............] - ETA: 0s - loss: 0.2950
6530/6530 [==============================] - 0s 45us/step - loss: 0.0239 - val_loss: 0.0890
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0747
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0393
3840/6530 [================>.............] - ETA: 0s - loss: 0.2739
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0246
3104/6530 [=============>................] - ETA: 0s - loss: 0.0393
4320/6530 [==================>...........] - ETA: 0s - loss: 0.2543
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0228
4064/6530 [=================>............] - ETA: 0s - loss: 0.0397
4800/6530 [=====================>........] - ETA: 0s - loss: 0.2383
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0216
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0401
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2250
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0219
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0393
5664/6530 [=========================>....] - ETA: 0s - loss: 0.2139
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0213
6530/6530 [==============================] - 0s 53us/step - loss: 0.0389 - val_loss: 0.0391
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0256
6530/6530 [==============================] - 0s 46us/step - loss: 0.0216 - val_loss: 0.0381
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0327
6144/6530 [===========================>..] - ETA: 0s - loss: 0.2033
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0359
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0206
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0360
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0201
3200/6530 [=============>................] - ETA: 0s - loss: 0.0356
6530/6530 [==============================] - 1s 213us/step - loss: 0.1954 - val_loss: 0.0593
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0972
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0193
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0357
 544/6530 [=>............................] - ETA: 0s - loss: 0.0690
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0201
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0356
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0679
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0195
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0356
6530/6530 [==============================] - 0s 45us/step - loss: 0.0199 - val_loss: 0.0694

1504/6530 [=====>........................] - ETA: 0s - loss: 0.0675
6530/6530 [==============================] - 0s 51us/step - loss: 0.0356 - val_loss: 0.0362
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0292
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0680
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0328
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0677
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0334
# training | RMSE: 0.2541, MAE: 0.2098
worker 2  xfile  [8, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3000335360122979}, 'layer_5_size': 67, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.2540957869363026, 'rmse': 0.2540957869363026, 'mae': 0.2097914383139169, 'early_stop': True}
vggnet done  2

2976/6530 [============>.................] - ETA: 0s - loss: 0.0674
3136/6530 [=============>................] - ETA: 0s - loss: 0.0336
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0669
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0329
4000/6530 [=================>............] - ETA: 0s - loss: 0.0659
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0328
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0653
6496/6530 [============================>.] - ETA: 0s - loss: 0.0332
6530/6530 [==============================] - 0s 49us/step - loss: 0.0331 - val_loss: 0.0346
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0281
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0635
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0322
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0627
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0312
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0619
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0313
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0313
6530/6530 [==============================] - 1s 107us/step - loss: 0.0611 - val_loss: 0.0396
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0795
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0312
 512/6530 [=>............................] - ETA: 0s - loss: 0.0454
6368/6530 [============================>.] - ETA: 0s - loss: 0.0313
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0469
6530/6530 [==============================] - 0s 50us/step - loss: 0.0312 - val_loss: 0.0330
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0436
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0472
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0321
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0476
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0304
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0470
2976/6530 [============>.................] - ETA: 0s - loss: 0.0294
2912/6530 [============>.................] - ETA: 0s - loss: 0.0467
4000/6530 [=================>............] - ETA: 0s - loss: 0.0296
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0458
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0295
3872/6530 [================>.............] - ETA: 0s - loss: 0.0458
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0297
6530/6530 [==============================] - 0s 52us/step - loss: 0.0298 - val_loss: 0.0317
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0207
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0457
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0302
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0457
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0278
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0453
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0279
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0449
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0287
6496/6530 [============================>.] - ETA: 0s - loss: 0.0445
6530/6530 [==============================] - 1s 108us/step - loss: 0.0446 - val_loss: 0.0479
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0620
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0287
 544/6530 [=>............................] - ETA: 0s - loss: 0.0393
6530/6530 [==============================] - 0s 48us/step - loss: 0.0287 - val_loss: 0.0304

1056/6530 [===>..........................] - ETA: 0s - loss: 0.0397
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0391
# training | RMSE: 0.1669, MAE: 0.1313
worker 0  xfile  [10, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.16854481038475316}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1461065261872081}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3046963572135841}, 'layer_5_size': 88, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.16694029510542602, 'rmse': 0.16694029510542602, 'mae': 0.1313486805059005, 'early_stop': False}
vggnet done  0

2080/6530 [========>.....................] - ETA: 0s - loss: 0.0381
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0379
3168/6530 [=============>................] - ETA: 0s - loss: 0.0377
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0375
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0375
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0375
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0376
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0372
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0371
6464/6530 [============================>.] - ETA: 0s - loss: 0.0370
6530/6530 [==============================] - 1s 107us/step - loss: 0.0370 - val_loss: 0.0324
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0541
 480/6530 [=>............................] - ETA: 0s - loss: 0.0355
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0361
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0355
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0350
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0343
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0339
3040/6530 [============>.................] - ETA: 0s - loss: 0.0334
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0333
3936/6530 [=================>............] - ETA: 0s - loss: 0.0331
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0328
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0327
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0326
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0327
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0325
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0322
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0321
6496/6530 [============================>.] - ETA: 0s - loss: 0.0319
6530/6530 [==============================] - 1s 143us/step - loss: 0.0320 - val_loss: 0.0241
Epoch 6/9

  32/6530 [..............................] - ETA: 1s - loss: 0.0181
 416/6530 [>.............................] - ETA: 0s - loss: 0.0289
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0300
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0294
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0296
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0303
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0308
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0305
3168/6530 [=============>................] - ETA: 0s - loss: 0.0297
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0291
3840/6530 [================>.............] - ETA: 0s - loss: 0.0293
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0293
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0293
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0295
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0298
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0297
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0295
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0294
6368/6530 [============================>.] - ETA: 0s - loss: 0.0292
6530/6530 [==============================] - 1s 156us/step - loss: 0.0291 - val_loss: 0.0304
Epoch 7/9

  32/6530 [..............................] - ETA: 1s - loss: 0.0237
 288/6530 [>.............................] - ETA: 1s - loss: 0.0277
 512/6530 [=>............................] - ETA: 1s - loss: 0.0282
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0286
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0287
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0292
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0291
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0287
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0292
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0291
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0289
3008/6530 [============>.................] - ETA: 0s - loss: 0.0286
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0285
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0285
3904/6530 [================>.............] - ETA: 0s - loss: 0.0281
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0281
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0278
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0276
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0276
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0276
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0273
6336/6530 [============================>.] - ETA: 0s - loss: 0.0273
6530/6530 [==============================] - 1s 181us/step - loss: 0.0272 - val_loss: 0.0259
Epoch 8/9

  32/6530 [..............................] - ETA: 1s - loss: 0.0369
 288/6530 [>.............................] - ETA: 1s - loss: 0.0272
 576/6530 [=>............................] - ETA: 1s - loss: 0.0277
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0265
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0275
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0264
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0256
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0254
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0248
3008/6530 [============>.................] - ETA: 0s - loss: 0.0250
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0251
3776/6530 [================>.............] - ETA: 0s - loss: 0.0254
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0254
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0255
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0256
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0257
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0259
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0256
6464/6530 [============================>.] - ETA: 0s - loss: 0.0259
6530/6530 [==============================] - 1s 152us/step - loss: 0.0259 - val_loss: 0.0211
Epoch 9/9

  32/6530 [..............................] - ETA: 1s - loss: 0.0123
 352/6530 [>.............................] - ETA: 1s - loss: 0.0219
 640/6530 [=>............................] - ETA: 1s - loss: 0.0224
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0237
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0233
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0227
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0234
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0234
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0232
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0233
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0235
2944/6530 [============>.................] - ETA: 0s - loss: 0.0234
3232/6530 [=============>................] - ETA: 0s - loss: 0.0239
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0237
3840/6530 [================>.............] - ETA: 0s - loss: 0.0235
4128/6530 [=================>............] - ETA: 0s - loss: 0.0236
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0235
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0235
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0235
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0233
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0233
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0234
6336/6530 [============================>.] - ETA: 0s - loss: 0.0232
6530/6530 [==============================] - 1s 190us/step - loss: 0.0231 - val_loss: 0.0230

# training | RMSE: 0.1450, MAE: 0.1169
worker 1  xfile  [9, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43759601245045443}, 'layer_1_size': 94, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 31, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 56, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.14501703859510628, 'rmse': 0.14501703859510628, 'mae': 0.11689789251576876, 'early_stop': False}
vggnet done  1
all of workers have been done
calculation finished
#0 epoch=9.0 loss={'loss': 0.09208078884797326, 'rmse': 0.09208078884797326, 'mae': 0.06978214485301047, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2806300923523829}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#1 epoch=9.0 loss={'loss': 0.09341991209338138, 'rmse': 0.09341991209338138, 'mae': 0.0721426525760442, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11698107846478517}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4994624948580816}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#2 epoch=9.0 loss={'loss': 0.3418541171732119, 'rmse': 0.3418541171732119, 'mae': 0.3226677289743417, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 5, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#4 epoch=9.0 loss={'loss': 0.11252647421811493, 'rmse': 0.11252647421811493, 'mae': 0.08643188075155847, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10607566151554111}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#6 epoch=9.0 loss={'loss': 0.1297972280525287, 'rmse': 0.1297972280525287, 'mae': 0.09858513556635924, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48890225339762805}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#5 epoch=9.0 loss={'loss': 0.1268035918949659, 'rmse': 0.1268035918949659, 'mae': 0.09626842418696946, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4062271963680081}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#7 epoch=9.0 loss={'loss': 0.13756803746049917, 'rmse': 0.13756803746049917, 'mae': 0.10743541403089522, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#3 epoch=9.0 loss={'loss': 0.14262074136702063, 'rmse': 0.14262074136702063, 'mae': 0.10873218895944155, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26857746513450387}, 'layer_4_size': 25, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.48790055374956387}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#8 epoch=9.0 loss={'loss': 0.2540957869363026, 'rmse': 0.2540957869363026, 'mae': 0.2097914383139169, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 96, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3000335360122979}, 'layer_5_size': 67, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#10 epoch=9.0 loss={'loss': 0.16694029510542602, 'rmse': 0.16694029510542602, 'mae': 0.1313486805059005, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.16854481038475316}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1461065261872081}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3046963572135841}, 'layer_5_size': 88, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#9 epoch=9.0 loss={'loss': 0.14501703859510628, 'rmse': 0.14501703859510628, 'mae': 0.11689789251576876, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43759601245045443}, 'layer_1_size': 94, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 31, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 56, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
get a list [results] of length 166
get a list [loss] of length 11
get a list [val_loss] of length 11
length of indices is (0, 1, 4, 5, 6, 7, 3, 9, 10, 8, 2)
length of indices is 11
length of T is 11
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2806300923523829}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [1, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11698107846478517}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4994624948580816}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [2, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10607566151554111}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]] 

*** 3.7777777777777777 configurations x 27.0 iterations each

10 | Thu Sep 27 23:08:59 2018 | lowest loss so far: 0.0675 (run 0)

{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  96 | activation: tanh    | extras: dropout - rate: 10.6% 
layer 2 | size:  78 | activation: tanh    | extras: None 
layer 3 | size:  76 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 2:27 - loss: 0.7090
 672/6530 [==>...........................] - ETA: 6s - loss: 0.5053  
1408/6530 [=====>........................] - ETA: 3s - loss: 0.3799
2304/6530 [=========>....................] - ETA: 1s - loss: 0.3207
3168/6530 [=============>................] - ETA: 0s - loss: 0.2841
4192/6530 [==================>...........] - ETA: 0s - loss: 0.2578
5280/6530 [=======================>......] - ETA: 0s - loss: 0.2365
6336/6530 [============================>.] - ETA: 0s - loss: 0.2219
6530/6530 [==============================] - 1s 174us/step - loss: 0.2200 - val_loss: 0.1801
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1740
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1422
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1360
3232/6530 [=============>................] - ETA: 0s - loss: 0.1348{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  63 | activation: relu    | extras: batchnorm 
layer 2 | size:  66 | activation: sigmoid | extras: dropout - rate: 11.7% 
layer 3 | size:  16 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 3:04 - loss: 0.5578
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1337
 608/6530 [=>............................] - ETA: 9s - loss: 0.2835  
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1335
1216/6530 [====>.........................] - ETA: 4s - loss: 0.1739
6400/6530 [============================>.] - ETA: 0s - loss: 0.1318
1856/6530 [=======>......................] - ETA: 2s - loss: 0.1292
6530/6530 [==============================] - 0s 50us/step - loss: 0.1319 - val_loss: 0.1955
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1738
2432/6530 [==========>...................] - ETA: 1s - loss: 0.1085
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1274{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  90 | activation: relu    | extras: batchnorm 
layer 2 | size:  64 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  57 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8ac550>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 128/6530 [..............................] - ETA: 50s - loss: 1.2484
3008/6530 [============>.................] - ETA: 1s - loss: 0.0952
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1247
2176/6530 [========>.....................] - ETA: 2s - loss: 0.9502 
3584/6530 [===============>..............] - ETA: 1s - loss: 0.0862
3232/6530 [=============>................] - ETA: 0s - loss: 0.1229
4224/6530 [==================>...........] - ETA: 0s - loss: 0.6678
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0793
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1208
6144/6530 [===========================>..] - ETA: 0s - loss: 0.5117
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0742
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1209
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0699
6530/6530 [==============================] - 1s 188us/step - loss: 0.4909 - val_loss: 0.1477
Epoch 2/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1489
6400/6530 [============================>.] - ETA: 0s - loss: 0.1194
6530/6530 [==============================] - 0s 51us/step - loss: 0.1194 - val_loss: 0.1961
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1852
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0667
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1332
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1158
6432/6530 [============================>.] - ETA: 0s - loss: 0.0637
4096/6530 [=================>............] - ETA: 0s - loss: 0.1271
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1169
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1224
6530/6530 [==============================] - 2s 238us/step - loss: 0.0631 - val_loss: 0.0320
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0328
6530/6530 [==============================] - 0s 28us/step - loss: 0.1213 - val_loss: 0.1074
Epoch 3/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1126
3072/6530 [=============>................] - ETA: 0s - loss: 0.1148
 640/6530 [=>............................] - ETA: 0s - loss: 0.0312
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1059
4128/6530 [=================>............] - ETA: 0s - loss: 0.1135
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0306
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1128
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1024
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0299
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1124
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1014
6530/6530 [==============================] - 0s 27us/step - loss: 0.1011 - val_loss: 0.0961
Epoch 4/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0980
6530/6530 [==============================] - 0s 52us/step - loss: 0.1123 - val_loss: 0.1089
Epoch 5/27

  32/6530 [..............................] - ETA: 1s - loss: 0.1275
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0303
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0965
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1102
2912/6530 [============>.................] - ETA: 0s - loss: 0.0303
3712/6530 [================>.............] - ETA: 0s - loss: 0.0927
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1112
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0302
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0918
2880/6530 [============>.................] - ETA: 0s - loss: 0.1101
4096/6530 [=================>............] - ETA: 0s - loss: 0.0300
6530/6530 [==============================] - 0s 29us/step - loss: 0.0909 - val_loss: 0.0923
Epoch 5/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0864
3936/6530 [=================>............] - ETA: 0s - loss: 0.1092
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0298
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0902
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1085
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0295
3968/6530 [=================>............] - ETA: 0s - loss: 0.0886
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1078
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0291
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0867
6530/6530 [==============================] - 0s 53us/step - loss: 0.1073 - val_loss: 0.1557
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1659
6530/6530 [==============================] - 0s 28us/step - loss: 0.0863 - val_loss: 0.0885
Epoch 6/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0768
6368/6530 [============================>.] - ETA: 0s - loss: 0.0292
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1099
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0830
6530/6530 [==============================] - 1s 93us/step - loss: 0.0289 - val_loss: 0.0245
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0206
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1060
3712/6530 [================>.............] - ETA: 0s - loss: 0.0806
 576/6530 [=>............................] - ETA: 0s - loss: 0.0238
2944/6530 [============>.................] - ETA: 0s - loss: 0.1037
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0801
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0238
6530/6530 [==============================] - 0s 29us/step - loss: 0.0796 - val_loss: 0.0854
Epoch 7/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0753
4000/6530 [=================>............] - ETA: 0s - loss: 0.1038
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0236
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0802
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1038
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0240
3840/6530 [================>.............] - ETA: 0s - loss: 0.0786
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1034
2912/6530 [============>.................] - ETA: 0s - loss: 0.0242
6530/6530 [==============================] - 0s 52us/step - loss: 0.1038 - val_loss: 0.1096
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1159
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0779
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0239
6530/6530 [==============================] - 0s 28us/step - loss: 0.0775 - val_loss: 0.0845
Epoch 8/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0752
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1013
3968/6530 [=================>............] - ETA: 0s - loss: 0.0239
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0812
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1006
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0237
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0787
3264/6530 [=============>................] - ETA: 0s - loss: 0.1002
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0235
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0770
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1004
6530/6530 [==============================] - 0s 27us/step - loss: 0.0769 - val_loss: 0.0773
Epoch 9/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0685
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0233
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1007
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0735
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0234
6528/6530 [============================>.] - ETA: 0s - loss: 0.1007
3968/6530 [=================>............] - ETA: 0s - loss: 0.0733
6530/6530 [==============================] - 0s 50us/step - loss: 0.1007 - val_loss: 0.1296
Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1072
6530/6530 [==============================] - 1s 93us/step - loss: 0.0231 - val_loss: 0.0208
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0181
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0726
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0983
 576/6530 [=>............................] - ETA: 0s - loss: 0.0190
6530/6530 [==============================] - 0s 28us/step - loss: 0.0726 - val_loss: 0.0778
Epoch 10/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0719
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0999
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0191
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0748
3104/6530 [=============>................] - ETA: 0s - loss: 0.1009
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0193
4096/6530 [=================>............] - ETA: 0s - loss: 0.0737
4064/6530 [=================>............] - ETA: 0s - loss: 0.0997
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0196
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0722
6530/6530 [==============================] - 0s 28us/step - loss: 0.0720 - val_loss: 0.0744
Epoch 11/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0633
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1001
2944/6530 [============>.................] - ETA: 0s - loss: 0.0199
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0701
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0996
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0198
6530/6530 [==============================] - 0s 52us/step - loss: 0.0993 - val_loss: 0.1401
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1428
4096/6530 [=================>............] - ETA: 0s - loss: 0.0701
4096/6530 [=================>............] - ETA: 0s - loss: 0.0196
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0980
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0694
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0197
6530/6530 [==============================] - 0s 27us/step - loss: 0.0694 - val_loss: 0.0760
Epoch 12/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0689
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0965
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0195
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0715
3200/6530 [=============>................] - ETA: 0s - loss: 0.0969
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0194
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0706
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0967
6368/6530 [============================>.] - ETA: 0s - loss: 0.0195
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0697
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0959
6530/6530 [==============================] - 0s 27us/step - loss: 0.0695 - val_loss: 0.0712
Epoch 13/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0615
6530/6530 [==============================] - 1s 92us/step - loss: 0.0194 - val_loss: 0.0162
Epoch 5/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0164
6400/6530 [============================>.] - ETA: 0s - loss: 0.0959
6530/6530 [==============================] - 0s 50us/step - loss: 0.0956 - val_loss: 0.1162

2176/6530 [========>.....................] - ETA: 0s - loss: 0.0681
 640/6530 [=>............................] - ETA: 0s - loss: 0.0166
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0676
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0166
# training | RMSE: 0.1487, MAE: 0.1158
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10607566151554111}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.1486641077220223, 'rmse': 0.1486641077220223, 'mae': 0.11579867915791348, 'early_stop': True}
vggnet done  2

6272/6530 [===========================>..] - ETA: 0s - loss: 0.0672
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0167
6530/6530 [==============================] - 0s 27us/step - loss: 0.0671 - val_loss: 0.0713
Epoch 14/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0615
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0170
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0670
3040/6530 [============>.................] - ETA: 0s - loss: 0.0168
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0669
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0167
6530/6530 [==============================] - 0s 25us/step - loss: 0.0664 - val_loss: 0.0700
Epoch 15/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0595
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0167
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0656
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0167
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0655
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0167
6530/6530 [==============================] - 0s 26us/step - loss: 0.0650 - val_loss: 0.0696
Epoch 16/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0603
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0165
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0664
6530/6530 [==============================] - 1s 88us/step - loss: 0.0165 - val_loss: 0.0143
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0123
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0656
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0147
6528/6530 [============================>.] - ETA: 0s - loss: 0.0645
6530/6530 [==============================] - 0s 26us/step - loss: 0.0645 - val_loss: 0.0688
Epoch 17/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0587
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0149
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0669
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0148
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0660
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0152
3168/6530 [=============>................] - ETA: 0s - loss: 0.0148
6530/6530 [==============================] - 0s 25us/step - loss: 0.0650 - val_loss: 0.0673
Epoch 18/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0579
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0149
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0653
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0150
3968/6530 [=================>............] - ETA: 0s - loss: 0.0649
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0151
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0637
6530/6530 [==============================] - 0s 27us/step - loss: 0.0634 - val_loss: 0.0679
Epoch 19/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0587
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0150
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0656
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0149
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0640
6530/6530 [==============================] - 1s 87us/step - loss: 0.0150 - val_loss: 0.0129
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0115
6530/6530 [==============================] - 0s 25us/step - loss: 0.0629 - val_loss: 0.0685
Epoch 20/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0600
 640/6530 [=>............................] - ETA: 0s - loss: 0.0142
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0682
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0143
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0656
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0142
6400/6530 [============================>.] - ETA: 0s - loss: 0.0648
6530/6530 [==============================] - 0s 26us/step - loss: 0.0647 - val_loss: 0.0694

2496/6530 [==========>...................] - ETA: 0s - loss: 0.0145Epoch 21/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0623
3104/6530 [=============>................] - ETA: 0s - loss: 0.0138
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0680
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0657
3744/6530 [================>.............] - ETA: 0s - loss: 0.0138
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0138
6400/6530 [============================>.] - ETA: 0s - loss: 0.0645
6530/6530 [==============================] - 0s 26us/step - loss: 0.0644 - val_loss: 0.0688
Epoch 22/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0582
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0138
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0656
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0137
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0637
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0139
6400/6530 [============================>.] - ETA: 0s - loss: 0.0627
6530/6530 [==============================] - 0s 26us/step - loss: 0.0626 - val_loss: 0.0688

6530/6530 [==============================] - 1s 85us/step - loss: 0.0138 - val_loss: 0.0120
Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0123
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0133
# training | RMSE: 0.0803, MAE: 0.0608
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2806300923523829}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.08025686929248477, 'rmse': 0.08025686929248477, 'mae': 0.06080904557481669, 'early_stop': True}
vggnet done  0

1312/6530 [=====>........................] - ETA: 0s - loss: 0.0132
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0134
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0135
3200/6530 [=============>................] - ETA: 0s - loss: 0.0130
3808/6530 [================>.............] - ETA: 0s - loss: 0.0131
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0134
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0133
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0132
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0132
6530/6530 [==============================] - 1s 87us/step - loss: 0.0133 - val_loss: 0.0128
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0111
 608/6530 [=>............................] - ETA: 0s - loss: 0.0130
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0127
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0129
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0128
3008/6530 [============>.................] - ETA: 0s - loss: 0.0127
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0127
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0128
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0128
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0128
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0126
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0127
6530/6530 [==============================] - 1s 99us/step - loss: 0.0127 - val_loss: 0.0103
Epoch 10/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0109
 512/6530 [=>............................] - ETA: 0s - loss: 0.0119
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0118
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0120
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0120
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0124
3008/6530 [============>.................] - ETA: 0s - loss: 0.0121
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0121
4064/6530 [=================>............] - ETA: 0s - loss: 0.0122
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0124
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0123
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0123
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0123
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0122
6496/6530 [============================>.] - ETA: 0s - loss: 0.0123
6530/6530 [==============================] - 1s 117us/step - loss: 0.0123 - val_loss: 0.0103
Epoch 11/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0093
 416/6530 [>.............................] - ETA: 0s - loss: 0.0117
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0122
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0119
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0120
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0121
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0118
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0121
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0119
3168/6530 [=============>................] - ETA: 0s - loss: 0.0116
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0117
3904/6530 [================>.............] - ETA: 0s - loss: 0.0118
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0118
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0119
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0119
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0118
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0118
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0118
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0119
6530/6530 [==============================] - 1s 158us/step - loss: 0.0119 - val_loss: 0.0097
Epoch 12/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0087
 352/6530 [>.............................] - ETA: 1s - loss: 0.0114
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0116
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0111
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0113
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0115
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0112
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0113
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0115
3008/6530 [============>.................] - ETA: 0s - loss: 0.0114
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0112
3712/6530 [================>.............] - ETA: 0s - loss: 0.0114
4064/6530 [=================>............] - ETA: 0s - loss: 0.0115
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0116
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0116
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0116
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0117
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0117
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0116
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0118
6530/6530 [==============================] - 1s 162us/step - loss: 0.0117 - val_loss: 0.0110
Epoch 13/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0105
 416/6530 [>.............................] - ETA: 0s - loss: 0.0114
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0116
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0112
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0111
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0112
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0109
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0111
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0111
3200/6530 [=============>................] - ETA: 0s - loss: 0.0108
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0110
3904/6530 [================>.............] - ETA: 0s - loss: 0.0111
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0112
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0112
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0112
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0112
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0112
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0112
6496/6530 [============================>.] - ETA: 0s - loss: 0.0114
6530/6530 [==============================] - 1s 152us/step - loss: 0.0113 - val_loss: 0.0096
Epoch 14/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0093
 416/6530 [>.............................] - ETA: 0s - loss: 0.0111
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0112
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0106
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0108
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0105
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0107
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0107
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0104
3776/6530 [================>.............] - ETA: 0s - loss: 0.0104
4128/6530 [=================>............] - ETA: 0s - loss: 0.0106
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0108
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0108
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0109
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0108
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0108
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0108
6530/6530 [==============================] - 1s 142us/step - loss: 0.0109 - val_loss: 0.0091
Epoch 15/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0077
 480/6530 [=>............................] - ETA: 0s - loss: 0.0107
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0105
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0105
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0106
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0104
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0105
3040/6530 [============>.................] - ETA: 0s - loss: 0.0104
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0103
3776/6530 [================>.............] - ETA: 0s - loss: 0.0104
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0106
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0108
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0107
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0108
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0108
6336/6530 [============================>.] - ETA: 0s - loss: 0.0109
6530/6530 [==============================] - 1s 128us/step - loss: 0.0109 - val_loss: 0.0095
Epoch 16/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0081
 416/6530 [>.............................] - ETA: 0s - loss: 0.0115
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0112
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0107
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0108
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0106
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0105
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0106
3008/6530 [============>.................] - ETA: 0s - loss: 0.0105
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0103
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0105
3968/6530 [=================>............] - ETA: 0s - loss: 0.0105
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0106
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0106
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0106
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0106
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0106
6336/6530 [============================>.] - ETA: 0s - loss: 0.0108
6530/6530 [==============================] - 1s 146us/step - loss: 0.0107 - val_loss: 0.0090
Epoch 17/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0068
 384/6530 [>.............................] - ETA: 0s - loss: 0.0109
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0106
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0100
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0102
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0103
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0100
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0103
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0102
3072/6530 [=============>................] - ETA: 0s - loss: 0.0100
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0099
3776/6530 [================>.............] - ETA: 0s - loss: 0.0100
4128/6530 [=================>............] - ETA: 0s - loss: 0.0101
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0103
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0103
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0103
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0103
6336/6530 [============================>.] - ETA: 0s - loss: 0.0105
6530/6530 [==============================] - 1s 145us/step - loss: 0.0104 - val_loss: 0.0084
Epoch 18/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0081
 544/6530 [=>............................] - ETA: 0s - loss: 0.0108
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0103
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0104
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0101
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0103
3168/6530 [=============>................] - ETA: 0s - loss: 0.0100
3712/6530 [================>.............] - ETA: 0s - loss: 0.0100
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0102
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0103
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0104
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0104
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0104
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0104
6400/6530 [============================>.] - ETA: 0s - loss: 0.0105
6530/6530 [==============================] - 1s 120us/step - loss: 0.0105 - val_loss: 0.0082
Epoch 19/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0076
 320/6530 [>.............................] - ETA: 1s - loss: 0.0113
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0112
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0103
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0102
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0102
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0100
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0103
2880/6530 [============>.................] - ETA: 0s - loss: 0.0101
3264/6530 [=============>................] - ETA: 0s - loss: 0.0099
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0100
4096/6530 [=================>............] - ETA: 0s - loss: 0.0102
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0104
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0103
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0102
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0102
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0102
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0104
6530/6530 [==============================] - 1s 147us/step - loss: 0.0104 - val_loss: 0.0082
Epoch 20/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0094
 448/6530 [=>............................] - ETA: 0s - loss: 0.0109
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0104
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0100
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0102
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0100
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0100
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0101
3232/6530 [=============>................] - ETA: 0s - loss: 0.0098
3744/6530 [================>.............] - ETA: 0s - loss: 0.0099
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0101
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0102
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0102
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0102
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0102
6530/6530 [==============================] - 1s 121us/step - loss: 0.0103 - val_loss: 0.0082
Epoch 21/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0080
 544/6530 [=>............................] - ETA: 0s - loss: 0.0108
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0101
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0100
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0096
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0098
3232/6530 [=============>................] - ETA: 0s - loss: 0.0095
3744/6530 [================>.............] - ETA: 0s - loss: 0.0095
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0098
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0098
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0098
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0098
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0100
6400/6530 [============================>.] - ETA: 0s - loss: 0.0101
6530/6530 [==============================] - 1s 111us/step - loss: 0.0101 - val_loss: 0.0081
Epoch 22/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0084
 448/6530 [=>............................] - ETA: 0s - loss: 0.0103
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0098
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0095
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0095
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0096
2912/6530 [============>.................] - ETA: 0s - loss: 0.0095
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0094
3808/6530 [================>.............] - ETA: 0s - loss: 0.0095
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0097
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0098
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0098
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0098
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0098
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 1s 118us/step - loss: 0.0100 - val_loss: 0.0078
Epoch 23/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0072
 544/6530 [=>............................] - ETA: 0s - loss: 0.0099
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0096
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0097
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0097
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0095
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0097
2848/6530 [============>.................] - ETA: 0s - loss: 0.0096
3232/6530 [=============>................] - ETA: 0s - loss: 0.0094
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0095
3936/6530 [=================>............] - ETA: 0s - loss: 0.0097
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0098
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0098
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0098
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0098
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0097
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0098
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 1s 146us/step - loss: 0.0100 - val_loss: 0.0083
Epoch 24/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0075
 448/6530 [=>............................] - ETA: 0s - loss: 0.0106
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0100
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0094
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0097
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0094
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0093
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0095
3040/6530 [============>.................] - ETA: 0s - loss: 0.0093
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0092
3744/6530 [================>.............] - ETA: 0s - loss: 0.0093
4064/6530 [=================>............] - ETA: 0s - loss: 0.0095
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0096
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0097
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0096
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0097
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0097
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0099
6530/6530 [==============================] - 1s 149us/step - loss: 0.0098 - val_loss: 0.0075
Epoch 25/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0070
 416/6530 [>.............................] - ETA: 0s - loss: 0.0102
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0100
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0095
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0096
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0095
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0093
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0093
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0094
2912/6530 [============>.................] - ETA: 0s - loss: 0.0093
3200/6530 [=============>................] - ETA: 0s - loss: 0.0093
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0093
3776/6530 [================>.............] - ETA: 0s - loss: 0.0093
4064/6530 [=================>............] - ETA: 0s - loss: 0.0095
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0095
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0096
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0095
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0096
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0096
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0097
6432/6530 [============================>.] - ETA: 0s - loss: 0.0098
6530/6530 [==============================] - 1s 167us/step - loss: 0.0097 - val_loss: 0.0077
Epoch 26/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0082
 384/6530 [>.............................] - ETA: 0s - loss: 0.0105
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0099
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0098
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0098
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0096
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0094
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0093
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0095
2944/6530 [============>.................] - ETA: 0s - loss: 0.0094
3264/6530 [=============>................] - ETA: 0s - loss: 0.0092
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0093
4000/6530 [=================>............] - ETA: 0s - loss: 0.0095
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0096
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0096
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0096
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0096
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0097
6530/6530 [==============================] - 1s 146us/step - loss: 0.0098 - val_loss: 0.0076
Epoch 27/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0072
 512/6530 [=>............................] - ETA: 0s - loss: 0.0101
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0094
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0092
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0093
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0091
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0092
2912/6530 [============>.................] - ETA: 0s - loss: 0.0091
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0090
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0092
3968/6530 [=================>............] - ETA: 0s - loss: 0.0093
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0093
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0094
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0094
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0094
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0094
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0095
6528/6530 [============================>.] - ETA: 0s - loss: 0.0095
6530/6530 [==============================] - 1s 143us/step - loss: 0.0095 - val_loss: 0.0075

# training | RMSE: 0.0810, MAE: 0.0631
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11698107846478517}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4994624948580816}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.08096879935001718, 'rmse': 0.08096879935001718, 'mae': 0.06305937178737675, 'early_stop': False}
vggnet done  1
all of workers have been done
calculation finished
#2 epoch=27.0 loss={'loss': 0.1486641077220223, 'rmse': 0.1486641077220223, 'mae': 0.11579867915791348, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10607566151554111}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#0 epoch=27.0 loss={'loss': 0.08025686929248477, 'rmse': 0.08025686929248477, 'mae': 0.06080904557481669, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2806300923523829}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#1 epoch=27.0 loss={'loss': 0.08096879935001718, 'rmse': 0.08096879935001718, 'mae': 0.06305937178737675, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11698107846478517}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4994624948580816}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
get a list [results] of length 169
get a list [loss] of length 3
get a list [val_loss] of length 3
length of indices is (0, 1, 2)
length of indices is 3
length of T is 3
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10607566151554111}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]] 

*** 1.259259259259259 configurations x 81.0 iterations each

2 | Thu Sep 27 23:09:23 2018 | lowest loss so far: 0.0675 (run 0)

vggnet done  1
vggnet done  2
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  96 | activation: tanh    | extras: dropout - rate: 10.6% 
layer 2 | size:  78 | activation: tanh    | extras: None 
layer 3 | size:  76 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  32/6530 [..............................] - ETA: 2:18 - loss: 0.7090
 928/6530 [===>..........................] - ETA: 4s - loss: 0.4496  
1760/6530 [=======>......................] - ETA: 2s - loss: 0.3509
2528/6530 [==========>...................] - ETA: 1s - loss: 0.3099
3424/6530 [==============>...............] - ETA: 0s - loss: 0.2766
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2537
5408/6530 [=======================>......] - ETA: 0s - loss: 0.2346
6496/6530 [============================>.] - ETA: 0s - loss: 0.2203
6530/6530 [==============================] - 1s 167us/step - loss: 0.2200 - val_loss: 0.1801
Epoch 2/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1740
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1429
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1359
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1345
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1334
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1337
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1325
6530/6530 [==============================] - 0s 57us/step - loss: 0.1319 - val_loss: 0.1955
Epoch 3/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1738
 800/6530 [==>...........................] - ETA: 0s - loss: 0.1274
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1255
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1245
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1238
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1220
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1205
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1207
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1206
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1196
6530/6530 [==============================] - 0s 77us/step - loss: 0.1194 - val_loss: 0.1961
Epoch 4/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1852
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1175
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1169
2976/6530 [============>.................] - ETA: 0s - loss: 0.1147
3936/6530 [=================>............] - ETA: 0s - loss: 0.1137
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1125
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1127
6530/6530 [==============================] - 0s 53us/step - loss: 0.1123 - val_loss: 0.1089
Epoch 5/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1275
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1076
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1112
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1104
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1102
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1088
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1085
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1084
6496/6530 [============================>.] - ETA: 0s - loss: 0.1074
6530/6530 [==============================] - 0s 66us/step - loss: 0.1073 - val_loss: 0.1557
Epoch 6/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1659
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1112
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1072
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1052
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1039
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1034
4128/6530 [=================>............] - ETA: 0s - loss: 0.1039
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1037
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1036
6464/6530 [============================>.] - ETA: 0s - loss: 0.1035
6530/6530 [==============================] - 0s 75us/step - loss: 0.1038 - val_loss: 0.1096
Epoch 7/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1159
 800/6530 [==>...........................] - ETA: 0s - loss: 0.1002
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1028
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1007
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0999
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1002
4096/6530 [=================>............] - ETA: 0s - loss: 0.1000
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1004
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1006
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1004
6530/6530 [==============================] - 1s 80us/step - loss: 0.1007 - val_loss: 0.1296
Epoch 8/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1072
 576/6530 [=>............................] - ETA: 0s - loss: 0.0983
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0984
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1005
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0998
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1006
3264/6530 [=============>................] - ETA: 0s - loss: 0.1004
3776/6530 [================>.............] - ETA: 0s - loss: 0.1004
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0996
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1000
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1000
6336/6530 [============================>.] - ETA: 0s - loss: 0.0995
6530/6530 [==============================] - 1s 94us/step - loss: 0.0993 - val_loss: 0.1401
Epoch 9/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1429
 576/6530 [=>............................] - ETA: 0s - loss: 0.1024
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0987
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0968
2880/6530 [============>.................] - ETA: 0s - loss: 0.0967
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0971
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0969
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0960
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0959
6530/6530 [==============================] - 0s 73us/step - loss: 0.0956 - val_loss: 0.1114

# training | RMSE: 0.1428, MAE: 0.1109
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10607566151554111}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.14279495514875382, 'rmse': 0.14279495514875382, 'mae': 0.11085109108446459, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=81.0 loss={'loss': 0.14279495514875382, 'rmse': 0.14279495514875382, 'mae': 0.11085109108446459, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10607566151554111}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
get a list [results] of length 170
get a list [loss] of length 1
get a list [val_loss] of length 1
length of indices is (0,)
length of indices is 1
length of T is 1
s=2
T is of size 15
T=[{'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47234950478948456}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 70, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.28486462957277}, 'layer_3_size': 3, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4760294151940936}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38379181805725937}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.151550479705256}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.302283486099243}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 57, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.330942552638289}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17444003372492284}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.37292523848627124}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10992466825922427}, 'layer_1_size': 23, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17662673073095092}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22893106919985234}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4247664644070097}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19334055170839967}, 'layer_2_size': 80, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42508411914061284}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 89, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36459418788069975}, 'layer_1_size': 27, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2333825191150376}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 56, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42127947953652023}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 80, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 99, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22024861517858152}, 'layer_4_size': 56, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3396711412072355}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10991956439681334}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25088382860678343}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3403550800994245}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 9, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3295619155514554}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14576819747292818}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21545192657439155}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 98, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]
newly formed T structure is:[[0, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47234950478948456}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 70, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.28486462957277}, 'layer_3_size': 3, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4760294151940936}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [1, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38379181805725937}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.151550479705256}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.302283486099243}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [2, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 57, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.330942552638289}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17444003372492284}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [3, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.37292523848627124}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [4, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10992466825922427}, 'layer_1_size': 23, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17662673073095092}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [5, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22893106919985234}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [6, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4247664644070097}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19334055170839967}, 'layer_2_size': 80, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [7, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42508411914061284}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 89, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [8, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36459418788069975}, 'layer_1_size': 27, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2333825191150376}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 56, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [9, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42127947953652023}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [10, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [11, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 80, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 99, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22024861517858152}, 'layer_4_size': 56, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3396711412072355}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [12, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10991956439681334}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25088382860678343}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [13, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3403550800994245}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 9, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3295619155514554}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [14, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14576819747292818}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21545192657439155}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 98, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]] 

*** 15 configurations x 9.0 iterations each

1 | Thu Sep 27 23:09:28 2018 | lowest loss so far: 0.0675 (run 0)

{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  52 | activation: relu    | extras: dropout - rate: 38.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8aceb8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 41s - loss: 0.7694{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  40 | activation: relu    | extras: dropout - rate: 47.2% 
layer 2 | size:  70 | activation: sigmoid | extras: None 
layer 3 | size:   3 | activation: tanh    | extras: dropout - rate: 28.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 21s - loss: 0.7281
3584/6530 [===============>..............] - ETA: 0s - loss: 0.6157 
6530/6530 [==============================] - 1s 145us/step - loss: 0.5147 - val_loss: 0.3163
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.3571
6530/6530 [==============================] - 1s 145us/step - loss: 0.4702 - val_loss: 0.3369
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.3365
6530/6530 [==============================] - 0s 7us/step - loss: 0.2863 - val_loss: 0.2581

4480/6530 [===================>..........] - ETA: 0s - loss: 0.2527Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2520
6530/6530 [==============================] - 0s 13us/step - loss: 0.2372 - val_loss: 0.1949
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1879
6530/6530 [==============================] - 0s 8us/step - loss: 0.2395 - val_loss: 0.2307
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2238
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1834
6530/6530 [==============================] - 0s 7us/step - loss: 0.2240 - val_loss: 0.2203
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2148
6530/6530 [==============================] - 0s 12us/step - loss: 0.1802 - val_loss: 0.1705
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:   4 | activation: tanh    | extras: None 
layer 2 | size:  57 | activation: tanh    | extras: None 
layer 3 | size:  87 | activation: tanh    | extras: dropout - rate: 33.1% 
layer 4 | size:  95 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 23s - loss: 0.3260Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1715
6530/6530 [==============================] - 0s 7us/step - loss: 0.2184 - val_loss: 0.2162
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2117
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0961 
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1634
6530/6530 [==============================] - 0s 12us/step - loss: 0.1625 - val_loss: 0.1596
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1598
6530/6530 [==============================] - 0s 7us/step - loss: 0.2162 - val_loss: 0.2141
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2102
6530/6530 [==============================] - 1s 158us/step - loss: 0.0906 - val_loss: 0.0557
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0572
6530/6530 [==============================] - 0s 7us/step - loss: 0.2145 - val_loss: 0.2119

4864/6530 [=====================>........] - ETA: 0s - loss: 0.1546Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2081
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0537
6530/6530 [==============================] - 0s 10us/step - loss: 0.0530 - val_loss: 0.0492
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0504
6530/6530 [==============================] - 0s 11us/step - loss: 0.1541 - val_loss: 0.1556
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1737
6530/6530 [==============================] - 0s 7us/step - loss: 0.2109 - val_loss: 0.2061
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2015
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0463
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1490
6530/6530 [==============================] - 0s 10us/step - loss: 0.0458 - val_loss: 0.0429
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0445
6530/6530 [==============================] - 0s 7us/step - loss: 0.2029 - val_loss: 0.1984

6530/6530 [==============================] - 0s 12us/step - loss: 0.1476 - val_loss: 0.1483
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1495
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0414
6530/6530 [==============================] - 0s 10us/step - loss: 0.0412 - val_loss: 0.0405

4736/6530 [====================>.........] - ETA: 0s - loss: 0.1421Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0404
6530/6530 [==============================] - 0s 12us/step - loss: 0.1422 - val_loss: 0.1441
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1400
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0405
6530/6530 [==============================] - 0s 10us/step - loss: 0.0405 - val_loss: 0.0399
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0415
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1381
6530/6530 [==============================] - 0s 12us/step - loss: 0.1379 - val_loss: 0.1420
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1407
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0401
6530/6530 [==============================] - 0s 10us/step - loss: 0.0399 - val_loss: 0.0392
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0407
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1330
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0395
6530/6530 [==============================] - 0s 12us/step - loss: 0.1334 - val_loss: 0.1358

6530/6530 [==============================] - 0s 10us/step - loss: 0.0394 - val_loss: 0.0387
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0390
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0395
6530/6530 [==============================] - 0s 10us/step - loss: 0.0395 - val_loss: 0.0381
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0396
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0391
6530/6530 [==============================] - 0s 10us/step - loss: 0.0389 - val_loss: 0.0375

# training | RMSE: 0.2446, MAE: 0.1982
worker 0  xfile  [0, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47234950478948456}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 70, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.28486462957277}, 'layer_3_size': 3, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4760294151940936}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.24459667905040078, 'rmse': 0.24459667905040078, 'mae': 0.1982495709945371, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  18 | activation: tanh    | extras: batchnorm 
layer 2 | size:  22 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f46042acda0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 6s - loss: 2.8082
# training | RMSE: 0.1710, MAE: 0.1303
worker 1  xfile  [1, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38379181805725937}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.151550479705256}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.302283486099243}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.17102672958721443, 'rmse': 0.17102672958721443, 'mae': 0.13033314902881205, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  23 | activation: sigmoid | extras: dropout - rate: 11.0% 
layer 2 | size:  45 | activation: relu    | extras: None 
layer 3 | size:  48 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 22s - loss: 0.5796
5120/6530 [======================>.......] - ETA: 0s - loss: 1.2177
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1742 
6530/6530 [==============================] - 0s 53us/step - loss: 1.0757 - val_loss: 0.4142
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.4303
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1198
5632/6530 [========================>.....] - ETA: 0s - loss: 0.3069
6530/6530 [==============================] - 0s 10us/step - loss: 0.2945 - val_loss: 0.1952
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2408
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1003
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1792
6530/6530 [==============================] - 0s 10us/step - loss: 0.1767 - val_loss: 0.1354
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1434
6530/6530 [==============================] - 0s 68us/step - loss: 0.0934 - val_loss: 0.0608
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0871
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1299
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0658
6530/6530 [==============================] - 0s 10us/step - loss: 0.1292 - val_loss: 0.1078
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1108
4096/6530 [=================>............] - ETA: 0s - loss: 0.0619
6400/6530 [============================>.] - ETA: 0s - loss: 0.1031
6530/6530 [==============================] - 0s 9us/step - loss: 0.1030 - val_loss: 0.0872
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0967
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0602
6400/6530 [============================>.] - ETA: 0s - loss: 0.0847
6530/6530 [==============================] - 0s 9us/step - loss: 0.0845 - val_loss: 0.0794

6530/6530 [==============================] - 0s 26us/step - loss: 0.0598 - val_loss: 0.0532
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0761Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0666
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0575
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0721
6530/6530 [==============================] - 0s 9us/step - loss: 0.0722 - val_loss: 0.0727
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0636
3776/6530 [================>.............] - ETA: 0s - loss: 0.0549
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0634
6530/6530 [==============================] - 0s 9us/step - loss: 0.0630 - val_loss: 0.0646
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0601
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0538
6530/6530 [==============================] - 0s 9us/step - loss: 0.0566 - val_loss: 0.0536

6530/6530 [==============================] - 0s 27us/step - loss: 0.0533 - val_loss: 0.0487
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0619
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0510
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0499
6530/6530 [==============================] - 0s 24us/step - loss: 0.0491 - val_loss: 0.0526
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0618
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0476
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0472
6528/6530 [============================>.] - ETA: 0s - loss: 0.0466
6530/6530 [==============================] - 0s 25us/step - loss: 0.0466 - val_loss: 0.0701
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0710
# training | RMSE: 0.1919, MAE: 0.1544
worker 2  xfile  [2, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 57, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.330942552638289}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17444003372492284}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.19185704096358724, 'rmse': 0.19185704096358724, 'mae': 0.15442511177878274, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  46 | activation: relu    | extras: None 
layer 2 | size:  15 | activation: relu    | extras: batchnorm 
layer 3 | size:  43 | activation: relu    | extras: batchnorm 
layer 4 | size:   9 | activation: sigmoid | extras: dropout - rate: 22.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8ac940>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 55s - loss: 0.4261
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0460
1024/6530 [===>..........................] - ETA: 3s - loss: 0.3784 
3840/6530 [================>.............] - ETA: 0s - loss: 0.0448
1984/6530 [========>.....................] - ETA: 1s - loss: 0.3303
# training | RMSE: 0.2279, MAE: 0.1796
worker 0  xfile  [3, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.37292523848627124}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.22788207175275904, 'rmse': 0.22788207175275904, 'mae': 0.17961456209394633, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  40 | activation: sigmoid | extras: dropout - rate: 42.5% 
layer 2 | size:  80 | activation: sigmoid | extras: dropout - rate: 19.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f46042acc88>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 36s - loss: 0.2611
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0452
2880/6530 [============>.................] - ETA: 0s - loss: 0.2870
1216/6530 [====>.........................] - ETA: 0s - loss: 0.2342 
6530/6530 [==============================] - 0s 28us/step - loss: 0.0453 - val_loss: 0.0601
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0626
3776/6530 [================>.............] - ETA: 0s - loss: 0.2463
2400/6530 [==========>...................] - ETA: 0s - loss: 0.2200
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0454
4672/6530 [====================>.........] - ETA: 0s - loss: 0.2134
3520/6530 [===============>..............] - ETA: 0s - loss: 0.2167
3776/6530 [================>.............] - ETA: 0s - loss: 0.0441
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1873
4544/6530 [===================>..........] - ETA: 0s - loss: 0.2152
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0449
6530/6530 [==============================] - 0s 28us/step - loss: 0.0447 - val_loss: 0.0675
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0671
6464/6530 [============================>.] - ETA: 0s - loss: 0.1667
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2122
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0453
6530/6530 [==============================] - 1s 150us/step - loss: 0.1652 - val_loss: 0.0344
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0287
6530/6530 [==============================] - 1s 78us/step - loss: 0.2095 - val_loss: 0.3121
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.3315
4096/6530 [=================>............] - ETA: 0s - loss: 0.0448
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0317
1120/6530 [====>.........................] - ETA: 0s - loss: 0.2058
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0449
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0299
6530/6530 [==============================] - 0s 28us/step - loss: 0.0447 - val_loss: 0.0728

2304/6530 [=========>....................] - ETA: 0s - loss: 0.1984
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0289
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1981
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0276
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1959
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0263
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1947
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0248
6530/6530 [==============================] - 0s 49us/step - loss: 0.1949 - val_loss: 0.2139
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.2281
6464/6530 [============================>.] - ETA: 0s - loss: 0.0237
6530/6530 [==============================] - 0s 58us/step - loss: 0.0236 - val_loss: 0.0206
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0145
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1909
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1902
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0154
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1922
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0148
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1911
3008/6530 [============>.................] - ETA: 0s - loss: 0.0143
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1909
3904/6530 [================>.............] - ETA: 0s - loss: 0.0144
6530/6530 [==============================] - 0s 44us/step - loss: 0.1902 - val_loss: 0.1923
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.2314
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0139
# training | RMSE: 0.2691, MAE: 0.2247
worker 1  xfile  [4, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10992466825922427}, 'layer_1_size': 23, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17662673073095092}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2690880923641579, 'rmse': 0.2690880923641579, 'mae': 0.22473889661224838, 'early_stop': True}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  25 | activation: relu    | extras: dropout - rate: 42.5% 
layer 2 | size:  36 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ef6a34a8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 5s - loss: 0.8132
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1839
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0136
6530/6530 [==============================] - 0s 42us/step - loss: 0.4596 - val_loss: 0.2396
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2911
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1877
6530/6530 [==============================] - 0s 56us/step - loss: 0.0132 - val_loss: 0.0143
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0104
6530/6530 [==============================] - 0s 8us/step - loss: 0.2676 - val_loss: 0.2090
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2589
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1874
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0115
6530/6530 [==============================] - 0s 7us/step - loss: 0.2440 - val_loss: 0.1941
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2335
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1856
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0110
6530/6530 [==============================] - 0s 7us/step - loss: 0.2225 - val_loss: 0.1868
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2219
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1865
2944/6530 [============>.................] - ETA: 0s - loss: 0.0108
6530/6530 [==============================] - 0s 8us/step - loss: 0.2116 - val_loss: 0.1827
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2137
6530/6530 [==============================] - 0s 46us/step - loss: 0.1864 - val_loss: 0.1659
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1678
3904/6530 [================>.............] - ETA: 0s - loss: 0.0109
6530/6530 [==============================] - 0s 7us/step - loss: 0.1979 - val_loss: 0.1788
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2083
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1858
6530/6530 [==============================] - 0s 8us/step - loss: 0.1901 - val_loss: 0.1731
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1761
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0106
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1842
6530/6530 [==============================] - 0s 8us/step - loss: 0.1836 - val_loss: 0.1717
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1784
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0106
3232/6530 [=============>................] - ETA: 0s - loss: 0.1825
6530/6530 [==============================] - 0s 8us/step - loss: 0.1791 - val_loss: 0.1705

6530/6530 [==============================] - 0s 58us/step - loss: 0.0104 - val_loss: 0.0102

4288/6530 [==================>...........] - ETA: 0s - loss: 0.1819Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0086
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1812
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0096
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0092
6530/6530 [==============================] - 0s 49us/step - loss: 0.1810 - val_loss: 0.1665
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1847
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0093
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1825
3776/6530 [================>.............] - ETA: 0s - loss: 0.0093
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1794
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0093
3840/6530 [================>.............] - ETA: 0s - loss: 0.1815
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0092
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1807
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1810
6530/6530 [==============================] - 0s 57us/step - loss: 0.0091 - val_loss: 0.0088
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0077
6530/6530 [==============================] - 0s 43us/step - loss: 0.1806 - val_loss: 0.1635
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1580
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0088
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1797
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0086
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1799
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0086
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1813
3776/6530 [================>.............] - ETA: 0s - loss: 0.0086
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1814
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0086
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1807
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0086
6530/6530 [==============================] - 0s 46us/step - loss: 0.1796 - val_loss: 0.1766
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.2297
6528/6530 [============================>.] - ETA: 0s - loss: 0.0085
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1773
6530/6530 [==============================] - 0s 58us/step - loss: 0.0085 - val_loss: 0.0084
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0072
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1769
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0084
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1771
# training | RMSE: 0.2053, MAE: 0.1678
worker 1  xfile  [7, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42508411914061284}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 89, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2053414868911826, 'rmse': 0.2053414868911826, 'mae': 0.1678078650773286, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  27 | activation: sigmoid | extras: dropout - rate: 36.5% 
layer 2 | size:  93 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  84 | activation: tanh    | extras: dropout - rate: 23.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f4607fc8a58>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 9s - loss: 0.7143
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0083
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1786
5120/6530 [======================>.......] - ETA: 0s - loss: 0.6053
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0081
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1773
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0082
6530/6530 [==============================] - 1s 77us/step - loss: 0.5485 - val_loss: 0.6300
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2794
6530/6530 [==============================] - 0s 47us/step - loss: 0.1776 - val_loss: 0.1836
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.2232
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0082
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2440
6530/6530 [==============================] - 0s 11us/step - loss: 0.2413 - val_loss: 0.4549
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2323
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1774
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0082
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2287
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1771
6530/6530 [==============================] - 0s 11us/step - loss: 0.2282 - val_loss: 0.2264
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2103
6400/6530 [============================>.] - ETA: 0s - loss: 0.0081
6530/6530 [==============================] - 0s 59us/step - loss: 0.0081 - val_loss: 0.0081
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0067
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1784
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2250
6530/6530 [==============================] - 0s 11us/step - loss: 0.2243 - val_loss: 0.2571
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2410
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0080
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1773
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2262
6530/6530 [==============================] - 0s 11us/step - loss: 0.2251 - val_loss: 0.3422

1728/6530 [======>.......................] - ETA: 0s - loss: 0.0081Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2118
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1778
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0078
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2246
6530/6530 [==============================] - 0s 47us/step - loss: 0.1777 - val_loss: 0.1776

6530/6530 [==============================] - 0s 11us/step - loss: 0.2244 - val_loss: 0.2545
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2321
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0079
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2259
6530/6530 [==============================] - 0s 11us/step - loss: 0.2228 - val_loss: 0.2864
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2190
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0079
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2197
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0080
6530/6530 [==============================] - 0s 11us/step - loss: 0.2197 - val_loss: 0.2491

6144/6530 [===========================>..] - ETA: 0s - loss: 0.0078
6530/6530 [==============================] - 0s 63us/step - loss: 0.0078 - val_loss: 0.0077
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0061
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0077
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0078
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0076
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0076
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0076
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0077
6464/6530 [============================>.] - ETA: 0s - loss: 0.0075
6530/6530 [==============================] - 0s 59us/step - loss: 0.0075 - val_loss: 0.0075

# training | RMSE: 0.2154, MAE: 0.1770
worker 0  xfile  [6, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4247664644070097}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19334055170839967}, 'layer_2_size': 80, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.21544413514659652, 'rmse': 0.21544413514659652, 'mae': 0.1769698141231248, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  77 | activation: relu    | extras: dropout - rate: 42.1% 
layer 2 | size:  33 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  69 | activation: relu    | extras: None 
layer 4 | size:  14 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ef57dc88>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 38s - loss: 0.6625
# training | RMSE: 0.3030, MAE: 0.2447
worker 1  xfile  [8, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36459418788069975}, 'layer_1_size': 27, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2333825191150376}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 56, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.3030415667887398, 'rmse': 0.3030415667887398, 'mae': 0.24472402780918828, 'early_stop': True}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  91 | activation: relu    | extras: batchnorm 
layer 2 | size:  79 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f45e46fff98>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 2:07 - loss: 1.6681
1216/6530 [====>.........................] - ETA: 1s - loss: 0.3648 
 368/6530 [>.............................] - ETA: 6s - loss: 0.5512  
2496/6530 [==========>...................] - ETA: 0s - loss: 0.3096
 704/6530 [==>...........................] - ETA: 3s - loss: 0.3155
3712/6530 [================>.............] - ETA: 0s - loss: 0.2850
1072/6530 [===>..........................] - ETA: 2s - loss: 0.2211
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2675
1440/6530 [=====>........................] - ETA: 1s - loss: 0.1741
6336/6530 [============================>.] - ETA: 0s - loss: 0.2532
1840/6530 [=======>......................] - ETA: 1s - loss: 0.1439
6530/6530 [==============================] - 1s 106us/step - loss: 0.2514 - val_loss: 0.1674
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2136
2240/6530 [=========>....................] - ETA: 1s - loss: 0.1246
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1979
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1120
3024/6530 [============>.................] - ETA: 0s - loss: 0.1005
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1923
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0929
3840/6530 [================>.............] - ETA: 0s - loss: 0.1920
3792/6530 [================>.............] - ETA: 0s - loss: 0.0867
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1897
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0820
6528/6530 [============================>.] - ETA: 0s - loss: 0.1869
6530/6530 [==============================] - 0s 42us/step - loss: 0.1869 - val_loss: 0.1405
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2209
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0779
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1772
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0746
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1710
# training | RMSE: 0.0799, MAE: 0.0625
worker 2  xfile  [5, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22893106919985234}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.07987750747457414, 'rmse': 0.07987750747457414, 'mae': 0.06254097162132036, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  80 | activation: relu    | extras: None 
layer 2 | size:  99 | activation: relu    | extras: None 
layer 3 | size:  47 | activation: relu    | extras: batchnorm 
layer 4 | size:  56 | activation: tanh    | extras: dropout - rate: 22.0% 
layer 5 | size:  76 | activation: tanh    | extras: dropout - rate: 34.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f46040eecf8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 40s - loss: 0.9952
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0716
3904/6530 [================>.............] - ETA: 0s - loss: 0.1695
1344/6530 [=====>........................] - ETA: 1s - loss: 0.4557 
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0692
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1680
2560/6530 [==========>...................] - ETA: 0s - loss: 0.3763
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0666
6336/6530 [============================>.] - ETA: 0s - loss: 0.1649
6530/6530 [==============================] - 0s 42us/step - loss: 0.1647 - val_loss: 0.1350
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1824
3776/6530 [================>.............] - ETA: 0s - loss: 0.3371
6400/6530 [============================>.] - ETA: 0s - loss: 0.0644
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1567
4992/6530 [=====================>........] - ETA: 0s - loss: 0.3118
6530/6530 [==============================] - 1s 194us/step - loss: 0.0638 - val_loss: 0.0271

2624/6530 [===========>..................] - ETA: 0s - loss: 0.1555Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0281
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2948
3904/6530 [================>.............] - ETA: 0s - loss: 0.1551
 416/6530 [>.............................] - ETA: 0s - loss: 0.0260
6530/6530 [==============================] - 1s 113us/step - loss: 0.2907 - val_loss: 0.2645
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2811
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1544
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0290
1216/6530 [====>.........................] - ETA: 0s - loss: 0.2203
6400/6530 [============================>.] - ETA: 0s - loss: 0.1533
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0284
6530/6530 [==============================] - 0s 42us/step - loss: 0.1529 - val_loss: 0.1842
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1455
2432/6530 [==========>...................] - ETA: 0s - loss: 0.2089
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0284
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1491
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2042
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0278
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1487
4800/6530 [=====================>........] - ETA: 0s - loss: 0.2001
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0277
3712/6530 [================>.............] - ETA: 0s - loss: 0.1473
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1990
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0280
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1472
6530/6530 [==============================] - 0s 46us/step - loss: 0.1974 - val_loss: 0.1629
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1934
3008/6530 [============>.................] - ETA: 0s - loss: 0.0271
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1461
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1852
6530/6530 [==============================] - 0s 44us/step - loss: 0.1456 - val_loss: 0.2273
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2375
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0269
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1818
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1471
3760/6530 [================>.............] - ETA: 0s - loss: 0.0268
3776/6530 [================>.............] - ETA: 0s - loss: 0.1775
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1442
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0269
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1743
3776/6530 [================>.............] - ETA: 0s - loss: 0.1434
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0271
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1735
6530/6530 [==============================] - 0s 43us/step - loss: 0.1734 - val_loss: 0.1506

5056/6530 [======================>.......] - ETA: 0s - loss: 0.1413Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1422
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0268
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1407
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1630
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0267
6530/6530 [==============================] - 0s 43us/step - loss: 0.1409 - val_loss: 0.1284
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1510
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0266
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1629
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1385
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0264
3840/6530 [================>.............] - ETA: 0s - loss: 0.1614
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1375
6496/6530 [============================>.] - ETA: 0s - loss: 0.0262
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1611
3968/6530 [=================>............] - ETA: 0s - loss: 0.1359
6530/6530 [==============================] - 1s 139us/step - loss: 0.0263 - val_loss: 0.0227
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0203
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1607
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1353
6530/6530 [==============================] - 0s 45us/step - loss: 0.1606 - val_loss: 0.1713
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2000
 368/6530 [>.............................] - ETA: 0s - loss: 0.0206
6528/6530 [============================>.] - ETA: 0s - loss: 0.1360
6530/6530 [==============================] - 0s 41us/step - loss: 0.1360 - val_loss: 0.1674
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1239
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1627
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0234
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1341
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1589
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0236
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1330
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1582
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0231
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1340
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1555
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0226
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1327
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1547
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0225
6530/6530 [==============================] - 0s 44us/step - loss: 0.1544 - val_loss: 0.1363
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1410
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1326
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0225
6530/6530 [==============================] - 0s 44us/step - loss: 0.1323 - val_loss: 0.1477
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1342
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1449
2992/6530 [============>.................] - ETA: 0s - loss: 0.0218
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1316
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1476
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0217
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1309
3776/6530 [================>.............] - ETA: 0s - loss: 0.1478
3744/6530 [================>.............] - ETA: 0s - loss: 0.0216
3840/6530 [================>.............] - ETA: 0s - loss: 0.1276
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1475
4096/6530 [=================>............] - ETA: 0s - loss: 0.0219
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1283
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1466
6530/6530 [==============================] - 0s 43us/step - loss: 0.1459 - val_loss: 0.1490

4496/6530 [===================>..........] - ETA: 0s - loss: 0.0220Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1544
6400/6530 [============================>.] - ETA: 0s - loss: 0.1274
6530/6530 [==============================] - 0s 42us/step - loss: 0.1276 - val_loss: 0.1239

4880/6530 [=====================>........] - ETA: 0s - loss: 0.0218
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1405
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0217
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1409
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0216
3776/6530 [================>.............] - ETA: 0s - loss: 0.1395
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0215
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1400
6448/6530 [============================>.] - ETA: 0s - loss: 0.0214
6336/6530 [============================>.] - ETA: 0s - loss: 0.1400
6530/6530 [==============================] - 0s 43us/step - loss: 0.1396 - val_loss: 0.1097
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1560
6530/6530 [==============================] - 1s 140us/step - loss: 0.0215 - val_loss: 0.0198
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0141
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1398
 400/6530 [>.............................] - ETA: 0s - loss: 0.0184
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1369
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0197
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1373
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0198
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1367
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0198
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1365
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0191
6530/6530 [==============================] - 0s 44us/step - loss: 0.1362 - val_loss: 0.1143
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1105
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0192
# training | RMSE: 0.1545, MAE: 0.1218
worker 0  xfile  [9, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42127947953652023}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.15453745372883795, 'rmse': 0.15453745372883795, 'mae': 0.1217984354008155, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size: 100 | activation: relu    | extras: None 
layer 2 | size:   8 | activation: tanh    | extras: dropout - rate: 11.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45c8345908>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 1:27 - loss: 1.2695
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1285
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0189
 592/6530 [=>............................] - ETA: 2s - loss: 0.6314  
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1317
3168/6530 [=============>................] - ETA: 0s - loss: 0.0185
1152/6530 [====>.........................] - ETA: 1s - loss: 0.4738
3776/6530 [================>.............] - ETA: 0s - loss: 0.1312
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0187
1712/6530 [======>.......................] - ETA: 1s - loss: 0.4014
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1301
3856/6530 [================>.............] - ETA: 0s - loss: 0.0187
2224/6530 [=========>....................] - ETA: 0s - loss: 0.3597
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1309
6530/6530 [==============================] - 0s 44us/step - loss: 0.1308 - val_loss: 0.1289

4224/6530 [==================>...........] - ETA: 0s - loss: 0.0188
2816/6530 [===========>..................] - ETA: 0s - loss: 0.3278
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0190
3360/6530 [==============>...............] - ETA: 0s - loss: 0.3061
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0188
3936/6530 [=================>............] - ETA: 0s - loss: 0.2885
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0188
4528/6530 [===================>..........] - ETA: 0s - loss: 0.2743
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0187
5104/6530 [======================>.......] - ETA: 0s - loss: 0.2634
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0185
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2530
6528/6530 [============================>.] - ETA: 0s - loss: 0.0187
6160/6530 [===========================>..] - ETA: 0s - loss: 0.2463
6530/6530 [==============================] - 1s 138us/step - loss: 0.0186 - val_loss: 0.0177
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0148
 432/6530 [>.............................] - ETA: 0s - loss: 0.0166
6530/6530 [==============================] - 1s 131us/step - loss: 0.2412 - val_loss: 0.1561
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1350
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0177
 608/6530 [=>............................] - ETA: 0s - loss: 0.1482
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0180
1200/6530 [====>.........................] - ETA: 0s - loss: 0.1456
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0180
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1469
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0173
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1480
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0175
2912/6530 [============>.................] - ETA: 0s - loss: 0.1466
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0172
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1454
3184/6530 [=============>................] - ETA: 0s - loss: 0.0168
4048/6530 [=================>............] - ETA: 0s - loss: 0.1442
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0171
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1429
4016/6530 [=================>............] - ETA: 0s - loss: 0.0171
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1419
# training | RMSE: 0.1525, MAE: 0.1205
worker 2  xfile  [11, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 80, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 99, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22024861517858152}, 'layer_4_size': 56, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3396711412072355}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.15254296748314108, 'rmse': 0.15254296748314108, 'mae': 0.12052500323516295, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  77 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  69 | activation: relu    | extras: dropout - rate: 34.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ac04b4e0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 9s - loss: 1.2001
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0172
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1407
5888/6530 [==========================>...] - ETA: 0s - loss: 0.4591
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0173
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1410
6530/6530 [==============================] - 0s 73us/step - loss: 0.4365 - val_loss: 0.2294
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2229
6530/6530 [==============================] - 1s 93us/step - loss: 0.1405 - val_loss: 0.1347

5184/6530 [======================>.......] - ETA: 0s - loss: 0.0172Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1168
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1958
6530/6530 [==============================] - 0s 10us/step - loss: 0.1946 - val_loss: 0.2279
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1685
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0170
 592/6530 [=>............................] - ETA: 0s - loss: 0.1276
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1758
6530/6530 [==============================] - 0s 10us/step - loss: 0.1752 - val_loss: 0.3363
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1552
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0170
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1269
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1668
6530/6530 [==============================] - 0s 9us/step - loss: 0.1662 - val_loss: 0.3982
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1763
6336/6530 [============================>.] - ETA: 0s - loss: 0.0170
1680/6530 [======>.......................] - ETA: 0s - loss: 0.1278
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1619
6530/6530 [==============================] - 0s 10us/step - loss: 0.1608 - val_loss: 0.4684

2240/6530 [=========>....................] - ETA: 0s - loss: 0.1286Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1510
6530/6530 [==============================] - 1s 135us/step - loss: 0.0171 - val_loss: 0.0177
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0182
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1284
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1520
 416/6530 [>.............................] - ETA: 0s - loss: 0.0155
6530/6530 [==============================] - 0s 10us/step - loss: 0.1520 - val_loss: 0.5598
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1566
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1269
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0169
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1495
6530/6530 [==============================] - 0s 9us/step - loss: 0.1498 - val_loss: 0.6163

3888/6530 [================>.............] - ETA: 0s - loss: 0.1252
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0171
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1243
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0168
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1241
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0162
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1225
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0164
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1232
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0161
6530/6530 [==============================] - 1s 93us/step - loss: 0.1229 - val_loss: 0.1203
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1068
3168/6530 [=============>................] - ETA: 0s - loss: 0.0157
 608/6530 [=>............................] - ETA: 0s - loss: 0.1166
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0159
1200/6530 [====>.........................] - ETA: 0s - loss: 0.1152
3968/6530 [=================>............] - ETA: 0s - loss: 0.0159
1744/6530 [=======>......................] - ETA: 0s - loss: 0.1163
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0160
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1161
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0163
2880/6530 [============>.................] - ETA: 0s - loss: 0.1150
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0161
# training | RMSE: 0.6531, MAE: 0.6134
worker 2  xfile  [13, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3403550800994245}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 9, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3295619155514554}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.6530748881526143, 'rmse': 0.6530748881526143, 'mae': 0.6134002469097932, 'early_stop': True}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  10 | activation: tanh    | extras: dropout - rate: 14.6% 
layer 2 | size:  80 | activation: relu    | extras: dropout - rate: 21.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ac063748>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 54s - loss: 0.8031
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1140
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0159
 992/6530 [===>..........................] - ETA: 1s - loss: 0.4164 
4080/6530 [=================>............] - ETA: 0s - loss: 0.1131
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0160
2048/6530 [========>.....................] - ETA: 0s - loss: 0.3593
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1127
6352/6530 [============================>.] - ETA: 0s - loss: 0.0159
3136/6530 [=============>................] - ETA: 0s - loss: 0.3326
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1117
6530/6530 [==============================] - 1s 134us/step - loss: 0.0160 - val_loss: 0.0171
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0180
4256/6530 [==================>...........] - ETA: 0s - loss: 0.3144
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1107
 400/6530 [>.............................] - ETA: 0s - loss: 0.0148
5408/6530 [=======================>......] - ETA: 0s - loss: 0.2993
6320/6530 [============================>.] - ETA: 0s - loss: 0.1115
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0157
6496/6530 [============================>.] - ETA: 0s - loss: 0.2901
6530/6530 [==============================] - 1s 92us/step - loss: 0.1113 - val_loss: 0.1114
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1064
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0163
6530/6530 [==============================] - 1s 96us/step - loss: 0.2898 - val_loss: 0.2191
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.3220
 576/6530 [=>............................] - ETA: 0s - loss: 0.1074
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0160
1024/6530 [===>..........................] - ETA: 0s - loss: 0.2369
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1066
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0153
2144/6530 [========>.....................] - ETA: 0s - loss: 0.2340
1680/6530 [======>.......................] - ETA: 0s - loss: 0.1073
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0153
3232/6530 [=============>................] - ETA: 0s - loss: 0.2341
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1059
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0155
4320/6530 [==================>...........] - ETA: 0s - loss: 0.2308
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1059
2976/6530 [============>.................] - ETA: 0s - loss: 0.0149
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2269
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1045
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0148
6464/6530 [============================>.] - ETA: 0s - loss: 0.2250
3888/6530 [================>.............] - ETA: 0s - loss: 0.1036
6530/6530 [==============================] - 0s 50us/step - loss: 0.2253 - val_loss: 0.1994
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.2099
3712/6530 [================>.............] - ETA: 0s - loss: 0.0149
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1029
1152/6530 [====>.........................] - ETA: 0s - loss: 0.2172
4112/6530 [=================>............] - ETA: 0s - loss: 0.0152
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1026
2272/6530 [=========>....................] - ETA: 0s - loss: 0.2115
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0153
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1022
3360/6530 [==============>...............] - ETA: 0s - loss: 0.2108
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0152
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1021
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2107
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0151
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2091
6530/6530 [==============================] - 1s 97us/step - loss: 0.1022 - val_loss: 0.1059
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1095
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0151
 576/6530 [=>............................] - ETA: 0s - loss: 0.1020
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0150
6530/6530 [==============================] - 0s 49us/step - loss: 0.2088 - val_loss: 0.1892
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.2303
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1004
6400/6530 [============================>.] - ETA: 0s - loss: 0.0150
1184/6530 [====>.........................] - ETA: 0s - loss: 0.2150
1712/6530 [======>.......................] - ETA: 0s - loss: 0.1007
6530/6530 [==============================] - 1s 141us/step - loss: 0.0151 - val_loss: 0.0166

2304/6530 [=========>....................] - ETA: 0s - loss: 0.2063Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0187
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0993
 400/6530 [>.............................] - ETA: 0s - loss: 0.0141
3456/6530 [==============>...............] - ETA: 0s - loss: 0.2044
2864/6530 [============>.................] - ETA: 0s - loss: 0.0987
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0151
4576/6530 [====================>.........] - ETA: 0s - loss: 0.2027
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0976
5664/6530 [=========================>....] - ETA: 0s - loss: 0.2009
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0156
3968/6530 [=================>............] - ETA: 0s - loss: 0.0968
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0151
6530/6530 [==============================] - 0s 48us/step - loss: 0.2002 - val_loss: 0.1916
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.2117
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0965
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0144
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1986
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0960
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0146
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1892
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0952
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0147
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1910
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0957
3024/6530 [============>.................] - ETA: 0s - loss: 0.0142
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1915
6530/6530 [==============================] - 1s 94us/step - loss: 0.0953 - val_loss: 0.0987
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0974
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0141
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1915
 576/6530 [=>............................] - ETA: 0s - loss: 0.0956
3792/6530 [================>.............] - ETA: 0s - loss: 0.0142
6530/6530 [==============================] - 0s 48us/step - loss: 0.1917 - val_loss: 0.1788
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1968
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0944
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0145
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1913
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0944
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0146
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1851
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0927
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0145
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1856
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0927
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0145
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1867
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0916
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0144
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1860
3856/6530 [================>.............] - ETA: 0s - loss: 0.0906
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0143
6530/6530 [==============================] - 0s 48us/step - loss: 0.1860 - val_loss: 0.1759
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.2106
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0901
6496/6530 [============================>.] - ETA: 0s - loss: 0.0143
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1905
6530/6530 [==============================] - 1s 138us/step - loss: 0.0144 - val_loss: 0.0145
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0154
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0898
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1813
 416/6530 [>.............................] - ETA: 0s - loss: 0.0131
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0898
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1824
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0896
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0146
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1813
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0149
6530/6530 [==============================] - 1s 96us/step - loss: 0.0897 - val_loss: 0.0961
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0990
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1809
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0146
 608/6530 [=>............................] - ETA: 0s - loss: 0.0905
6530/6530 [==============================] - 0s 49us/step - loss: 0.1818 - val_loss: 0.1666
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1812
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0138
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0902
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1885
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0140
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0901
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1799
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0140
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0884
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1798
3152/6530 [=============>................] - ETA: 0s - loss: 0.0136
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0889
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1804
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0136
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0874
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1803
3920/6530 [=================>............] - ETA: 0s - loss: 0.0138
3840/6530 [================>.............] - ETA: 0s - loss: 0.0865
6528/6530 [============================>.] - ETA: 0s - loss: 0.1801
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0139
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0857
6530/6530 [==============================] - 0s 52us/step - loss: 0.1802 - val_loss: 0.1605
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1776
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0141
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0856
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1791
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0139
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0856
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1711
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0139
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0853
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1695
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0138
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1727
6530/6530 [==============================] - 1s 96us/step - loss: 0.0854 - val_loss: 0.0925
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0967
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0137
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1719
 576/6530 [=>............................] - ETA: 0s - loss: 0.0897
6512/6530 [============================>.] - ETA: 0s - loss: 0.0138
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0887
6530/6530 [==============================] - 0s 48us/step - loss: 0.1720 - val_loss: 0.1632

6530/6530 [==============================] - 1s 141us/step - loss: 0.0139 - val_loss: 0.0128

1680/6530 [======>.......................] - ETA: 0s - loss: 0.0870
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0849
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0848
# training | RMSE: 0.2000, MAE: 0.1585
worker 2  xfile  [14, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14576819747292818}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21545192657439155}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 98, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.20002036237180107, 'rmse': 0.20002036237180107, 'mae': 0.1585141016899747, 'early_stop': False}
vggnet done  2

3344/6530 [==============>...............] - ETA: 0s - loss: 0.0840
3888/6530 [================>.............] - ETA: 0s - loss: 0.0830
# training | RMSE: 0.1097, MAE: 0.0871
worker 1  xfile  [10, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.1097469492008388, 'rmse': 0.1097469492008388, 'mae': 0.08712660607188898, 'early_stop': False}
vggnet done  1

4512/6530 [===================>..........] - ETA: 0s - loss: 0.0826
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0821
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0819
6464/6530 [============================>.] - ETA: 0s - loss: 0.0819
6530/6530 [==============================] - 1s 90us/step - loss: 0.0819 - val_loss: 0.0883

# training | RMSE: 0.1066, MAE: 0.0797
worker 0  xfile  [12, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10991956439681334}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25088382860678343}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.10657822867430503, 'rmse': 0.10657822867430503, 'mae': 0.07965697446151117, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=9.0 loss={'loss': 0.24459667905040078, 'rmse': 0.24459667905040078, 'mae': 0.1982495709945371, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47234950478948456}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 70, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.28486462957277}, 'layer_3_size': 3, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4760294151940936}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#1 epoch=9.0 loss={'loss': 0.17102672958721443, 'rmse': 0.17102672958721443, 'mae': 0.13033314902881205, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38379181805725937}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.151550479705256}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.302283486099243}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#2 epoch=9.0 loss={'loss': 0.19185704096358724, 'rmse': 0.19185704096358724, 'mae': 0.15442511177878274, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 57, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.330942552638289}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17444003372492284}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#3 epoch=9.0 loss={'loss': 0.22788207175275904, 'rmse': 0.22788207175275904, 'mae': 0.17961456209394633, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.37292523848627124}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#4 epoch=9.0 loss={'loss': 0.2690880923641579, 'rmse': 0.2690880923641579, 'mae': 0.22473889661224838, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10992466825922427}, 'layer_1_size': 23, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.17662673073095092}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#7 epoch=9.0 loss={'loss': 0.2053414868911826, 'rmse': 0.2053414868911826, 'mae': 0.1678078650773286, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42508411914061284}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 89, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#6 epoch=9.0 loss={'loss': 0.21544413514659652, 'rmse': 0.21544413514659652, 'mae': 0.1769698141231248, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4247664644070097}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19334055170839967}, 'layer_2_size': 80, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#8 epoch=9.0 loss={'loss': 0.3030415667887398, 'rmse': 0.3030415667887398, 'mae': 0.24472402780918828, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36459418788069975}, 'layer_1_size': 27, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2333825191150376}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 56, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#5 epoch=9.0 loss={'loss': 0.07987750747457414, 'rmse': 0.07987750747457414, 'mae': 0.06254097162132036, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22893106919985234}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#9 epoch=9.0 loss={'loss': 0.15453745372883795, 'rmse': 0.15453745372883795, 'mae': 0.1217984354008155, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42127947953652023}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#11 epoch=9.0 loss={'loss': 0.15254296748314108, 'rmse': 0.15254296748314108, 'mae': 0.12052500323516295, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 80, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 99, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22024861517858152}, 'layer_4_size': 56, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3396711412072355}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#13 epoch=9.0 loss={'loss': 0.6530748881526143, 'rmse': 0.6530748881526143, 'mae': 0.6134002469097932, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3403550800994245}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 9, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3295619155514554}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#14 epoch=9.0 loss={'loss': 0.20002036237180107, 'rmse': 0.20002036237180107, 'mae': 0.1585141016899747, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14576819747292818}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21545192657439155}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 98, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#10 epoch=9.0 loss={'loss': 0.1097469492008388, 'rmse': 0.1097469492008388, 'mae': 0.08712660607188898, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#12 epoch=9.0 loss={'loss': 0.10657822867430503, 'rmse': 0.10657822867430503, 'mae': 0.07965697446151117, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10991956439681334}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25088382860678343}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
get a list [results] of length 185
get a list [loss] of length 15
get a list [val_loss] of length 15
length of indices is (5, 12, 10, 11, 9, 1, 2, 14, 7, 6, 3, 0, 4, 8, 13)
length of indices is 15
length of T is 15
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22893106919985234}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [1, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10991956439681334}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25088382860678343}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [2, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [3, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42127947953652023}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [4, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38379181805725937}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.151550479705256}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.302283486099243}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]] 

*** 5.0 configurations x 27.0 iterations each

13 | Thu Sep 27 23:09:44 2018 | lowest loss so far: 0.0675 (run 0)

{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size: 100 | activation: relu    | extras: None 
layer 2 | size:   8 | activation: tanh    | extras: dropout - rate: 11.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 4:51 - loss: 0.6351
 480/6530 [=>............................] - ETA: 9s - loss: 0.3222  
 944/6530 [===>..........................] - ETA: 4s - loss: 0.2692
1424/6530 [=====>........................] - ETA: 3s - loss: 0.2418
1872/6530 [=======>......................] - ETA: 2s - loss: 0.2275
2448/6530 [==========>...................] - ETA: 1s - loss: 0.2170{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  91 | activation: relu    | extras: batchnorm 
layer 2 | size:  79 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 5:38 - loss: 0.1747
3040/6530 [============>.................] - ETA: 1s - loss: 0.2061
 432/6530 [>.............................] - ETA: 12s - loss: 0.0637 
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1997
 816/6530 [==>...........................] - ETA: 6s - loss: 0.0580 
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1946
1216/6530 [====>.........................] - ETA: 4s - loss: 0.0523
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1904
1568/6530 [======>.......................] - ETA: 3s - loss: 0.0490
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1853
1968/6530 [========>.....................] - ETA: 2s - loss: 0.0474
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1815
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0459
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0450
6530/6530 [==============================] - 1s 208us/step - loss: 0.1780 - val_loss: 0.1459
Epoch 2/27

  16/6530 [..............................] - ETA: 0s - loss: 0.1541
3152/6530 [=============>................] - ETA: 1s - loss: 0.0434
 592/6530 [=>............................] - ETA: 0s - loss: 0.1373
3536/6530 [===============>..............] - ETA: 1s - loss: 0.0427
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1324
3936/6530 [=================>............] - ETA: 0s - loss: 0.0418
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1302{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  46 | activation: relu    | extras: None 
layer 2 | size:  15 | activation: relu    | extras: batchnorm 
layer 3 | size:  43 | activation: relu    | extras: batchnorm 
layer 4 | size:   9 | activation: sigmoid | extras: dropout - rate: 22.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c894668>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  64/6530 [..............................] - ETA: 1:48 - loss: 0.5729
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0411
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1304
 960/6530 [===>..........................] - ETA: 6s - loss: 0.5255  
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0408
2864/6530 [============>.................] - ETA: 0s - loss: 0.1298
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0404
1920/6530 [=======>......................] - ETA: 2s - loss: 0.4626
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1288
2816/6530 [===========>..................] - ETA: 1s - loss: 0.4123
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0399
3968/6530 [=================>............] - ETA: 0s - loss: 0.1286
3712/6530 [================>.............] - ETA: 0s - loss: 0.3594
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0393
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1281
4544/6530 [===================>..........] - ETA: 0s - loss: 0.3177
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0388
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1276
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2773
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1262
6530/6530 [==============================] - 2s 267us/step - loss: 0.0383 - val_loss: 0.0342
Epoch 2/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0282
6464/6530 [============================>.] - ETA: 0s - loss: 0.2446
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1263
 400/6530 [>.............................] - ETA: 0s - loss: 0.0264
6530/6530 [==============================] - 1s 92us/step - loss: 0.1255 - val_loss: 0.1223
Epoch 3/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0967
6530/6530 [==============================] - 2s 230us/step - loss: 0.2425 - val_loss: 0.0578
Epoch 2/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0457
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0294
 608/6530 [=>............................] - ETA: 0s - loss: 0.1118
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0450
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0286
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1111
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0403
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0276
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1102
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0376
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1106
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0274
3712/6530 [================>.............] - ETA: 0s - loss: 0.0354
2832/6530 [============>.................] - ETA: 0s - loss: 0.1112
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0274
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0334
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1106
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0278
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0318
3936/6530 [=================>............] - ETA: 0s - loss: 0.1110
2992/6530 [============>.................] - ETA: 0s - loss: 0.0272
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0302
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1101
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0269
6530/6530 [==============================] - 0s 61us/step - loss: 0.0295 - val_loss: 0.0518
Epoch 3/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0181
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1103
3728/6530 [================>.............] - ETA: 0s - loss: 0.0267
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0169
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1097
4112/6530 [=================>............] - ETA: 0s - loss: 0.0267
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0159
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0269
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1101
2880/6530 [============>.................] - ETA: 0s - loss: 0.0152
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0267
6530/6530 [==============================] - 1s 94us/step - loss: 0.1095 - val_loss: 0.1106
Epoch 4/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0728
3840/6530 [================>.............] - ETA: 0s - loss: 0.0146
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0266
 592/6530 [=>............................] - ETA: 0s - loss: 0.1010
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0142
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0264
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1015
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0139
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0261
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1007
6530/6530 [==============================] - 0s 58us/step - loss: 0.0136 - val_loss: 0.0167
Epoch 4/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0123
6512/6530 [============================>.] - ETA: 0s - loss: 0.0260
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1012
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0118
6530/6530 [==============================] - 1s 138us/step - loss: 0.0260 - val_loss: 0.0282
Epoch 3/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0265
2944/6530 [============>.................] - ETA: 0s - loss: 0.1006
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0113
 416/6530 [>.............................] - ETA: 0s - loss: 0.0221
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1008
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0111
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0234
4064/6530 [=================>............] - ETA: 0s - loss: 0.1010
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0109
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0230
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1008
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0107
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0223
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1006
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0107
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0221
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1002
6400/6530 [============================>.] - ETA: 0s - loss: 0.0105
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0220
6368/6530 [============================>.] - ETA: 0s - loss: 0.1004
6530/6530 [==============================] - 0s 59us/step - loss: 0.0105 - val_loss: 0.0106
Epoch 5/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0103
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0221
6530/6530 [==============================] - 1s 92us/step - loss: 0.1000 - val_loss: 0.1042
Epoch 5/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0670
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0097
3168/6530 [=============>................] - ETA: 0s - loss: 0.0216
 592/6530 [=>............................] - ETA: 0s - loss: 0.0931
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0095
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0217
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0945
2880/6530 [============>.................] - ETA: 0s - loss: 0.0094
3952/6530 [=================>............] - ETA: 0s - loss: 0.0217
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0941
3840/6530 [================>.............] - ETA: 0s - loss: 0.0095
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0217
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0946
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0094
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0221
2976/6530 [============>.................] - ETA: 0s - loss: 0.0937
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0094
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0218
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0940
6530/6530 [==============================] - 0s 56us/step - loss: 0.0092 - val_loss: 0.0090
Epoch 6/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0087
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0216
4128/6530 [=================>............] - ETA: 0s - loss: 0.0942
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0086
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0216
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0941
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0087
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0215
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0940
2880/6530 [============>.................] - ETA: 0s - loss: 0.0086
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0938
6530/6530 [==============================] - 1s 135us/step - loss: 0.0215 - val_loss: 0.0237
Epoch 4/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0233
3840/6530 [================>.............] - ETA: 0s - loss: 0.0088
6352/6530 [============================>.] - ETA: 0s - loss: 0.0940
 416/6530 [>.............................] - ETA: 0s - loss: 0.0194
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0086
6530/6530 [==============================] - 1s 91us/step - loss: 0.0937 - val_loss: 0.0979
Epoch 6/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0631
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0205
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0087
 576/6530 [=>............................] - ETA: 0s - loss: 0.0879
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0203
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0907
6530/6530 [==============================] - 0s 57us/step - loss: 0.0085 - val_loss: 0.0084
Epoch 7/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0079
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0198
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0896
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0080
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0193
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0905
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0082
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0195
2864/6530 [============>.................] - ETA: 0s - loss: 0.0899
2944/6530 [============>.................] - ETA: 0s - loss: 0.0082
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0193
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0897
3904/6530 [================>.............] - ETA: 0s - loss: 0.0083
3088/6530 [=============>................] - ETA: 0s - loss: 0.0189
4032/6530 [=================>............] - ETA: 0s - loss: 0.0900
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0082
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0191
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0896
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0082
3840/6530 [================>.............] - ETA: 0s - loss: 0.0191
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0895
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0192
6530/6530 [==============================] - 0s 57us/step - loss: 0.0080 - val_loss: 0.0082
Epoch 8/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0074
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0894
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0194
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0076
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0897
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0192
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0079
6530/6530 [==============================] - 1s 92us/step - loss: 0.0894 - val_loss: 0.0947
Epoch 7/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0607
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0192
2944/6530 [============>.................] - ETA: 0s - loss: 0.0078
 592/6530 [=>............................] - ETA: 0s - loss: 0.0847
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0189
3840/6530 [================>.............] - ETA: 0s - loss: 0.0079
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0859
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0188
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0078
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0857
6530/6530 [==============================] - 1s 136us/step - loss: 0.0189 - val_loss: 0.0195

5760/6530 [=========================>....] - ETA: 0s - loss: 0.0078Epoch 5/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0188
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0866
 416/6530 [>.............................] - ETA: 0s - loss: 0.0176
6530/6530 [==============================] - 0s 58us/step - loss: 0.0077 - val_loss: 0.0078

2928/6530 [============>.................] - ETA: 0s - loss: 0.0856Epoch 9/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0070
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0185
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0856
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0073
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0188
4096/6530 [=================>............] - ETA: 0s - loss: 0.0861
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0075
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0181
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0857
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0076
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0176
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0858
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0076
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0179
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0856
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0076
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0177
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0858
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0076
3056/6530 [=============>................] - ETA: 0s - loss: 0.0173
6530/6530 [==============================] - 1s 94us/step - loss: 0.0855 - val_loss: 0.0903
Epoch 8/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0565
6400/6530 [============================>.] - ETA: 0s - loss: 0.0075
6530/6530 [==============================] - 0s 59us/step - loss: 0.0075 - val_loss: 0.0073
Epoch 10/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0066
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0176
 592/6530 [=>............................] - ETA: 0s - loss: 0.0815
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0067
3824/6530 [================>.............] - ETA: 0s - loss: 0.0175
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0838
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0072
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0176
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0836
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0073
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0178
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0835
3712/6530 [================>.............] - ETA: 0s - loss: 0.0074
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0175
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0836
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0073
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0175
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0829
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0073
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0174
4016/6530 [=================>............] - ETA: 0s - loss: 0.0834
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0173
6530/6530 [==============================] - 0s 58us/step - loss: 0.0072 - val_loss: 0.0077
Epoch 11/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0064
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0832
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0065
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0831
6530/6530 [==============================] - 1s 137us/step - loss: 0.0173 - val_loss: 0.0178
Epoch 6/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0172
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0828
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0070
 384/6530 [>.............................] - ETA: 0s - loss: 0.0163
6320/6530 [============================>.] - ETA: 0s - loss: 0.0830
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0070
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0172
6530/6530 [==============================] - 1s 93us/step - loss: 0.0829 - val_loss: 0.0885
Epoch 9/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0540
3712/6530 [================>.............] - ETA: 0s - loss: 0.0072
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0175
 592/6530 [=>............................] - ETA: 0s - loss: 0.0780
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0071
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0168
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0800
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0071
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0165
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0805
6528/6530 [============================>.] - ETA: 0s - loss: 0.0070
6530/6530 [==============================] - 0s 58us/step - loss: 0.0070 - val_loss: 0.0074

2384/6530 [=========>....................] - ETA: 0s - loss: 0.0165Epoch 12/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0061
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0810
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0165
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0064
2896/6530 [============>.................] - ETA: 0s - loss: 0.0805
3136/6530 [=============>................] - ETA: 0s - loss: 0.0161
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0068
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0805
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0163
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0069
4000/6530 [=================>............] - ETA: 0s - loss: 0.0810
3888/6530 [================>.............] - ETA: 0s - loss: 0.0164
3776/6530 [================>.............] - ETA: 0s - loss: 0.0070
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0806
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0165
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0069
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0807
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0167
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0070
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0806
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0165
6530/6530 [==============================] - 0s 58us/step - loss: 0.0069 - val_loss: 0.0072
Epoch 13/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0060
6320/6530 [============================>.] - ETA: 0s - loss: 0.0808
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0163
6530/6530 [==============================] - 1s 93us/step - loss: 0.0806 - val_loss: 0.0884
Epoch 10/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0463
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0065
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0162
 592/6530 [=>............................] - ETA: 0s - loss: 0.0780
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0067
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0162
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0790
2880/6530 [============>.................] - ETA: 0s - loss: 0.0068
6530/6530 [==============================] - 1s 135us/step - loss: 0.0162 - val_loss: 0.0162
Epoch 7/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0146
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0791
3712/6530 [================>.............] - ETA: 0s - loss: 0.0069
 416/6530 [>.............................] - ETA: 0s - loss: 0.0148
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0798
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0068
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0161
2864/6530 [============>.................] - ETA: 0s - loss: 0.0792
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0069
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0165
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0792
6528/6530 [============================>.] - ETA: 0s - loss: 0.0068
6530/6530 [==============================] - 0s 58us/step - loss: 0.0068 - val_loss: 0.0072
Epoch 14/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0058
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0160
4016/6530 [=================>............] - ETA: 0s - loss: 0.0794
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0064
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0155
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0789
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0067
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0155
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0792
2880/6530 [============>.................] - ETA: 0s - loss: 0.0068
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0154
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0788
3840/6530 [================>.............] - ETA: 0s - loss: 0.0069
3216/6530 [=============>................] - ETA: 0s - loss: 0.0151
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0791
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0068
6530/6530 [==============================] - 1s 92us/step - loss: 0.0789 - val_loss: 0.0847
Epoch 11/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0455
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0153
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0068
 576/6530 [=>............................] - ETA: 0s - loss: 0.0763
4016/6530 [=================>............] - ETA: 0s - loss: 0.0155
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0785
6530/6530 [==============================] - 0s 58us/step - loss: 0.0067 - val_loss: 0.0072
Epoch 15/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0058
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0156
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0781
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0062
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0157
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0780
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0155
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0065
2832/6530 [============>.................] - ETA: 0s - loss: 0.0782
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0153
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0066
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0775
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0152
3776/6530 [================>.............] - ETA: 0s - loss: 0.0067
3968/6530 [=================>............] - ETA: 0s - loss: 0.0779
6320/6530 [============================>.] - ETA: 0s - loss: 0.0152
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0067
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0774
6530/6530 [==============================] - 1s 134us/step - loss: 0.0153 - val_loss: 0.0166

5696/6530 [=========================>....] - ETA: 0s - loss: 0.0067Epoch 8/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0139
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0776
 400/6530 [>.............................] - ETA: 0s - loss: 0.0141
6530/6530 [==============================] - 0s 57us/step - loss: 0.0066 - val_loss: 0.0071
Epoch 16/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0056
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0773
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0152
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0061
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0776
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0157
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0065
6530/6530 [==============================] - 1s 93us/step - loss: 0.0774 - val_loss: 0.0858
Epoch 12/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0420
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0153
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0065
 544/6530 [=>............................] - ETA: 0s - loss: 0.0738
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0147
3712/6530 [================>.............] - ETA: 0s - loss: 0.0066
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0756
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0148
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0066
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0755
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0148
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0066
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0762
3136/6530 [=============>................] - ETA: 0s - loss: 0.0145
6528/6530 [============================>.] - ETA: 0s - loss: 0.0065
6530/6530 [==============================] - 0s 57us/step - loss: 0.0065 - val_loss: 0.0070
Epoch 17/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0055
2912/6530 [============>.................] - ETA: 0s - loss: 0.0756
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0146
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0062
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0756
3936/6530 [=================>............] - ETA: 0s - loss: 0.0147
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0064
4080/6530 [=================>............] - ETA: 0s - loss: 0.0760
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0148
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0758
2880/6530 [============>.................] - ETA: 0s - loss: 0.0065
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0150
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0755
3712/6530 [================>.............] - ETA: 0s - loss: 0.0066
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0148
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0755
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0065
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0145
6416/6530 [============================>.] - ETA: 0s - loss: 0.0759
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0066
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0145
6530/6530 [==============================] - 1s 91us/step - loss: 0.0757 - val_loss: 0.0834
Epoch 13/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0422
6528/6530 [============================>.] - ETA: 0s - loss: 0.0065
6530/6530 [==============================] - 0s 58us/step - loss: 0.0065 - val_loss: 0.0072

6336/6530 [============================>.] - ETA: 0s - loss: 0.0145Epoch 18/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0054
 560/6530 [=>............................] - ETA: 0s - loss: 0.0725
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0061
6530/6530 [==============================] - 1s 134us/step - loss: 0.0146 - val_loss: 0.0160
Epoch 9/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0131
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0741
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0064
 432/6530 [>.............................] - ETA: 0s - loss: 0.0133
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0744
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0065
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0146
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0753
3776/6530 [================>.............] - ETA: 0s - loss: 0.0065
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0151
2896/6530 [============>.................] - ETA: 0s - loss: 0.0750
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0065
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0146
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0748
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0065
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0141
4112/6530 [=================>............] - ETA: 0s - loss: 0.0750
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0142
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0748
6530/6530 [==============================] - 0s 58us/step - loss: 0.0064 - val_loss: 0.0072
Epoch 19/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0055
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0140
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0746
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0061
3184/6530 [=============>................] - ETA: 0s - loss: 0.0138
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0745
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0063
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0140
6432/6530 [============================>.] - ETA: 0s - loss: 0.0748
2944/6530 [============>.................] - ETA: 0s - loss: 0.0064
6530/6530 [==============================] - 1s 91us/step - loss: 0.0747 - val_loss: 0.0829
Epoch 14/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0504
3968/6530 [=================>............] - ETA: 0s - loss: 0.0141
3904/6530 [================>.............] - ETA: 0s - loss: 0.0065
 624/6530 [=>............................] - ETA: 0s - loss: 0.0726
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0142
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0065
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0738
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0144
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0064
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0743
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0141
6530/6530 [==============================] - 0s 57us/step - loss: 0.0064 - val_loss: 0.0077
Epoch 20/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0055
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0742
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0140
 640/6530 [=>............................] - ETA: 0s - loss: 0.0061
2896/6530 [============>.................] - ETA: 0s - loss: 0.0735
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0139
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0063
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0733
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0140
4064/6530 [=================>............] - ETA: 0s - loss: 0.0736
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0064
6530/6530 [==============================] - 1s 136us/step - loss: 0.0140 - val_loss: 0.0153
Epoch 10/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0118
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0065
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0735
 416/6530 [>.............................] - ETA: 0s - loss: 0.0128
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0735
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0064
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0144
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0735
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0065
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0147
6336/6530 [============================>.] - ETA: 0s - loss: 0.0738
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0063
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0141
6530/6530 [==============================] - 0s 62us/step - loss: 0.0063 - val_loss: 0.0073
Epoch 21/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0054
6530/6530 [==============================] - 1s 92us/step - loss: 0.0737 - val_loss: 0.0820
Epoch 15/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0401
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0136
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0059
 544/6530 [=>............................] - ETA: 0s - loss: 0.0714
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0137
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0062
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0727
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0136
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0732
2880/6530 [============>.................] - ETA: 0s - loss: 0.0062
3120/6530 [=============>................] - ETA: 0s - loss: 0.0133
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0728
3712/6530 [================>.............] - ETA: 0s - loss: 0.0063
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0135
2864/6530 [============>.................] - ETA: 0s - loss: 0.0725
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0063
3872/6530 [================>.............] - ETA: 0s - loss: 0.0136
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0726
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0063
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0138
4016/6530 [=================>............] - ETA: 0s - loss: 0.0728
6528/6530 [============================>.] - ETA: 0s - loss: 0.0063
6530/6530 [==============================] - 0s 58us/step - loss: 0.0062 - val_loss: 0.0072

4688/6530 [====================>.........] - ETA: 0s - loss: 0.0139
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0724
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0137
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0724
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0135
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0723
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0135
6384/6530 [============================>.] - ETA: 0s - loss: 0.0725
6530/6530 [==============================] - 1s 91us/step - loss: 0.0724 - val_loss: 0.0816
Epoch 16/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0475
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0135
 576/6530 [=>............................] - ETA: 0s - loss: 0.0711
6530/6530 [==============================] - 1s 135us/step - loss: 0.0135 - val_loss: 0.0149
Epoch 11/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0104
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0720
 416/6530 [>.............................] - ETA: 0s - loss: 0.0124
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0722
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0138
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0722
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0142
2960/6530 [============>.................] - ETA: 0s - loss: 0.0712
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0137
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0714
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0132
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0718
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0132
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0718
2832/6530 [============>.................] - ETA: 0s - loss: 0.0131
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0714
3216/6530 [=============>................] - ETA: 0s - loss: 0.0129
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0715
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0131
6464/6530 [============================>.] - ETA: 0s - loss: 0.0717
4000/6530 [=================>............] - ETA: 0s - loss: 0.0133
6530/6530 [==============================] - 1s 90us/step - loss: 0.0717 - val_loss: 0.0818
Epoch 17/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0467
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0133
 560/6530 [=>............................] - ETA: 0s - loss: 0.0709
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0134
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0713
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0132
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0713
# training | RMSE: 0.0711, MAE: 0.0556
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22893106919985234}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.07106116969524705, 'rmse': 0.07106116969524705, 'mae': 0.05561185286981979, 'early_stop': True}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  77 | activation: relu    | extras: dropout - rate: 42.1% 
layer 2 | size:  33 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  69 | activation: relu    | extras: None 
layer 4 | size:  14 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c894940>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  64/6530 [..............................] - ETA: 40s - loss: 0.5176
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0130
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0712
1280/6530 [====>.........................] - ETA: 1s - loss: 0.3285 
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0130
2880/6530 [============>.................] - ETA: 0s - loss: 0.0705
2560/6530 [==========>...................] - ETA: 0s - loss: 0.2975
6320/6530 [============================>.] - ETA: 0s - loss: 0.0130
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0707
3776/6530 [================>.............] - ETA: 0s - loss: 0.2770
4016/6530 [=================>............] - ETA: 0s - loss: 0.0708
6530/6530 [==============================] - 1s 134us/step - loss: 0.0131 - val_loss: 0.0133
Epoch 12/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0088
4928/6530 [=====================>........] - ETA: 0s - loss: 0.2617
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0706
 416/6530 [>.............................] - ETA: 0s - loss: 0.0122
6144/6530 [===========================>..] - ETA: 0s - loss: 0.2517
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0706
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0136
6530/6530 [==============================] - 1s 110us/step - loss: 0.2487 - val_loss: 0.1709
Epoch 2/27

  64/6530 [..............................] - ETA: 0s - loss: 0.2461
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0704
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0138
1344/6530 [=====>........................] - ETA: 0s - loss: 0.2039
6336/6530 [============================>.] - ETA: 0s - loss: 0.0706
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0132
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1993
6530/6530 [==============================] - 1s 93us/step - loss: 0.0705 - val_loss: 0.0809
Epoch 18/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0434
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0127
3840/6530 [================>.............] - ETA: 0s - loss: 0.1927
 608/6530 [=>............................] - ETA: 0s - loss: 0.0690
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0129
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1886
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0697
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0128
6336/6530 [============================>.] - ETA: 0s - loss: 0.1861
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0700
3072/6530 [=============>................] - ETA: 0s - loss: 0.0126
6530/6530 [==============================] - 0s 43us/step - loss: 0.1857 - val_loss: 0.1438
Epoch 3/27

  64/6530 [..............................] - ETA: 0s - loss: 0.2003
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0701
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0127
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1784
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0698
3856/6530 [================>.............] - ETA: 0s - loss: 0.0128
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1717
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0694
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0130
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1703
3920/6530 [=================>............] - ETA: 0s - loss: 0.0702
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0131
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1675
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0698
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0129
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1648
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0695
6530/6530 [==============================] - 0s 45us/step - loss: 0.1642 - val_loss: 0.1353
Epoch 4/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1503
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0127
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0697
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1595
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0127
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0699
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1569
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0127
6530/6530 [==============================] - 1s 94us/step - loss: 0.0698 - val_loss: 0.0812
Epoch 19/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0471
3776/6530 [================>.............] - ETA: 0s - loss: 0.1553
6530/6530 [==============================] - 1s 136us/step - loss: 0.0127 - val_loss: 0.0142
Epoch 13/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0088
 576/6530 [=>............................] - ETA: 0s - loss: 0.0687
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1533
 416/6530 [>.............................] - ETA: 0s - loss: 0.0119
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0695
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1519
6530/6530 [==============================] - 0s 43us/step - loss: 0.1516 - val_loss: 0.1491
Epoch 5/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1876
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0134
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0693
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1415
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0137
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0696
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1431
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0130
2832/6530 [============>.................] - ETA: 0s - loss: 0.0694
3776/6530 [================>.............] - ETA: 0s - loss: 0.1431
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0125
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0688
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1415
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0126
3984/6530 [=================>............] - ETA: 0s - loss: 0.0693
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1410
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0125
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0689
6530/6530 [==============================] - 0s 44us/step - loss: 0.1407 - val_loss: 0.1734
Epoch 6/27

  64/6530 [..............................] - ETA: 0s - loss: 0.2154
3136/6530 [=============>................] - ETA: 0s - loss: 0.0123
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0691
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1333
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0124
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0689
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1349
3888/6530 [================>.............] - ETA: 0s - loss: 0.0126
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0691
3840/6530 [================>.............] - ETA: 0s - loss: 0.1377
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0127
6530/6530 [==============================] - 1s 92us/step - loss: 0.0691 - val_loss: 0.0801
Epoch 20/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0473
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1375
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0128
 592/6530 [=>............................] - ETA: 0s - loss: 0.0683
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1366
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0126
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0693
6530/6530 [==============================] - 0s 44us/step - loss: 0.1366 - val_loss: 0.1539
Epoch 7/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1843
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0124
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0692
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1351
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0123
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0689
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1365
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0124
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0688
3840/6530 [================>.............] - ETA: 0s - loss: 0.1354
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0681
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1340
6530/6530 [==============================] - 1s 136us/step - loss: 0.0124 - val_loss: 0.0128
Epoch 14/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0080
3984/6530 [=================>............] - ETA: 0s - loss: 0.0686
6336/6530 [============================>.] - ETA: 0s - loss: 0.1325
 432/6530 [>.............................] - ETA: 0s - loss: 0.0119
6530/6530 [==============================] - 0s 43us/step - loss: 0.1323 - val_loss: 0.1305
Epoch 8/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1107
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0680
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0127
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1287
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0683
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0133
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1272
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0681
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0128
3840/6530 [================>.............] - ETA: 0s - loss: 0.1278
6320/6530 [============================>.] - ETA: 0s - loss: 0.0683
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0122
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1273
6530/6530 [==============================] - 1s 92us/step - loss: 0.0683 - val_loss: 0.0791
Epoch 21/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0442
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0123
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1267
 592/6530 [=>............................] - ETA: 0s - loss: 0.0647
6530/6530 [==============================] - 0s 44us/step - loss: 0.1262 - val_loss: 0.1359
Epoch 9/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1262
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0123
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0666
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1305
3136/6530 [=============>................] - ETA: 0s - loss: 0.0120
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0668
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1258
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0121
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0673
3776/6530 [================>.............] - ETA: 0s - loss: 0.1242
3888/6530 [================>.............] - ETA: 0s - loss: 0.0123
2880/6530 [============>.................] - ETA: 0s - loss: 0.0665
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1233
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0124
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0666
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1231
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0125
6530/6530 [==============================] - 0s 43us/step - loss: 0.1232 - val_loss: 0.1533
Epoch 10/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1449
4016/6530 [=================>............] - ETA: 0s - loss: 0.0668
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0123
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1240
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0668
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0121
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1231
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0668
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0120
3904/6530 [================>.............] - ETA: 0s - loss: 0.1234
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0668
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0121
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1222
6368/6530 [============================>.] - ETA: 0s - loss: 0.0671
6530/6530 [==============================] - 1s 91us/step - loss: 0.0671 - val_loss: 0.0793
Epoch 22/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0455
6400/6530 [============================>.] - ETA: 0s - loss: 0.1212
6530/6530 [==============================] - 1s 137us/step - loss: 0.0121 - val_loss: 0.0132
Epoch 15/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0077
6530/6530 [==============================] - 0s 43us/step - loss: 0.1212 - val_loss: 0.1330
Epoch 11/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1371
 608/6530 [=>............................] - ETA: 0s - loss: 0.0667
 416/6530 [>.............................] - ETA: 0s - loss: 0.0116
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1202
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0676
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0127
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1220
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0672
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0131
3776/6530 [================>.............] - ETA: 0s - loss: 0.1203
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0675
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0125
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1188
2896/6530 [============>.................] - ETA: 0s - loss: 0.0666
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0120
6400/6530 [============================>.] - ETA: 0s - loss: 0.1188
6530/6530 [==============================] - 0s 42us/step - loss: 0.1187 - val_loss: 0.1199

3472/6530 [==============>...............] - ETA: 0s - loss: 0.0666Epoch 12/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1254
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0121
4064/6530 [=================>............] - ETA: 0s - loss: 0.0669
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1144
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0120
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0667
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1157
3136/6530 [=============>................] - ETA: 0s - loss: 0.0117
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0666
3840/6530 [================>.............] - ETA: 0s - loss: 0.1166
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0118
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0664
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1168
3936/6530 [=================>............] - ETA: 0s - loss: 0.0120
6320/6530 [============================>.] - ETA: 0s - loss: 0.0666
6400/6530 [============================>.] - ETA: 0s - loss: 0.1172
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0121
6530/6530 [==============================] - 0s 42us/step - loss: 0.1171 - val_loss: 0.1435
Epoch 13/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1445
6530/6530 [==============================] - 1s 92us/step - loss: 0.0665 - val_loss: 0.0766
Epoch 23/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0446
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0122
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1216
 608/6530 [=>............................] - ETA: 0s - loss: 0.0654
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0119
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1187
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0667
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0118
3840/6530 [================>.............] - ETA: 0s - loss: 0.1179
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0667
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0118
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1176
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0667
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0118
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1177
2944/6530 [============>.................] - ETA: 0s - loss: 0.0657
6530/6530 [==============================] - 0s 44us/step - loss: 0.1175 - val_loss: 0.1367
Epoch 14/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1177
6530/6530 [==============================] - 1s 134us/step - loss: 0.0118 - val_loss: 0.0125
Epoch 16/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0069
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0658
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1103
 400/6530 [>.............................] - ETA: 0s - loss: 0.0115
4096/6530 [=================>............] - ETA: 0s - loss: 0.0660
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1101
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0124
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0658
3776/6530 [================>.............] - ETA: 0s - loss: 0.1118
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0127
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0656
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1115
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0122
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0658
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1119
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0117
6530/6530 [==============================] - 0s 44us/step - loss: 0.1122 - val_loss: 0.1248
Epoch 15/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1057
6352/6530 [============================>.] - ETA: 0s - loss: 0.0660
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0118
6530/6530 [==============================] - 1s 91us/step - loss: 0.0659 - val_loss: 0.0781
Epoch 24/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0498
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1117
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0116
 576/6530 [=>............................] - ETA: 0s - loss: 0.0640
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1103
3184/6530 [=============>................] - ETA: 0s - loss: 0.0114
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0655
3840/6530 [================>.............] - ETA: 0s - loss: 0.1112
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0116
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0654
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1115
3968/6530 [=================>............] - ETA: 0s - loss: 0.0117
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0659
6336/6530 [============================>.] - ETA: 0s - loss: 0.1132
6530/6530 [==============================] - 0s 43us/step - loss: 0.1129 - val_loss: 0.1239
Epoch 16/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1396
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0117
2864/6530 [============>.................] - ETA: 0s - loss: 0.0652
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1095
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0119
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0652
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1108
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0117
4016/6530 [=================>............] - ETA: 0s - loss: 0.0653
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1106
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0115
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0650
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1103
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0115
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0651
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1105
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0115
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0649
6530/6530 [==============================] - 0s 44us/step - loss: 0.1101 - val_loss: 0.1197
Epoch 17/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1210
6336/6530 [============================>.] - ETA: 0s - loss: 0.0652
6530/6530 [==============================] - 1s 136us/step - loss: 0.0116 - val_loss: 0.0116

1280/6530 [====>.........................] - ETA: 0s - loss: 0.1111Epoch 17/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0063
6530/6530 [==============================] - 1s 92us/step - loss: 0.0652 - val_loss: 0.0765
Epoch 25/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0467
 400/6530 [>.............................] - ETA: 0s - loss: 0.0113
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1130
 576/6530 [=>............................] - ETA: 0s - loss: 0.0649
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0121
3840/6530 [================>.............] - ETA: 0s - loss: 0.1122
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0662
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0125
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1112
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0656
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0120
6336/6530 [============================>.] - ETA: 0s - loss: 0.1111
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0658
6530/6530 [==============================] - 0s 43us/step - loss: 0.1110 - val_loss: 0.1108
Epoch 18/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1133
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0114
2848/6530 [============>.................] - ETA: 0s - loss: 0.0651
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1082
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0115
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0649
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1113
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0114
4032/6530 [=================>............] - ETA: 0s - loss: 0.0650
3776/6530 [================>.............] - ETA: 0s - loss: 0.1100
3200/6530 [=============>................] - ETA: 0s - loss: 0.0111
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0647
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1088
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0113
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0647
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1090
6530/6530 [==============================] - 0s 43us/step - loss: 0.1094 - val_loss: 0.1316

4016/6530 [=================>............] - ETA: 0s - loss: 0.0115Epoch 19/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1357
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0648
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0115
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1094
6352/6530 [============================>.] - ETA: 0s - loss: 0.0650
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0116
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1081
6530/6530 [==============================] - 1s 92us/step - loss: 0.0650 - val_loss: 0.0762
Epoch 26/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0454
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0114
3904/6530 [================>.............] - ETA: 0s - loss: 0.1088
 608/6530 [=>............................] - ETA: 0s - loss: 0.0655
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0112
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1086
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0674
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0112
6464/6530 [============================>.] - ETA: 0s - loss: 0.1078
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0662
6530/6530 [==============================] - 0s 42us/step - loss: 0.1080 - val_loss: 0.1187
Epoch 20/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1080
6416/6530 [============================>.] - ETA: 0s - loss: 0.0112
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0658
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1118
6530/6530 [==============================] - 1s 133us/step - loss: 0.0113 - val_loss: 0.0112
Epoch 18/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0055
2944/6530 [============>.................] - ETA: 0s - loss: 0.0649
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1090
 352/6530 [>.............................] - ETA: 0s - loss: 0.0113
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0649
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1073
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0122
4048/6530 [=================>............] - ETA: 0s - loss: 0.0653
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1081
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0124
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0648
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1076
6530/6530 [==============================] - 0s 44us/step - loss: 0.1076 - val_loss: 0.1332

1536/6530 [======>.......................] - ETA: 0s - loss: 0.0118Epoch 21/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1423
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0649
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0113
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1051
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0647
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0113
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1077
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0649
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0113
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1061
6530/6530 [==============================] - 1s 93us/step - loss: 0.0649 - val_loss: 0.0751
Epoch 27/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0434
3056/6530 [=============>................] - ETA: 0s - loss: 0.0110
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1067
 592/6530 [=>............................] - ETA: 0s - loss: 0.0641
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0110
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1061
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0652
6530/6530 [==============================] - 0s 44us/step - loss: 0.1061 - val_loss: 0.1270
Epoch 22/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1051
3840/6530 [================>.............] - ETA: 0s - loss: 0.0111
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0647
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1030
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0112
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0647
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1051
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0113
2848/6530 [============>.................] - ETA: 0s - loss: 0.0640
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1064
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0112
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0634
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1065
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0111
4016/6530 [=================>............] - ETA: 0s - loss: 0.0639
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1062
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0109
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0637
6530/6530 [==============================] - 0s 45us/step - loss: 0.1060 - val_loss: 0.1163

6176/6530 [===========================>..] - ETA: 0s - loss: 0.0109
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0637
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0637
6530/6530 [==============================] - 1s 137us/step - loss: 0.0110 - val_loss: 0.0113
Epoch 19/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0047
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0639
 416/6530 [>.............................] - ETA: 0s - loss: 0.0109
6530/6530 [==============================] - 1s 92us/step - loss: 0.0639 - val_loss: 0.0742

 848/6530 [==>...........................] - ETA: 0s - loss: 0.0114
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0120
# training | RMSE: 0.0841, MAE: 0.0617
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10991956439681334}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25088382860678343}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.08414640972664571, 'rmse': 0.08414640972664571, 'mae': 0.06167691201259393, 'early_stop': False}
vggnet done  1

1632/6530 [======>.......................] - ETA: 0s - loss: 0.0115
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0112
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0110
2896/6530 [============>.................] - ETA: 0s - loss: 0.0108
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0106
3792/6530 [================>.............] - ETA: 0s - loss: 0.0107
# training | RMSE: 0.1461, MAE: 0.1109
worker 0  xfile  [3, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42127947953652023}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.1460962944937521, 'rmse': 0.1460962944937521, 'mae': 0.11090611605186398, 'early_stop': True}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  52 | activation: relu    | extras: dropout - rate: 38.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ff4bf828>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 128/6530 [..............................] - ETA: 9s - loss: 0.7113
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0110
5120/6530 [======================>.......] - ETA: 0s - loss: 0.5274
6530/6530 [==============================] - 0s 43us/step - loss: 0.4807 - val_loss: 0.2635
Epoch 2/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2803
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0110
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2293
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0109
6530/6530 [==============================] - 0s 11us/step - loss: 0.2228 - val_loss: 0.1937
Epoch 3/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2205
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0107
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1898
6530/6530 [==============================] - 0s 11us/step - loss: 0.1864 - val_loss: 0.1727
Epoch 4/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1675
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0107
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1712
6352/6530 [============================>.] - ETA: 0s - loss: 0.0107
6530/6530 [==============================] - 0s 11us/step - loss: 0.1693 - val_loss: 0.1636
Epoch 5/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1526
6530/6530 [==============================] - 1s 125us/step - loss: 0.0108 - val_loss: 0.0106
Epoch 20/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0045
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1600
6530/6530 [==============================] - 0s 11us/step - loss: 0.1591 - val_loss: 0.1568
Epoch 6/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1537
 416/6530 [>.............................] - ETA: 0s - loss: 0.0108
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1520
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0115
6530/6530 [==============================] - 0s 11us/step - loss: 0.1518 - val_loss: 0.1497
Epoch 7/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1356
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0118
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1481
6530/6530 [==============================] - 0s 11us/step - loss: 0.1471 - val_loss: 0.1469
Epoch 8/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1538
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0113
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1434
6530/6530 [==============================] - 0s 11us/step - loss: 0.1425 - val_loss: 0.1424
Epoch 9/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1357
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0110
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1401
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0108
6530/6530 [==============================] - 0s 11us/step - loss: 0.1383 - val_loss: 0.1401
Epoch 10/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1339
2896/6530 [============>.................] - ETA: 0s - loss: 0.0106
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1338
6530/6530 [==============================] - 0s 11us/step - loss: 0.1344 - val_loss: 0.1356
Epoch 11/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1270
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0104
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1313
3744/6530 [================>.............] - ETA: 0s - loss: 0.0105
6530/6530 [==============================] - 0s 11us/step - loss: 0.1310 - val_loss: 0.1326
Epoch 12/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1304
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0108
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1286
6530/6530 [==============================] - 0s 11us/step - loss: 0.1275 - val_loss: 0.1295
Epoch 13/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1054
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0108
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1230
6530/6530 [==============================] - 0s 11us/step - loss: 0.1229 - val_loss: 0.1252
Epoch 14/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1045
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0107
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1209
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0106
6530/6530 [==============================] - 0s 11us/step - loss: 0.1204 - val_loss: 0.1219
Epoch 15/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1220
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0105
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1168
6530/6530 [==============================] - 0s 11us/step - loss: 0.1161 - val_loss: 0.1193
Epoch 16/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1027
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0105
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1144
6530/6530 [==============================] - 1s 126us/step - loss: 0.0105 - val_loss: 0.0097
Epoch 21/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0040
6530/6530 [==============================] - 0s 11us/step - loss: 0.1131 - val_loss: 0.1161
Epoch 17/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0928
 448/6530 [=>............................] - ETA: 0s - loss: 0.0112
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1105
6530/6530 [==============================] - 0s 11us/step - loss: 0.1102 - val_loss: 0.1134
Epoch 18/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1080
 848/6530 [==>...........................] - ETA: 0s - loss: 0.0109
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1083
6530/6530 [==============================] - 0s 11us/step - loss: 0.1071 - val_loss: 0.1117
Epoch 19/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1094
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0114
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1067
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0109
6530/6530 [==============================] - 0s 11us/step - loss: 0.1055 - val_loss: 0.1090
Epoch 20/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1069
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0106
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1022
6530/6530 [==============================] - 0s 11us/step - loss: 0.1019 - val_loss: 0.1072
Epoch 21/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0888
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0107
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1005
6530/6530 [==============================] - 0s 11us/step - loss: 0.0997 - val_loss: 0.1050

3008/6530 [============>.................] - ETA: 0s - loss: 0.0103Epoch 22/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0879
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0103
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0993
6530/6530 [==============================] - 0s 12us/step - loss: 0.0980 - val_loss: 0.1031
Epoch 23/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0868
3840/6530 [================>.............] - ETA: 0s - loss: 0.0103
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0954
6530/6530 [==============================] - 0s 11us/step - loss: 0.0960 - val_loss: 0.1016
Epoch 24/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1025
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0105
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0952
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0106
6530/6530 [==============================] - 0s 11us/step - loss: 0.0950 - val_loss: 0.1007
Epoch 25/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0953
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0105
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0933
6530/6530 [==============================] - 0s 11us/step - loss: 0.0930 - val_loss: 0.0994
Epoch 26/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0926
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0103
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0900
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0102
6530/6530 [==============================] - 0s 11us/step - loss: 0.0910 - val_loss: 0.0983
Epoch 27/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0916
6320/6530 [============================>.] - ETA: 0s - loss: 0.0102
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0895
6530/6530 [==============================] - 0s 11us/step - loss: 0.0896 - val_loss: 0.0976

6530/6530 [==============================] - 1s 124us/step - loss: 0.0103 - val_loss: 0.0103
Epoch 22/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0041
# training | RMSE: 0.1226, MAE: 0.0884
worker 0  xfile  [4, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38379181805725937}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.151550479705256}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.302283486099243}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.12262754696939128, 'rmse': 0.12262754696939128, 'mae': 0.08839522069293657, 'early_stop': False}
vggnet done  0

 448/6530 [=>............................] - ETA: 0s - loss: 0.0110
 880/6530 [===>..........................] - ETA: 0s - loss: 0.0107
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0111
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0106
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0104
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0104
2896/6530 [============>.................] - ETA: 0s - loss: 0.0102
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0100
3744/6530 [================>.............] - ETA: 0s - loss: 0.0101
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0104
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0104
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0103
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0102
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0100
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0100
6432/6530 [============================>.] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 1s 132us/step - loss: 0.0101 - val_loss: 0.0104
Epoch 23/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0042
 400/6530 [>.............................] - ETA: 0s - loss: 0.0106
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0108
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0110
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0107
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0104
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0102
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0101
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0100
3136/6530 [=============>................] - ETA: 0s - loss: 0.0098
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0099
3856/6530 [================>.............] - ETA: 0s - loss: 0.0099
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0101
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0101
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0100
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0100
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0098
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0098
6400/6530 [============================>.] - ETA: 0s - loss: 0.0098
6530/6530 [==============================] - 1s 149us/step - loss: 0.0099 - val_loss: 0.0105
Epoch 24/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0038
 368/6530 [>.............................] - ETA: 0s - loss: 0.0103
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0109
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0105
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0107
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0104
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0098
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0099
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0100
2880/6530 [============>.................] - ETA: 0s - loss: 0.0098
3120/6530 [=============>................] - ETA: 0s - loss: 0.0096
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0096
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0097
3792/6530 [================>.............] - ETA: 0s - loss: 0.0096
4048/6530 [=================>............] - ETA: 0s - loss: 0.0099
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0099
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0099
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0099
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0098
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0098
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0097
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0096
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0096
6352/6530 [============================>.] - ETA: 0s - loss: 0.0096
6530/6530 [==============================] - 1s 192us/step - loss: 0.0097 - val_loss: 0.0110
Epoch 25/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0038
 272/6530 [>.............................] - ETA: 1s - loss: 0.0102
 560/6530 [=>............................] - ETA: 1s - loss: 0.0102
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0101
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0104
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0103
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0100
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0098
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0097
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0099
2832/6530 [============>.................] - ETA: 0s - loss: 0.0096
3152/6530 [=============>................] - ETA: 0s - loss: 0.0094
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0095
3776/6530 [================>.............] - ETA: 0s - loss: 0.0094
4112/6530 [=================>............] - ETA: 0s - loss: 0.0097
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0096
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0097
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0096
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0096
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0094
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0095
6352/6530 [============================>.] - ETA: 0s - loss: 0.0095
6530/6530 [==============================] - 1s 176us/step - loss: 0.0095 - val_loss: 0.0102

# training | RMSE: 0.0945, MAE: 0.0745
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.09450126591551929, 'rmse': 0.09450126591551929, 'mae': 0.07445976163484638, 'early_stop': True}
vggnet done  2
all of workers have been done
calculation finished
#0 epoch=27.0 loss={'loss': 0.07106116969524705, 'rmse': 0.07106116969524705, 'mae': 0.05561185286981979, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22893106919985234}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#3 epoch=27.0 loss={'loss': 0.1460962944937521, 'rmse': 0.1460962944937521, 'mae': 0.11090611605186398, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.42127947953652023}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#1 epoch=27.0 loss={'loss': 0.08414640972664571, 'rmse': 0.08414640972664571, 'mae': 0.06167691201259393, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 100, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10991956439681334}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25088382860678343}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#4 epoch=27.0 loss={'loss': 0.12262754696939128, 'rmse': 0.12262754696939128, 'mae': 0.08839522069293657, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38379181805725937}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.151550479705256}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.302283486099243}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#2 epoch=27.0 loss={'loss': 0.09450126591551929, 'rmse': 0.09450126591551929, 'mae': 0.07445976163484638, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
get a list [results] of length 190
get a list [loss] of length 5
get a list [val_loss] of length 5
length of indices is (0, 1, 2, 4, 3)
length of indices is 5
length of T is 5
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]] 

*** 1.6666666666666665 configurations x 81.0 iterations each

3 | Thu Sep 27 23:10:09 2018 | lowest loss so far: 0.0675 (run 0)

vggnet done  1
vggnet done  2
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  91 | activation: relu    | extras: batchnorm 
layer 2 | size:  79 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 6:31 - loss: 0.1747
 304/6530 [>.............................] - ETA: 20s - loss: 0.0672 
 592/6530 [=>............................] - ETA: 10s - loss: 0.0615
 864/6530 [==>...........................] - ETA: 7s - loss: 0.0570 
1136/6530 [====>.........................] - ETA: 5s - loss: 0.0539
1424/6530 [=====>........................] - ETA: 4s - loss: 0.0509
1712/6530 [======>.......................] - ETA: 3s - loss: 0.0484
1984/6530 [========>.....................] - ETA: 3s - loss: 0.0471
2256/6530 [=========>....................] - ETA: 2s - loss: 0.0462
2528/6530 [==========>...................] - ETA: 2s - loss: 0.0458
2848/6530 [============>.................] - ETA: 1s - loss: 0.0446
3152/6530 [=============>................] - ETA: 1s - loss: 0.0434
3456/6530 [==============>...............] - ETA: 1s - loss: 0.0430
3744/6530 [================>.............] - ETA: 1s - loss: 0.0420
4048/6530 [=================>............] - ETA: 1s - loss: 0.0417
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0411
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0406
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0401
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0396
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0393
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0388
6448/6530 [============================>.] - ETA: 0s - loss: 0.0384
6530/6530 [==============================] - 2s 324us/step - loss: 0.0383 - val_loss: 0.0346
Epoch 2/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0292
 304/6530 [>.............................] - ETA: 1s - loss: 0.0249
 608/6530 [=>............................] - ETA: 1s - loss: 0.0289
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0280
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0281
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0274
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0273
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0275
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0276
2912/6530 [============>.................] - ETA: 0s - loss: 0.0272
3200/6530 [=============>................] - ETA: 0s - loss: 0.0268
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0268
3904/6530 [================>.............] - ETA: 0s - loss: 0.0266
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0266
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0269
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0266
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0266
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0265
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0263
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0261
6384/6530 [============================>.] - ETA: 0s - loss: 0.0260
6530/6530 [==============================] - 1s 165us/step - loss: 0.0260 - val_loss: 0.0277
Epoch 3/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0255
 320/6530 [>.............................] - ETA: 1s - loss: 0.0216
 624/6530 [=>............................] - ETA: 0s - loss: 0.0234
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0228
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0230
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0225
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0221
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0223
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0223
2976/6530 [============>.................] - ETA: 0s - loss: 0.0219
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0217
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0218
3888/6530 [================>.............] - ETA: 0s - loss: 0.0218
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0218
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0221
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0219
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0218
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0217
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0216
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0215
6448/6530 [============================>.] - ETA: 0s - loss: 0.0214
6530/6530 [==============================] - 1s 166us/step - loss: 0.0215 - val_loss: 0.0249
Epoch 4/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0225
 272/6530 [>.............................] - ETA: 1s - loss: 0.0191
 528/6530 [=>............................] - ETA: 1s - loss: 0.0201
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0205
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0207
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0200
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0198
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0194
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0196
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0194
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0192
3104/6530 [=============>................] - ETA: 0s - loss: 0.0188
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0190
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0190
3936/6530 [=================>............] - ETA: 0s - loss: 0.0191
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0192
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0194
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0193
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0192
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0191
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0190
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0189
6480/6530 [============================>.] - ETA: 0s - loss: 0.0188
6530/6530 [==============================] - 1s 182us/step - loss: 0.0189 - val_loss: 0.0206
Epoch 5/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0188
 336/6530 [>.............................] - ETA: 0s - loss: 0.0178
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0185
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0181
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0189
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0182
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0177
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0178
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0178
2848/6530 [============>.................] - ETA: 0s - loss: 0.0175
3120/6530 [=============>................] - ETA: 0s - loss: 0.0172
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0173
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0174
3952/6530 [=================>............] - ETA: 0s - loss: 0.0174
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0176
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0178
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0177
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0175
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0175
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0174
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0173
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0173
6530/6530 [==============================] - 1s 179us/step - loss: 0.0173 - val_loss: 0.0179
Epoch 6/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0161
 304/6530 [>.............................] - ETA: 1s - loss: 0.0157
 576/6530 [=>............................] - ETA: 1s - loss: 0.0166
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0170
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0173
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0176
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0169
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0165
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0167
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0165
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0164
3024/6530 [============>.................] - ETA: 0s - loss: 0.0161
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0161
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0162
3856/6530 [================>.............] - ETA: 0s - loss: 0.0163
4112/6530 [=================>............] - ETA: 0s - loss: 0.0165
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0163
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0166
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0165
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0163
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0162
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0161
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0161
6530/6530 [==============================] - 1s 186us/step - loss: 0.0162 - val_loss: 0.0167
Epoch 7/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0137
 352/6530 [>.............................] - ETA: 0s - loss: 0.0151
 608/6530 [=>............................] - ETA: 1s - loss: 0.0158
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0158
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0167
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0164
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0160
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0155
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0157
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0156
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0155
2976/6530 [============>.................] - ETA: 0s - loss: 0.0153
3232/6530 [=============>................] - ETA: 0s - loss: 0.0151
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0153
3712/6530 [================>.............] - ETA: 0s - loss: 0.0153
3936/6530 [=================>............] - ETA: 0s - loss: 0.0154
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0155
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0156
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0157
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0155
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0155
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0153
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0152
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0153
6530/6530 [==============================] - 1s 196us/step - loss: 0.0153 - val_loss: 0.0158
Epoch 8/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0130
 240/6530 [>.............................] - ETA: 1s - loss: 0.0139
 480/6530 [=>............................] - ETA: 1s - loss: 0.0148
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0156
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0150
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0158
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0155
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0151
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0150
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0149
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0149
2992/6530 [============>.................] - ETA: 0s - loss: 0.0146
3264/6530 [=============>................] - ETA: 0s - loss: 0.0145
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0146
3792/6530 [================>.............] - ETA: 0s - loss: 0.0147
4096/6530 [=================>............] - ETA: 0s - loss: 0.0150
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0148
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0150
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0149
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0149
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0148
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0146
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0146
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0146
6512/6530 [============================>.] - ETA: 0s - loss: 0.0146
6530/6530 [==============================] - 1s 198us/step - loss: 0.0147 - val_loss: 0.0158
Epoch 9/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0118
 384/6530 [>.............................] - ETA: 0s - loss: 0.0135
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0149
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0155
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0152
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0148
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0142
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0145
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0144
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0143
2992/6530 [============>.................] - ETA: 0s - loss: 0.0141
3264/6530 [=============>................] - ETA: 0s - loss: 0.0140
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0141
3776/6530 [================>.............] - ETA: 0s - loss: 0.0141
4080/6530 [=================>............] - ETA: 0s - loss: 0.0144
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0143
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0145
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0143
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0143
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0141
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0140
6368/6530 [============================>.] - ETA: 0s - loss: 0.0140
6530/6530 [==============================] - 1s 175us/step - loss: 0.0141 - val_loss: 0.0163
Epoch 10/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0112
 352/6530 [>.............................] - ETA: 0s - loss: 0.0131
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0145
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0144
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0148
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0141
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0138
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0138
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0138
2896/6530 [============>.................] - ETA: 0s - loss: 0.0136
3232/6530 [=============>................] - ETA: 0s - loss: 0.0134
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0135
3856/6530 [================>.............] - ETA: 0s - loss: 0.0137
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0139
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0140
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0138
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0137
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0136
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0135
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0136
6530/6530 [==============================] - 1s 161us/step - loss: 0.0136 - val_loss: 0.0165
Epoch 11/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0109
 320/6530 [>.............................] - ETA: 1s - loss: 0.0127
 640/6530 [=>............................] - ETA: 0s - loss: 0.0136
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0138
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0144
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0142
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0138
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0133
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0134
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0134
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0134
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0132
3056/6530 [=============>................] - ETA: 0s - loss: 0.0131
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0130
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0132
3792/6530 [================>.............] - ETA: 0s - loss: 0.0131
4032/6530 [=================>............] - ETA: 0s - loss: 0.0136
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0134
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0136
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0135
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0133
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0132
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0132
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0131
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0132
6528/6530 [============================>.] - ETA: 0s - loss: 0.0132
6530/6530 [==============================] - 1s 205us/step - loss: 0.0132 - val_loss: 0.0151
Epoch 12/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0091
 272/6530 [>.............................] - ETA: 1s - loss: 0.0129
 560/6530 [=>............................] - ETA: 1s - loss: 0.0127
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0136
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0142
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0138
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0135
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0128
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0131
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0130
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0129
3040/6530 [============>.................] - ETA: 0s - loss: 0.0127
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0127
3712/6530 [================>.............] - ETA: 0s - loss: 0.0128
4048/6530 [=================>............] - ETA: 0s - loss: 0.0132
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0130
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0132
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0131
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0130
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0128
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0128
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0128
6530/6530 [==============================] - 1s 180us/step - loss: 0.0129 - val_loss: 0.0138
Epoch 13/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0090
 304/6530 [>.............................] - ETA: 1s - loss: 0.0122
 592/6530 [=>............................] - ETA: 1s - loss: 0.0128
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0131
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0138
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0133
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0129
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0127
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0126
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0126
3008/6530 [============>.................] - ETA: 0s - loss: 0.0124
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0123
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0125
4016/6530 [=================>............] - ETA: 0s - loss: 0.0127
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0127
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0129
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0127
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0126
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0125
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0124
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0124
6448/6530 [============================>.] - ETA: 0s - loss: 0.0124
6530/6530 [==============================] - 1s 174us/step - loss: 0.0125 - val_loss: 0.0127
Epoch 14/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0083
 256/6530 [>.............................] - ETA: 1s - loss: 0.0123
 480/6530 [=>............................] - ETA: 1s - loss: 0.0127
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0132
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0129
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0132
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0128
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0122
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0123
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0122
3024/6530 [============>.................] - ETA: 0s - loss: 0.0121
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0119
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0122
3952/6530 [=================>............] - ETA: 0s - loss: 0.0123
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0124
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0125
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0125
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0124
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0123
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0121
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0121
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0121
6530/6530 [==============================] - 1s 180us/step - loss: 0.0122 - val_loss: 0.0115
Epoch 15/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0071
 320/6530 [>.............................] - ETA: 1s - loss: 0.0117
 624/6530 [=>............................] - ETA: 0s - loss: 0.0124
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0125
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0131
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0130
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0125
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0119
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0121
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0121
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0120
2976/6530 [============>.................] - ETA: 0s - loss: 0.0118
3248/6530 [=============>................] - ETA: 0s - loss: 0.0117
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0119
3792/6530 [================>.............] - ETA: 0s - loss: 0.0119
4096/6530 [=================>............] - ETA: 0s - loss: 0.0122
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0122
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0123
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0121
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0120
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0118
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0118
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0118
6530/6530 [==============================] - 1s 188us/step - loss: 0.0119 - val_loss: 0.0113
Epoch 16/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0071
 288/6530 [>.............................] - ETA: 1s - loss: 0.0117
 528/6530 [=>............................] - ETA: 1s - loss: 0.0119
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0126
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0119
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0128
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0126
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0122
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0120
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0116
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0117
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0119
2912/6530 [============>.................] - ETA: 0s - loss: 0.0115
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0114
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0116
3920/6530 [=================>............] - ETA: 0s - loss: 0.0117
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0118
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0120
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0118
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0117
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0116
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0115
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0115
6384/6530 [============================>.] - ETA: 0s - loss: 0.0115
6530/6530 [==============================] - 1s 192us/step - loss: 0.0116 - val_loss: 0.0109
Epoch 17/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0064
 288/6530 [>.............................] - ETA: 1s - loss: 0.0115
 560/6530 [=>............................] - ETA: 1s - loss: 0.0114
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0119
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0125
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0122
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0118
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0116
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0115
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0114
2976/6530 [============>.................] - ETA: 0s - loss: 0.0112
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0111
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0113
4080/6530 [=================>............] - ETA: 0s - loss: 0.0116
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0116
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0116
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0115
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0113
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0113
6336/6530 [============================>.] - ETA: 0s - loss: 0.0113
6530/6530 [==============================] - 1s 159us/step - loss: 0.0113 - val_loss: 0.0098
Epoch 18/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0059
 320/6530 [>.............................] - ETA: 1s - loss: 0.0108
 576/6530 [=>............................] - ETA: 1s - loss: 0.0116
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0116
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0122
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0121
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0117
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0110
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0113
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0112
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0113
2880/6530 [============>.................] - ETA: 0s - loss: 0.0110
3104/6530 [=============>................] - ETA: 0s - loss: 0.0109
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0108
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0110
3904/6530 [================>.............] - ETA: 0s - loss: 0.0111
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0113
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0112
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0114
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0113
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0112
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0111
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0110
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0110
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0110
6530/6530 [==============================] - 1s 204us/step - loss: 0.0111 - val_loss: 0.0098
Epoch 19/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0058
 256/6530 [>.............................] - ETA: 1s - loss: 0.0110
 496/6530 [=>............................] - ETA: 1s - loss: 0.0115
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0117
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0119
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0117
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0113
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0110
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0110
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0110
2864/6530 [============>.................] - ETA: 0s - loss: 0.0108
3152/6530 [=============>................] - ETA: 0s - loss: 0.0107
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0108
3776/6530 [================>.............] - ETA: 0s - loss: 0.0108
4112/6530 [=================>............] - ETA: 0s - loss: 0.0110
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0110
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0111
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0110
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0109
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0108
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0107
6432/6530 [============================>.] - ETA: 0s - loss: 0.0108
6530/6530 [==============================] - 1s 174us/step - loss: 0.0108 - val_loss: 0.0095
Epoch 20/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0055
 384/6530 [>.............................] - ETA: 0s - loss: 0.0107
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0116
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0117
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0114
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0110
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0108
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0108
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0107
3008/6530 [============>.................] - ETA: 0s - loss: 0.0105
3216/6530 [=============>................] - ETA: 0s - loss: 0.0105
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0106
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0106
3824/6530 [================>.............] - ETA: 0s - loss: 0.0106
4032/6530 [=================>............] - ETA: 0s - loss: 0.0109
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0108
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0109
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0108
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0108
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0107
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0105
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0105
6320/6530 [============================>.] - ETA: 0s - loss: 0.0105
6530/6530 [==============================] - 1s 184us/step - loss: 0.0106 - val_loss: 0.0092
Epoch 21/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0049
 368/6530 [>.............................] - ETA: 0s - loss: 0.0106
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0115
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0111
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0114
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0110
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0104
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0106
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0106
2928/6530 [============>.................] - ETA: 0s - loss: 0.0103
3216/6530 [=============>................] - ETA: 0s - loss: 0.0102
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0103
3872/6530 [================>.............] - ETA: 0s - loss: 0.0104
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0105
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0107
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0105
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0104
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0103
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0103
6368/6530 [============================>.] - ETA: 0s - loss: 0.0103
6530/6530 [==============================] - 1s 161us/step - loss: 0.0103 - val_loss: 0.0088
Epoch 22/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0046
 272/6530 [>.............................] - ETA: 1s - loss: 0.0104
 528/6530 [=>............................] - ETA: 1s - loss: 0.0107
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0111
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0108
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0112
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0109
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0106
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0104
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0104
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0105
2928/6530 [============>.................] - ETA: 0s - loss: 0.0101
3248/6530 [=============>................] - ETA: 0s - loss: 0.0100
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0101
3968/6530 [=================>............] - ETA: 0s - loss: 0.0102
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0103
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0104
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0103
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0102
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0100
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0100
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0100
6528/6530 [============================>.] - ETA: 0s - loss: 0.0101
6530/6530 [==============================] - 1s 180us/step - loss: 0.0101 - val_loss: 0.0094
Epoch 23/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0049
 240/6530 [>.............................] - ETA: 1s - loss: 0.0097
 480/6530 [=>............................] - ETA: 1s - loss: 0.0108
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0109
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0104
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0109
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0108
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0105
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0102
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0102
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0102
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0103
2880/6530 [============>.................] - ETA: 0s - loss: 0.0100
3152/6530 [=============>................] - ETA: 0s - loss: 0.0098
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0099
3792/6530 [================>.............] - ETA: 0s - loss: 0.0099
4128/6530 [=================>............] - ETA: 0s - loss: 0.0101
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0101
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0102
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0101
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0100
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0098
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0098
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0098
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0098
6528/6530 [============================>.] - ETA: 0s - loss: 0.0099
6530/6530 [==============================] - 1s 205us/step - loss: 0.0099 - val_loss: 0.0091
Epoch 24/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0048
 320/6530 [>.............................] - ETA: 1s - loss: 0.0097
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0105
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0100
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0106
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0103
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0098
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0099
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0101
2832/6530 [============>.................] - ETA: 0s - loss: 0.0099
3088/6530 [=============>................] - ETA: 0s - loss: 0.0097
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0096
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0097
3904/6530 [================>.............] - ETA: 0s - loss: 0.0097
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0099
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0098
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0099
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0098
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0098
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0097
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0096
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0096
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0096
6336/6530 [============================>.] - ETA: 0s - loss: 0.0096
6530/6530 [==============================] - 1s 195us/step - loss: 0.0097 - val_loss: 0.0096
Epoch 25/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0045
 272/6530 [>.............................] - ETA: 1s - loss: 0.0099
 544/6530 [=>............................] - ETA: 1s - loss: 0.0101
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0102
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0103
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0103
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0100
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0097
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0097
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0097
3024/6530 [============>.................] - ETA: 0s - loss: 0.0095
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0094
3760/6530 [================>.............] - ETA: 0s - loss: 0.0095
4096/6530 [=================>............] - ETA: 0s - loss: 0.0096
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0096
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0097
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0096
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0095
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0094
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0094
6512/6530 [============================>.] - ETA: 0s - loss: 0.0094
6530/6530 [==============================] - 1s 162us/step - loss: 0.0095 - val_loss: 0.0096
Epoch 26/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0051
 384/6530 [>.............................] - ETA: 0s - loss: 0.0097
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0102
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0097
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0100
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0099
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0097
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0095
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0095
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0097
2848/6530 [============>.................] - ETA: 0s - loss: 0.0095
3136/6530 [=============>................] - ETA: 0s - loss: 0.0092
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0093
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0094
3984/6530 [=================>............] - ETA: 0s - loss: 0.0094
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0094
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0095
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0095
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0094
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0092
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0092
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0092
6530/6530 [==============================] - 1s 177us/step - loss: 0.0093 - val_loss: 0.0098

# training | RMSE: 0.0927, MAE: 0.0729
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.09271198201583747, 'rmse': 0.09271198201583747, 'mae': 0.07286129616406364, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=81.0 loss={'loss': 0.09271198201583747, 'rmse': 0.09271198201583747, 'mae': 0.07286129616406364, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
get a list [results] of length 191
get a list [loss] of length 1
get a list [val_loss] of length 1
length of indices is (0,)
length of indices is 1
length of T is 1
s=1
T is of size 8
T=[{'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 35, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20722056181944334}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 53, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3917465634993933}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 29, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15337783986534193}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 92, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 59, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14008507478600118}, 'layer_2_size': 32, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 88, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.43079533423603034}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.34262296990548324}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3881724853455274}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28576535767268696}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 91, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 35, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [1, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20722056181944334}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 53, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [2, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3917465634993933}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 29, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15337783986534193}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [3, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 92, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 59, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [4, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14008507478600118}, 'layer_2_size': 32, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 88, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [5, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.43079533423603034}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.34262296990548324}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [6, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3881724853455274}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28576535767268696}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [7, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 91, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]] 

*** 8 configurations x 27.0 iterations each

1 | Thu Sep 27 23:10:41 2018 | lowest loss so far: 0.0675 (run 0)

{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  99 | activation: sigmoid | extras: None 
layer 2 | size:  77 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 256/6530 [>.............................] - ETA: 17s - loss: 0.8089
5376/6530 [=======================>......] - ETA: 0s - loss: 0.3295 
6530/6530 [==============================] - 1s 124us/step - loss: 0.3087 - val_loss: 0.2133
Epoch 2/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.2110
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2154
6530/6530 [==============================] - 0s 11us/step - loss: 0.2151 - val_loss: 0.2117
Epoch 3/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.2096
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2142
6530/6530 [==============================] - 0s 11us/step - loss: 0.2136 - val_loss: 0.2100
Epoch 4/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.2081
6530/6530 [==============================] - 0s 7us/step - loss: 0.2121 - val_loss: 0.2082
Epoch 5/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.2061
6530/6530 [==============================] - 0s 7us/step - loss: 0.2102 - val_loss: 0.2060
Epoch 6/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.2039
6530/6530 [==============================] - 0s 8us/step - loss: 0.2076 - val_loss: 0.2025
Epoch 7/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.2010
6530/6530 [==============================] - 0s 7us/step - loss: 0.2042 - val_loss: 0.1983
Epoch 8/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1975
6530/6530 [==============================] - 0s 7us/step - loss: 0.1998 - val_loss: 0.1932
Epoch 9/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1933
6530/6530 [==============================] - 0s 8us/step - loss: 0.1943 - val_loss: 0.1878
Epoch 10/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1884
6530/6530 [==============================] - 0s 6us/step - loss: 0.1882 - val_loss: 0.1819
Epoch 11/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1835
6530/6530 [==============================] - 0s 7us/step - loss: 0.1815 - val_loss: 0.1756
Epoch 12/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1779{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  46 | activation: sigmoid | extras: None 
layer 2 | size:  53 | activation: relu    | extras: dropout - rate: 39.2% 
layer 3 | size:  95 | activation: tanh    | extras: None 
layer 4 | size:  29 | activation: tanh    | extras: batchnorm 
layer 5 | size:  78 | activation: relu    | extras: dropout - rate: 15.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 3:10 - loss: 5.2489
6530/6530 [==============================] - 0s 7us/step - loss: 0.1750 - val_loss: 0.1738
Epoch 13/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1756
 640/6530 [=>............................] - ETA: 9s - loss: 0.9980  
6530/6530 [==============================] - 0s 7us/step - loss: 0.1703 - val_loss: 0.1692
Epoch 14/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1713
6530/6530 [==============================] - 0s 7us/step - loss: 0.1666 - val_loss: 0.1647
Epoch 15/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1667
1248/6530 [====>.........................] - ETA: 4s - loss: 0.6323
6530/6530 [==============================] - 0s 6us/step - loss: 0.1656 - val_loss: 0.1624
Epoch 16/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1642
1888/6530 [=======>......................] - ETA: 2s - loss: 0.4729
6530/6530 [==============================] - 0s 7us/step - loss: 0.1652 - val_loss: 0.1607
Epoch 17/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1619
2496/6530 [==========>...................] - ETA: 1s - loss: 0.3880
6530/6530 [==============================] - 0s 7us/step - loss: 0.1649 - val_loss: 0.1604
Epoch 18/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1610
3136/6530 [=============>................] - ETA: 1s - loss: 0.3336
6530/6530 [==============================] - 0s 7us/step - loss: 0.1640 - val_loss: 0.1613
Epoch 19/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1614
3744/6530 [================>.............] - ETA: 0s - loss: 0.2970
6530/6530 [==============================] - 0s 7us/step - loss: 0.1636 - val_loss: 0.1608
Epoch 20/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1605
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2684
6530/6530 [==============================] - 0s 8us/step - loss: 0.1635 - val_loss: 0.1632
Epoch 21/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1624
4960/6530 [=====================>........] - ETA: 0s - loss: 0.2460
6530/6530 [==============================] - 0s 7us/step - loss: 0.1649 - val_loss: 0.1625
Epoch 22/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1616
6530/6530 [==============================] - 0s 7us/step - loss: 0.1634 - val_loss: 0.1633

5600/6530 [========================>.....] - ETA: 0s - loss: 0.2280
6240/6530 [===========================>..] - ETA: 0s - loss: 0.2121
6530/6530 [==============================] - 2s 234us/step - loss: 0.2058 - val_loss: 0.0675
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0717
 576/6530 [=>............................] - ETA: 0s - loss: 0.0786
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0789{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  74 | activation: relu    | extras: batchnorm 
layer 2 | size:  20 | activation: relu    | extras: batchnorm 
layer 3 | size:  14 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  35 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 8:27 - loss: 0.5029
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0767
 208/6530 [..............................] - ETA: 39s - loss: 0.4654 
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0760
 400/6530 [>.............................] - ETA: 20s - loss: 0.3567
2912/6530 [============>.................] - ETA: 0s - loss: 0.0743
 592/6530 [=>............................] - ETA: 14s - loss: 0.2705
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0726
 784/6530 [==>...........................] - ETA: 10s - loss: 0.2227
4032/6530 [=================>............] - ETA: 0s - loss: 0.0713
 976/6530 [===>..........................] - ETA: 8s - loss: 0.1887 
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0704
1168/6530 [====>.........................] - ETA: 7s - loss: 0.1667
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0686
1344/6530 [=====>........................] - ETA: 6s - loss: 0.1507
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0679
1520/6530 [=====>........................] - ETA: 5s - loss: 0.1380
6496/6530 [============================>.] - ETA: 0s - loss: 0.0676
6530/6530 [==============================] - 1s 90us/step - loss: 0.0676 - val_loss: 0.0600
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0601
1680/6530 [======>.......................] - ETA: 4s - loss: 0.1285
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0618
1856/6530 [=======>......................] - ETA: 4s - loss: 0.1204
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0598
2048/6530 [========>.....................] - ETA: 3s - loss: 0.1127
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0609
2240/6530 [=========>....................] - ETA: 3s - loss: 0.1066
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0596
2432/6530 [==========>...................] - ETA: 3s - loss: 0.1011
3104/6530 [=============>................] - ETA: 0s - loss: 0.0582
2624/6530 [===========>..................] - ETA: 2s - loss: 0.0973
3712/6530 [================>.............] - ETA: 0s - loss: 0.0573
2800/6530 [===========>..................] - ETA: 2s - loss: 0.0932
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0562
2992/6530 [============>.................] - ETA: 2s - loss: 0.0891
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0560
3168/6530 [=============>................] - ETA: 2s - loss: 0.0857
# training | RMSE: 0.2030, MAE: 0.1627
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20722056181944334}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 53, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.20296877881688113, 'rmse': 0.20296877881688113, 'mae': 0.16269820066336607, 'early_stop': True}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  92 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  59 | activation: tanh    | extras: None 
layer 3 | size:   4 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  92 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c894b70>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 3:06 - loss: 0.5205
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0556
3344/6530 [==============>...............] - ETA: 2s - loss: 0.0829
 272/6530 [>.............................] - ETA: 11s - loss: 0.4797 
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0549
3520/6530 [===============>..............] - ETA: 1s - loss: 0.0803
 528/6530 [=>............................] - ETA: 6s - loss: 0.3775 
6530/6530 [==============================] - 1s 86us/step - loss: 0.0544 - val_loss: 0.0610
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1083
3696/6530 [===============>..............] - ETA: 1s - loss: 0.0778
 784/6530 [==>...........................] - ETA: 4s - loss: 0.3002
 640/6530 [=>............................] - ETA: 0s - loss: 0.0531
3872/6530 [================>.............] - ETA: 1s - loss: 0.0759
1040/6530 [===>..........................] - ETA: 3s - loss: 0.2419
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0496
4048/6530 [=================>............] - ETA: 1s - loss: 0.0739
1280/6530 [====>.........................] - ETA: 2s - loss: 0.2044
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0506
4224/6530 [==================>...........] - ETA: 1s - loss: 0.0720
1552/6530 [======>.......................] - ETA: 2s - loss: 0.1754
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0492
4384/6530 [===================>..........] - ETA: 1s - loss: 0.0705
1808/6530 [=======>......................] - ETA: 2s - loss: 0.1571
3104/6530 [=============>................] - ETA: 0s - loss: 0.0471
4560/6530 [===================>..........] - ETA: 1s - loss: 0.0690
2064/6530 [========>.....................] - ETA: 1s - loss: 0.1430
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0480
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0676
2320/6530 [=========>....................] - ETA: 1s - loss: 0.1322
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0472
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0658
2592/6530 [==========>...................] - ETA: 1s - loss: 0.1233
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0474
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0647
2864/6530 [============>.................] - ETA: 1s - loss: 0.1149
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0470
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0633
3136/6530 [=============>................] - ETA: 1s - loss: 0.1078
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0474
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0621
3392/6530 [==============>...............] - ETA: 1s - loss: 0.1031
6530/6530 [==============================] - 1s 88us/step - loss: 0.0472 - val_loss: 0.0367
Epoch 5/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1204
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0609
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0989
 608/6530 [=>............................] - ETA: 0s - loss: 0.0453
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0599
3904/6530 [================>.............] - ETA: 0s - loss: 0.0950
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0442
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0589
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0912
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0454
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0578
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0880
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0453
6352/6530 [============================>.] - ETA: 0s - loss: 0.0569
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0852
3136/6530 [=============>................] - ETA: 0s - loss: 0.0446
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0824
3744/6530 [================>.............] - ETA: 0s - loss: 0.0443
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0797
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0443
6530/6530 [==============================] - 3s 493us/step - loss: 0.0561 - val_loss: 0.0218
Epoch 2/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0230
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0774
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0433
 192/6530 [..............................] - ETA: 1s - loss: 0.0243
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0755
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0434
 384/6530 [>.............................] - ETA: 1s - loss: 0.0233
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0736
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0431
6320/6530 [============================>.] - ETA: 0s - loss: 0.0719
 576/6530 [=>............................] - ETA: 1s - loss: 0.0234
6530/6530 [==============================] - 1s 86us/step - loss: 0.0432 - val_loss: 0.0326
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0520
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0243
 640/6530 [=>............................] - ETA: 0s - loss: 0.0403
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0243
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0402
6530/6530 [==============================] - 2s 279us/step - loss: 0.0705 - val_loss: 0.0276
Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0364
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0249
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0392
 288/6530 [>.............................] - ETA: 1s - loss: 0.0310
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0243
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0393
 560/6530 [=>............................] - ETA: 1s - loss: 0.0313
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0239
3040/6530 [============>.................] - ETA: 0s - loss: 0.0398
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0326
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0404
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0239
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0322
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0399
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0238
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0313
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0399
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0238
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0308
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0396
2256/6530 [=========>....................] - ETA: 1s - loss: 0.0238
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0306
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0397
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0239
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0302
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0241
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0305
6530/6530 [==============================] - 1s 89us/step - loss: 0.0391 - val_loss: 0.0426
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0401
2816/6530 [===========>..................] - ETA: 1s - loss: 0.0238
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0305
 640/6530 [=>............................] - ETA: 0s - loss: 0.0375
3008/6530 [============>.................] - ETA: 1s - loss: 0.0232
2928/6530 [============>.................] - ETA: 0s - loss: 0.0299
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0375
3200/6530 [=============>................] - ETA: 0s - loss: 0.0230
3184/6530 [=============>................] - ETA: 0s - loss: 0.0294
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0376
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0297
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0228
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0367
3712/6530 [================>.............] - ETA: 0s - loss: 0.0294
3104/6530 [=============>................] - ETA: 0s - loss: 0.0367
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0228
3984/6530 [=================>............] - ETA: 0s - loss: 0.0293
3744/6530 [================>.............] - ETA: 0s - loss: 0.0364
3776/6530 [================>.............] - ETA: 0s - loss: 0.0225
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0292
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0370
3936/6530 [=================>............] - ETA: 0s - loss: 0.0227
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0294
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0371
4128/6530 [=================>............] - ETA: 0s - loss: 0.0227
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0293
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0373
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0227
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0291
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0370
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0228
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0288
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0228
6530/6530 [==============================] - 1s 89us/step - loss: 0.0377 - val_loss: 0.0815
Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0544
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0286
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0227
 640/6530 [=>............................] - ETA: 0s - loss: 0.0357
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0285
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0226
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0340
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0283
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0225
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0358
6336/6530 [============================>.] - ETA: 0s - loss: 0.0281
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0224
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0359
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0222
3136/6530 [=============>................] - ETA: 0s - loss: 0.0357
6530/6530 [==============================] - 1s 203us/step - loss: 0.0280 - val_loss: 0.0212
Epoch 3/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0252
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0223
3776/6530 [================>.............] - ETA: 0s - loss: 0.0349
 304/6530 [>.............................] - ETA: 1s - loss: 0.0242
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0221
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0352
 576/6530 [=>............................] - ETA: 1s - loss: 0.0245
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0219
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0350
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0259
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0218
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0355
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0268
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0354
6496/6530 [============================>.] - ETA: 0s - loss: 0.0218
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0260
6530/6530 [==============================] - 1s 87us/step - loss: 0.0353 - val_loss: 0.0411
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0856
6530/6530 [==============================] - 2s 294us/step - loss: 0.0218 - val_loss: 0.0223

1632/6530 [======>.......................] - ETA: 0s - loss: 0.0260Epoch 3/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0220
 640/6530 [=>............................] - ETA: 0s - loss: 0.0328
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0254
 176/6530 [..............................] - ETA: 1s - loss: 0.0191
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0344
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0256
 368/6530 [>.............................] - ETA: 1s - loss: 0.0189
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0335
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0255
 560/6530 [=>............................] - ETA: 1s - loss: 0.0186
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0338
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0253
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0192
2976/6530 [============>.................] - ETA: 0s - loss: 0.0341
2976/6530 [============>.................] - ETA: 0s - loss: 0.0249
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0197
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0335
3200/6530 [=============>................] - ETA: 0s - loss: 0.0246
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0202
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0333
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0250
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0197
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0329
3744/6530 [================>.............] - ETA: 0s - loss: 0.0247
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0193
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0333
4016/6530 [=================>............] - ETA: 0s - loss: 0.0249
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0191
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0336
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0249
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0190
6530/6530 [==============================] - 1s 89us/step - loss: 0.0338 - val_loss: 0.0506
Epoch 10/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0688
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0251
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0189
 640/6530 [=>............................] - ETA: 0s - loss: 0.0319
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0251
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0188
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0333
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0248
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0190
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0330
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0248
2608/6530 [==========>...................] - ETA: 1s - loss: 0.0190
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0337
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0245
2784/6530 [===========>..................] - ETA: 1s - loss: 0.0188
3072/6530 [=============>................] - ETA: 0s - loss: 0.0331
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0245
2976/6530 [============>.................] - ETA: 1s - loss: 0.0184
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0327
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0243
3168/6530 [=============>................] - ETA: 0s - loss: 0.0181
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0325
6400/6530 [============================>.] - ETA: 0s - loss: 0.0241
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0180
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0325
6530/6530 [==============================] - 1s 200us/step - loss: 0.0243 - val_loss: 0.0185
Epoch 4/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0250
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0180
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0324
 288/6530 [>.............................] - ETA: 1s - loss: 0.0223
3744/6530 [================>.............] - ETA: 0s - loss: 0.0179
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0323
 560/6530 [=>............................] - ETA: 1s - loss: 0.0224
3936/6530 [=================>............] - ETA: 0s - loss: 0.0180
6530/6530 [==============================] - 1s 88us/step - loss: 0.0323 - val_loss: 0.0518

 832/6530 [==>...........................] - ETA: 1s - loss: 0.0244
4128/6530 [=================>............] - ETA: 0s - loss: 0.0180
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0254
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0181
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0246
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0182
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0245
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0182
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0237
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0181
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0238
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0182
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0239
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0180
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0237
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0181
3024/6530 [============>.................] - ETA: 0s - loss: 0.0233
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0180
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0233
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0180
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0235
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0178
3888/6530 [================>.............] - ETA: 0s - loss: 0.0235
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0177
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0235
6400/6530 [============================>.] - ETA: 0s - loss: 0.0176
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0235
6530/6530 [==============================] - 2s 286us/step - loss: 0.0177 - val_loss: 0.0210
Epoch 4/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0220
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0237
 192/6530 [..............................] - ETA: 1s - loss: 0.0170
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0235
 384/6530 [>.............................] - ETA: 1s - loss: 0.0165
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0233
 560/6530 [=>............................] - ETA: 1s - loss: 0.0164
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0232
# training | RMSE: 0.2306, MAE: 0.1918
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3917465634993933}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 29, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15337783986534193}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.23057683067695642, 'rmse': 0.23057683067695642, 'mae': 0.1917885754819075, 'early_stop': True}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  99 | activation: tanh    | extras: batchnorm 
layer 2 | size:  32 | activation: sigmoid | extras: dropout - rate: 14.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c894f98>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 2:22 - loss: 0.2080
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0172
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0233
 336/6530 [>.............................] - ETA: 7s - loss: 0.0995  
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0171
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0231
 672/6530 [==>...........................] - ETA: 3s - loss: 0.0815
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0181
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0231
 992/6530 [===>..........................] - ETA: 2s - loss: 0.0727
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0176
6496/6530 [============================>.] - ETA: 0s - loss: 0.0230
1344/6530 [=====>........................] - ETA: 2s - loss: 0.0665
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0171
6530/6530 [==============================] - 1s 198us/step - loss: 0.0231 - val_loss: 0.0173
Epoch 5/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0232
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0624
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0168
 272/6530 [>.............................] - ETA: 1s - loss: 0.0223
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0593
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0167
 528/6530 [=>............................] - ETA: 1s - loss: 0.0221
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0573
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0165
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0237
2704/6530 [===========>..................] - ETA: 1s - loss: 0.0563
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0164
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0244
3056/6530 [=============>................] - ETA: 0s - loss: 0.0539
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0163
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0242
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0528
2544/6530 [==========>...................] - ETA: 1s - loss: 0.0164
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0234
3760/6530 [================>.............] - ETA: 0s - loss: 0.0517
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0235
2720/6530 [===========>..................] - ETA: 1s - loss: 0.0163
4096/6530 [=================>............] - ETA: 0s - loss: 0.0508
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0229
2912/6530 [============>.................] - ETA: 1s - loss: 0.0160
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0497
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0231
3104/6530 [=============>................] - ETA: 0s - loss: 0.0157
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0488
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0232
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0155
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0478
2896/6530 [============>.................] - ETA: 0s - loss: 0.0227
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0157
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0469
3152/6530 [=============>................] - ETA: 0s - loss: 0.0225
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0157
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0463
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0226
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0456
3840/6530 [================>.............] - ETA: 0s - loss: 0.0156
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0227
4032/6530 [=================>............] - ETA: 0s - loss: 0.0158
3936/6530 [=================>............] - ETA: 0s - loss: 0.0228
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0157
6530/6530 [==============================] - 1s 212us/step - loss: 0.0449 - val_loss: 0.0260
Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0266
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0228
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0157
 352/6530 [>.............................] - ETA: 0s - loss: 0.0296
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0229
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0159
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0325
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0230
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0158
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0317
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0229
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0157
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0310
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0226
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0157
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0299
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0225
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0156
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0295
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0226
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0156
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0293
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0224
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0157
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0290
6336/6530 [============================>.] - ETA: 0s - loss: 0.0224
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0156
3152/6530 [=============>................] - ETA: 0s - loss: 0.0285
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0155
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0284
6530/6530 [==============================] - 1s 202us/step - loss: 0.0224 - val_loss: 0.0178
Epoch 6/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0220
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0154
3888/6530 [================>.............] - ETA: 0s - loss: 0.0281
 288/6530 [>.............................] - ETA: 1s - loss: 0.0208
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0279
6416/6530 [============================>.] - ETA: 0s - loss: 0.0154
 544/6530 [=>............................] - ETA: 1s - loss: 0.0211
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0280
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0231
6530/6530 [==============================] - 2s 293us/step - loss: 0.0155 - val_loss: 0.0203
Epoch 5/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0170
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0279
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0241
 192/6530 [..............................] - ETA: 1s - loss: 0.0153
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0276
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0233
 368/6530 [>.............................] - ETA: 1s - loss: 0.0148
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0274
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0232
 544/6530 [=>............................] - ETA: 1s - loss: 0.0148
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0274
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0225
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0155
6336/6530 [============================>.] - ETA: 0s - loss: 0.0273
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0223
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0153
6530/6530 [==============================] - 1s 151us/step - loss: 0.0272 - val_loss: 0.0192
Epoch 3/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0180
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0225
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0157
 368/6530 [>.............................] - ETA: 0s - loss: 0.0231
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0226
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0159
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0250
2896/6530 [============>.................] - ETA: 0s - loss: 0.0222
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0156
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0248
3152/6530 [=============>................] - ETA: 0s - loss: 0.0219
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0153
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0240
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0220
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0151
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0237
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0221
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0149
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0236
3936/6530 [=================>............] - ETA: 0s - loss: 0.0223
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0148
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0235
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0223
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0148
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0233
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0224
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0148
3152/6530 [=============>................] - ETA: 0s - loss: 0.0229
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0225
2720/6530 [===========>..................] - ETA: 1s - loss: 0.0147
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0230
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0223
2912/6530 [============>.................] - ETA: 1s - loss: 0.0144
3840/6530 [================>.............] - ETA: 0s - loss: 0.0227
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0221
3072/6530 [=============>................] - ETA: 1s - loss: 0.0141
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0227
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0219
3248/6530 [=============>................] - ETA: 0s - loss: 0.0140
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0228
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0220
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0141
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0229
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0219
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0141
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0226
6336/6530 [============================>.] - ETA: 0s - loss: 0.0219
3808/6530 [================>.............] - ETA: 0s - loss: 0.0140
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0226
6530/6530 [==============================] - 1s 202us/step - loss: 0.0219 - val_loss: 0.0193

4000/6530 [=================>............] - ETA: 0s - loss: 0.0142Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0209
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0227
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0142
 288/6530 [>.............................] - ETA: 1s - loss: 0.0204
6352/6530 [============================>.] - ETA: 0s - loss: 0.0226
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0142
 560/6530 [=>............................] - ETA: 1s - loss: 0.0205
6530/6530 [==============================] - 1s 151us/step - loss: 0.0226 - val_loss: 0.0170
Epoch 4/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0156
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0144
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0224
 368/6530 [>.............................] - ETA: 0s - loss: 0.0203
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0235
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0144
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0221
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0142
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0228
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0220
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0143
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0227
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0213
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0142
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0220
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0208
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0142
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0220
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0210
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0142
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0220
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0208
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0142
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0219
2864/6530 [============>.................] - ETA: 0s - loss: 0.0205
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0141
2928/6530 [============>.................] - ETA: 0s - loss: 0.0216
3168/6530 [=============>................] - ETA: 0s - loss: 0.0203
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0140
3200/6530 [=============>................] - ETA: 0s - loss: 0.0214
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0204
6368/6530 [============================>.] - ETA: 0s - loss: 0.0140
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0217
3888/6530 [================>.............] - ETA: 0s - loss: 0.0203
3712/6530 [================>.............] - ETA: 0s - loss: 0.0216
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0202
6530/6530 [==============================] - 2s 294us/step - loss: 0.0141 - val_loss: 0.0187
Epoch 6/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0169
3968/6530 [=================>............] - ETA: 0s - loss: 0.0218
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0203
 176/6530 [..............................] - ETA: 2s - loss: 0.0130
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0219
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0203
 352/6530 [>.............................] - ETA: 1s - loss: 0.0127
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0221
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0202
 528/6530 [=>............................] - ETA: 1s - loss: 0.0133
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0220
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0201
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0134
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0218
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0202
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0134
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0217
6352/6530 [============================>.] - ETA: 0s - loss: 0.0202
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0143
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0215
6530/6530 [==============================] - 1s 152us/step - loss: 0.0202 - val_loss: 0.0156
Epoch 5/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0145
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0143
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0216
 352/6530 [>.............................] - ETA: 0s - loss: 0.0182
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0141
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0215
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0198
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0138
6368/6530 [============================>.] - ETA: 0s - loss: 0.0214
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0197
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0136
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0194
6530/6530 [==============================] - 1s 203us/step - loss: 0.0215 - val_loss: 0.0242
Epoch 8/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0202
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0134
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0191
 256/6530 [>.............................] - ETA: 1s - loss: 0.0209
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0134
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0190
 496/6530 [=>............................] - ETA: 1s - loss: 0.0213
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0134
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0190
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0222
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0135
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0188
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0225
2720/6530 [===========>..................] - ETA: 1s - loss: 0.0134
3104/6530 [=============>................] - ETA: 0s - loss: 0.0185
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0228
2896/6530 [============>.................] - ETA: 1s - loss: 0.0131
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0187
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0219
3072/6530 [=============>................] - ETA: 1s - loss: 0.0129
3824/6530 [================>.............] - ETA: 0s - loss: 0.0185
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0220
3248/6530 [=============>................] - ETA: 0s - loss: 0.0128
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0184
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0215
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0129
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0185
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0216
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0129
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0186
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0217
3792/6530 [================>.............] - ETA: 0s - loss: 0.0128
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0184
2864/6530 [============>.................] - ETA: 0s - loss: 0.0214
3968/6530 [=================>............] - ETA: 0s - loss: 0.0130
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0183
3120/6530 [=============>................] - ETA: 0s - loss: 0.0211
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0131
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0185
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0212
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0131
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0185
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0214
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0132
3920/6530 [=================>............] - ETA: 0s - loss: 0.0216
6530/6530 [==============================] - 1s 153us/step - loss: 0.0185 - val_loss: 0.0145
Epoch 6/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0134
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0131
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0216
 384/6530 [>.............................] - ETA: 0s - loss: 0.0176
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0130
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0216
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0186
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0130
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0217
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0187
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0130
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0216
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0179
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0131
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0213
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0177
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0130
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0213
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0177
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0130
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0213
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0176
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0129
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0212
2896/6530 [============>.................] - ETA: 0s - loss: 0.0174
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0212
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0129
3264/6530 [=============>................] - ETA: 0s - loss: 0.0172
6384/6530 [============================>.] - ETA: 0s - loss: 0.0128
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0173
6530/6530 [==============================] - 1s 203us/step - loss: 0.0212 - val_loss: 0.0281
Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0197
4000/6530 [=================>............] - ETA: 0s - loss: 0.0172
6530/6530 [==============================] - 2s 296us/step - loss: 0.0129 - val_loss: 0.0149

 288/6530 [>.............................] - ETA: 1s - loss: 0.0202Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0148
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0171
 560/6530 [=>............................] - ETA: 1s - loss: 0.0201
 208/6530 [..............................] - ETA: 1s - loss: 0.0123
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0173
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0218
 384/6530 [>.............................] - ETA: 1s - loss: 0.0116
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0172
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0228
 576/6530 [=>............................] - ETA: 1s - loss: 0.0122
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0171
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0221
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0123
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0171
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0221
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0124
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0172
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0213
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0130
6464/6530 [============================>.] - ETA: 0s - loss: 0.0171
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0212
6530/6530 [==============================] - 1s 148us/step - loss: 0.0172 - val_loss: 0.0136

1328/6530 [=====>........................] - ETA: 1s - loss: 0.0131Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0121
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0213
 352/6530 [>.............................] - ETA: 0s - loss: 0.0162
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0128
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0213
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0177
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0126
2928/6530 [============>.................] - ETA: 0s - loss: 0.0210
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0176
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0124
3200/6530 [=============>................] - ETA: 0s - loss: 0.0208
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0169
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0124
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0211
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0167
2256/6530 [=========>....................] - ETA: 1s - loss: 0.0123
3728/6530 [================>.............] - ETA: 0s - loss: 0.0210
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0165
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0124
4000/6530 [=================>............] - ETA: 0s - loss: 0.0213
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0166
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0124
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0213
2832/6530 [============>.................] - ETA: 0s - loss: 0.0163
2832/6530 [============>.................] - ETA: 1s - loss: 0.0123
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0215
3184/6530 [=============>................] - ETA: 0s - loss: 0.0161
3008/6530 [============>.................] - ETA: 1s - loss: 0.0121
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0214
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0162
3184/6530 [=============>................] - ETA: 0s - loss: 0.0120
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0213
3872/6530 [================>.............] - ETA: 0s - loss: 0.0161
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0120
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0211
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0161
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0120
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0209
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0162
3728/6530 [================>.............] - ETA: 0s - loss: 0.0120
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0211
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0162
3920/6530 [=================>............] - ETA: 0s - loss: 0.0121
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0209
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0160
4112/6530 [=================>............] - ETA: 0s - loss: 0.0122
6384/6530 [============================>.] - ETA: 0s - loss: 0.0209
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0160
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0122
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0161
6530/6530 [==============================] - 1s 202us/step - loss: 0.0210 - val_loss: 0.0302

4480/6530 [===================>..........] - ETA: 0s - loss: 0.0123
6352/6530 [============================>.] - ETA: 0s - loss: 0.0161
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0122
6530/6530 [==============================] - 1s 150us/step - loss: 0.0162 - val_loss: 0.0128
Epoch 8/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0110
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0122
 352/6530 [>.............................] - ETA: 0s - loss: 0.0154
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0122
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0166
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0121
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0165
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0122
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0161
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0121
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0158
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0122
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0158
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0121
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0157
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0120
2848/6530 [============>.................] - ETA: 0s - loss: 0.0155
6320/6530 [============================>.] - ETA: 0s - loss: 0.0121
3216/6530 [=============>................] - ETA: 0s - loss: 0.0153
# training | RMSE: 0.1687, MAE: 0.1421
worker 1  xfile  [3, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 92, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 59, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.16868788520154937, 'rmse': 0.16868788520154937, 'mae': 0.14206331461948535, 'early_stop': True}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  99 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f46042aa160>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 27s - loss: 2.1249
6512/6530 [============================>.] - ETA: 0s - loss: 0.0121
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0154
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1272 
6530/6530 [==============================] - 2s 291us/step - loss: 0.0121 - val_loss: 0.0129
Epoch 8/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0089
3936/6530 [=================>............] - ETA: 0s - loss: 0.0153
2912/6530 [============>.................] - ETA: 0s - loss: 0.0922
 192/6530 [..............................] - ETA: 1s - loss: 0.0113
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0153
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0781
 368/6530 [>.............................] - ETA: 1s - loss: 0.0108
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0153
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0702
 544/6530 [=>............................] - ETA: 1s - loss: 0.0112
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0153
6530/6530 [==============================] - 0s 60us/step - loss: 0.0669 - val_loss: 0.0484
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0445
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0116
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0152
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0455
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0115
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0152
2912/6530 [============>.................] - ETA: 0s - loss: 0.0445
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0122
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0153
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0442
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0123
6352/6530 [============================>.] - ETA: 0s - loss: 0.0153
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0430
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0121
6530/6530 [==============================] - 1s 151us/step - loss: 0.0153 - val_loss: 0.0123
Epoch 9/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0104
6530/6530 [==============================] - 0s 40us/step - loss: 0.0426 - val_loss: 0.0489
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0497
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0120
 304/6530 [>.............................] - ETA: 1s - loss: 0.0140
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0401
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0118
 640/6530 [=>............................] - ETA: 0s - loss: 0.0161
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0414
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0116
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0157
3936/6530 [=================>............] - ETA: 0s - loss: 0.0418
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0116
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0156
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0404
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0116
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0153
6530/6530 [==============================] - 0s 40us/step - loss: 0.0405 - val_loss: 0.0412
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0453
2512/6530 [==========>...................] - ETA: 1s - loss: 0.0118
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0148
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0402
2688/6530 [===========>..................] - ETA: 1s - loss: 0.0117
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0148
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0399
2864/6530 [============>.................] - ETA: 1s - loss: 0.0116
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0149
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0396
3040/6530 [============>.................] - ETA: 1s - loss: 0.0114
3040/6530 [============>.................] - ETA: 0s - loss: 0.0146
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0391
3216/6530 [=============>................] - ETA: 0s - loss: 0.0113
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0146
6530/6530 [==============================] - 0s 38us/step - loss: 0.0391 - val_loss: 0.0500
Epoch 5/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0657
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0113
3744/6530 [================>.............] - ETA: 0s - loss: 0.0145
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0393
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0114
4096/6530 [=================>............] - ETA: 0s - loss: 0.0146
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0380
3760/6530 [================>.............] - ETA: 0s - loss: 0.0114
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0145
4032/6530 [=================>............] - ETA: 0s - loss: 0.0382
3936/6530 [=================>............] - ETA: 0s - loss: 0.0114
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0146
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0382
4112/6530 [=================>............] - ETA: 0s - loss: 0.0115
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0145
6530/6530 [==============================] - 0s 39us/step - loss: 0.0382 - val_loss: 0.0580
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0521
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0116
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0145
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0368
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0117
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0145
2880/6530 [============>.................] - ETA: 0s - loss: 0.0375
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0116
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0146
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0371
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0115
6480/6530 [============================>.] - ETA: 0s - loss: 0.0145
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0373
6530/6530 [==============================] - 1s 155us/step - loss: 0.0146 - val_loss: 0.0117
Epoch 10/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0093
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0116
6530/6530 [==============================] - 0s 38us/step - loss: 0.0374 - val_loss: 0.0407
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0309
 368/6530 [>.............................] - ETA: 0s - loss: 0.0149
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0115
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0369
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0158
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0116
2912/6530 [============>.................] - ETA: 0s - loss: 0.0355
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0154
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0115
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0363
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0149
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0116
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0363
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0146
6530/6530 [==============================] - 0s 37us/step - loss: 0.0366 - val_loss: 0.0380
Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0415
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0115
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0144
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0382
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0114
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0144
2880/6530 [============>.................] - ETA: 0s - loss: 0.0370
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0115
2832/6530 [============>.................] - ETA: 0s - loss: 0.0142
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0362
6480/6530 [============================>.] - ETA: 0s - loss: 0.0114
3184/6530 [=============>................] - ETA: 0s - loss: 0.0140
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0362
6530/6530 [==============================] - 2s 297us/step - loss: 0.0115 - val_loss: 0.0104
Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0048
6530/6530 [==============================] - 0s 37us/step - loss: 0.0360 - val_loss: 0.0409
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0317
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0141
 192/6530 [..............................] - ETA: 1s - loss: 0.0103
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0377
3872/6530 [================>.............] - ETA: 0s - loss: 0.0140
 384/6530 [>.............................] - ETA: 1s - loss: 0.0098
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0372
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0140
 576/6530 [=>............................] - ETA: 1s - loss: 0.0105
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0361
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0141
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0106
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0355
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0140
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0110
6336/6530 [============================>.] - ETA: 0s - loss: 0.0353
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0139
6530/6530 [==============================] - 0s 42us/step - loss: 0.0352 - val_loss: 0.0406
Epoch 10/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0544
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0114
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0139
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0362
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0117
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0140
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0361
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0114
6352/6530 [============================>.] - ETA: 0s - loss: 0.0140
4032/6530 [=================>............] - ETA: 0s - loss: 0.0359
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0113
6530/6530 [==============================] - 1s 151us/step - loss: 0.0141 - val_loss: 0.0120

5472/6530 [========================>.....] - ETA: 0s - loss: 0.0355Epoch 11/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0102
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0112
6530/6530 [==============================] - 0s 39us/step - loss: 0.0348 - val_loss: 0.0368

 368/6530 [>.............................] - ETA: 0s - loss: 0.0153Epoch 11/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0323
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0111
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0335
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0162
2224/6530 [=========>....................] - ETA: 1s - loss: 0.0110
2848/6530 [============>.................] - ETA: 0s - loss: 0.0340
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0156
2400/6530 [==========>...................] - ETA: 1s - loss: 0.0111
4064/6530 [=================>............] - ETA: 0s - loss: 0.0341
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0150
2576/6530 [==========>...................] - ETA: 1s - loss: 0.0113
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0339
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0145
2752/6530 [===========>..................] - ETA: 1s - loss: 0.0111
6530/6530 [==============================] - 0s 39us/step - loss: 0.0341 - val_loss: 0.0375
Epoch 12/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0346
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0143
2928/6530 [============>.................] - ETA: 1s - loss: 0.0109
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0326
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0142
3120/6530 [=============>................] - ETA: 0s - loss: 0.0108
2880/6530 [============>.................] - ETA: 0s - loss: 0.0339
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0140
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0339
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0108
3152/6530 [=============>................] - ETA: 0s - loss: 0.0137
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0335
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0108
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0137
6530/6530 [==============================] - 0s 37us/step - loss: 0.0337 - val_loss: 0.0378
Epoch 13/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0432
3856/6530 [================>.............] - ETA: 0s - loss: 0.0136
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0109
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0327
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0136
3872/6530 [================>.............] - ETA: 0s - loss: 0.0109
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0332
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0137
4048/6530 [=================>............] - ETA: 0s - loss: 0.0110
4128/6530 [=================>............] - ETA: 0s - loss: 0.0325
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0136
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0110
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0329
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0135
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0110
6530/6530 [==============================] - 0s 39us/step - loss: 0.0332 - val_loss: 0.0353
Epoch 14/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0377
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0135
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0110
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0329
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0136
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0110
2880/6530 [============>.................] - ETA: 0s - loss: 0.0331
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0136
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0109
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0330
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0110
6530/6530 [==============================] - 1s 151us/step - loss: 0.0136 - val_loss: 0.0109
Epoch 12/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0087
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0329
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0110
 368/6530 [>.............................] - ETA: 0s - loss: 0.0140
6530/6530 [==============================] - 0s 38us/step - loss: 0.0328 - val_loss: 0.0343
Epoch 15/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0332
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0110
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0147
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0306
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0110
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0144
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0315
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0110
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0139
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0321
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0110
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0135
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0327
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0110
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0134
6530/6530 [==============================] - 0s 39us/step - loss: 0.0324 - val_loss: 0.0342
Epoch 16/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0406
6400/6530 [============================>.] - ETA: 0s - loss: 0.0109
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0134
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0322
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0132
2880/6530 [============>.................] - ETA: 0s - loss: 0.0320
6530/6530 [==============================] - 2s 295us/step - loss: 0.0110 - val_loss: 0.0104
Epoch 10/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0042
3120/6530 [=============>................] - ETA: 0s - loss: 0.0130
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0323
 192/6530 [..............................] - ETA: 1s - loss: 0.0096
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0130
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0325
 368/6530 [>.............................] - ETA: 1s - loss: 0.0093
6530/6530 [==============================] - 0s 37us/step - loss: 0.0321 - val_loss: 0.0359
Epoch 17/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0402
3808/6530 [================>.............] - ETA: 0s - loss: 0.0130
 544/6530 [=>............................] - ETA: 1s - loss: 0.0100
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0306
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0130
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0103
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0314
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0131
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0104
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0318
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0130
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0110
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0315
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0129
6530/6530 [==============================] - 0s 38us/step - loss: 0.0318 - val_loss: 0.0355
Epoch 18/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0339
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0112
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0129
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0324
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0110
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0130
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0324
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0108
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0130
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0322
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0108
6530/6530 [==============================] - 1s 153us/step - loss: 0.0131 - val_loss: 0.0106
Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0083
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0320
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0106
 368/6530 [>.............................] - ETA: 0s - loss: 0.0130
6530/6530 [==============================] - 0s 39us/step - loss: 0.0314 - val_loss: 0.0338
Epoch 19/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0288
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0107
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0141
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0331
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0106
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0138
2848/6530 [============>.................] - ETA: 0s - loss: 0.0332
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0107
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0135
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0320
2672/6530 [===========>..................] - ETA: 1s - loss: 0.0107
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0131
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0314
2848/6530 [============>.................] - ETA: 1s - loss: 0.0105
6530/6530 [==============================] - 0s 38us/step - loss: 0.0311 - val_loss: 0.0336
Epoch 20/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0229
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0129
3024/6530 [============>.................] - ETA: 1s - loss: 0.0104
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0297
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0129
3200/6530 [=============>................] - ETA: 0s - loss: 0.0103
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0297
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0128
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0103
4096/6530 [=================>............] - ETA: 0s - loss: 0.0306
3136/6530 [=============>................] - ETA: 0s - loss: 0.0125
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0104
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0308
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0126
3728/6530 [================>.............] - ETA: 0s - loss: 0.0104
3824/6530 [================>.............] - ETA: 0s - loss: 0.0126
6530/6530 [==============================] - 0s 40us/step - loss: 0.0308 - val_loss: 0.0340
Epoch 21/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0263
3888/6530 [================>.............] - ETA: 0s - loss: 0.0105
4128/6530 [=================>............] - ETA: 0s - loss: 0.0126
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0290
4064/6530 [=================>............] - ETA: 0s - loss: 0.0106
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0126
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0304
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0106
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0126
4064/6530 [=================>............] - ETA: 0s - loss: 0.0307
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0106
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0125
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0303
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0106
6530/6530 [==============================] - 0s 39us/step - loss: 0.0305 - val_loss: 0.0323
Epoch 22/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0253
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0125
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0106
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0308
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0126
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0105
2848/6530 [============>.................] - ETA: 0s - loss: 0.0312
6320/6530 [============================>.] - ETA: 0s - loss: 0.0126
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0106
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0309
6530/6530 [==============================] - 1s 151us/step - loss: 0.0127 - val_loss: 0.0104
Epoch 14/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0079
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0106
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0304
 368/6530 [>.............................] - ETA: 0s - loss: 0.0127
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0105
6530/6530 [==============================] - 0s 38us/step - loss: 0.0302 - val_loss: 0.0324
Epoch 23/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0308
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0140
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0106
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0313
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0131
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0106
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0290
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0132
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0105
3872/6530 [================>.............] - ETA: 0s - loss: 0.0298
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0129
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0106
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0298
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0125
6416/6530 [============================>.] - ETA: 0s - loss: 0.0105
6530/6530 [==============================] - 0s 40us/step - loss: 0.0300 - val_loss: 0.0329
Epoch 24/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0342
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0125
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0297
6530/6530 [==============================] - 2s 299us/step - loss: 0.0106 - val_loss: 0.0105

2688/6530 [===========>..................] - ETA: 0s - loss: 0.0126Epoch 11/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0055
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0297
3040/6530 [============>.................] - ETA: 0s - loss: 0.0123
 192/6530 [..............................] - ETA: 1s - loss: 0.0099
4128/6530 [=================>............] - ETA: 0s - loss: 0.0297
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0122
 368/6530 [>.............................] - ETA: 1s - loss: 0.0095
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0303
3744/6530 [================>.............] - ETA: 0s - loss: 0.0122
 544/6530 [=>............................] - ETA: 1s - loss: 0.0101
6530/6530 [==============================] - 0s 39us/step - loss: 0.0298 - val_loss: 0.0327
Epoch 25/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0222
4112/6530 [=================>............] - ETA: 0s - loss: 0.0123
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0102
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0295
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0123
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0103
2944/6530 [============>.................] - ETA: 0s - loss: 0.0297
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0123
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0110
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0298
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0122
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0111
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0297
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0122
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0109
6530/6530 [==============================] - 0s 37us/step - loss: 0.0296 - val_loss: 0.0322
Epoch 26/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0288
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0122
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0107
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0285
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0122
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0107
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0291
6464/6530 [============================>.] - ETA: 0s - loss: 0.0123
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0104
4096/6530 [=================>............] - ETA: 0s - loss: 0.0291
6530/6530 [==============================] - 1s 156us/step - loss: 0.0123 - val_loss: 0.0102
Epoch 15/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0076
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0104
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0294
 352/6530 [>.............................] - ETA: 0s - loss: 0.0122
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0104
6530/6530 [==============================] - 0s 39us/step - loss: 0.0294 - val_loss: 0.0332
Epoch 27/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0194
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0133
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0104
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0285
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0128
2672/6530 [===========>..................] - ETA: 1s - loss: 0.0104
2848/6530 [============>.................] - ETA: 0s - loss: 0.0284
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0129
2848/6530 [============>.................] - ETA: 1s - loss: 0.0102
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0285
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0126
3024/6530 [============>.................] - ETA: 1s - loss: 0.0101
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0288
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0123
3200/6530 [=============>................] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 0s 39us/step - loss: 0.0292 - val_loss: 0.0329

2416/6530 [==========>...................] - ETA: 0s - loss: 0.0122
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0100
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0121
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0101
3136/6530 [=============>................] - ETA: 0s - loss: 0.0118
3744/6530 [================>.............] - ETA: 0s - loss: 0.0101
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0119
3936/6530 [=================>............] - ETA: 0s - loss: 0.0102
3856/6530 [================>.............] - ETA: 0s - loss: 0.0119
4112/6530 [=================>............] - ETA: 0s - loss: 0.0102
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0119
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0103
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0120
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0103
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0119
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0103
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0119
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0102
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0119
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0102
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0120
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0102
6432/6530 [============================>.] - ETA: 0s - loss: 0.0119
6530/6530 [==============================] - 1s 148us/step - loss: 0.0120 - val_loss: 0.0101

5440/6530 [=======================>......] - ETA: 0s - loss: 0.0103Epoch 16/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0074
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0103
 368/6530 [>.............................] - ETA: 0s - loss: 0.0120
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0103
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0130
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0128
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0102
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0125
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0102
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0122
6400/6530 [============================>.] - ETA: 0s - loss: 0.0102
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0120
6530/6530 [==============================] - 2s 293us/step - loss: 0.0103 - val_loss: 0.0119
Epoch 12/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0073
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0120
 192/6530 [..............................] - ETA: 1s - loss: 0.0096
2896/6530 [============>.................] - ETA: 0s - loss: 0.0117
 368/6530 [>.............................] - ETA: 1s - loss: 0.0091
3248/6530 [=============>................] - ETA: 0s - loss: 0.0115
 560/6530 [=>............................] - ETA: 1s - loss: 0.0096
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0116
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0097
3936/6530 [=================>............] - ETA: 0s - loss: 0.0117
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0101
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0117
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0105
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0117
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0107
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0116
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0104
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0116
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0103
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0116
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0101
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0117
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0101
6528/6530 [============================>.] - ETA: 0s - loss: 0.0117
6530/6530 [==============================] - 1s 146us/step - loss: 0.0117 - val_loss: 0.0099

# training | RMSE: 0.1756, MAE: 0.1368
worker 1  xfile  [5, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.43079533423603034}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.34262296990548324}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.17558661923879246, 'rmse': 0.17558661923879246, 'mae': 0.13678029067552216, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  54 | activation: tanh    | extras: dropout - rate: 38.8% 
layer 2 | size:  24 | activation: tanh    | extras: None 
layer 3 | size:  44 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  43 | activation: tanh    | extras: batchnorm 
layer 5 | size:   3 | activation: tanh    | extras: dropout - rate: 28.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f45ec745128>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  64/6530 [..............................] - ETA: 1:04 - loss: 0.7251Epoch 17/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0070
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0100
 832/6530 [==>...........................] - ETA: 4s - loss: 0.6990  
 368/6530 [>.............................] - ETA: 0s - loss: 0.0118
2416/6530 [==========>...................] - ETA: 1s - loss: 0.0100
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0129
1536/6530 [======>.......................] - ETA: 2s - loss: 0.6670
2576/6530 [==========>...................] - ETA: 1s - loss: 0.0101
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0124
2304/6530 [=========>....................] - ETA: 1s - loss: 0.6334
2752/6530 [===========>..................] - ETA: 1s - loss: 0.0100
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0123
3136/6530 [=============>................] - ETA: 0s - loss: 0.5765
2928/6530 [============>.................] - ETA: 1s - loss: 0.0098
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0119
3904/6530 [================>.............] - ETA: 0s - loss: 0.5222
3104/6530 [=============>................] - ETA: 0s - loss: 0.0097
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0117
4672/6530 [====================>.........] - ETA: 0s - loss: 0.4769
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0097
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0117
5440/6530 [=======================>......] - ETA: 0s - loss: 0.4412
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0097
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0115
6208/6530 [===========================>..] - ETA: 0s - loss: 0.4118
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0098
3136/6530 [=============>................] - ETA: 0s - loss: 0.0113
3824/6530 [================>.............] - ETA: 0s - loss: 0.0098
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0114
6530/6530 [==============================] - 1s 176us/step - loss: 0.4009 - val_loss: 0.1878
Epoch 2/27

  64/6530 [..............................] - ETA: 0s - loss: 0.2425
4000/6530 [=================>............] - ETA: 0s - loss: 0.0099
3888/6530 [================>.............] - ETA: 0s - loss: 0.0114
 832/6530 [==>...........................] - ETA: 0s - loss: 0.2056
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0099
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0114
1600/6530 [======>.......................] - ETA: 0s - loss: 0.2017
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0099
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0114
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1988
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0100
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0114
3264/6530 [=============>................] - ETA: 0s - loss: 0.1963
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0100
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0113
4032/6530 [=================>............] - ETA: 0s - loss: 0.1940
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0099
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0113
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1905
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0099
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0114
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1874
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0099
6368/6530 [============================>.] - ETA: 0s - loss: 0.0114
6336/6530 [============================>.] - ETA: 0s - loss: 0.1854
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0099
6530/6530 [==============================] - 0s 70us/step - loss: 0.1849 - val_loss: 0.1515

6530/6530 [==============================] - 1s 150us/step - loss: 0.0115 - val_loss: 0.0098
Epoch 3/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1776Epoch 18/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0068
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0100
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1798
 336/6530 [>.............................] - ETA: 1s - loss: 0.0110
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0100
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1780
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0124
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0099
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1749
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0122
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0099
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0120
3200/6530 [=============>................] - ETA: 0s - loss: 0.1721
6384/6530 [============================>.] - ETA: 0s - loss: 0.0099
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0116
3968/6530 [=================>............] - ETA: 0s - loss: 0.1718
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0115
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1708
6530/6530 [==============================] - 2s 295us/step - loss: 0.0100 - val_loss: 0.0116
Epoch 13/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0053
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0115
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1702
 192/6530 [..............................] - ETA: 1s - loss: 0.0089
2864/6530 [============>.................] - ETA: 0s - loss: 0.0112
6400/6530 [============================>.] - ETA: 0s - loss: 0.1695
 368/6530 [>.............................] - ETA: 1s - loss: 0.0088
6530/6530 [==============================] - 0s 69us/step - loss: 0.1695 - val_loss: 0.1466
Epoch 4/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1941
3216/6530 [=============>................] - ETA: 0s - loss: 0.0110
 560/6530 [=>............................] - ETA: 1s - loss: 0.0093
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1673
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0111
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0094
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1633
3936/6530 [=================>............] - ETA: 0s - loss: 0.0112
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0096
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1640
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0112
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0102
3200/6530 [=============>................] - ETA: 0s - loss: 0.1619
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0112
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0104
4032/6530 [=================>............] - ETA: 0s - loss: 0.1620
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0111
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0101
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1616
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0111
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0100
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1616
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0111
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0100
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0112
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1615
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0097
6530/6530 [==============================] - 0s 71us/step - loss: 0.1615 - val_loss: 0.1422
Epoch 5/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1711
6336/6530 [============================>.] - ETA: 0s - loss: 0.0111
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0097
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1595
6530/6530 [==============================] - 1s 151us/step - loss: 0.0112 - val_loss: 0.0097
Epoch 19/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0065
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0097
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1580
 368/6530 [>.............................] - ETA: 0s - loss: 0.0112
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1589
2560/6530 [==========>...................] - ETA: 1s - loss: 0.0098
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0123
3136/6530 [=============>................] - ETA: 0s - loss: 0.1557
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0097
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0120
3904/6530 [================>.............] - ETA: 0s - loss: 0.1563
2912/6530 [============>.................] - ETA: 1s - loss: 0.0095
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0116
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1555
3088/6530 [=============>................] - ETA: 1s - loss: 0.0094
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0114
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1562
3264/6530 [=============>................] - ETA: 0s - loss: 0.0094
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0113
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1563
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0095
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0112
6530/6530 [==============================] - 0s 69us/step - loss: 0.1563 - val_loss: 0.1385
Epoch 6/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1780
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0095
2864/6530 [============>.................] - ETA: 0s - loss: 0.0110
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1601
3824/6530 [================>.............] - ETA: 0s - loss: 0.0095
3184/6530 [=============>................] - ETA: 0s - loss: 0.0108
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1560
4016/6530 [=================>............] - ETA: 0s - loss: 0.0096
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0108
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1562
3904/6530 [================>.............] - ETA: 0s - loss: 0.0109
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0096
3200/6530 [=============>................] - ETA: 0s - loss: 0.1534
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0110
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0096
4032/6530 [=================>............] - ETA: 0s - loss: 0.1539
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0110
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0097
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1536
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0109
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0097
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1531
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0109
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0096
6400/6530 [============================>.] - ETA: 0s - loss: 0.1527
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0109
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0096
6530/6530 [==============================] - 0s 69us/step - loss: 0.1526 - val_loss: 0.1331
Epoch 7/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1482
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0109
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0096
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1527
6416/6530 [============================>.] - ETA: 0s - loss: 0.0109
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0096
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1503
6530/6530 [==============================] - 1s 149us/step - loss: 0.0110 - val_loss: 0.0097
Epoch 20/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0063
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0097
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1507
 384/6530 [>.............................] - ETA: 0s - loss: 0.0112
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0097
3200/6530 [=============>................] - ETA: 0s - loss: 0.1500
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0119
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0096
3968/6530 [=================>............] - ETA: 0s - loss: 0.1503
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0117
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0096
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1493
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0114
6400/6530 [============================>.] - ETA: 0s - loss: 0.0096
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1489
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0111
6400/6530 [============================>.] - ETA: 0s - loss: 0.1487
6530/6530 [==============================] - 2s 295us/step - loss: 0.0097 - val_loss: 0.0102
Epoch 14/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0051
6530/6530 [==============================] - 0s 69us/step - loss: 0.1487 - val_loss: 0.1282

2192/6530 [=========>....................] - ETA: 0s - loss: 0.0111Epoch 8/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1391
 192/6530 [..............................] - ETA: 1s - loss: 0.0086
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0110
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1498
 368/6530 [>.............................] - ETA: 1s - loss: 0.0084
2880/6530 [============>.................] - ETA: 0s - loss: 0.0107
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1457
 544/6530 [=>............................] - ETA: 1s - loss: 0.0091
3216/6530 [=============>................] - ETA: 0s - loss: 0.0106
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1483
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0092
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0107
3136/6530 [=============>................] - ETA: 0s - loss: 0.1472
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0094
3936/6530 [=================>............] - ETA: 0s - loss: 0.0108
3904/6530 [================>.............] - ETA: 0s - loss: 0.1468
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0099
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0107
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1447
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0101
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0107
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1452
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0100
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0106
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1452
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0099
6530/6530 [==============================] - 0s 71us/step - loss: 0.1449 - val_loss: 0.1198

5328/6530 [=======================>......] - ETA: 0s - loss: 0.0106Epoch 9/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1469
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0099
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0107
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1440
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0096
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0107
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1439
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0095
6384/6530 [============================>.] - ETA: 0s - loss: 0.0107
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1445
3200/6530 [=============>................] - ETA: 0s - loss: 0.1429
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0094
6530/6530 [==============================] - 1s 150us/step - loss: 0.0108 - val_loss: 0.0095
Epoch 21/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0059
3968/6530 [=================>............] - ETA: 0s - loss: 0.1432
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0095
 384/6530 [>.............................] - ETA: 0s - loss: 0.0111
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1427
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0095
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0116
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1425
2848/6530 [============>.................] - ETA: 1s - loss: 0.0094
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0114
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1421
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0112
3040/6530 [============>.................] - ETA: 1s - loss: 0.0092
6530/6530 [==============================] - 0s 69us/step - loss: 0.1419 - val_loss: 0.1173
Epoch 10/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1580
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0109
3216/6530 [=============>................] - ETA: 0s - loss: 0.0092
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1371
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0108
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0092
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1387
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0108
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0093
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1386
2896/6530 [============>.................] - ETA: 0s - loss: 0.0105
3760/6530 [================>.............] - ETA: 0s - loss: 0.0093
3136/6530 [=============>................] - ETA: 0s - loss: 0.1377
3264/6530 [=============>................] - ETA: 0s - loss: 0.0104
3952/6530 [=================>............] - ETA: 0s - loss: 0.0093
3904/6530 [================>.............] - ETA: 0s - loss: 0.1383
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0105
4128/6530 [=================>............] - ETA: 0s - loss: 0.0094
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1373
4000/6530 [=================>............] - ETA: 0s - loss: 0.0106
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0094
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1373
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0105
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0094
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1371
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0105
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0094
6530/6530 [==============================] - 0s 69us/step - loss: 0.1371 - val_loss: 0.1161
Epoch 11/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1498
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0104
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0094
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1344
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0104
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0094
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1360
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0105
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0094
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1374
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0105
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0094
3136/6530 [=============>................] - ETA: 0s - loss: 0.1349
6512/6530 [============================>.] - ETA: 0s - loss: 0.0105
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0094
3904/6530 [================>.............] - ETA: 0s - loss: 0.1353
6530/6530 [==============================] - 1s 147us/step - loss: 0.0106 - val_loss: 0.0097
Epoch 22/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0060
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1350
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0095
 352/6530 [>.............................] - ETA: 0s - loss: 0.0105
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0094
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1343
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0117
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0094
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1342
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0112
6530/6530 [==============================] - 0s 69us/step - loss: 0.1343 - val_loss: 0.1133
Epoch 12/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1304
6352/6530 [============================>.] - ETA: 0s - loss: 0.0094
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0112
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1329
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0109
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1312
6530/6530 [==============================] - 2s 296us/step - loss: 0.0095 - val_loss: 0.0102
Epoch 15/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0043
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0107
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1330
 208/6530 [..............................] - ETA: 1s - loss: 0.0084
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0107
3072/6530 [=============>................] - ETA: 0s - loss: 0.1313
 384/6530 [>.............................] - ETA: 1s - loss: 0.0080
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0105
3840/6530 [================>.............] - ETA: 0s - loss: 0.1317
 576/6530 [=>............................] - ETA: 1s - loss: 0.0088
3136/6530 [=============>................] - ETA: 0s - loss: 0.0103
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1317
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0088
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0103
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1321
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0092
3840/6530 [================>.............] - ETA: 0s - loss: 0.0104
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1318
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0096
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0104
6530/6530 [==============================] - 0s 70us/step - loss: 0.1313 - val_loss: 0.1104
Epoch 13/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1406
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0098
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0103
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1350
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0096
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0103
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1321
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0095
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0102
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1319
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0095
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0103
3200/6530 [=============>................] - ETA: 0s - loss: 0.1304
2016/6530 [========>.....................] - ETA: 1s - loss: 0.0092
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0103
3968/6530 [=================>............] - ETA: 0s - loss: 0.1307
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0092
6352/6530 [============================>.] - ETA: 0s - loss: 0.0103
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1301
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0092
6530/6530 [==============================] - 1s 150us/step - loss: 0.0104 - val_loss: 0.0094
Epoch 23/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0056
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1300
2560/6530 [==========>...................] - ETA: 1s - loss: 0.0093
 368/6530 [>.............................] - ETA: 0s - loss: 0.0105
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1294
2752/6530 [===========>..................] - ETA: 1s - loss: 0.0092
6530/6530 [==============================] - 0s 69us/step - loss: 0.1294 - val_loss: 0.1107

 736/6530 [==>...........................] - ETA: 0s - loss: 0.0113Epoch 14/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1307
2928/6530 [============>.................] - ETA: 1s - loss: 0.0090
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1295
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0109
3104/6530 [=============>................] - ETA: 0s - loss: 0.0090
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1273
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0107
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0089
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0105
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1275
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0090
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0104
3136/6530 [=============>................] - ETA: 0s - loss: 0.1259
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0090
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0104
3840/6530 [================>.............] - ETA: 0s - loss: 0.1269
3824/6530 [================>.............] - ETA: 0s - loss: 0.0090
2848/6530 [============>.................] - ETA: 0s - loss: 0.0102
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1266
4016/6530 [=================>............] - ETA: 0s - loss: 0.0090
3216/6530 [=============>................] - ETA: 0s - loss: 0.0100
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1262
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0091
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0101
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1264
6530/6530 [==============================] - 0s 70us/step - loss: 0.1263 - val_loss: 0.1041

4384/6530 [===================>..........] - ETA: 0s - loss: 0.0090Epoch 15/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1361
3936/6530 [=================>............] - ETA: 0s - loss: 0.0102
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0091
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1291
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0101
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0091
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1272
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0101
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0090
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1264
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0100
3072/6530 [=============>................] - ETA: 0s - loss: 0.1246
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0091
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0101
3840/6530 [================>.............] - ETA: 0s - loss: 0.1247
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0091
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0100
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1245
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0091
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0101
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1249
6432/6530 [============================>.] - ETA: 0s - loss: 0.0101
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0091
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1248
6530/6530 [==============================] - 1s 149us/step - loss: 0.0102 - val_loss: 0.0094
Epoch 24/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0053
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0091
6530/6530 [==============================] - 0s 70us/step - loss: 0.1249 - val_loss: 0.1042
Epoch 16/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1210
 368/6530 [>.............................] - ETA: 0s - loss: 0.0100
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0091
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1250
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0110
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0091
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1246
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0107
6384/6530 [============================>.] - ETA: 0s - loss: 0.0091
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0105
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1249
6530/6530 [==============================] - 2s 294us/step - loss: 0.0092 - val_loss: 0.0106

1824/6530 [=======>......................] - ETA: 0s - loss: 0.0103Epoch 16/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0061
3264/6530 [=============>................] - ETA: 0s - loss: 0.1237
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0103
 208/6530 [..............................] - ETA: 1s - loss: 0.0080
4032/6530 [=================>............] - ETA: 0s - loss: 0.1230
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0102
 400/6530 [>.............................] - ETA: 1s - loss: 0.0078
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1235
2896/6530 [============>.................] - ETA: 0s - loss: 0.0099
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1236
 592/6530 [=>............................] - ETA: 1s - loss: 0.0085
3264/6530 [=============>................] - ETA: 0s - loss: 0.0098
6464/6530 [============================>.] - ETA: 0s - loss: 0.1233
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0084
6530/6530 [==============================] - 0s 69us/step - loss: 0.1233 - val_loss: 0.1024
Epoch 17/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1086
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0099
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0088
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1205
3952/6530 [=================>............] - ETA: 0s - loss: 0.0100
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0093
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1205
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0099
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0095
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1216
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0099
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0093
3136/6530 [=============>................] - ETA: 0s - loss: 0.1193
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0098
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0093
3904/6530 [================>.............] - ETA: 0s - loss: 0.1205
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0098
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0092
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1205
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0098
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0091
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1212
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0099
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0090
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1212
6368/6530 [============================>.] - ETA: 0s - loss: 0.0099
6530/6530 [==============================] - 0s 69us/step - loss: 0.1209 - val_loss: 0.1011

2416/6530 [==========>...................] - ETA: 1s - loss: 0.0090Epoch 18/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1225
6530/6530 [==============================] - 1s 150us/step - loss: 0.0100 - val_loss: 0.0093
Epoch 25/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0050
2592/6530 [==========>...................] - ETA: 1s - loss: 0.0091
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1226
 384/6530 [>.............................] - ETA: 0s - loss: 0.0100
2784/6530 [===========>..................] - ETA: 1s - loss: 0.0089
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1218
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0108
2960/6530 [============>.................] - ETA: 1s - loss: 0.0088
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1228
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0105
3136/6530 [=============>................] - ETA: 0s - loss: 0.0087
3200/6530 [=============>................] - ETA: 0s - loss: 0.1216
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0103
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0087
4032/6530 [=================>............] - ETA: 0s - loss: 0.1227
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0101
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0087
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1222
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0100
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0088
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1220
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0100
3856/6530 [================>.............] - ETA: 0s - loss: 0.0088
6400/6530 [============================>.] - ETA: 0s - loss: 0.1225
2848/6530 [============>.................] - ETA: 0s - loss: 0.0098
6530/6530 [==============================] - 0s 71us/step - loss: 0.1225 - val_loss: 0.1015

4032/6530 [=================>............] - ETA: 0s - loss: 0.0089Epoch 19/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1127
3168/6530 [=============>................] - ETA: 0s - loss: 0.0096
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0088
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1201
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0097
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1175
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0088
3872/6530 [================>.............] - ETA: 0s - loss: 0.0097
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0088
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1196
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0098
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0089
3136/6530 [=============>................] - ETA: 0s - loss: 0.1191
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0097
3840/6530 [================>.............] - ETA: 0s - loss: 0.1204
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0088
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0097
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1203
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0088
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0097
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1211
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0088
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0097
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1208
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0088
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0097
6530/6530 [==============================] - 0s 71us/step - loss: 0.1207 - val_loss: 0.0982
Epoch 20/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1093
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0089
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0097
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1196
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0089
6530/6530 [==============================] - 1s 152us/step - loss: 0.0098 - val_loss: 0.0093
Epoch 26/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0048
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1180
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0089
 384/6530 [>.............................] - ETA: 0s - loss: 0.0098
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1188
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0089
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0105
3136/6530 [=============>................] - ETA: 0s - loss: 0.1171
6368/6530 [============================>.] - ETA: 0s - loss: 0.0089
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0100
3904/6530 [================>.............] - ETA: 0s - loss: 0.1185
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0102
6530/6530 [==============================] - 2s 294us/step - loss: 0.0089 - val_loss: 0.0105

4672/6530 [====================>.........] - ETA: 0s - loss: 0.1184Epoch 17/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0050
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0100
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1185
 192/6530 [..............................] - ETA: 1s - loss: 0.0080
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0097
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1180
 368/6530 [>.............................] - ETA: 1s - loss: 0.0077
6530/6530 [==============================] - 0s 69us/step - loss: 0.1180 - val_loss: 0.0977
Epoch 21/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1189
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0098
 560/6530 [=>............................] - ETA: 1s - loss: 0.0083
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1183
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0097
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0086
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1185
3136/6530 [=============>................] - ETA: 0s - loss: 0.0095
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0088
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1180
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0095
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0094
3200/6530 [=============>................] - ETA: 0s - loss: 0.1163
3824/6530 [================>.............] - ETA: 0s - loss: 0.0096
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0096
3968/6530 [=================>............] - ETA: 0s - loss: 0.1171
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0096
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0094
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1166
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0096
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0092
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1168
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0095
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0092
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1170
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0095
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0090
6530/6530 [==============================] - 0s 69us/step - loss: 0.1172 - val_loss: 0.0970
Epoch 22/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1280
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0095
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0090
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1163
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0096
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0089
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1145
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0096
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0090
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1149
6528/6530 [============================>.] - ETA: 0s - loss: 0.0097
2688/6530 [===========>..................] - ETA: 1s - loss: 0.0090
3072/6530 [=============>................] - ETA: 0s - loss: 0.1153
6530/6530 [==============================] - 1s 156us/step - loss: 0.0097 - val_loss: 0.0092
Epoch 27/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0046
2864/6530 [============>.................] - ETA: 1s - loss: 0.0088
3840/6530 [================>.............] - ETA: 0s - loss: 0.1168
 368/6530 [>.............................] - ETA: 0s - loss: 0.0094
3056/6530 [=============>................] - ETA: 1s - loss: 0.0087
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1171
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0103
3232/6530 [=============>................] - ETA: 0s - loss: 0.0086
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1168
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0100
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0086
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1168
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0100
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0087
6530/6530 [==============================] - 0s 70us/step - loss: 0.1167 - val_loss: 0.0967
Epoch 23/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1254
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0097
3760/6530 [================>.............] - ETA: 0s - loss: 0.0087
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1172
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0096
3936/6530 [=================>............] - ETA: 0s - loss: 0.0087
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1169
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0097
4128/6530 [=================>............] - ETA: 0s - loss: 0.0087
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1166
2864/6530 [============>.................] - ETA: 0s - loss: 0.0094
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0087
3200/6530 [=============>................] - ETA: 0s - loss: 0.1161
3200/6530 [=============>................] - ETA: 0s - loss: 0.0093
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0087
4032/6530 [=================>............] - ETA: 0s - loss: 0.1172
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0093
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0087
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1169
3920/6530 [=================>............] - ETA: 0s - loss: 0.0095
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0087
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1167
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0095
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0086
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1170
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0094
6530/6530 [==============================] - 0s 69us/step - loss: 0.1170 - val_loss: 0.0977
Epoch 24/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1087
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0087
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0093
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1136
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0087
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0094
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1117
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0087
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0094
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1118
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0088
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0094
3136/6530 [=============>................] - ETA: 0s - loss: 0.1116
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0087
6352/6530 [============================>.] - ETA: 0s - loss: 0.0094
3904/6530 [================>.............] - ETA: 0s - loss: 0.1124
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0087
6530/6530 [==============================] - 1s 152us/step - loss: 0.0095 - val_loss: 0.0092

4672/6530 [====================>.........] - ETA: 0s - loss: 0.1130
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0087
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1135
6448/6530 [============================>.] - ETA: 0s - loss: 0.0087
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1135
6530/6530 [==============================] - 2s 298us/step - loss: 0.0088 - val_loss: 0.0098
Epoch 18/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0046
6530/6530 [==============================] - 0s 71us/step - loss: 0.1137 - val_loss: 0.0955
Epoch 25/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1001
 208/6530 [..............................] - ETA: 1s - loss: 0.0079
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1133
 384/6530 [>.............................] - ETA: 1s - loss: 0.0075
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1131
 576/6530 [=>............................] - ETA: 1s - loss: 0.0082
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1137
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0082
3264/6530 [=============>................] - ETA: 0s - loss: 0.1133
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0087
4032/6530 [=================>............] - ETA: 0s - loss: 0.1139
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0093
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1135
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0094
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1135
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0092
6400/6530 [============================>.] - ETA: 0s - loss: 0.1133
6530/6530 [==============================] - 0s 68us/step - loss: 0.1133 - val_loss: 0.0945
Epoch 26/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1163
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0091
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1166
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0090
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1144
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0089
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1141
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0089
2944/6530 [============>.................] - ETA: 0s - loss: 0.1130
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0088
3712/6530 [================>.............] - ETA: 0s - loss: 0.1127
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0089
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1123
2704/6530 [===========>..................] - ETA: 1s - loss: 0.0088
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1127
2880/6530 [============>.................] - ETA: 1s - loss: 0.0087
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1128
3056/6530 [=============>................] - ETA: 1s - loss: 0.0085
6530/6530 [==============================] - 0s 69us/step - loss: 0.1125 - val_loss: 0.0933
Epoch 27/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1133
3232/6530 [=============>................] - ETA: 0s - loss: 0.0085
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1120
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0085
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1110
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0085
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1120
3760/6530 [================>.............] - ETA: 0s - loss: 0.0085
3136/6530 [=============>................] - ETA: 0s - loss: 0.1119
3936/6530 [=================>............] - ETA: 0s - loss: 0.0085
3840/6530 [================>.............] - ETA: 0s - loss: 0.1133
4112/6530 [=================>............] - ETA: 0s - loss: 0.0086
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1132
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0085
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1130
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0085
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1122
6530/6530 [==============================] - 0s 70us/step - loss: 0.1120 - val_loss: 0.0954

4640/6530 [====================>.........] - ETA: 0s - loss: 0.0086
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0085
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0085
# training | RMSE: 0.1145, MAE: 0.0890
worker 1  xfile  [6, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3881724853455274}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28576535767268696}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.11454248579730282, 'rmse': 0.11454248579730282, 'mae': 0.0889903846511492, 'early_stop': False}
vggnet done  1

5168/6530 [======================>.......] - ETA: 0s - loss: 0.0085
# training | RMSE: 0.0830, MAE: 0.0656
worker 2  xfile  [4, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14008507478600118}, 'layer_2_size': 32, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 88, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.08297062310034203, 'rmse': 0.08297062310034203, 'mae': 0.06560969164497767, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  82 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  35 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  45 | activation: tanh    | extras: batchnorm 
layer 4 | size:  91 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f45e759fa58>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 256/6530 [>.............................] - ETA: 15s - loss: 1.2005
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0086
3840/6530 [================>.............] - ETA: 0s - loss: 0.1517 
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0085
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0086
6530/6530 [==============================] - 1s 118us/step - loss: 0.1043 - val_loss: 0.0333
Epoch 2/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0326
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0086
3840/6530 [================>.............] - ETA: 0s - loss: 0.0270
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0085
6530/6530 [==============================] - 0s 15us/step - loss: 0.0262 - val_loss: 0.0278
Epoch 3/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0224
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0086
3840/6530 [================>.............] - ETA: 0s - loss: 0.0208
6480/6530 [============================>.] - ETA: 0s - loss: 0.0086
6530/6530 [==============================] - 0s 15us/step - loss: 0.0207 - val_loss: 0.0254
Epoch 4/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0182
6530/6530 [==============================] - 2s 299us/step - loss: 0.0086 - val_loss: 0.0142
Epoch 19/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0070
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0182
6530/6530 [==============================] - 0s 15us/step - loss: 0.0181 - val_loss: 0.0226
Epoch 5/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0163
 208/6530 [..............................] - ETA: 1s - loss: 0.0079
3840/6530 [================>.............] - ETA: 0s - loss: 0.0164
 400/6530 [>.............................] - ETA: 1s - loss: 0.0075
6530/6530 [==============================] - 0s 15us/step - loss: 0.0166 - val_loss: 0.0201
Epoch 6/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0152
 576/6530 [=>............................] - ETA: 1s - loss: 0.0083
4096/6530 [=================>............] - ETA: 0s - loss: 0.0153
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0082
6530/6530 [==============================] - 0s 15us/step - loss: 0.0155 - val_loss: 0.0184
Epoch 7/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0143
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0087
3840/6530 [================>.............] - ETA: 0s - loss: 0.0145
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0091
6530/6530 [==============================] - 0s 15us/step - loss: 0.0147 - val_loss: 0.0172
Epoch 8/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0137
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0092
4096/6530 [=================>............] - ETA: 0s - loss: 0.0139
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0090
6530/6530 [==============================] - 0s 14us/step - loss: 0.0141 - val_loss: 0.0163
Epoch 9/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0132
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0089
3840/6530 [================>.............] - ETA: 0s - loss: 0.0134
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0087
6530/6530 [==============================] - 0s 15us/step - loss: 0.0136 - val_loss: 0.0157
Epoch 10/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0127
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0086
4096/6530 [=================>............] - ETA: 0s - loss: 0.0130
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0085
6530/6530 [==============================] - 0s 14us/step - loss: 0.0131 - val_loss: 0.0151
Epoch 11/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0123
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0086
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0127
6530/6530 [==============================] - 0s 16us/step - loss: 0.0128 - val_loss: 0.0147

2672/6530 [===========>..................] - ETA: 1s - loss: 0.0086Epoch 12/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0119
2864/6530 [============>.................] - ETA: 1s - loss: 0.0084
3840/6530 [================>.............] - ETA: 0s - loss: 0.0122
6530/6530 [==============================] - 0s 15us/step - loss: 0.0124 - val_loss: 0.0143
Epoch 13/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0116
3056/6530 [=============>................] - ETA: 0s - loss: 0.0083
4096/6530 [=================>............] - ETA: 0s - loss: 0.0120
3232/6530 [=============>................] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 0s 14us/step - loss: 0.0121 - val_loss: 0.0140
Epoch 14/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0113
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0083
3840/6530 [================>.............] - ETA: 0s - loss: 0.0116
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 0s 15us/step - loss: 0.0118 - val_loss: 0.0137
Epoch 15/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0110
3792/6530 [================>.............] - ETA: 0s - loss: 0.0083
4096/6530 [=================>............] - ETA: 0s - loss: 0.0114
3984/6530 [=================>............] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 0s 15us/step - loss: 0.0116 - val_loss: 0.0134
Epoch 16/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0108
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0083
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0112
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132
Epoch 17/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0105
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0084
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0110
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0129
Epoch 18/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0103
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0083
3840/6530 [================>.............] - ETA: 0s - loss: 0.0107
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0127
Epoch 19/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0101
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0083
3840/6530 [================>.............] - ETA: 0s - loss: 0.0105
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 0s 15us/step - loss: 0.0107 - val_loss: 0.0125
Epoch 20/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0099
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0084
4096/6530 [=================>............] - ETA: 0s - loss: 0.0104
6530/6530 [==============================] - 0s 15us/step - loss: 0.0106 - val_loss: 0.0123

5888/6530 [==========================>...] - ETA: 0s - loss: 0.0084Epoch 21/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0098
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0083
3840/6530 [================>.............] - ETA: 0s - loss: 0.0102
6530/6530 [==============================] - 0s 15us/step - loss: 0.0104 - val_loss: 0.0121
Epoch 22/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0096
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0083
4096/6530 [=================>............] - ETA: 0s - loss: 0.0101
6464/6530 [============================>.] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 0s 15us/step - loss: 0.0102 - val_loss: 0.0120
Epoch 23/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0095
6530/6530 [==============================] - 2s 282us/step - loss: 0.0084 - val_loss: 0.0100
Epoch 20/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0049
4096/6530 [=================>............] - ETA: 0s - loss: 0.0100
 208/6530 [..............................] - ETA: 1s - loss: 0.0080
6530/6530 [==============================] - 0s 15us/step - loss: 0.0101 - val_loss: 0.0118
Epoch 24/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0094
 400/6530 [>.............................] - ETA: 1s - loss: 0.0076
4096/6530 [=================>............] - ETA: 0s - loss: 0.0098
 576/6530 [=>............................] - ETA: 1s - loss: 0.0083
6530/6530 [==============================] - 0s 15us/step - loss: 0.0099 - val_loss: 0.0117
Epoch 25/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0092
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0082
4096/6530 [=================>............] - ETA: 0s - loss: 0.0097
6530/6530 [==============================] - 0s 14us/step - loss: 0.0098 - val_loss: 0.0115
Epoch 26/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0091
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0087
4096/6530 [=================>............] - ETA: 0s - loss: 0.0096
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0091
6530/6530 [==============================] - 0s 15us/step - loss: 0.0097 - val_loss: 0.0114
Epoch 27/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0090
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0092
3840/6530 [================>.............] - ETA: 0s - loss: 0.0094
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0090
6530/6530 [==============================] - 0s 15us/step - loss: 0.0095 - val_loss: 0.0113

1696/6530 [======>.......................] - ETA: 1s - loss: 0.0089
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0087
# training | RMSE: 0.0942, MAE: 0.0732
worker 2  xfile  [7, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 91, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.09424707076037812, 'rmse': 0.09424707076037812, 'mae': 0.07319544578230802, 'early_stop': False}
vggnet done  2

2048/6530 [========>.....................] - ETA: 1s - loss: 0.0087
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0085
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0085
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0085
2832/6530 [============>.................] - ETA: 1s - loss: 0.0084
3008/6530 [============>.................] - ETA: 0s - loss: 0.0082
3184/6530 [=============>................] - ETA: 0s - loss: 0.0082
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0081
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0082
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0082
3808/6530 [================>.............] - ETA: 0s - loss: 0.0082
3968/6530 [=================>............] - ETA: 0s - loss: 0.0083
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0083
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0082
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0082
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0082
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0082
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0082
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0082
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0083
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0082
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0083
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0083
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0082
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0082
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0082
6384/6530 [============================>.] - ETA: 0s - loss: 0.0082
6528/6530 [============================>.] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 2s 319us/step - loss: 0.0083 - val_loss: 0.0102
Epoch 21/27

  16/6530 [..............................] - ETA: 3s - loss: 0.0062
 160/6530 [..............................] - ETA: 2s - loss: 0.0080
 320/6530 [>.............................] - ETA: 2s - loss: 0.0077
 464/6530 [=>............................] - ETA: 2s - loss: 0.0081
 608/6530 [=>............................] - ETA: 2s - loss: 0.0084
 736/6530 [==>...........................] - ETA: 2s - loss: 0.0082
 880/6530 [===>..........................] - ETA: 2s - loss: 0.0084
 992/6530 [===>..........................] - ETA: 2s - loss: 0.0085
1120/6530 [====>.........................] - ETA: 2s - loss: 0.0089
1232/6530 [====>.........................] - ETA: 2s - loss: 0.0089
1360/6530 [=====>........................] - ETA: 2s - loss: 0.0090
1472/6530 [=====>........................] - ETA: 2s - loss: 0.0088
1584/6530 [======>.......................] - ETA: 2s - loss: 0.0087
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0087
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0086
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0085
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0085
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0084
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0084
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0083
2416/6530 [==========>...................] - ETA: 1s - loss: 0.0083
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0084
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0084
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0083
2848/6530 [============>.................] - ETA: 1s - loss: 0.0082
2976/6530 [============>.................] - ETA: 1s - loss: 0.0081
3136/6530 [=============>................] - ETA: 1s - loss: 0.0081
3312/6530 [==============>...............] - ETA: 1s - loss: 0.0080
3488/6530 [===============>..............] - ETA: 1s - loss: 0.0081
3648/6530 [===============>..............] - ETA: 1s - loss: 0.0081
3808/6530 [================>.............] - ETA: 1s - loss: 0.0080
3952/6530 [=================>............] - ETA: 1s - loss: 0.0081
4080/6530 [=================>............] - ETA: 1s - loss: 0.0082
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0081
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0081
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0081
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0081
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0081
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0081
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0081
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0081
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0081
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0082
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0081
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0081
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0082
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0081
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0081
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0081
6336/6530 [============================>.] - ETA: 0s - loss: 0.0081
6464/6530 [============================>.] - ETA: 0s - loss: 0.0081
6530/6530 [==============================] - 3s 423us/step - loss: 0.0082 - val_loss: 0.0101
Epoch 22/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0065
 144/6530 [..............................] - ETA: 2s - loss: 0.0079
 304/6530 [>.............................] - ETA: 2s - loss: 0.0075
 432/6530 [>.............................] - ETA: 2s - loss: 0.0077
 576/6530 [=>............................] - ETA: 2s - loss: 0.0081
 720/6530 [==>...........................] - ETA: 2s - loss: 0.0081
 864/6530 [==>...........................] - ETA: 2s - loss: 0.0081
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0085
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0088
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0089
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0087
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0086
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0085
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0084
2016/6530 [========>.....................] - ETA: 1s - loss: 0.0084
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0083
2288/6530 [=========>....................] - ETA: 1s - loss: 0.0082
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0082
2608/6530 [==========>...................] - ETA: 1s - loss: 0.0083
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0082
2944/6530 [============>.................] - ETA: 1s - loss: 0.0080
3104/6530 [=============>................] - ETA: 1s - loss: 0.0079
3248/6530 [=============>................] - ETA: 1s - loss: 0.0079
3360/6530 [==============>...............] - ETA: 1s - loss: 0.0079
3456/6530 [==============>...............] - ETA: 1s - loss: 0.0079
3568/6530 [===============>..............] - ETA: 1s - loss: 0.0080
3680/6530 [===============>..............] - ETA: 1s - loss: 0.0080
3824/6530 [================>.............] - ETA: 1s - loss: 0.0080
3968/6530 [=================>............] - ETA: 0s - loss: 0.0080
4096/6530 [=================>............] - ETA: 0s - loss: 0.0080
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0080
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0079
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0080
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0080
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0080
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0080
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0080
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0080
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0080
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0081
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0080
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0080
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0081
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0081
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0080
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0080
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0080
6368/6530 [============================>.] - ETA: 0s - loss: 0.0080
6480/6530 [============================>.] - ETA: 0s - loss: 0.0080
6530/6530 [==============================] - 3s 402us/step - loss: 0.0081 - val_loss: 0.0110

# training | RMSE: 0.0956, MAE: 0.0755
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 35, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.0956248480093816, 'rmse': 0.0956248480093816, 'mae': 0.07545087932634503, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#1 epoch=27.0 loss={'loss': 0.20296877881688113, 'rmse': 0.20296877881688113, 'mae': 0.16269820066336607, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20722056181944334}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 53, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#2 epoch=27.0 loss={'loss': 0.23057683067695642, 'rmse': 0.23057683067695642, 'mae': 0.1917885754819075, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3917465634993933}, 'layer_2_size': 53, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 29, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15337783986534193}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#3 epoch=27.0 loss={'loss': 0.16868788520154937, 'rmse': 0.16868788520154937, 'mae': 0.14206331461948535, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 92, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 59, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#5 epoch=27.0 loss={'loss': 0.17558661923879246, 'rmse': 0.17558661923879246, 'mae': 0.13678029067552216, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.43079533423603034}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 15, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.34262296990548324}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#4 epoch=27.0 loss={'loss': 0.08297062310034203, 'rmse': 0.08297062310034203, 'mae': 0.06560969164497767, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14008507478600118}, 'layer_2_size': 32, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 88, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#6 epoch=27.0 loss={'loss': 0.11454248579730282, 'rmse': 0.11454248579730282, 'mae': 0.0889903846511492, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3881724853455274}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28576535767268696}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#7 epoch=27.0 loss={'loss': 0.09424707076037812, 'rmse': 0.09424707076037812, 'mae': 0.07319544578230802, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 35, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 91, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#0 epoch=27.0 loss={'loss': 0.0956248480093816, 'rmse': 0.0956248480093816, 'mae': 0.07545087932634503, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 74, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 35, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 51, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
get a list [results] of length 199
get a list [loss] of length 8
get a list [val_loss] of length 8
length of indices is (4, 7, 0, 6, 3, 5, 1, 2)
length of indices is 8
length of T is 8
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14008507478600118}, 'layer_2_size': 32, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 88, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [1, 81.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3881724853455274}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28576535767268696}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]] 

*** 2.6666666666666665 configurations x 81.0 iterations each

1 | Thu Sep 27 23:11:28 2018 | lowest loss so far: 0.0675 (run 0)

vggnet done  2
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  99 | activation: tanh    | extras: batchnorm 
layer 2 | size:  32 | activation: sigmoid | extras: dropout - rate: 14.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8b02b0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 6:17 - loss: 0.4510
 272/6530 [>.............................] - ETA: 22s - loss: 0.2574 
 560/6530 [=>............................] - ETA: 11s - loss: 0.1587
 864/6530 [==>...........................] - ETA: 7s - loss: 0.1224 
1168/6530 [====>.........................] - ETA: 5s - loss: 0.1051
1488/6530 [=====>........................] - ETA: 4s - loss: 0.0919
1808/6530 [=======>......................] - ETA: 3s - loss: 0.0838
2096/6530 [========>.....................] - ETA: 2s - loss: 0.0784
2400/6530 [==========>...................] - ETA: 2s - loss: 0.0748
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0706
3104/6530 [=============>................] - ETA: 1s - loss: 0.0669
3424/6530 [==============>...............] - ETA: 1s - loss: 0.0651
3776/6530 [================>.............] - ETA: 1s - loss: 0.0631{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  54 | activation: tanh    | extras: dropout - rate: 38.8% 
layer 2 | size:  24 | activation: tanh    | extras: None 
layer 3 | size:  44 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  43 | activation: tanh    | extras: batchnorm 
layer 5 | size:   3 | activation: tanh    | extras: dropout - rate: 28.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  64/6530 [..............................] - ETA: 1:49 - loss: 1.3329
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0615
 896/6530 [===>..........................] - ETA: 7s - loss: 0.9464  
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0598
1728/6530 [======>.......................] - ETA: 3s - loss: 0.8084
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0579
2560/6530 [==========>...................] - ETA: 1s - loss: 0.7242
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0566
3328/6530 [==============>...............] - ETA: 1s - loss: 0.6708
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0554
4032/6530 [=================>............] - ETA: 0s - loss: 0.6294
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0545
4800/6530 [=====================>........] - ETA: 0s - loss: 0.5927
6320/6530 [============================>.] - ETA: 0s - loss: 0.0535
5568/6530 [========================>.....] - ETA: 0s - loss: 0.5665
6272/6530 [===========================>..] - ETA: 0s - loss: 0.5446
6530/6530 [==============================] - 2s 306us/step - loss: 0.0530 - val_loss: 0.0322
Epoch 2/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0467
 384/6530 [>.............................] - ETA: 0s - loss: 0.0337
6530/6530 [==============================] - 2s 242us/step - loss: 0.5360 - val_loss: 0.3501
Epoch 2/81

  64/6530 [..............................] - ETA: 0s - loss: 0.3206
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0361
 896/6530 [===>..........................] - ETA: 0s - loss: 0.3451
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0362
1728/6530 [======>.......................] - ETA: 0s - loss: 0.3353
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0339
2496/6530 [==========>...................] - ETA: 0s - loss: 0.3331
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0333
3328/6530 [==============>...............] - ETA: 0s - loss: 0.3213
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0324
4160/6530 [==================>...........] - ETA: 0s - loss: 0.3124
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0325
4928/6530 [=====================>........] - ETA: 0s - loss: 0.3060
2944/6530 [============>.................] - ETA: 0s - loss: 0.0319
5696/6530 [=========================>....] - ETA: 0s - loss: 0.3003
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0314
6464/6530 [============================>.] - ETA: 0s - loss: 0.2970
6530/6530 [==============================] - 0s 67us/step - loss: 0.2963 - val_loss: 0.2342
Epoch 3/81

  64/6530 [..............................] - ETA: 0s - loss: 0.2408
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0315
 896/6530 [===>..........................] - ETA: 0s - loss: 0.2554
4016/6530 [=================>............] - ETA: 0s - loss: 0.0312
1728/6530 [======>.......................] - ETA: 0s - loss: 0.2514
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0309
2496/6530 [==========>...................] - ETA: 0s - loss: 0.2496
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0308
3328/6530 [==============>...............] - ETA: 0s - loss: 0.2467
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0304
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2448
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0302
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2412
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0302
5824/6530 [=========================>....] - ETA: 0s - loss: 0.2382
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0299
6530/6530 [==============================] - 0s 65us/step - loss: 0.2379 - val_loss: 0.2129
Epoch 4/81

  64/6530 [..............................] - ETA: 0s - loss: 0.2464
6530/6530 [==============================] - 1s 143us/step - loss: 0.0298 - val_loss: 0.0227
Epoch 3/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0321
 896/6530 [===>..........................] - ETA: 0s - loss: 0.2218
 400/6530 [>.............................] - ETA: 0s - loss: 0.0261
1728/6530 [======>.......................] - ETA: 0s - loss: 0.2175
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0263
2560/6530 [==========>...................] - ETA: 0s - loss: 0.2186
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0262
3328/6530 [==============>...............] - ETA: 0s - loss: 0.2177
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0250
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2169
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0245
4928/6530 [=====================>........] - ETA: 0s - loss: 0.2162
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0241
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2142
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0243
6400/6530 [============================>.] - ETA: 0s - loss: 0.2133
2912/6530 [============>.................] - ETA: 0s - loss: 0.0240
6530/6530 [==============================] - 0s 67us/step - loss: 0.2134 - val_loss: 0.1843
Epoch 5/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1938
3264/6530 [=============>................] - ETA: 0s - loss: 0.0236
 832/6530 [==>...........................] - ETA: 0s - loss: 0.2011
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0238
1664/6530 [======>.......................] - ETA: 0s - loss: 0.2028
4000/6530 [=================>............] - ETA: 0s - loss: 0.0237
2496/6530 [==========>...................] - ETA: 0s - loss: 0.2017
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0234
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1977
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0235
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1970
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0233
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1957
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0232
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1956
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0231
6530/6530 [==============================] - 0s 66us/step - loss: 0.1956 - val_loss: 0.1888
Epoch 6/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1838
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0231
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1895
6530/6530 [==============================] - 1s 145us/step - loss: 0.0231 - val_loss: 0.0181
Epoch 4/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0242
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1887
 400/6530 [>.............................] - ETA: 0s - loss: 0.0217
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1904
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0219
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1897
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0218
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1880
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0210
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1875
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0204
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1869
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0204
6530/6530 [==============================] - 0s 65us/step - loss: 0.1859 - val_loss: 0.1562
Epoch 7/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1653
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0206
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1708
2992/6530 [============>.................] - ETA: 0s - loss: 0.0202
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1735
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0198
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1790
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0201
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1778
4080/6530 [=================>............] - ETA: 0s - loss: 0.0201
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1775
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0200
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1762
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0200
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1766
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0198
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0197
6530/6530 [==============================] - 0s 66us/step - loss: 0.1763 - val_loss: 0.1577
Epoch 8/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1904
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0198
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1746
6320/6530 [============================>.] - ETA: 0s - loss: 0.0197
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1727
6530/6530 [==============================] - 1s 142us/step - loss: 0.0197 - val_loss: 0.0162
Epoch 5/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0197
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1759
 384/6530 [>.............................] - ETA: 0s - loss: 0.0186
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1731
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0194
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1733
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0191
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1742
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0186
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1732
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0181
6530/6530 [==============================] - 0s 65us/step - loss: 0.1734 - val_loss: 0.1535
Epoch 9/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1643
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0183
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1713
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0183
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1675
3072/6530 [=============>................] - ETA: 0s - loss: 0.0180
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1693
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0179
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1662
3840/6530 [================>.............] - ETA: 0s - loss: 0.0181
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1667
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0180
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1678
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0181
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1678
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0179
6530/6530 [==============================] - 0s 63us/step - loss: 0.1679 - val_loss: 0.1650
Epoch 10/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1778
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0179
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1692
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0177
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1662
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0178
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1677
6448/6530 [============================>.] - ETA: 0s - loss: 0.0177
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1654
6530/6530 [==============================] - 1s 139us/step - loss: 0.0178 - val_loss: 0.0152
Epoch 6/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0173
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1644
 400/6530 [>.............................] - ETA: 0s - loss: 0.0169
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1645
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0177
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1636
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0176
6530/6530 [==============================] - 0s 66us/step - loss: 0.1635 - val_loss: 0.1601
Epoch 11/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1929
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0171
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1643
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0168
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1611
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0170
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1617
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0171
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1602
3056/6530 [=============>................] - ETA: 0s - loss: 0.0167
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1594
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0166
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1598
3824/6530 [================>.............] - ETA: 0s - loss: 0.0168
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1593
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0166
6530/6530 [==============================] - 0s 64us/step - loss: 0.1590 - val_loss: 0.1474
Epoch 12/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1792
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0167
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1584
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0166
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1560
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0166
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1583
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0164
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0165
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1569
6496/6530 [============================>.] - ETA: 0s - loss: 0.0164
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1568
6530/6530 [==============================] - 1s 139us/step - loss: 0.0165 - val_loss: 0.0136
Epoch 7/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0155
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1575
 400/6530 [>.............................] - ETA: 0s - loss: 0.0174
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1561
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0176
6530/6530 [==============================] - 0s 64us/step - loss: 0.1559 - val_loss: 0.1527
Epoch 13/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1805
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0172
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1573
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0167
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1519
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0163
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1527
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0165
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1518
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0165
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1515
3024/6530 [============>.................] - ETA: 0s - loss: 0.0161
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1520
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0158
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1517
3776/6530 [================>.............] - ETA: 0s - loss: 0.0160
6530/6530 [==============================] - 0s 65us/step - loss: 0.1518 - val_loss: 0.1587
Epoch 14/81

  64/6530 [..............................] - ETA: 0s - loss: 0.2048
4128/6530 [=================>............] - ETA: 0s - loss: 0.0160
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1600
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0160
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1551
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0159
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1551
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0158
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1535
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0157
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1520
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0157
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1520
6368/6530 [============================>.] - ETA: 0s - loss: 0.0157
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1522
6530/6530 [==============================] - 1s 141us/step - loss: 0.0157 - val_loss: 0.0132
Epoch 8/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0143
6530/6530 [==============================] - 0s 64us/step - loss: 0.1517 - val_loss: 0.1357
Epoch 15/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1741
 416/6530 [>.............................] - ETA: 0s - loss: 0.0150
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1475
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0156
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1439
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0156
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0153
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1451
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0150
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1448
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0153
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1453
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1461
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0153
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1455
3120/6530 [=============>................] - ETA: 0s - loss: 0.0149
6530/6530 [==============================] - 0s 63us/step - loss: 0.1454 - val_loss: 0.1388

3504/6530 [===============>..............] - ETA: 0s - loss: 0.0149Epoch 16/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1848
3888/6530 [================>.............] - ETA: 0s - loss: 0.0151
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1431
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0150
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1423
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0150
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1432
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0149
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1431
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0149
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1423
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0148
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1435
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0148
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1436
6480/6530 [============================>.] - ETA: 0s - loss: 0.0148
6530/6530 [==============================] - 0s 66us/step - loss: 0.1437 - val_loss: 0.1282
Epoch 17/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1433
6530/6530 [==============================] - 1s 138us/step - loss: 0.0149 - val_loss: 0.0122
Epoch 9/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0133
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1420
 400/6530 [>.............................] - ETA: 0s - loss: 0.0147
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1409
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0152
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1393
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0151
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1390
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0149
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1406
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0145
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1421
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0148
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1416
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0149
6530/6530 [==============================] - 0s 64us/step - loss: 0.1418 - val_loss: 0.1329

3040/6530 [============>.................] - ETA: 0s - loss: 0.0145Epoch 18/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1413
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0143
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1406
3792/6530 [================>.............] - ETA: 0s - loss: 0.0145
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1380
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0145
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1395
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0145
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1400
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0143
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1392
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0143
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1399
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0142
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1400
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0143
6530/6530 [==============================] - 0s 63us/step - loss: 0.1401 - val_loss: 0.1405
Epoch 19/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1346
6432/6530 [============================>.] - ETA: 0s - loss: 0.0142
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1409
6530/6530 [==============================] - 1s 140us/step - loss: 0.0143 - val_loss: 0.0118
Epoch 10/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0126
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1394
 368/6530 [>.............................] - ETA: 0s - loss: 0.0134
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1384
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0144
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1382
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0144
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1383
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0142
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1396
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0139
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1390
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0142
6530/6530 [==============================] - 0s 64us/step - loss: 0.1384 - val_loss: 0.1229
Epoch 20/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1359
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0143
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1379
2976/6530 [============>.................] - ETA: 0s - loss: 0.0140
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1359
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0137
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1378
3712/6530 [================>.............] - ETA: 0s - loss: 0.0138
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1371
4080/6530 [=================>............] - ETA: 0s - loss: 0.0139
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1374
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0139
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1378
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0138
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1373
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0137
6530/6530 [==============================] - 0s 65us/step - loss: 0.1365 - val_loss: 0.1219
Epoch 21/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1364
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0136
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1377
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0137
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1353
6320/6530 [============================>.] - ETA: 0s - loss: 0.0136
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1345
6530/6530 [==============================] - 1s 143us/step - loss: 0.0137 - val_loss: 0.0114
Epoch 11/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0120
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1356
 368/6530 [>.............................] - ETA: 0s - loss: 0.0129
4096/6530 [=================>............] - ETA: 0s - loss: 0.1354
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0141
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1365
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0141
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1359
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0138
6464/6530 [============================>.] - ETA: 0s - loss: 0.1354
6530/6530 [==============================] - 0s 66us/step - loss: 0.1353 - val_loss: 0.1240

1824/6530 [=======>......................] - ETA: 0s - loss: 0.0135Epoch 22/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1281
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0138
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1324
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0138
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1338
2912/6530 [============>.................] - ETA: 0s - loss: 0.0136
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1343
3248/6530 [=============>................] - ETA: 0s - loss: 0.0132
3264/6530 [=============>................] - ETA: 0s - loss: 0.1327
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0134
4032/6530 [=================>............] - ETA: 0s - loss: 0.1327
3984/6530 [=================>............] - ETA: 0s - loss: 0.0135
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1334
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0133
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1328
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0134
6336/6530 [============================>.] - ETA: 0s - loss: 0.1327
6530/6530 [==============================] - 0s 69us/step - loss: 0.1325 - val_loss: 0.1296
Epoch 23/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1310
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0133
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1296
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0132
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1284
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0132
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1299
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0132
3200/6530 [=============>................] - ETA: 0s - loss: 0.1295
6512/6530 [============================>.] - ETA: 0s - loss: 0.0132
6530/6530 [==============================] - 1s 146us/step - loss: 0.0133 - val_loss: 0.0112
Epoch 12/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0115
4032/6530 [=================>............] - ETA: 0s - loss: 0.1303
 384/6530 [>.............................] - ETA: 0s - loss: 0.0124
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1305
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0137
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1296
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0136
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1303
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0135
6530/6530 [==============================] - 0s 70us/step - loss: 0.1301 - val_loss: 0.1143
Epoch 24/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1313
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0133
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1335
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0129
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1312
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0132
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1304
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0133
2944/6530 [============>.................] - ETA: 0s - loss: 0.1301
2992/6530 [============>.................] - ETA: 0s - loss: 0.0131
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1299
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0128
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1291
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0130
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1292
3984/6530 [=================>............] - ETA: 0s - loss: 0.0131
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1287
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0130
6530/6530 [==============================] - 0s 74us/step - loss: 0.1289 - val_loss: 0.1182
Epoch 25/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1339
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0129
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1276
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0129
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1253
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0128
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1246
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0127
3136/6530 [=============>................] - ETA: 0s - loss: 0.1242
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0128
3904/6530 [================>.............] - ETA: 0s - loss: 0.1252
6336/6530 [============================>.] - ETA: 0s - loss: 0.0128
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1257
6530/6530 [==============================] - 1s 159us/step - loss: 0.0128 - val_loss: 0.0109
Epoch 13/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0111
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1257
 336/6530 [>.............................] - ETA: 0s - loss: 0.0118
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1254
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0129
6530/6530 [==============================] - 0s 72us/step - loss: 0.1257 - val_loss: 0.1155
Epoch 26/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1588
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0130
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1286
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0131
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1269
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0127
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1266
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0128
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1266
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0130
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1256
2912/6530 [============>.................] - ETA: 0s - loss: 0.0127
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1262
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0125
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1263
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0126
6530/6530 [==============================] - 0s 65us/step - loss: 0.1262 - val_loss: 0.1157
Epoch 27/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1303
4032/6530 [=================>............] - ETA: 0s - loss: 0.0127
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1276
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0126
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1244
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0126
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1244
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0125
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1239
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0124
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1228
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0124
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1228
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0124
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1233
6530/6530 [==============================] - 1s 143us/step - loss: 0.0125 - val_loss: 0.0107
Epoch 14/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0108
6530/6530 [==============================] - 0s 65us/step - loss: 0.1235 - val_loss: 0.1226
Epoch 28/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1297
 384/6530 [>.............................] - ETA: 0s - loss: 0.0117
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1227
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0126
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1221
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0127
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1229
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0126
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1221
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0122
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1204
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0126
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1204
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0125
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1202
3088/6530 [=============>................] - ETA: 0s - loss: 0.0122
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0122
6530/6530 [==============================] - 0s 64us/step - loss: 0.1200 - val_loss: 0.1479

3792/6530 [================>.............] - ETA: 0s - loss: 0.0122
4112/6530 [=================>............] - ETA: 0s - loss: 0.0123
# training | RMSE: 0.1751, MAE: 0.1438
worker 1  xfile  [1, 81.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3881724853455274}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28576535767268696}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.17509792431590374, 'rmse': 0.17509792431590374, 'mae': 0.14375044991703162, 'early_stop': True}
vggnet done  1

4432/6530 [===================>..........] - ETA: 0s - loss: 0.0122
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0122
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0121
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0120
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0121
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0120
6530/6530 [==============================] - 1s 143us/step - loss: 0.0121 - val_loss: 0.0106
Epoch 15/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0105
 384/6530 [>.............................] - ETA: 0s - loss: 0.0114
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0124
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0125
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0124
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0121
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0121
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0122
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0121
3152/6530 [=============>................] - ETA: 0s - loss: 0.0119
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0118
3872/6530 [================>.............] - ETA: 0s - loss: 0.0119
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0119
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0119
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0118
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0118
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0117
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0117
6528/6530 [============================>.] - ETA: 0s - loss: 0.0118
6530/6530 [==============================] - 1s 146us/step - loss: 0.0118 - val_loss: 0.0105
Epoch 16/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0102
 384/6530 [>.............................] - ETA: 0s - loss: 0.0112
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0122
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0119
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0121
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0119
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0118
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0116
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0120
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0120
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0118
3024/6530 [============>.................] - ETA: 0s - loss: 0.0117
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0115
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0116
3840/6530 [================>.............] - ETA: 0s - loss: 0.0116
4080/6530 [=================>............] - ETA: 0s - loss: 0.0117
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0116
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0116
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0116
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0115
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0115
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0115
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0114
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0114
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0114
6352/6530 [============================>.] - ETA: 0s - loss: 0.0114
6530/6530 [==============================] - 1s 210us/step - loss: 0.0115 - val_loss: 0.0103
Epoch 17/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0100
 256/6530 [>.............................] - ETA: 1s - loss: 0.0116
 544/6530 [=>............................] - ETA: 1s - loss: 0.0115
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0118
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0120
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0119
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0118
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0115
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0115
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0117
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0117
2944/6530 [============>.................] - ETA: 0s - loss: 0.0115
3232/6530 [=============>................] - ETA: 0s - loss: 0.0113
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0113
3824/6530 [================>.............] - ETA: 0s - loss: 0.0114
4096/6530 [=================>............] - ETA: 0s - loss: 0.0114
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0114
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0114
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0113
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0112
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0112
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0112
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0111
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0112
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0112
6432/6530 [============================>.] - ETA: 0s - loss: 0.0112
6530/6530 [==============================] - 1s 208us/step - loss: 0.0112 - val_loss: 0.0102
Epoch 18/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0099
 288/6530 [>.............................] - ETA: 1s - loss: 0.0108
 608/6530 [=>............................] - ETA: 1s - loss: 0.0116
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0113
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0116
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0114
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0112
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0112
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0114
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0112
3088/6530 [=============>................] - ETA: 0s - loss: 0.0111
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0109
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0110
3760/6530 [================>.............] - ETA: 0s - loss: 0.0111
4032/6530 [=================>............] - ETA: 0s - loss: 0.0112
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0111
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0111
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0110
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0110
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0109
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0108
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0109
6384/6530 [============================>.] - ETA: 0s - loss: 0.0109
6530/6530 [==============================] - 1s 183us/step - loss: 0.0110 - val_loss: 0.0101
Epoch 19/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0097
 320/6530 [>.............................] - ETA: 1s - loss: 0.0105
 624/6530 [=>............................] - ETA: 1s - loss: 0.0114
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0111
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0114
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0113
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0111
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0108
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0111
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0112
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0111
2976/6530 [============>.................] - ETA: 0s - loss: 0.0109
3232/6530 [=============>................] - ETA: 0s - loss: 0.0107
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0108
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0108
3904/6530 [================>.............] - ETA: 0s - loss: 0.0108
4128/6530 [=================>............] - ETA: 0s - loss: 0.0109
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0108
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0108
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0107
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0107
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0107
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0106
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0107
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0107
6530/6530 [==============================] - 1s 202us/step - loss: 0.0107 - val_loss: 0.0101
Epoch 20/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0096
 272/6530 [>.............................] - ETA: 1s - loss: 0.0108
 544/6530 [=>............................] - ETA: 1s - loss: 0.0107
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0109
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0112
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0111
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0109
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0106
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0108
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0109
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0107
3120/6530 [=============>................] - ETA: 0s - loss: 0.0106
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0105
3760/6530 [================>.............] - ETA: 0s - loss: 0.0106
4048/6530 [=================>............] - ETA: 0s - loss: 0.0107
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0106
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0106
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0105
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0105
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0104
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0104
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0104
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0104
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0104
6530/6530 [==============================] - 1s 195us/step - loss: 0.0105 - val_loss: 0.0099
Epoch 21/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0095
 288/6530 [>.............................] - ETA: 1s - loss: 0.0106
 576/6530 [=>............................] - ETA: 1s - loss: 0.0110
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0109
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0112
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0110
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0106
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0104
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0108
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0108
2832/6530 [============>.................] - ETA: 0s - loss: 0.0106
3088/6530 [=============>................] - ETA: 0s - loss: 0.0105
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0103
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0103
3776/6530 [================>.............] - ETA: 0s - loss: 0.0104
4000/6530 [=================>............] - ETA: 0s - loss: 0.0105
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0104
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0104
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0104
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0103
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0103
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0102
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0102
6368/6530 [============================>.] - ETA: 0s - loss: 0.0102
6530/6530 [==============================] - 1s 190us/step - loss: 0.0103 - val_loss: 0.0099
Epoch 22/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0093
 352/6530 [>.............................] - ETA: 0s - loss: 0.0099
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0105
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0104
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0106
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0104
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0102
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0102
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0104
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0105
2832/6530 [============>.................] - ETA: 0s - loss: 0.0103
3104/6530 [=============>................] - ETA: 0s - loss: 0.0101
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0100
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0101
3904/6530 [================>.............] - ETA: 0s - loss: 0.0101
4128/6530 [=================>............] - ETA: 0s - loss: 0.0102
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0101
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0101
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0101
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0100
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0100
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0099
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0099
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0100
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0100
6368/6530 [============================>.] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 1s 213us/step - loss: 0.0101 - val_loss: 0.0098
Epoch 23/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0092
 224/6530 [>.............................] - ETA: 1s - loss: 0.0097
 448/6530 [=>............................] - ETA: 1s - loss: 0.0103
 656/6530 [==>...........................] - ETA: 1s - loss: 0.0104
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0100
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0105
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0104
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0102
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0099
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0100
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0102
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0102
3024/6530 [============>.................] - ETA: 0s - loss: 0.0100
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0098
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0099
3872/6530 [================>.............] - ETA: 0s - loss: 0.0099
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0100
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0100
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0099
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0099
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0099
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0098
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0098
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0098
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0098
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0098
6448/6530 [============================>.] - ETA: 0s - loss: 0.0098
6530/6530 [==============================] - 1s 216us/step - loss: 0.0099 - val_loss: 0.0098
Epoch 24/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0090
 272/6530 [>.............................] - ETA: 1s - loss: 0.0098
 544/6530 [=>............................] - ETA: 1s - loss: 0.0098
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0098
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0101
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0101
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0100
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0097
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0099
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0100
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0100
2864/6530 [============>.................] - ETA: 0s - loss: 0.0099
3072/6530 [=============>................] - ETA: 0s - loss: 0.0098
3264/6530 [=============>................] - ETA: 0s - loss: 0.0097
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0097
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0097
3824/6530 [================>.............] - ETA: 0s - loss: 0.0097
4032/6530 [=================>............] - ETA: 0s - loss: 0.0098
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0097
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0098
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0097
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0097
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0096
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0096
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0096
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0096
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0096
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0096
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0096
6480/6530 [============================>.] - ETA: 0s - loss: 0.0096
6530/6530 [==============================] - 2s 244us/step - loss: 0.0097 - val_loss: 0.0097
Epoch 25/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0088
 192/6530 [..............................] - ETA: 1s - loss: 0.0095
 400/6530 [>.............................] - ETA: 1s - loss: 0.0097
 592/6530 [=>............................] - ETA: 1s - loss: 0.0098
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0096
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0098
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0100
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0099
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0098
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0096
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0095
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0098
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0098
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0098
2928/6530 [============>.................] - ETA: 0s - loss: 0.0096
3168/6530 [=============>................] - ETA: 0s - loss: 0.0095
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0095
3744/6530 [================>.............] - ETA: 0s - loss: 0.0095
4048/6530 [=================>............] - ETA: 0s - loss: 0.0096
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0096
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0096
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0095
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0095
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0094
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0094
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0095
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0094
6530/6530 [==============================] - 1s 218us/step - loss: 0.0095 - val_loss: 0.0097
Epoch 26/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0086
 320/6530 [>.............................] - ETA: 1s - loss: 0.0092
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0097
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0095
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0097
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0097
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0093
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0094
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0096
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0097
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0096
3008/6530 [============>.................] - ETA: 0s - loss: 0.0095
3264/6530 [=============>................] - ETA: 0s - loss: 0.0093
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0093
3760/6530 [================>.............] - ETA: 0s - loss: 0.0094
3984/6530 [=================>............] - ETA: 0s - loss: 0.0095
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0094
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0094
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0094
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0093
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0093
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0092
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0092
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0093
6352/6530 [============================>.] - ETA: 0s - loss: 0.0093
6530/6530 [==============================] - 1s 203us/step - loss: 0.0094 - val_loss: 0.0096
Epoch 27/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0084
 272/6530 [>.............................] - ETA: 1s - loss: 0.0092
 576/6530 [=>............................] - ETA: 1s - loss: 0.0094
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0091
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0096
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0094
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0092
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0093
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0094
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0096
2848/6530 [============>.................] - ETA: 0s - loss: 0.0094
3104/6530 [=============>................] - ETA: 0s - loss: 0.0092
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0091
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0092
3904/6530 [================>.............] - ETA: 0s - loss: 0.0092
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0092
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0093
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0092
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0091
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0091
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0091
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0091
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0091
6368/6530 [============================>.] - ETA: 0s - loss: 0.0091
6530/6530 [==============================] - 1s 195us/step - loss: 0.0092 - val_loss: 0.0096
Epoch 28/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0082
 240/6530 [>.............................] - ETA: 1s - loss: 0.0090
 480/6530 [=>............................] - ETA: 1s - loss: 0.0095
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0093
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0092
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0094
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0092
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0090
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0090
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0093
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0093
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0093
2992/6530 [============>.................] - ETA: 0s - loss: 0.0091
3216/6530 [=============>................] - ETA: 0s - loss: 0.0090
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0090
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0090
3808/6530 [================>.............] - ETA: 0s - loss: 0.0090
4016/6530 [=================>............] - ETA: 0s - loss: 0.0091
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0090
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0091
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0091
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0090
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0090
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0090
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0089
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0089
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0090
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0090
6368/6530 [============================>.] - ETA: 0s - loss: 0.0090
6530/6530 [==============================] - 2s 238us/step - loss: 0.0090 - val_loss: 0.0096
Epoch 29/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0081
 256/6530 [>.............................] - ETA: 1s - loss: 0.0090
 496/6530 [=>............................] - ETA: 1s - loss: 0.0092
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0091
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0089
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0093
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0091
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0090
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0088
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0092
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0091
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0092
2912/6530 [============>.................] - ETA: 0s - loss: 0.0090
3168/6530 [=============>................] - ETA: 0s - loss: 0.0089
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0088
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0089
3920/6530 [=================>............] - ETA: 0s - loss: 0.0089
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0089
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0089
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0089
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0089
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0088
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0088
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0088
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0088
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0088
6448/6530 [============================>.] - ETA: 0s - loss: 0.0089
6530/6530 [==============================] - 1s 216us/step - loss: 0.0089 - val_loss: 0.0095
Epoch 30/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0079
 224/6530 [>.............................] - ETA: 1s - loss: 0.0084
 448/6530 [=>............................] - ETA: 1s - loss: 0.0090
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0090
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0086
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0091
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0090
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0090
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0088
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0086
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0090
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0090
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0089
3056/6530 [=============>................] - ETA: 0s - loss: 0.0088
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0087
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0087
3872/6530 [================>.............] - ETA: 0s - loss: 0.0088
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0088
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0088
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0088
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0087
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0087
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0087
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0086
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0087
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0087
6416/6530 [============================>.] - ETA: 0s - loss: 0.0087
6530/6530 [==============================] - 1s 215us/step - loss: 0.0088 - val_loss: 0.0095
Epoch 31/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0077
 304/6530 [>.............................] - ETA: 1s - loss: 0.0085
 592/6530 [=>............................] - ETA: 1s - loss: 0.0089
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0086
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0088
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0089
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0088
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0087
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0084
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0089
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0088
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0088
2912/6530 [============>.................] - ETA: 0s - loss: 0.0087
3088/6530 [=============>................] - ETA: 0s - loss: 0.0087
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0085
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0086
3744/6530 [================>.............] - ETA: 0s - loss: 0.0086
3968/6530 [=================>............] - ETA: 0s - loss: 0.0087
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0086
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0087
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0086
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0085
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0085
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0085
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0085
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0086
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0086
6512/6530 [============================>.] - ETA: 0s - loss: 0.0086
6530/6530 [==============================] - 1s 220us/step - loss: 0.0086 - val_loss: 0.0095
Epoch 32/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0076
 288/6530 [>.............................] - ETA: 1s - loss: 0.0084
 560/6530 [=>............................] - ETA: 1s - loss: 0.0086
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0085
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0089
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0087
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0086
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0083
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0086
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0087
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0088
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0086
2992/6530 [============>.................] - ETA: 0s - loss: 0.0086
3200/6530 [=============>................] - ETA: 0s - loss: 0.0085
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0084
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0085
3808/6530 [================>.............] - ETA: 0s - loss: 0.0085
3984/6530 [=================>............] - ETA: 0s - loss: 0.0086
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0085
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0085
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0085
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0085
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0085
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0084
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0084
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0084
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0084
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0084
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0084
6384/6530 [============================>.] - ETA: 0s - loss: 0.0084
6530/6530 [==============================] - 2s 246us/step - loss: 0.0085 - val_loss: 0.0094
Epoch 33/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0074
 224/6530 [>.............................] - ETA: 1s - loss: 0.0079
 464/6530 [=>............................] - ETA: 1s - loss: 0.0088
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0085
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0084
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0086
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0085
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0082
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0084
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0085
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0087
2912/6530 [============>.................] - ETA: 0s - loss: 0.0085
3200/6530 [=============>................] - ETA: 0s - loss: 0.0083
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0083
3792/6530 [================>.............] - ETA: 0s - loss: 0.0083
4096/6530 [=================>............] - ETA: 0s - loss: 0.0084
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0084
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0084
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0083
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0083
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0083
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0082
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0083
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0083
6320/6530 [============================>.] - ETA: 0s - loss: 0.0083
6528/6530 [============================>.] - ETA: 0s - loss: 0.0084
6530/6530 [==============================] - 1s 207us/step - loss: 0.0084 - val_loss: 0.0094
Epoch 34/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0073
 240/6530 [>.............................] - ETA: 1s - loss: 0.0081
 416/6530 [>.............................] - ETA: 1s - loss: 0.0084
 608/6530 [=>............................] - ETA: 1s - loss: 0.0085
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0081
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0082
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0084
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0083
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0083
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0081
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0084
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0084
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0085
2928/6530 [============>.................] - ETA: 0s - loss: 0.0083
3184/6530 [=============>................] - ETA: 0s - loss: 0.0082
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0082
3760/6530 [================>.............] - ETA: 0s - loss: 0.0082
4048/6530 [=================>............] - ETA: 0s - loss: 0.0083
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0082
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0083
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0082
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0082
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0081
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0081
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0081
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0082
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0082
6352/6530 [============================>.] - ETA: 0s - loss: 0.0082
6528/6530 [============================>.] - ETA: 0s - loss: 0.0082
6530/6530 [==============================] - 2s 232us/step - loss: 0.0082 - val_loss: 0.0094
Epoch 35/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0071
 224/6530 [>.............................] - ETA: 1s - loss: 0.0077
 416/6530 [>.............................] - ETA: 1s - loss: 0.0083
 592/6530 [=>............................] - ETA: 1s - loss: 0.0084
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0080
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0081
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0084
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0083
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0083
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0080
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0080
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0083
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0083
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0084
2880/6530 [============>.................] - ETA: 0s - loss: 0.0082
3088/6530 [=============>................] - ETA: 0s - loss: 0.0082
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0081
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0081
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0081
3904/6530 [================>.............] - ETA: 0s - loss: 0.0081
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0081
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0082
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0081
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0080
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0080
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0080
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0080
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0081
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0081
6530/6530 [==============================] - 2s 237us/step - loss: 0.0081 - val_loss: 0.0094
Epoch 36/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0070
 272/6530 [>.............................] - ETA: 1s - loss: 0.0081
 544/6530 [=>............................] - ETA: 1s - loss: 0.0081
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0079
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0080
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0082
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0081
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0081
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0079
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0080
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0082
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0083
2864/6530 [============>.................] - ETA: 0s - loss: 0.0081
3136/6530 [=============>................] - ETA: 0s - loss: 0.0080
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0080
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0080
3984/6530 [=================>............] - ETA: 0s - loss: 0.0081
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0080
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0081
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0080
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0079
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0079
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0079
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0079
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0079
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0079
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0079
6384/6530 [============================>.] - ETA: 0s - loss: 0.0080
6530/6530 [==============================] - 1s 225us/step - loss: 0.0080 - val_loss: 0.0094
Epoch 37/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0068
 320/6530 [>.............................] - ETA: 1s - loss: 0.0078
 624/6530 [=>............................] - ETA: 1s - loss: 0.0082
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0077
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0081
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0080
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0077
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0081
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0081
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0081
2944/6530 [============>.................] - ETA: 0s - loss: 0.0080
3200/6530 [=============>................] - ETA: 0s - loss: 0.0079
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0079
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0079
3856/6530 [================>.............] - ETA: 0s - loss: 0.0079
4048/6530 [=================>............] - ETA: 0s - loss: 0.0080
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0079
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0079
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0079
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0079
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0078
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0078
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0078
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0078
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0078
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0078
6400/6530 [============================>.] - ETA: 0s - loss: 0.0078
6530/6530 [==============================] - 1s 219us/step - loss: 0.0079 - val_loss: 0.0093
Epoch 38/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0067
 208/6530 [..............................] - ETA: 1s - loss: 0.0074
 400/6530 [>.............................] - ETA: 1s - loss: 0.0080
 608/6530 [=>............................] - ETA: 1s - loss: 0.0081
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0076
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0077
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0080
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0079
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0078
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0076
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0078
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0079
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0080
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0080
2944/6530 [============>.................] - ETA: 0s - loss: 0.0079
3184/6530 [=============>................] - ETA: 0s - loss: 0.0077
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0077
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0078
3872/6530 [================>.............] - ETA: 0s - loss: 0.0078
4064/6530 [=================>............] - ETA: 0s - loss: 0.0078
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0078
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0078
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0078
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0077
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0077
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0077
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0077
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0077
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0077
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0077
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0077
6432/6530 [============================>.] - ETA: 0s - loss: 0.0078
6530/6530 [==============================] - 2s 258us/step - loss: 0.0078 - val_loss: 0.0093
Epoch 39/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0066
 288/6530 [>.............................] - ETA: 1s - loss: 0.0075
 544/6530 [=>............................] - ETA: 1s - loss: 0.0078
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0076
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0075
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0079
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0078
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0077
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0076
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0075
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0077
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0078
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0079
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0078
3008/6530 [============>.................] - ETA: 0s - loss: 0.0077
3248/6530 [=============>................] - ETA: 0s - loss: 0.0076
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0077
3728/6530 [================>.............] - ETA: 0s - loss: 0.0076
4000/6530 [=================>............] - ETA: 0s - loss: 0.0078
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0077
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0077
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0077
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0076
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0076
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0076
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0076
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0076
6530/6530 [==============================] - 1s 218us/step - loss: 0.0077 - val_loss: 0.0093
Epoch 40/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0065
 304/6530 [>.............................] - ETA: 1s - loss: 0.0076
 624/6530 [=>............................] - ETA: 0s - loss: 0.0079
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0074
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0078
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0077
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0074
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0077
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0078
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0078
3024/6530 [============>.................] - ETA: 0s - loss: 0.0077
3264/6530 [=============>................] - ETA: 0s - loss: 0.0076
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0076
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0076
3888/6530 [================>.............] - ETA: 0s - loss: 0.0076
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0076
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0076
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0076
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0075
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0075
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0075
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0075
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0075
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0075
6368/6530 [============================>.] - ETA: 0s - loss: 0.0075
6530/6530 [==============================] - 1s 202us/step - loss: 0.0076 - val_loss: 0.0093
Epoch 41/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0063
 240/6530 [>.............................] - ETA: 1s - loss: 0.0073
 512/6530 [=>............................] - ETA: 1s - loss: 0.0077
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0073
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0077
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0076
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0075
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0073
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0076
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0077
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0076
3040/6530 [============>.................] - ETA: 0s - loss: 0.0075
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0074
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0075
3904/6530 [================>.............] - ETA: 0s - loss: 0.0075
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0075
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0075
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0075
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0074
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0074
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0074
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0074
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0074
6400/6530 [============================>.] - ETA: 0s - loss: 0.0074
6530/6530 [==============================] - 1s 193us/step - loss: 0.0075 - val_loss: 0.0093
Epoch 42/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0062
 240/6530 [>.............................] - ETA: 1s - loss: 0.0074
 480/6530 [=>............................] - ETA: 1s - loss: 0.0079
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0075
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0073
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0077
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0075
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0075
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0073
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0074
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0075
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0076
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0076
2976/6530 [============>.................] - ETA: 0s - loss: 0.0075
3264/6530 [=============>................] - ETA: 0s - loss: 0.0074
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0074
3792/6530 [================>.............] - ETA: 0s - loss: 0.0074
4064/6530 [=================>............] - ETA: 0s - loss: 0.0074
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0074
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0074
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0073
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0073
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0073
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0073
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0073
6400/6530 [============================>.] - ETA: 0s - loss: 0.0074
6530/6530 [==============================] - 1s 210us/step - loss: 0.0074 - val_loss: 0.0093
Epoch 43/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0061
 288/6530 [>.............................] - ETA: 1s - loss: 0.0072
 528/6530 [=>............................] - ETA: 1s - loss: 0.0074
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0072
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0072
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0075
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0074
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0074
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0071
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0073
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0074
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0075
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0075
3040/6530 [============>.................] - ETA: 0s - loss: 0.0074
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0073
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0073
3808/6530 [================>.............] - ETA: 0s - loss: 0.0073
4096/6530 [=================>............] - ETA: 0s - loss: 0.0073
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0073
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0073
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0072
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0072
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0072
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0072
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0072
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0073
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0072
6432/6530 [============================>.] - ETA: 0s - loss: 0.0073
6530/6530 [==============================] - 1s 229us/step - loss: 0.0073 - val_loss: 0.0093
Epoch 44/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0060
 208/6530 [..............................] - ETA: 1s - loss: 0.0069
 400/6530 [>.............................] - ETA: 1s - loss: 0.0075
 592/6530 [=>............................] - ETA: 1s - loss: 0.0075
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0071
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0070
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0075
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0073
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0073
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0072
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0070
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0072
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0073
2512/6530 [==========>...................] - ETA: 1s - loss: 0.0074
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0074
2944/6530 [============>.................] - ETA: 0s - loss: 0.0073
3168/6530 [=============>................] - ETA: 0s - loss: 0.0072
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0072
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0072
3872/6530 [================>.............] - ETA: 0s - loss: 0.0072
4096/6530 [=================>............] - ETA: 0s - loss: 0.0072
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0072
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0072
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0072
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0071
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0071
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0071
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0071
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0071
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0072
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0072
6448/6530 [============================>.] - ETA: 0s - loss: 0.0072
6530/6530 [==============================] - 2s 260us/step - loss: 0.0072 - val_loss: 0.0093
Epoch 45/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0059
 240/6530 [>.............................] - ETA: 1s - loss: 0.0069
 480/6530 [=>............................] - ETA: 1s - loss: 0.0076
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0072
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0069
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0073
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0072
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0071
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0069
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0071
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0072
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0073
2832/6530 [============>.................] - ETA: 0s - loss: 0.0073
3024/6530 [============>.................] - ETA: 0s - loss: 0.0072
3232/6530 [=============>................] - ETA: 0s - loss: 0.0071
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0071
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0071
3824/6530 [================>.............] - ETA: 0s - loss: 0.0071
4032/6530 [=================>............] - ETA: 0s - loss: 0.0072
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0071
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0071
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0071
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0071
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0070
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0070
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0070
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0071
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0071
6448/6530 [============================>.] - ETA: 0s - loss: 0.0071
6530/6530 [==============================] - 2s 234us/step - loss: 0.0071 - val_loss: 0.0093
Epoch 46/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0058
 256/6530 [>.............................] - ETA: 1s - loss: 0.0073
 528/6530 [=>............................] - ETA: 1s - loss: 0.0073
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0069
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0071
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0072
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0071
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0070
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0069
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0072
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0072
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0072
2864/6530 [============>.................] - ETA: 0s - loss: 0.0072
3088/6530 [=============>................] - ETA: 0s - loss: 0.0071
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0070
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0071
3856/6530 [================>.............] - ETA: 0s - loss: 0.0071
4128/6530 [=================>............] - ETA: 0s - loss: 0.0071
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0071
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0070
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0070
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0070
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0069
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0070
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0070
6480/6530 [============================>.] - ETA: 0s - loss: 0.0070
6530/6530 [==============================] - 1s 207us/step - loss: 0.0071 - val_loss: 0.0093
Epoch 47/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0057
 304/6530 [>.............................] - ETA: 1s - loss: 0.0069
 592/6530 [=>............................] - ETA: 1s - loss: 0.0072
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0067
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0070
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0071
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0070
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0069
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0068
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0070
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0071
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0071
2848/6530 [============>.................] - ETA: 0s - loss: 0.0071
3120/6530 [=============>................] - ETA: 0s - loss: 0.0070
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0070
3728/6530 [================>.............] - ETA: 0s - loss: 0.0069
4000/6530 [=================>............] - ETA: 0s - loss: 0.0070
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0069
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0069
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0069
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0069
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0069
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0068
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0069
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0069
6530/6530 [==============================] - 1s 204us/step - loss: 0.0070 - val_loss: 0.0093
Epoch 48/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0057
 256/6530 [>.............................] - ETA: 1s - loss: 0.0071
 496/6530 [=>............................] - ETA: 1s - loss: 0.0073
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0069
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0069
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0070
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0070
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0068
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0069
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0070
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0071
2880/6530 [============>.................] - ETA: 0s - loss: 0.0070
3104/6530 [=============>................] - ETA: 0s - loss: 0.0069
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0069
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0069
3728/6530 [================>.............] - ETA: 0s - loss: 0.0069
3904/6530 [================>.............] - ETA: 0s - loss: 0.0069
4096/6530 [=================>............] - ETA: 0s - loss: 0.0069
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0069
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0069
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0068
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0068
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0068
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0068
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0068
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0068
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0068
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0069
6512/6530 [============================>.] - ETA: 0s - loss: 0.0069
6530/6530 [==============================] - 1s 229us/step - loss: 0.0069 - val_loss: 0.0093
Epoch 49/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0055
 320/6530 [>.............................] - ETA: 1s - loss: 0.0068
 608/6530 [=>............................] - ETA: 1s - loss: 0.0070
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0066
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0069
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0069
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0068
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0067
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0066
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0067
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0069
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0069
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0069
2944/6530 [============>.................] - ETA: 0s - loss: 0.0069
3136/6530 [=============>................] - ETA: 0s - loss: 0.0068
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0068
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0068
3744/6530 [================>.............] - ETA: 0s - loss: 0.0068
3984/6530 [=================>............] - ETA: 0s - loss: 0.0068
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0068
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0068
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0068
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0067
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0067
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0067
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0067
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0067
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0067
6432/6530 [============================>.] - ETA: 0s - loss: 0.0068
6530/6530 [==============================] - 2s 233us/step - loss: 0.0068 - val_loss: 0.0094

# training | RMSE: 0.0804, MAE: 0.0634
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14008507478600118}, 'layer_2_size': 32, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 88, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.08041192755728124, 'rmse': 0.08041192755728124, 'mae': 0.06343755121335048, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#1 epoch=81.0 loss={'loss': 0.17509792431590374, 'rmse': 0.17509792431590374, 'mae': 0.14375044991703162, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3881724853455274}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28576535767268696}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#0 epoch=81.0 loss={'loss': 0.08041192755728124, 'rmse': 0.08041192755728124, 'mae': 0.06343755121335048, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14008507478600118}, 'layer_2_size': 32, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 88, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
get a list [results] of length 201
get a list [loss] of length 2
get a list [val_loss] of length 2
length of indices is (0, 1)
length of indices is 2
length of T is 2
s=0
T is of size 5
T=[{'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24230766296308875}, 'layer_2_size': 81, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26425376474083073}, 'layer_4_size': 23, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3683057751741984}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 67, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2672256338472979}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10409852991432521}, 'layer_4_size': 43, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20537784995900182}, 'layer_2_size': 39, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3182253766971461}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27793970530039186}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3014747197109999}, 'layer_2_size': 79, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37619115972170725}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.432264982318203}, 'layer_4_size': 18, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31335263854200923}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]
newly formed T structure is:[[0, 81, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24230766296308875}, 'layer_2_size': 81, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26425376474083073}, 'layer_4_size': 23, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [1, 81, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3683057751741984}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 67, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2672256338472979}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10409852991432521}, 'layer_4_size': 43, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [2, 81, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20537784995900182}, 'layer_2_size': 39, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3182253766971461}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [3, 81, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27793970530039186}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [4, 81, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3014747197109999}, 'layer_2_size': 79, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37619115972170725}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.432264982318203}, 'layer_4_size': 18, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31335263854200923}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]] 

*** 5 configurations x 81.0 iterations each

1 | Thu Sep 27 23:12:33 2018 | lowest loss so far: 0.0675 (run 0)

{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  12 | activation: sigmoid | extras: dropout - rate: 36.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/81

 128/6530 [..............................] - ETA: 40s - loss: 0.5483
3456/6530 [==============>...............] - ETA: 0s - loss: 0.3830 
6530/6530 [==============================] - 1s 141us/step - loss: 0.3146 - val_loss: 0.2298
Epoch 2/81

 128/6530 [..............................] - ETA: 0s - loss: 0.2578
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2331
6530/6530 [==============================] - 0s 11us/step - loss: 0.2321 - val_loss: 0.2259
Epoch 3/81

 128/6530 [..............................] - ETA: 0s - loss: 0.2502
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2272
6530/6530 [==============================] - 0s 10us/step - loss: 0.2281 - val_loss: 0.2229
Epoch 4/81

 128/6530 [..............................] - ETA: 0s - loss: 0.2438
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2258
6530/6530 [==============================] - 0s 10us/step - loss: 0.2252 - val_loss: 0.2198
Epoch 5/81

 128/6530 [..............................] - ETA: 0s - loss: 0.2349
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2221
6530/6530 [==============================] - 0s 10us/step - loss: 0.2221 - val_loss: 0.2169
Epoch 6/81

 128/6530 [..............................] - ETA: 0s - loss: 0.2172{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  56 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  81 | activation: relu    | extras: dropout - rate: 24.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

 256/6530 [>.............................] - ETA: 23s - loss: 1.7339
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2200
6144/6530 [===========================>..] - ETA: 0s - loss: 0.4136 
6530/6530 [==============================] - 0s 10us/step - loss: 0.2190 - val_loss: 0.2137
Epoch 7/81

 128/6530 [..............................] - ETA: 0s - loss: 0.2244
6530/6530 [==============================] - 1s 162us/step - loss: 0.3973 - val_loss: 0.1201
Epoch 2/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1104
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2161
6530/6530 [==============================] - 0s 10us/step - loss: 0.2159 - val_loss: 0.2106
Epoch 8/81

 128/6530 [..............................] - ETA: 0s - loss: 0.2066
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1069
6530/6530 [==============================] - 0s 10us/step - loss: 0.1054 - val_loss: 0.0909
Epoch 3/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0707{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  63 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  39 | activation: sigmoid | extras: dropout - rate: 20.5% 
layer 3 | size:  71 | activation: sigmoid | extras: None 
layer 4 | size:   7 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f4681b45240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 5:59 - loss: 0.6531
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2125
6530/6530 [==============================] - 0s 11us/step - loss: 0.2130 - val_loss: 0.2072
Epoch 9/81

 128/6530 [..............................] - ETA: 0s - loss: 0.2139
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0822
 336/6530 [>.............................] - ETA: 17s - loss: 0.2653 
6530/6530 [==============================] - 0s 10us/step - loss: 0.0821 - val_loss: 0.0775
Epoch 4/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0889
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2123
 656/6530 [==>...........................] - ETA: 8s - loss: 0.2355 
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0729
6530/6530 [==============================] - 0s 11us/step - loss: 0.2098 - val_loss: 0.2041
Epoch 10/81

 128/6530 [..............................] - ETA: 0s - loss: 0.2221
6530/6530 [==============================] - 0s 10us/step - loss: 0.0731 - val_loss: 0.0719
Epoch 5/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0701
 992/6530 [===>..........................] - ETA: 5s - loss: 0.2189
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2068
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0679
6530/6530 [==============================] - 0s 10us/step - loss: 0.0677 - val_loss: 0.0686

6530/6530 [==============================] - 0s 11us/step - loss: 0.2066 - val_loss: 0.2009
Epoch 11/81

 128/6530 [..............................] - ETA: 0s - loss: 0.2035Epoch 6/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0543
1328/6530 [=====>........................] - ETA: 4s - loss: 0.2042
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2038
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0640
6530/6530 [==============================] - 0s 10us/step - loss: 0.0643 - val_loss: 0.0641
Epoch 7/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0550
1648/6530 [======>.......................] - ETA: 3s - loss: 0.1965
6530/6530 [==============================] - 0s 12us/step - loss: 0.2036 - val_loss: 0.1981
Epoch 12/81

 128/6530 [..............................] - ETA: 0s - loss: 0.2080
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0610
1968/6530 [========>.....................] - ETA: 2s - loss: 0.1919
6530/6530 [==============================] - 0s 10us/step - loss: 0.0609 - val_loss: 0.0623
Epoch 8/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0624
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2016
6530/6530 [==============================] - 0s 11us/step - loss: 0.2010 - val_loss: 0.1961
Epoch 13/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1851
2304/6530 [=========>....................] - ETA: 2s - loss: 0.1882
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0581
6530/6530 [==============================] - 0s 10us/step - loss: 0.0578 - val_loss: 0.0605
Epoch 9/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0549
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1992
6530/6530 [==============================] - 0s 11us/step - loss: 0.1989 - val_loss: 0.1940
Epoch 14/81

 128/6530 [..............................] - ETA: 0s - loss: 0.2370
2656/6530 [===========>..................] - ETA: 1s - loss: 0.1868
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0544
6530/6530 [==============================] - 0s 9us/step - loss: 0.0543 - val_loss: 0.0580
Epoch 10/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0616
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1966
2976/6530 [============>.................] - ETA: 1s - loss: 0.1842
6530/6530 [==============================] - 0s 11us/step - loss: 0.1967 - val_loss: 0.1919
Epoch 15/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1813
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0520
6530/6530 [==============================] - 0s 9us/step - loss: 0.0520 - val_loss: 0.0551
Epoch 11/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0466
3328/6530 [==============>...............] - ETA: 1s - loss: 0.1821
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1952
6530/6530 [==============================] - 0s 11us/step - loss: 0.1947 - val_loss: 0.1900
Epoch 16/81

 128/6530 [..............................] - ETA: 0s - loss: 0.2068
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0498
6530/6530 [==============================] - 0s 9us/step - loss: 0.0499 - val_loss: 0.0543
Epoch 12/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0509
3664/6530 [===============>..............] - ETA: 1s - loss: 0.1808
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1922
6530/6530 [==============================] - 0s 10us/step - loss: 0.1928 - val_loss: 0.1882
Epoch 17/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1837
6400/6530 [============================>.] - ETA: 0s - loss: 0.0483
4000/6530 [=================>............] - ETA: 0s - loss: 0.1800
6530/6530 [==============================] - 0s 9us/step - loss: 0.0484 - val_loss: 0.0521
Epoch 13/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0488
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1904
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1790
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0462
6530/6530 [==============================] - 0s 9us/step - loss: 0.0461 - val_loss: 0.0498

6530/6530 [==============================] - 0s 11us/step - loss: 0.1909 - val_loss: 0.1864
Epoch 18/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1725Epoch 14/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0493
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1789
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0455
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1887
6530/6530 [==============================] - 0s 9us/step - loss: 0.0453 - val_loss: 0.0475
Epoch 15/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0370
6530/6530 [==============================] - 0s 11us/step - loss: 0.1889 - val_loss: 0.1844
Epoch 19/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1741
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1778
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0427
6530/6530 [==============================] - 0s 9us/step - loss: 0.0429 - val_loss: 0.0475

5504/6530 [========================>.....] - ETA: 0s - loss: 0.1873Epoch 16/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0464
6530/6530 [==============================] - 0s 10us/step - loss: 0.1869 - val_loss: 0.1825
Epoch 20/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1857
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1776
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0413
6530/6530 [==============================] - 0s 9us/step - loss: 0.0413 - val_loss: 0.0464
Epoch 17/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0456
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1850
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1765
6530/6530 [==============================] - 0s 10us/step - loss: 0.1850 - val_loss: 0.1806
Epoch 21/81

 128/6530 [..............................] - ETA: 0s - loss: 0.2064
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0405
6530/6530 [==============================] - 0s 10us/step - loss: 0.0406 - val_loss: 0.0450
Epoch 18/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0401
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1761
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1842
6530/6530 [==============================] - 0s 10us/step - loss: 0.1836 - val_loss: 0.1790
Epoch 22/81

 128/6530 [..............................] - ETA: 0s - loss: 0.2044
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0390
6400/6530 [============================>.] - ETA: 0s - loss: 0.1751
6530/6530 [==============================] - 0s 11us/step - loss: 0.0388 - val_loss: 0.0433
Epoch 19/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0346
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1817
6530/6530 [==============================] - 0s 11us/step - loss: 0.1819 - val_loss: 0.1775
Epoch 23/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1874
6400/6530 [============================>.] - ETA: 0s - loss: 0.0370
6530/6530 [==============================] - 0s 9us/step - loss: 0.0371 - val_loss: 0.0426
Epoch 20/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0323
6530/6530 [==============================] - 2s 298us/step - loss: 0.1748 - val_loss: 0.1634
Epoch 2/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1930
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1800
6530/6530 [==============================] - 0s 10us/step - loss: 0.1804 - val_loss: 0.1761
Epoch 24/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1746
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0361
6530/6530 [==============================] - 0s 9us/step - loss: 0.0363 - val_loss: 0.0406

 368/6530 [>.............................] - ETA: 0s - loss: 0.1669Epoch 21/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0384
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1797
6530/6530 [==============================] - 0s 10us/step - loss: 0.1788 - val_loss: 0.1746
Epoch 25/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1612
 720/6530 [==>...........................] - ETA: 0s - loss: 0.1749
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0348
6530/6530 [==============================] - 0s 10us/step - loss: 0.0348 - val_loss: 0.0406
Epoch 22/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0373
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1761
6530/6530 [==============================] - 0s 11us/step - loss: 0.1774 - val_loss: 0.1732

1072/6530 [===>..........................] - ETA: 0s - loss: 0.1751Epoch 26/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1712
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0341
6530/6530 [==============================] - 0s 10us/step - loss: 0.0341 - val_loss: 0.0398
Epoch 23/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0429
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1706
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1764
6530/6530 [==============================] - 0s 12us/step - loss: 0.1763 - val_loss: 0.1719
Epoch 27/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1802
6400/6530 [============================>.] - ETA: 0s - loss: 0.0336
6530/6530 [==============================] - 0s 9us/step - loss: 0.0336 - val_loss: 0.0386
Epoch 24/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0326
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1675
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1750
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0326
6530/6530 [==============================] - 0s 10us/step - loss: 0.1745 - val_loss: 0.1705

6530/6530 [==============================] - 0s 9us/step - loss: 0.0324 - val_loss: 0.0377
Epoch 28/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1733Epoch 25/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0292
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1674
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1738
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0313
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1679
6530/6530 [==============================] - 0s 10us/step - loss: 0.0313 - val_loss: 0.0355
Epoch 26/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0284
6530/6530 [==============================] - 0s 11us/step - loss: 0.1735 - val_loss: 0.1694
Epoch 29/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1543
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1676
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0304
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1728
6530/6530 [==============================] - 0s 10us/step - loss: 0.0307 - val_loss: 0.0347
Epoch 27/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0364
6530/6530 [==============================] - 0s 11us/step - loss: 0.1721 - val_loss: 0.1683
Epoch 30/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1786
3120/6530 [=============>................] - ETA: 0s - loss: 0.1663
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0299
6530/6530 [==============================] - 0s 9us/step - loss: 0.0298 - val_loss: 0.0347
Epoch 28/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0306
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1710
6530/6530 [==============================] - 0s 10us/step - loss: 0.1710 - val_loss: 0.1674
Epoch 31/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1473
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1672
6400/6530 [============================>.] - ETA: 0s - loss: 0.0291
6530/6530 [==============================] - 0s 9us/step - loss: 0.0291 - val_loss: 0.0342
Epoch 29/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0295
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1701
3792/6530 [================>.............] - ETA: 0s - loss: 0.1667
6530/6530 [==============================] - 0s 10us/step - loss: 0.1701 - val_loss: 0.1665
Epoch 32/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1457
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0279
6530/6530 [==============================] - 0s 9us/step - loss: 0.0280 - val_loss: 0.0333
Epoch 30/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0259
4128/6530 [=================>............] - ETA: 0s - loss: 0.1673
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1690
6530/6530 [==============================] - 0s 10us/step - loss: 0.1692 - val_loss: 0.1655
Epoch 33/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1733
6400/6530 [============================>.] - ETA: 0s - loss: 0.0270
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1679
6530/6530 [==============================] - 0s 9us/step - loss: 0.0270 - val_loss: 0.0325
Epoch 31/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0257
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1690
6530/6530 [==============================] - 0s 11us/step - loss: 0.1682 - val_loss: 0.1649
Epoch 34/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1615
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1675
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0268
6530/6530 [==============================] - 0s 9us/step - loss: 0.0268 - val_loss: 0.0337
Epoch 32/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0257
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1670
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1673
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0255
6530/6530 [==============================] - 0s 11us/step - loss: 0.1673 - val_loss: 0.1640
Epoch 35/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1663
6530/6530 [==============================] - 0s 12us/step - loss: 0.0261 - val_loss: 0.0314
Epoch 33/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0259
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1672
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1667
6530/6530 [==============================] - 0s 10us/step - loss: 0.1664 - val_loss: 0.1635
Epoch 36/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1583
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0259
6530/6530 [==============================] - 0s 9us/step - loss: 0.0259 - val_loss: 0.0309
Epoch 34/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0261
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1670
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1664
6530/6530 [==============================] - 0s 10us/step - loss: 0.1657 - val_loss: 0.1632

6144/6530 [===========================>..] - ETA: 0s - loss: 0.0249Epoch 37/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1705
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1670
6530/6530 [==============================] - 0s 9us/step - loss: 0.0250 - val_loss: 0.0309
Epoch 35/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0257
6448/6530 [============================>.] - ETA: 0s - loss: 0.1666
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1663
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0248
6530/6530 [==============================] - 0s 9us/step - loss: 0.0247 - val_loss: 0.0297
Epoch 36/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0219
6530/6530 [==============================] - 0s 11us/step - loss: 0.1653 - val_loss: 0.1626
Epoch 38/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1510
6530/6530 [==============================] - 1s 156us/step - loss: 0.1664 - val_loss: 0.1628
Epoch 3/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1997
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0244
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1643
6530/6530 [==============================] - 0s 9us/step - loss: 0.0244 - val_loss: 0.0283
Epoch 37/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0202
6530/6530 [==============================] - 0s 11us/step - loss: 0.1646 - val_loss: 0.1622
Epoch 39/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1486
 352/6530 [>.............................] - ETA: 0s - loss: 0.1648
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0232
6530/6530 [==============================] - 0s 10us/step - loss: 0.0231 - val_loss: 0.0286
Epoch 38/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0208
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1640
 688/6530 [==>...........................] - ETA: 0s - loss: 0.1737
6530/6530 [==============================] - 0s 10us/step - loss: 0.1641 - val_loss: 0.1619
Epoch 40/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1648
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0233
6530/6530 [==============================] - 0s 10us/step - loss: 0.0235 - val_loss: 0.0281
Epoch 39/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0227
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1751
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1642
6530/6530 [==============================] - 0s 11us/step - loss: 0.1636 - val_loss: 0.1616
Epoch 41/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1648
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0225
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1694
6530/6530 [==============================] - 0s 9us/step - loss: 0.0225 - val_loss: 0.0266
Epoch 40/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0220
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1633
6530/6530 [==============================] - 0s 10us/step - loss: 0.1632 - val_loss: 0.1613
Epoch 42/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1826
1712/6530 [======>.......................] - ETA: 0s - loss: 0.1670
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0222
6530/6530 [==============================] - 0s 10us/step - loss: 0.0222 - val_loss: 0.0274
Epoch 41/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0182
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1638
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1669
6530/6530 [==============================] - 0s 11us/step - loss: 0.1629 - val_loss: 0.1611
Epoch 43/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1645
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0215
6530/6530 [==============================] - 0s 9us/step - loss: 0.0215 - val_loss: 0.0254
Epoch 42/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0234
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1666
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1634
6530/6530 [==============================] - 0s 10us/step - loss: 0.1625 - val_loss: 0.1610
Epoch 44/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1700
6400/6530 [============================>.] - ETA: 0s - loss: 0.0208
6530/6530 [==============================] - 0s 9us/step - loss: 0.0208 - val_loss: 0.0252
Epoch 43/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0176
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1665
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1634
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0204
6530/6530 [==============================] - 0s 11us/step - loss: 0.1623 - val_loss: 0.1608
Epoch 45/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1745
6530/6530 [==============================] - 0s 10us/step - loss: 0.0206 - val_loss: 0.0249
Epoch 44/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0175
3088/6530 [=============>................] - ETA: 0s - loss: 0.1651
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1613
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0202
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1658
6530/6530 [==============================] - 0s 9us/step - loss: 0.0202 - val_loss: 0.0248
Epoch 45/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0191
6530/6530 [==============================] - 0s 11us/step - loss: 0.1621 - val_loss: 0.1608
Epoch 46/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1551
3760/6530 [================>.............] - ETA: 0s - loss: 0.1654
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0204
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1610
6530/6530 [==============================] - 0s 10us/step - loss: 0.0206 - val_loss: 0.0239
Epoch 46/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0213
6530/6530 [==============================] - 0s 11us/step - loss: 0.1618 - val_loss: 0.1606
Epoch 47/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1515
4112/6530 [=================>............] - ETA: 0s - loss: 0.1660
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0197
6530/6530 [==============================] - 0s 10us/step - loss: 0.0199 - val_loss: 0.0235
Epoch 47/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0187
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1616
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1663
6530/6530 [==============================] - 0s 11us/step - loss: 0.1617 - val_loss: 0.1606
Epoch 48/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1770
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0190
6530/6530 [==============================] - 0s 9us/step - loss: 0.0193 - val_loss: 0.0230
Epoch 48/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0190
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1662
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1612
6530/6530 [==============================] - 0s 10us/step - loss: 0.1614 - val_loss: 0.1605
Epoch 49/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1726
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0187
6530/6530 [==============================] - 0s 9us/step - loss: 0.0188 - val_loss: 0.0232

5120/6530 [======================>.......] - ETA: 0s - loss: 0.1661Epoch 49/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0204
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1624
6530/6530 [==============================] - 0s 10us/step - loss: 0.1612 - val_loss: 0.1605
Epoch 50/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1570
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1660
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0183
6530/6530 [==============================] - 0s 9us/step - loss: 0.0183 - val_loss: 0.0232
Epoch 50/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0190
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1616
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1658
6530/6530 [==============================] - 0s 10us/step - loss: 0.1611 - val_loss: 0.1605
Epoch 51/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1564
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0188
6530/6530 [==============================] - 0s 9us/step - loss: 0.0187 - val_loss: 0.0238
Epoch 51/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0134
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1658
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1611
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0182
6530/6530 [==============================] - 0s 11us/step - loss: 0.1612 - val_loss: 0.1604
Epoch 52/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1818
6530/6530 [==============================] - 0s 9us/step - loss: 0.0184 - val_loss: 0.0224
Epoch 52/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0165
6480/6530 [============================>.] - ETA: 0s - loss: 0.1653
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1607
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0180
6530/6530 [==============================] - 0s 9us/step - loss: 0.0180 - val_loss: 0.0217

6530/6530 [==============================] - 0s 10us/step - loss: 0.1607 - val_loss: 0.1605
Epoch 53/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0156Epoch 53/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1531
6530/6530 [==============================] - 1s 155us/step - loss: 0.1653 - val_loss: 0.1613
Epoch 4/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1940
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0181
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1601
6530/6530 [==============================] - 0s 9us/step - loss: 0.0180 - val_loss: 0.0209

 352/6530 [>.............................] - ETA: 1s - loss: 0.1656Epoch 54/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0167
6530/6530 [==============================] - 0s 10us/step - loss: 0.1607 - val_loss: 0.1604
Epoch 54/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1617
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0173
 688/6530 [==>...........................] - ETA: 0s - loss: 0.1727
6530/6530 [==============================] - 0s 9us/step - loss: 0.0173 - val_loss: 0.0202

5376/6530 [=======================>......] - ETA: 0s - loss: 0.1593Epoch 55/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0203
6530/6530 [==============================] - 0s 10us/step - loss: 0.1606 - val_loss: 0.1605
Epoch 55/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1514
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1725
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0172
6530/6530 [==============================] - 0s 10us/step - loss: 0.0174 - val_loss: 0.0209
Epoch 56/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0148
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1600
6530/6530 [==============================] - 0s 11us/step - loss: 0.1606 - val_loss: 0.1605
Epoch 56/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1772
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1668
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0165
6530/6530 [==============================] - 0s 10us/step - loss: 0.0165 - val_loss: 0.0213
Epoch 57/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0142
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1597
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1650
6530/6530 [==============================] - 0s 10us/step - loss: 0.1604 - val_loss: 0.1606
Epoch 57/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1641
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0164
6530/6530 [==============================] - 0s 9us/step - loss: 0.0165 - val_loss: 0.0200
Epoch 58/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0136
2064/6530 [========>.....................] - ETA: 0s - loss: 0.1649
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1603
6530/6530 [==============================] - 0s 11us/step - loss: 0.1603 - val_loss: 0.1605
Epoch 58/81

 128/6530 [..............................] - ETA: 0s - loss: 0.1397
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0163
6530/6530 [==============================] - 0s 9us/step - loss: 0.0163 - val_loss: 0.0216
Epoch 59/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0152
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1647
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1607
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0162
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1651
6530/6530 [==============================] - 0s 12us/step - loss: 0.1603 - val_loss: 0.1607

6530/6530 [==============================] - 0s 9us/step - loss: 0.0161 - val_loss: 0.0208
Epoch 60/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0176
3072/6530 [=============>................] - ETA: 0s - loss: 0.1634
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0159
6530/6530 [==============================] - 0s 9us/step - loss: 0.0158 - val_loss: 0.0204
Epoch 61/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0169
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1641
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0155
6530/6530 [==============================] - 0s 9us/step - loss: 0.0156 - val_loss: 0.0196
Epoch 62/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0164
3696/6530 [===============>..............] - ETA: 0s - loss: 0.1639
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0149
6530/6530 [==============================] - 0s 9us/step - loss: 0.0149 - val_loss: 0.0186
Epoch 63/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0116
4016/6530 [=================>............] - ETA: 0s - loss: 0.1640
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0148
6530/6530 [==============================] - 0s 10us/step - loss: 0.0148 - val_loss: 0.0186
Epoch 64/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0137
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1642
# training | RMSE: 0.2038, MAE: 0.1602
worker 1  xfile  [1, 81, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3683057751741984}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 67, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2672256338472979}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10409852991432521}, 'layer_4_size': 43, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.20384992456194864, 'rmse': 0.20384992456194864, 'mae': 0.16019485835484962, 'early_stop': True}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  89 | activation: tanh    | extras: dropout - rate: 27.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8aa7b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  32/6530 [..............................] - ETA: 20s - loss: 0.5673
6400/6530 [============================>.] - ETA: 0s - loss: 0.0156
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1650
6530/6530 [==============================] - 0s 9us/step - loss: 0.0157 - val_loss: 0.0184
Epoch 65/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0138
1440/6530 [=====>........................] - ETA: 0s - loss: 0.3523 
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1649
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0153
6530/6530 [==============================] - 0s 9us/step - loss: 0.0151 - val_loss: 0.0179
Epoch 66/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0140
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1650
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0146
6530/6530 [==============================] - 0s 9us/step - loss: 0.0146 - val_loss: 0.0181
Epoch 67/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0124
2400/6530 [==========>...................] - ETA: 0s - loss: 0.2417
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1644
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0142
6530/6530 [==============================] - 0s 9us/step - loss: 0.0144 - val_loss: 0.0177
Epoch 68/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0120
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1719
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1645
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0145
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1413
6530/6530 [==============================] - 0s 10us/step - loss: 0.0149 - val_loss: 0.0190
Epoch 69/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0128
6384/6530 [============================>.] - ETA: 0s - loss: 0.1638
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1196
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0144
6530/6530 [==============================] - 1s 159us/step - loss: 0.1637 - val_loss: 0.1583

6530/6530 [==============================] - 0s 10us/step - loss: 0.0144 - val_loss: 0.0185
Epoch 70/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0166Epoch 5/81

  16/6530 [..............................] - ETA: 1s - loss: 0.2011
6530/6530 [==============================] - 0s 68us/step - loss: 0.1153 - val_loss: 0.0449
Epoch 2/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0532
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0141
 368/6530 [>.............................] - ETA: 0s - loss: 0.1622
6530/6530 [==============================] - 0s 9us/step - loss: 0.0143 - val_loss: 0.0173
Epoch 71/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0133
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0418
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1721
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0139
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0416
6530/6530 [==============================] - 0s 9us/step - loss: 0.0140 - val_loss: 0.0171
Epoch 72/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0134
1008/6530 [===>..........................] - ETA: 0s - loss: 0.1702
3936/6530 [=================>............] - ETA: 0s - loss: 0.0415
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0141
6530/6530 [==============================] - 0s 10us/step - loss: 0.0141 - val_loss: 0.0177
Epoch 73/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0146
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1648
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0418
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0140
6530/6530 [==============================] - 0s 9us/step - loss: 0.0141 - val_loss: 0.0173
Epoch 74/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0123
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1637
6530/6530 [==============================] - 0s 40us/step - loss: 0.0416 - val_loss: 0.0449
Epoch 3/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0531
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0134
6530/6530 [==============================] - 0s 10us/step - loss: 0.0134 - val_loss: 0.0168

2000/6530 [========>.....................] - ETA: 0s - loss: 0.1637Epoch 75/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0126
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0417
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1629
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0129
6530/6530 [==============================] - 0s 11us/step - loss: 0.0131 - val_loss: 0.0168
Epoch 76/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0120
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0413
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1640
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0136
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0413
6530/6530 [==============================] - 0s 9us/step - loss: 0.0136 - val_loss: 0.0174
Epoch 77/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0112
2960/6530 [============>.................] - ETA: 0s - loss: 0.1626
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0417
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0133
6530/6530 [==============================] - 0s 9us/step - loss: 0.0133 - val_loss: 0.0164
Epoch 78/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0118
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1617
6368/6530 [============================>.] - ETA: 0s - loss: 0.0418
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0127
6530/6530 [==============================] - 0s 42us/step - loss: 0.0416 - val_loss: 0.0448
Epoch 4/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0531
6530/6530 [==============================] - 0s 9us/step - loss: 0.0127 - val_loss: 0.0161
Epoch 79/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0112
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1620
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0417
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0133
3952/6530 [=================>............] - ETA: 0s - loss: 0.1621
6530/6530 [==============================] - 0s 9us/step - loss: 0.0132 - val_loss: 0.0166
Epoch 80/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0155
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0418
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1620
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0128
6530/6530 [==============================] - 0s 9us/step - loss: 0.0127 - val_loss: 0.0157
Epoch 81/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0126
3968/6530 [=================>............] - ETA: 0s - loss: 0.0415
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1629
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0130
6530/6530 [==============================] - 0s 10us/step - loss: 0.0131 - val_loss: 0.0168

5312/6530 [=======================>......] - ETA: 0s - loss: 0.0418
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1628
6530/6530 [==============================] - 0s 40us/step - loss: 0.0416 - val_loss: 0.0447
Epoch 5/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0530
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1627
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0409
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1623
2880/6530 [============>.................] - ETA: 0s - loss: 0.0411
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1622
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0412
6416/6530 [============================>.] - ETA: 0s - loss: 0.1617
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0416
6530/6530 [==============================] - 1s 158us/step - loss: 0.1615 - val_loss: 0.1558
Epoch 6/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1982
6530/6530 [==============================] - 0s 37us/step - loss: 0.0416 - val_loss: 0.0447
Epoch 6/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0529
 384/6530 [>.............................] - ETA: 0s - loss: 0.1641
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0415
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1701
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0417
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1687
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0413
1392/6530 [=====>........................] - ETA: 0s - loss: 0.1643
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0416
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1620
6530/6530 [==============================] - 0s 38us/step - loss: 0.0415 - val_loss: 0.0446
Epoch 7/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0528
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1619
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0411
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1620
2976/6530 [============>.................] - ETA: 0s - loss: 0.0411
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1615
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0414
3120/6530 [=============>................] - ETA: 0s - loss: 0.1596
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0416
6530/6530 [==============================] - 0s 36us/step - loss: 0.0415 - val_loss: 0.0446

3472/6530 [==============>...............] - ETA: 0s - loss: 0.1609Epoch 8/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0527
3824/6530 [================>.............] - ETA: 0s - loss: 0.1603
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0411
# training | RMSE: 0.1078, MAE: 0.0846
worker 0  xfile  [0, 81, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24230766296308875}, 'layer_2_size': 81, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26425376474083073}, 'layer_4_size': 23, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.10782994300053479, 'rmse': 0.10782994300053479, 'mae': 0.08458231751992143, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  65 | activation: relu    | extras: None 
layer 2 | size:  79 | activation: sigmoid | extras: dropout - rate: 30.1% 
layer 3 | size:  73 | activation: tanh    | extras: dropout - rate: 37.6% 
layer 4 | size:  18 | activation: relu    | extras: dropout - rate: 43.2% 
layer 5 | size:  46 | activation: sigmoid | extras: dropout - rate: 31.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f460c8aa780>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 2:22 - loss: 1.6081
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1605
2880/6530 [============>.................] - ETA: 0s - loss: 0.0410
 320/6530 [>.............................] - ETA: 7s - loss: 0.5390  
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0412
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1611
 640/6530 [=>............................] - ETA: 4s - loss: 0.3542
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0415
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1609
 944/6530 [===>..........................] - ETA: 2s - loss: 0.2792
6530/6530 [==============================] - 0s 38us/step - loss: 0.0415 - val_loss: 0.0445
Epoch 9/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0526
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1609
1248/6530 [====>.........................] - ETA: 2s - loss: 0.2384
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0410
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1604
1552/6530 [======>.......................] - ETA: 1s - loss: 0.2108
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0411
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1606
1856/6530 [=======>......................] - ETA: 1s - loss: 0.1914
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0412
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1602
2160/6530 [========>.....................] - ETA: 1s - loss: 0.1775
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0414
6530/6530 [==============================] - 1s 152us/step - loss: 0.1599 - val_loss: 0.1552
Epoch 7/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1991
2464/6530 [==========>...................] - ETA: 1s - loss: 0.1658
6530/6530 [==============================] - 0s 39us/step - loss: 0.0414 - val_loss: 0.0444
Epoch 10/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0525
 368/6530 [>.............................] - ETA: 0s - loss: 0.1597
2752/6530 [===========>..................] - ETA: 1s - loss: 0.1578
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0415
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1668
3040/6530 [============>.................] - ETA: 0s - loss: 0.1494
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0412
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1672
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1436
4096/6530 [=================>............] - ETA: 0s - loss: 0.0412
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1625
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1382
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0413
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1607
3936/6530 [=================>............] - ETA: 0s - loss: 0.1333
6530/6530 [==============================] - 0s 39us/step - loss: 0.0414 - val_loss: 0.0444
Epoch 11/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0525
2064/6530 [========>.....................] - ETA: 0s - loss: 0.1607
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1288
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0414
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1608
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1251
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0411
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1609
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1214
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0411
3024/6530 [============>.................] - ETA: 0s - loss: 0.1594
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1181
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0415
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1595
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1153
6530/6530 [==============================] - 0s 39us/step - loss: 0.0413 - val_loss: 0.0443
Epoch 12/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0524
3728/6530 [================>.............] - ETA: 0s - loss: 0.1592
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1124
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0410
4080/6530 [=================>............] - ETA: 0s - loss: 0.1597
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1099
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0409
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1595
6352/6530 [============================>.] - ETA: 0s - loss: 0.1075
3936/6530 [=================>............] - ETA: 0s - loss: 0.0411
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1602
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0414
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1599
6530/6530 [==============================] - 2s 235us/step - loss: 0.1059 - val_loss: 0.0454
Epoch 2/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0835
6530/6530 [==============================] - 0s 40us/step - loss: 0.0412 - val_loss: 0.0442
Epoch 13/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0522
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1596
 320/6530 [>.............................] - ETA: 1s - loss: 0.0627
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0412
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1595
 624/6530 [=>............................] - ETA: 0s - loss: 0.0680
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0409
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1592
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0648
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0409
6528/6530 [============================>.] - ETA: 0s - loss: 0.1589
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0624
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0411
6530/6530 [==============================] - 1s 154us/step - loss: 0.1588 - val_loss: 0.1531
Epoch 8/81

  16/6530 [..............................] - ETA: 1s - loss: 0.2038
6530/6530 [==============================] - 0s 38us/step - loss: 0.0411 - val_loss: 0.0441
Epoch 14/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0521
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0603
 336/6530 [>.............................] - ETA: 0s - loss: 0.1558
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0411
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0597
 688/6530 [==>...........................] - ETA: 0s - loss: 0.1658
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0407
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0593
1040/6530 [===>..........................] - ETA: 0s - loss: 0.1653
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0408
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0594
1360/6530 [=====>........................] - ETA: 0s - loss: 0.1606
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0410
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0590
1712/6530 [======>.......................] - ETA: 0s - loss: 0.1590
6530/6530 [==============================] - 0s 39us/step - loss: 0.0410 - val_loss: 0.0439
Epoch 15/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0518
3008/6530 [============>.................] - ETA: 0s - loss: 0.0577
2064/6530 [========>.....................] - ETA: 0s - loss: 0.1586
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0409
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0566
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1588
2880/6530 [============>.................] - ETA: 0s - loss: 0.0404
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0558
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1587
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0405
3904/6530 [================>.............] - ETA: 0s - loss: 0.0554
3120/6530 [=============>................] - ETA: 0s - loss: 0.1570
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0408
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0551
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1583
6530/6530 [==============================] - 0s 37us/step - loss: 0.0408 - val_loss: 0.0436
Epoch 16/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0515
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0548
3776/6530 [================>.............] - ETA: 0s - loss: 0.1577
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0405
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0547
4128/6530 [=================>............] - ETA: 0s - loss: 0.1583
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0403
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0542
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1589
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0403
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0540
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1589
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0404
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0533
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1584
6530/6530 [==============================] - 0s 38us/step - loss: 0.0404 - val_loss: 0.0431
Epoch 17/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0509
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0531
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1582
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0399
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0530
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1587
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0398
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1581
6530/6530 [==============================] - 1s 177us/step - loss: 0.0524 - val_loss: 0.0365

4192/6530 [==================>...........] - ETA: 0s - loss: 0.0396Epoch 3/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0569
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0398
 320/6530 [>.............................] - ETA: 1s - loss: 0.0446
6530/6530 [==============================] - 1s 154us/step - loss: 0.1575 - val_loss: 0.1530
Epoch 9/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1877
6530/6530 [==============================] - 0s 39us/step - loss: 0.0397 - val_loss: 0.0422
Epoch 18/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0498
 624/6530 [=>............................] - ETA: 1s - loss: 0.0466
 336/6530 [>.............................] - ETA: 0s - loss: 0.1572
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0387
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0475
 656/6530 [==>...........................] - ETA: 0s - loss: 0.1672
2848/6530 [============>.................] - ETA: 0s - loss: 0.0387
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0466
1008/6530 [===>..........................] - ETA: 0s - loss: 0.1637
4096/6530 [=================>............] - ETA: 0s - loss: 0.0388
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0459
1328/6530 [=====>........................] - ETA: 0s - loss: 0.1600
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0389
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0451
1648/6530 [======>.......................] - ETA: 0s - loss: 0.1590
6530/6530 [==============================] - 0s 40us/step - loss: 0.0386 - val_loss: 0.0408
Epoch 19/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0480
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0449
1968/6530 [========>.....................] - ETA: 0s - loss: 0.1590
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0449
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0372
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1588
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0375
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0451
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1590
4096/6530 [=================>............] - ETA: 0s - loss: 0.0372
2944/6530 [============>.................] - ETA: 0s - loss: 0.0442
3024/6530 [============>.................] - ETA: 0s - loss: 0.1573
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0372
3248/6530 [=============>................] - ETA: 0s - loss: 0.0437
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1574
6530/6530 [==============================] - 0s 39us/step - loss: 0.0370 - val_loss: 0.0387
Epoch 20/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0453
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0434
3712/6530 [================>.............] - ETA: 0s - loss: 0.1576
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0353
3856/6530 [================>.............] - ETA: 0s - loss: 0.0433
4064/6530 [=================>............] - ETA: 0s - loss: 0.1579
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0354
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0431
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1580
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0350
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0431
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1583
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0349
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0430
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1583
6530/6530 [==============================] - 0s 39us/step - loss: 0.0347 - val_loss: 0.0360
Epoch 21/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0417
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0429
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1582
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0326
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0429
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1578
2880/6530 [============>.................] - ETA: 0s - loss: 0.0327
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0430
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1576
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0324
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0432
6512/6530 [============================>.] - ETA: 0s - loss: 0.1573
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0324
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0431
6530/6530 [==============================] - 1s 156us/step - loss: 0.1572 - val_loss: 0.1517
Epoch 10/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1944
6530/6530 [==============================] - 0s 38us/step - loss: 0.0322 - val_loss: 0.0333
Epoch 22/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0378
6496/6530 [============================>.] - ETA: 0s - loss: 0.0426
 368/6530 [>.............................] - ETA: 0s - loss: 0.1583
6530/6530 [==============================] - 1s 180us/step - loss: 0.0426 - val_loss: 0.0381

1280/6530 [====>.........................] - ETA: 0s - loss: 0.0294Epoch 4/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0550
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1662
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0307
 320/6530 [>.............................] - ETA: 1s - loss: 0.0381
1040/6530 [===>..........................] - ETA: 0s - loss: 0.1652
4128/6530 [=================>............] - ETA: 0s - loss: 0.0301
 608/6530 [=>............................] - ETA: 1s - loss: 0.0401
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1604
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0300
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0401
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1580
6530/6530 [==============================] - 0s 39us/step - loss: 0.0298 - val_loss: 0.0308
Epoch 23/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0341
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0399
2064/6530 [========>.....................] - ETA: 0s - loss: 0.1578
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0271
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0386
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1578
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0286
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0384
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1577
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0280
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0387
3120/6530 [=============>................] - ETA: 0s - loss: 0.1559
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0280
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0396
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1572
6530/6530 [==============================] - 0s 39us/step - loss: 0.0279 - val_loss: 0.0290
Epoch 24/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0310
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0395
3808/6530 [================>.............] - ETA: 0s - loss: 0.1566
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0259
3008/6530 [============>.................] - ETA: 0s - loss: 0.0387
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1571
2848/6530 [============>.................] - ETA: 0s - loss: 0.0269
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0387
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1580
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0266
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0383
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1578
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0266
3904/6530 [================>.............] - ETA: 0s - loss: 0.0385
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1576
6530/6530 [==============================] - 0s 38us/step - loss: 0.0266 - val_loss: 0.0278
Epoch 25/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0287
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0383
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1571
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0248
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0384
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1576
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0264
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0383
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1574
4064/6530 [=================>............] - ETA: 0s - loss: 0.0258
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0384
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0258
6530/6530 [==============================] - 1s 153us/step - loss: 0.1569 - val_loss: 0.1509
Epoch 11/81

  16/6530 [..............................] - ETA: 1s - loss: 0.2029
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0383
6530/6530 [==============================] - 0s 39us/step - loss: 0.0257 - val_loss: 0.0270
Epoch 26/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0270
 352/6530 [>.............................] - ETA: 0s - loss: 0.1563
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0382
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0240
 688/6530 [==>...........................] - ETA: 0s - loss: 0.1633
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0381
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0255
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1627
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0381
3936/6530 [=================>............] - ETA: 0s - loss: 0.0253
1360/6530 [=====>........................] - ETA: 0s - loss: 0.1572
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0252
6530/6530 [==============================] - 1s 179us/step - loss: 0.0377 - val_loss: 0.0290
Epoch 5/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0422
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1571
 320/6530 [>.............................] - ETA: 1s - loss: 0.0340
6530/6530 [==============================] - 0s 40us/step - loss: 0.0251 - val_loss: 0.0264
Epoch 27/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0257
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1560
 624/6530 [=>............................] - ETA: 1s - loss: 0.0353
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0229
2384/6530 [=========>....................] - ETA: 0s - loss: 0.1563
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0356
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0253
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1566
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0354
4000/6530 [=================>............] - ETA: 0s - loss: 0.0247
3072/6530 [=============>................] - ETA: 0s - loss: 0.1544
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0345
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0247
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1555
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0353
6530/6530 [==============================] - 0s 39us/step - loss: 0.0246 - val_loss: 0.0259
Epoch 28/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0246
3760/6530 [================>.............] - ETA: 0s - loss: 0.1554
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0347
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0228
4080/6530 [=================>............] - ETA: 0s - loss: 0.1563
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0351
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0246
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1565
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0241
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0353
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1571
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0242
3056/6530 [=============>................] - ETA: 0s - loss: 0.0347
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1568
6530/6530 [==============================] - 0s 38us/step - loss: 0.0242 - val_loss: 0.0254
Epoch 29/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0237
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0348
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1567
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0222
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0347
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1567
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0244
3952/6530 [=================>............] - ETA: 0s - loss: 0.0345
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1563
4064/6530 [=================>............] - ETA: 0s - loss: 0.0238
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0343
6512/6530 [============================>.] - ETA: 0s - loss: 0.1560
6530/6530 [==============================] - 1s 155us/step - loss: 0.1559 - val_loss: 0.1507

5408/6530 [=======================>......] - ETA: 0s - loss: 0.0238Epoch 12/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1858
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0344
 368/6530 [>.............................] - ETA: 0s - loss: 0.1543
6530/6530 [==============================] - 0s 40us/step - loss: 0.0237 - val_loss: 0.0250
Epoch 30/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0228
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0344
 720/6530 [==>...........................] - ETA: 0s - loss: 0.1622
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0220
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0342
1072/6530 [===>..........................] - ETA: 0s - loss: 0.1615
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0237
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0343
1424/6530 [=====>........................] - ETA: 0s - loss: 0.1574
4096/6530 [=================>............] - ETA: 0s - loss: 0.0233
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0341
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1551
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0234
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0344
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1553
6530/6530 [==============================] - 0s 39us/step - loss: 0.0232 - val_loss: 0.0245
Epoch 31/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0219
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0345
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1556
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0217
6530/6530 [==============================] - 1s 179us/step - loss: 0.0341 - val_loss: 0.0259
Epoch 6/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0277
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1554
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0231
 304/6530 [>.............................] - ETA: 1s - loss: 0.0285
3152/6530 [=============>................] - ETA: 0s - loss: 0.1535
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0228
 608/6530 [=>............................] - ETA: 1s - loss: 0.0300
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1548
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0228
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0302
6530/6530 [==============================] - 0s 38us/step - loss: 0.0228 - val_loss: 0.0241
Epoch 32/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0211
3824/6530 [================>.............] - ETA: 0s - loss: 0.1548
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0300
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0212
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1550
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0303
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0228
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1557
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0306
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0224
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1557
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0301
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0224
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1555
6530/6530 [==============================] - 0s 38us/step - loss: 0.0223 - val_loss: 0.0237
Epoch 33/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0204
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0310
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1553
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0209
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0312
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1555
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0227
3008/6530 [============>.................] - ETA: 0s - loss: 0.0306
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1553
4064/6530 [=================>............] - ETA: 0s - loss: 0.0221
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0307
6530/6530 [==============================] - 1s 153us/step - loss: 0.1549 - val_loss: 0.1493
Epoch 13/81

  16/6530 [..............................] - ETA: 1s - loss: 0.2015
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0222
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0304
 320/6530 [>.............................] - ETA: 1s - loss: 0.1478
6530/6530 [==============================] - 0s 39us/step - loss: 0.0220 - val_loss: 0.0234
Epoch 34/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0197
3904/6530 [================>.............] - ETA: 0s - loss: 0.0306
 640/6530 [=>............................] - ETA: 0s - loss: 0.1613
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0207
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0304
 976/6530 [===>..........................] - ETA: 0s - loss: 0.1595
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0222
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0305
1328/6530 [=====>........................] - ETA: 0s - loss: 0.1568
3968/6530 [=================>............] - ETA: 0s - loss: 0.0219
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0305
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1546
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0219
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0307
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1543
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0308
6530/6530 [==============================] - 0s 41us/step - loss: 0.0216 - val_loss: 0.0231
Epoch 35/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0192
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1538
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0307
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0205
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1556
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0309
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0219
2944/6530 [============>.................] - ETA: 0s - loss: 0.1544
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0310
4032/6530 [=================>............] - ETA: 0s - loss: 0.0216
3264/6530 [=============>................] - ETA: 0s - loss: 0.1529
6496/6530 [============================>.] - ETA: 0s - loss: 0.0305
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0216
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1538
6530/6530 [==============================] - 1s 179us/step - loss: 0.0305 - val_loss: 0.0238
Epoch 7/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0318
6530/6530 [==============================] - 0s 40us/step - loss: 0.0214 - val_loss: 0.0229
Epoch 36/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0189
3936/6530 [=================>............] - ETA: 0s - loss: 0.1540
 320/6530 [>.............................] - ETA: 1s - loss: 0.0278
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0203
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1541
 608/6530 [=>............................] - ETA: 1s - loss: 0.0282
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0217
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1550
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0284
3936/6530 [=================>............] - ETA: 0s - loss: 0.0215
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1550
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0279
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0214
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1549
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0276
6530/6530 [==============================] - 0s 40us/step - loss: 0.0212 - val_loss: 0.0227
Epoch 37/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0185
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1543
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0279
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0202
5968/6530 [==========================>...] - ETA: 0s - loss: 0.1546
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0278
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0215
6320/6530 [============================>.] - ETA: 0s - loss: 0.1541
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0284
4096/6530 [=================>............] - ETA: 0s - loss: 0.0212
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0287
6530/6530 [==============================] - 1s 159us/step - loss: 0.1538 - val_loss: 0.1497
Epoch 14/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1900
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0212
2992/6530 [============>.................] - ETA: 0s - loss: 0.0285
 368/6530 [>.............................] - ETA: 0s - loss: 0.1559
6530/6530 [==============================] - 0s 40us/step - loss: 0.0210 - val_loss: 0.0226
Epoch 38/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0183
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0283
 720/6530 [==>...........................] - ETA: 0s - loss: 0.1623
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0201
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0281
1072/6530 [===>..........................] - ETA: 0s - loss: 0.1616
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0214
3872/6530 [================>.............] - ETA: 0s - loss: 0.0282
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1568
4064/6530 [=================>............] - ETA: 0s - loss: 0.0211
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0278
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1554
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0211
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0279
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1557
6530/6530 [==============================] - 0s 39us/step - loss: 0.0209 - val_loss: 0.0224
Epoch 39/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0181
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0279
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1555
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0202
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0279
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1551
2848/6530 [============>.................] - ETA: 0s - loss: 0.0211
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0280
3168/6530 [=============>................] - ETA: 0s - loss: 0.1533
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0209
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0279
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1542
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0209
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0280
3872/6530 [================>.............] - ETA: 0s - loss: 0.1542
6530/6530 [==============================] - 0s 39us/step - loss: 0.0207 - val_loss: 0.0223
Epoch 40/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0180
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0283
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1541
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0201
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1549
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0210
6530/6530 [==============================] - 1s 178us/step - loss: 0.0281 - val_loss: 0.0291
Epoch 8/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0211
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1546
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0208
 320/6530 [>.............................] - ETA: 1s - loss: 0.0268
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1547
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0208
 624/6530 [=>............................] - ETA: 1s - loss: 0.0281
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1544
6530/6530 [==============================] - 0s 39us/step - loss: 0.0206 - val_loss: 0.0222
Epoch 41/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0179
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0266
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1546
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0200
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0269
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1543
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0211
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0264
3808/6530 [================>.............] - ETA: 0s - loss: 0.0209
6530/6530 [==============================] - 1s 154us/step - loss: 0.1541 - val_loss: 0.1487

1792/6530 [=======>......................] - ETA: 0s - loss: 0.0263Epoch 15/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1842
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0209
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0263
 336/6530 [>.............................] - ETA: 0s - loss: 0.1529
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0206
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0267
 688/6530 [==>...........................] - ETA: 0s - loss: 0.1604
6530/6530 [==============================] - 0s 42us/step - loss: 0.0205 - val_loss: 0.0221
Epoch 42/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0178
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0272
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1603
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0198
1360/6530 [=====>........................] - ETA: 0s - loss: 0.1553
2912/6530 [============>.................] - ETA: 0s - loss: 0.0268
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0208
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1539
3216/6530 [=============>................] - ETA: 0s - loss: 0.0266
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0206
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1533
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0265
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0206
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1535
6530/6530 [==============================] - 0s 38us/step - loss: 0.0204 - val_loss: 0.0220
Epoch 43/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0177
3792/6530 [================>.............] - ETA: 0s - loss: 0.0264
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1530
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0199
4080/6530 [=================>............] - ETA: 0s - loss: 0.0262
3136/6530 [=============>................] - ETA: 0s - loss: 0.1511
2848/6530 [============>.................] - ETA: 0s - loss: 0.0206
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0260
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1528
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0205
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0264
3824/6530 [================>.............] - ETA: 0s - loss: 0.1528
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0205
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0262
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1530
6530/6530 [==============================] - 0s 39us/step - loss: 0.0203 - val_loss: 0.0219
Epoch 44/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0176
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0262
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1540
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0263
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0197
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1538
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0264
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0205
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1538
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0265
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0203
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1538
6496/6530 [============================>.] - ETA: 0s - loss: 0.0263
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0204
6530/6530 [==============================] - 1s 180us/step - loss: 0.0263 - val_loss: 0.0211
Epoch 9/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0121
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1542
6530/6530 [==============================] - 0s 40us/step - loss: 0.0202 - val_loss: 0.0218
Epoch 45/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0176
 320/6530 [>.............................] - ETA: 1s - loss: 0.0229
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1538
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0197
 608/6530 [=>............................] - ETA: 1s - loss: 0.0242
2848/6530 [============>.................] - ETA: 0s - loss: 0.0203
6530/6530 [==============================] - 1s 152us/step - loss: 0.1535 - val_loss: 0.1476
Epoch 16/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1898
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0238
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0203
 368/6530 [>.............................] - ETA: 0s - loss: 0.1508
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0238
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0203
 720/6530 [==>...........................] - ETA: 0s - loss: 0.1588
6530/6530 [==============================] - 0s 38us/step - loss: 0.0200 - val_loss: 0.0217

1504/6530 [=====>........................] - ETA: 0s - loss: 0.0235Epoch 46/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0175
1072/6530 [===>..........................] - ETA: 0s - loss: 0.1578
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0239
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0194
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1526
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0239
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0205
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1517
4064/6530 [=================>............] - ETA: 0s - loss: 0.0202
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0246
2064/6530 [========>.....................] - ETA: 0s - loss: 0.1520
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0202
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0253
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1522
6530/6530 [==============================] - 0s 39us/step - loss: 0.0199 - val_loss: 0.0216
Epoch 47/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0174
2960/6530 [============>.................] - ETA: 0s - loss: 0.0248
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1524
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0193
3248/6530 [=============>................] - ETA: 0s - loss: 0.0247
3056/6530 [=============>................] - ETA: 0s - loss: 0.1504
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0201
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0247
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1516
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0200
3840/6530 [================>.............] - ETA: 0s - loss: 0.0246
3744/6530 [================>.............] - ETA: 0s - loss: 0.1517
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0201
4128/6530 [=================>............] - ETA: 0s - loss: 0.0246
4032/6530 [=================>............] - ETA: 0s - loss: 0.1522
6530/6530 [==============================] - 0s 39us/step - loss: 0.0198 - val_loss: 0.0215
Epoch 48/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0173
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0245
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1523
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0193
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0246
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1534
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0203
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0246
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1533
4032/6530 [=================>............] - ETA: 0s - loss: 0.0200
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0245
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1534
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0200
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0246
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1529
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0247
6530/6530 [==============================] - 0s 40us/step - loss: 0.0196 - val_loss: 0.0214
Epoch 49/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0172
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1530
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0247
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0192
6464/6530 [============================>.] - ETA: 0s - loss: 0.1527
6496/6530 [============================>.] - ETA: 0s - loss: 0.0245
2848/6530 [============>.................] - ETA: 0s - loss: 0.0198
6530/6530 [==============================] - 1s 155us/step - loss: 0.1525 - val_loss: 0.1470
Epoch 17/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1857
6530/6530 [==============================] - 1s 179us/step - loss: 0.0245 - val_loss: 0.0196
Epoch 10/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0156
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0197
 368/6530 [>.............................] - ETA: 0s - loss: 0.1491
 304/6530 [>.............................] - ETA: 1s - loss: 0.0219
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0197
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1573
 592/6530 [=>............................] - ETA: 1s - loss: 0.0231
6530/6530 [==============================] - 0s 38us/step - loss: 0.0195 - val_loss: 0.0213
Epoch 50/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0171
1040/6530 [===>..........................] - ETA: 0s - loss: 0.1572
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0230
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0191
1392/6530 [=====>........................] - ETA: 0s - loss: 0.1532
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0227
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0198
1712/6530 [======>.......................] - ETA: 0s - loss: 0.1522
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0226
4128/6530 [=================>............] - ETA: 0s - loss: 0.0196
2064/6530 [========>.....................] - ETA: 0s - loss: 0.1523
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0227
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0196
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1530
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0229
6530/6530 [==============================] - 0s 39us/step - loss: 0.0194 - val_loss: 0.0212
Epoch 51/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0170
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1526
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0235
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0190
3088/6530 [=============>................] - ETA: 0s - loss: 0.1508
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0240
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0196
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1518
3008/6530 [============>.................] - ETA: 0s - loss: 0.0237
3968/6530 [=================>............] - ETA: 0s - loss: 0.0196
3776/6530 [================>.............] - ETA: 0s - loss: 0.1515
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0195
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0238
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1521
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0235
6530/6530 [==============================] - 0s 41us/step - loss: 0.0192 - val_loss: 0.0211
Epoch 52/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0169
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1528
3920/6530 [=================>............] - ETA: 0s - loss: 0.0236
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0189
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1530
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0236
2848/6530 [============>.................] - ETA: 0s - loss: 0.0193
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1530
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0234
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0193
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1529
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0235
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0193
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1530
6530/6530 [==============================] - 0s 38us/step - loss: 0.0191 - val_loss: 0.0210

5088/6530 [======================>.......] - ETA: 0s - loss: 0.0236Epoch 53/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0168
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1526
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0238
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0188
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0237
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0192
6530/6530 [==============================] - 1s 154us/step - loss: 0.1523 - val_loss: 0.1471
Epoch 18/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1788
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0238
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0192
 368/6530 [>.............................] - ETA: 0s - loss: 0.1519
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0192
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0239
 688/6530 [==>...........................] - ETA: 0s - loss: 0.1578
6530/6530 [==============================] - 0s 39us/step - loss: 0.0190 - val_loss: 0.0209
Epoch 54/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0167
1040/6530 [===>..........................] - ETA: 0s - loss: 0.1572
6530/6530 [==============================] - 1s 178us/step - loss: 0.0236 - val_loss: 0.0196
Epoch 11/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0178
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0187
1392/6530 [=====>........................] - ETA: 0s - loss: 0.1531
 320/6530 [>.............................] - ETA: 1s - loss: 0.0226
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0194
1712/6530 [======>.......................] - ETA: 0s - loss: 0.1523
 560/6530 [=>............................] - ETA: 1s - loss: 0.0216
4000/6530 [=================>............] - ETA: 0s - loss: 0.0191
2064/6530 [========>.....................] - ETA: 0s - loss: 0.1514
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0212
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0191
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1518
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0216
6432/6530 [============================>.] - ETA: 0s - loss: 0.0189
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1517
6530/6530 [==============================] - 0s 41us/step - loss: 0.0188 - val_loss: 0.0208
Epoch 55/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0166
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0210
3024/6530 [============>.................] - ETA: 0s - loss: 0.1504
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0185
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0208
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1504
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0190
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0209
3696/6530 [===============>..............] - ETA: 0s - loss: 0.1511
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0189
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0211
4048/6530 [=================>............] - ETA: 0s - loss: 0.1514
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0189
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0218
6530/6530 [==============================] - 0s 39us/step - loss: 0.0187 - val_loss: 0.0207

4400/6530 [===================>..........] - ETA: 0s - loss: 0.1514Epoch 56/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0165
2912/6530 [============>.................] - ETA: 0s - loss: 0.0215
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0185
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1521
3168/6530 [=============>................] - ETA: 0s - loss: 0.0213
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0190
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1519
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0214
4000/6530 [=================>............] - ETA: 0s - loss: 0.0189
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1524
3776/6530 [================>.............] - ETA: 0s - loss: 0.0213
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0189
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1519
4080/6530 [=================>............] - ETA: 0s - loss: 0.0214
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1520
6530/6530 [==============================] - 0s 39us/step - loss: 0.0186 - val_loss: 0.0206
Epoch 57/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0164
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0212
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0184
6368/6530 [============================>.] - ETA: 0s - loss: 0.1517
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0214
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0189
6530/6530 [==============================] - 1s 159us/step - loss: 0.1516 - val_loss: 0.1460
Epoch 19/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1861
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0215
3968/6530 [=================>............] - ETA: 0s - loss: 0.0188
 352/6530 [>.............................] - ETA: 0s - loss: 0.1477
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0217
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0188
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1559
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0216
6530/6530 [==============================] - 0s 40us/step - loss: 0.0185 - val_loss: 0.0205
Epoch 58/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0163
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1540
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0218
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0182
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1506
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0218
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0186
1712/6530 [======>.......................] - ETA: 0s - loss: 0.1502
6480/6530 [============================>.] - ETA: 0s - loss: 0.0217
4128/6530 [=================>............] - ETA: 0s - loss: 0.0186
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1497
6530/6530 [==============================] - 1s 180us/step - loss: 0.0217 - val_loss: 0.0223
Epoch 12/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0234
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0185
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1506
 320/6530 [>.............................] - ETA: 1s - loss: 0.0206
6530/6530 [==============================] - 0s 38us/step - loss: 0.0184 - val_loss: 0.0204
Epoch 59/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0162
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1509
 624/6530 [=>............................] - ETA: 0s - loss: 0.0202
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0182
3104/6530 [=============>................] - ETA: 0s - loss: 0.1494
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0202
2848/6530 [============>.................] - ETA: 0s - loss: 0.0184
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1506
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0199
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0184
3792/6530 [================>.............] - ETA: 0s - loss: 0.1504
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0193
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0185
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1508
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0200
6530/6530 [==============================] - 0s 39us/step - loss: 0.0183 - val_loss: 0.0203
Epoch 60/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0160
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1519
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0199
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0181
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1519
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0204
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0185
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1515
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0209
4032/6530 [=================>............] - ETA: 0s - loss: 0.0184
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1510
3040/6530 [============>.................] - ETA: 0s - loss: 0.0206
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0185
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1514
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0209
6530/6530 [==============================] - 0s 39us/step - loss: 0.0182 - val_loss: 0.0202
Epoch 61/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0159
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1511
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0207
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0180
3952/6530 [=================>............] - ETA: 0s - loss: 0.0206
6530/6530 [==============================] - 1s 152us/step - loss: 0.1509 - val_loss: 0.1451
Epoch 20/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1875
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0184
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0207
 352/6530 [>.............................] - ETA: 0s - loss: 0.1505
4128/6530 [=================>............] - ETA: 0s - loss: 0.0183
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0207
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1578
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0183
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0208
1040/6530 [===>..........................] - ETA: 0s - loss: 0.1568
6530/6530 [==============================] - 0s 39us/step - loss: 0.0181 - val_loss: 0.0202
Epoch 62/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0158
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0207
1392/6530 [=====>........................] - ETA: 0s - loss: 0.1524
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0182
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0208
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1520
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0184
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0208
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1513
4064/6530 [=================>............] - ETA: 0s - loss: 0.0182
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0209
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1517
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0182
6336/6530 [============================>.] - ETA: 0s - loss: 0.0209
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1520
6530/6530 [==============================] - 0s 39us/step - loss: 0.0180 - val_loss: 0.0201
Epoch 63/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0157
3072/6530 [=============>................] - ETA: 0s - loss: 0.1499
6530/6530 [==============================] - 1s 176us/step - loss: 0.0208 - val_loss: 0.0173
Epoch 13/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0202
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0179
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1502
 304/6530 [>.............................] - ETA: 1s - loss: 0.0194
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0181
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1506
 608/6530 [=>............................] - ETA: 1s - loss: 0.0194
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0181
4032/6530 [=================>............] - ETA: 0s - loss: 0.1510
 880/6530 [===>..........................] - ETA: 0s - loss: 0.0195
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0182
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1508
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0188
6530/6530 [==============================] - 0s 40us/step - loss: 0.0179 - val_loss: 0.0200
Epoch 64/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0156
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1522
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0191
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0181
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1518
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0194
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0181
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1521
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0195
3968/6530 [=================>............] - ETA: 0s - loss: 0.0181
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1514
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0198
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0181
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1514
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0203
6530/6530 [==============================] - 0s 39us/step - loss: 0.0178 - val_loss: 0.0200
Epoch 65/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0155
6336/6530 [============================>.] - ETA: 0s - loss: 0.1509
2960/6530 [============>.................] - ETA: 0s - loss: 0.0200
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0179
3264/6530 [=============>................] - ETA: 0s - loss: 0.0198
6530/6530 [==============================] - 1s 161us/step - loss: 0.1509 - val_loss: 0.1447

2720/6530 [===========>..................] - ETA: 0s - loss: 0.0180Epoch 21/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1686
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0197
4128/6530 [=================>............] - ETA: 0s - loss: 0.0179
 384/6530 [>.............................] - ETA: 0s - loss: 0.1457
3872/6530 [================>.............] - ETA: 0s - loss: 0.0198
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0179
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1536
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0197
6530/6530 [==============================] - 0s 38us/step - loss: 0.0177 - val_loss: 0.0199
Epoch 66/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0154
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1526
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0196
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0177
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1501
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0197
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0179
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1485
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0199
4096/6530 [=================>............] - ETA: 0s - loss: 0.0179
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1485
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0200
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0178
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1493
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0199
6530/6530 [==============================] - 0s 40us/step - loss: 0.0176 - val_loss: 0.0198
Epoch 67/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0152
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1496
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0200
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0177
3152/6530 [=============>................] - ETA: 0s - loss: 0.1477
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0200
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0179
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1491
4128/6530 [=================>............] - ETA: 0s - loss: 0.0178
3840/6530 [================>.............] - ETA: 0s - loss: 0.1490
6530/6530 [==============================] - 1s 178us/step - loss: 0.0198 - val_loss: 0.0170
Epoch 14/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0183
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0177
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1492
 320/6530 [>.............................] - ETA: 1s - loss: 0.0201
6530/6530 [==============================] - 0s 39us/step - loss: 0.0176 - val_loss: 0.0198
Epoch 68/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0151
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1497
 624/6530 [=>............................] - ETA: 1s - loss: 0.0188
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0176
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1499
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0183
2848/6530 [============>.................] - ETA: 0s - loss: 0.0175
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1498
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0179
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0176
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1496
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0179
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0177
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1498
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0181
6530/6530 [==============================] - 0s 39us/step - loss: 0.0175 - val_loss: 0.0197
Epoch 69/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0150
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1495
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0179
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0175
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0184
6530/6530 [==============================] - 1s 154us/step - loss: 0.1494 - val_loss: 0.1436
Epoch 22/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1979
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0177
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0187
 352/6530 [>.............................] - ETA: 0s - loss: 0.1501
4000/6530 [=================>............] - ETA: 0s - loss: 0.0176
2976/6530 [============>.................] - ETA: 0s - loss: 0.0184
 688/6530 [==>...........................] - ETA: 0s - loss: 0.1559
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0177
3264/6530 [=============>................] - ETA: 0s - loss: 0.0184
1008/6530 [===>..........................] - ETA: 0s - loss: 0.1544
6530/6530 [==============================] - 0s 40us/step - loss: 0.0174 - val_loss: 0.0197
Epoch 70/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0149
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0186
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1517
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0175
3824/6530 [================>.............] - ETA: 0s - loss: 0.0186
1680/6530 [======>.......................] - ETA: 0s - loss: 0.1507
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0175
4080/6530 [=================>............] - ETA: 0s - loss: 0.0185
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1503
4032/6530 [=================>............] - ETA: 0s - loss: 0.0175
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0184
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1506
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0177
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0186
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1508
6530/6530 [==============================] - 0s 39us/step - loss: 0.0173 - val_loss: 0.0196
Epoch 71/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0148
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0186
3040/6530 [============>.................] - ETA: 0s - loss: 0.1489
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0175
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0187
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1489
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0176
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0187
3712/6530 [================>.............] - ETA: 0s - loss: 0.1493
4096/6530 [=================>............] - ETA: 0s - loss: 0.0175
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0188
4080/6530 [=================>............] - ETA: 0s - loss: 0.1501
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0175
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0189
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1502
6530/6530 [==============================] - 0s 39us/step - loss: 0.0173 - val_loss: 0.0195
Epoch 72/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0146
6432/6530 [============================>.] - ETA: 0s - loss: 0.0189
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1511
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0174
6530/6530 [==============================] - 1s 183us/step - loss: 0.0189 - val_loss: 0.0168
Epoch 15/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0105
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1509
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0174
 320/6530 [>.............................] - ETA: 1s - loss: 0.0175
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1509
3904/6530 [================>.............] - ETA: 0s - loss: 0.0174
 608/6530 [=>............................] - ETA: 1s - loss: 0.0177
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1506
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0174
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0177
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1506
6530/6530 [==============================] - 0s 40us/step - loss: 0.0172 - val_loss: 0.0194
Epoch 73/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0145
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0171
6464/6530 [============================>.] - ETA: 0s - loss: 0.1502
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0173
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0169
6530/6530 [==============================] - 1s 158us/step - loss: 0.1501 - val_loss: 0.1438
Epoch 23/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1733
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0174
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0171
 352/6530 [>.............................] - ETA: 0s - loss: 0.1467
4032/6530 [=================>............] - ETA: 0s - loss: 0.0173
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0172
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1542
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0173
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0175
1008/6530 [===>..........................] - ETA: 0s - loss: 0.1519
6496/6530 [============================>.] - ETA: 0s - loss: 0.0171
6530/6530 [==============================] - 0s 41us/step - loss: 0.0171 - val_loss: 0.0194
Epoch 74/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0144
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0178
1360/6530 [=====>........................] - ETA: 0s - loss: 0.1486
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0174
2880/6530 [============>.................] - ETA: 0s - loss: 0.0174
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1485
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0172
3152/6530 [=============>................] - ETA: 0s - loss: 0.0173
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1481
3840/6530 [================>.............] - ETA: 0s - loss: 0.0172
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0177
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1485
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0172
3712/6530 [================>.............] - ETA: 0s - loss: 0.0175
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1490
6530/6530 [==============================] - 0s 41us/step - loss: 0.0170 - val_loss: 0.0193
Epoch 75/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0143
3984/6530 [=================>............] - ETA: 0s - loss: 0.0176
3024/6530 [============>.................] - ETA: 0s - loss: 0.1475
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0173
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0176
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1475
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0170
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0175
3712/6530 [================>.............] - ETA: 0s - loss: 0.1479
4064/6530 [=================>............] - ETA: 0s - loss: 0.0171
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0177
4064/6530 [=================>............] - ETA: 0s - loss: 0.1482
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0171
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0176
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1482
6530/6530 [==============================] - 0s 40us/step - loss: 0.0169 - val_loss: 0.0192
Epoch 76/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0142
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0177
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1493
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0171
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0178
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1491
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0169
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0179
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1495
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0170
6368/6530 [============================>.] - ETA: 0s - loss: 0.0180
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1491
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0170
6530/6530 [==============================] - 1s 182us/step - loss: 0.0179 - val_loss: 0.0152
Epoch 16/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0195
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1491
6530/6530 [==============================] - 0s 38us/step - loss: 0.0169 - val_loss: 0.0191
Epoch 77/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0141
 304/6530 [>.............................] - ETA: 1s - loss: 0.0160
6432/6530 [============================>.] - ETA: 0s - loss: 0.1489
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0171
6530/6530 [==============================] - 1s 157us/step - loss: 0.1487 - val_loss: 0.1426

 576/6530 [=>............................] - ETA: 1s - loss: 0.0168Epoch 24/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1793
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0168
 880/6530 [===>..........................] - ETA: 0s - loss: 0.0166
 368/6530 [>.............................] - ETA: 0s - loss: 0.1483
4096/6530 [=================>............] - ETA: 0s - loss: 0.0170
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0169
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1533
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0171
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0168
1072/6530 [===>..........................] - ETA: 0s - loss: 0.1531
6530/6530 [==============================] - 0s 39us/step - loss: 0.0168 - val_loss: 0.0190
Epoch 78/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0139
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0169
1424/6530 [=====>........................] - ETA: 0s - loss: 0.1489
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0169
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0169
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1475
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0169
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0175
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1472
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0168
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0175
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1487
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0169
2976/6530 [============>.................] - ETA: 0s - loss: 0.0172
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1486
6530/6530 [==============================] - 0s 39us/step - loss: 0.0167 - val_loss: 0.0190
Epoch 79/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0138
3248/6530 [=============>................] - ETA: 0s - loss: 0.0174
3120/6530 [=============>................] - ETA: 0s - loss: 0.1473
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0168
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0172
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1485
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0168
3840/6530 [================>.............] - ETA: 0s - loss: 0.0172
3840/6530 [================>.............] - ETA: 0s - loss: 0.1481
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0168
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0172
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1485
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0168
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0170
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1493
6530/6530 [==============================] - 0s 39us/step - loss: 0.0166 - val_loss: 0.0189
Epoch 80/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0137
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0172
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1495
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0169
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0172
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1496
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0167
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1494
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0171
4096/6530 [=================>............] - ETA: 0s - loss: 0.0167
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0170
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1499
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0167
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0170
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1495
6530/6530 [==============================] - 0s 39us/step - loss: 0.0166 - val_loss: 0.0188
Epoch 81/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0136
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0172
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0170
6530/6530 [==============================] - 1s 155us/step - loss: 0.1492 - val_loss: 0.1425
Epoch 25/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1803
6464/6530 [============================>.] - ETA: 0s - loss: 0.0172
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0168
6530/6530 [==============================] - 1s 179us/step - loss: 0.0172 - val_loss: 0.0157
Epoch 17/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0091
 320/6530 [>.............................] - ETA: 1s - loss: 0.1434
4064/6530 [=================>............] - ETA: 0s - loss: 0.0166
 304/6530 [>.............................] - ETA: 1s - loss: 0.0152
 656/6530 [==>...........................] - ETA: 0s - loss: 0.1537
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0167
 592/6530 [=>............................] - ETA: 1s - loss: 0.0165
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1498
6530/6530 [==============================] - 0s 39us/step - loss: 0.0165 - val_loss: 0.0187

 880/6530 [===>..........................] - ETA: 0s - loss: 0.0168
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1499
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0163
1632/6530 [======>.......................] - ETA: 0s - loss: 0.1477
# training | RMSE: 0.1303, MAE: 0.1007
worker 1  xfile  [3, 81, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27793970530039186}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.1303229629236031, 'rmse': 0.1303229629236031, 'mae': 0.10065355875744092, 'early_stop': False}
vggnet done  1

1456/6530 [=====>........................] - ETA: 0s - loss: 0.0162
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1478
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0162
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1485
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0162
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1485
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0162
3056/6530 [=============>................] - ETA: 0s - loss: 0.1465
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0163
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1477
3008/6530 [============>.................] - ETA: 0s - loss: 0.0161
3808/6530 [================>.............] - ETA: 0s - loss: 0.1479
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0162
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1481
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0161
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1490
3952/6530 [=================>............] - ETA: 0s - loss: 0.0162
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1491
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0161
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1491
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1488
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0162
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1491
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0162
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1487
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0161
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0159
6530/6530 [==============================] - 1s 152us/step - loss: 0.1486 - val_loss: 0.1415
Epoch 26/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1672
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0160
 384/6530 [>.............................] - ETA: 0s - loss: 0.1424
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0161
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1491
6448/6530 [============================>.] - ETA: 0s - loss: 0.0162
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1499
6530/6530 [==============================] - 1s 172us/step - loss: 0.0162 - val_loss: 0.0185
Epoch 18/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0176
1520/6530 [=====>........................] - ETA: 0s - loss: 0.1462
 336/6530 [>.............................] - ETA: 0s - loss: 0.0149
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1463
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0153
2256/6530 [=========>....................] - ETA: 0s - loss: 0.1468
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0157
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1478
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0161
2992/6530 [============>.................] - ETA: 0s - loss: 0.1462
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0157
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1459
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0155
3728/6530 [================>.............] - ETA: 0s - loss: 0.1465
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0157
4080/6530 [=================>............] - ETA: 0s - loss: 0.1471
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0158
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1474
2832/6530 [============>.................] - ETA: 0s - loss: 0.0160
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1479
3152/6530 [=============>................] - ETA: 0s - loss: 0.0156
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1478
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0160
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1479
3760/6530 [================>.............] - ETA: 0s - loss: 0.0160
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1486
4080/6530 [=================>............] - ETA: 0s - loss: 0.0161
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1481
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0158
6530/6530 [==============================] - 1s 145us/step - loss: 0.1477 - val_loss: 0.1407
Epoch 27/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1739
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0161
 368/6530 [>.............................] - ETA: 0s - loss: 0.1463
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0161
 752/6530 [==>...........................] - ETA: 0s - loss: 0.1515
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0161
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1508
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0160
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0160
1520/6530 [=====>........................] - ETA: 0s - loss: 0.1474
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1468
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0162
2256/6530 [=========>....................] - ETA: 0s - loss: 0.1467
6530/6530 [==============================] - 1s 167us/step - loss: 0.0160 - val_loss: 0.0189
Epoch 19/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0085
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1476
 336/6530 [>.............................] - ETA: 0s - loss: 0.0142
2992/6530 [============>.................] - ETA: 0s - loss: 0.1460
 640/6530 [=>............................] - ETA: 0s - loss: 0.0150
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1457
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0152
3728/6530 [================>.............] - ETA: 0s - loss: 0.1462
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0152
4112/6530 [=================>............] - ETA: 0s - loss: 0.1464
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0152
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1471
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0155
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1471
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0157
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1472
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0159
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1470
2864/6530 [============>.................] - ETA: 0s - loss: 0.0157
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1473
3184/6530 [=============>................] - ETA: 0s - loss: 0.0156
6384/6530 [============================>.] - ETA: 0s - loss: 0.1469
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0158
6530/6530 [==============================] - 1s 142us/step - loss: 0.1468 - val_loss: 0.1403
Epoch 28/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1885
3760/6530 [================>.............] - ETA: 0s - loss: 0.0157
 400/6530 [>.............................] - ETA: 0s - loss: 0.1511
4064/6530 [=================>............] - ETA: 0s - loss: 0.0157
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1513
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0155
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1507
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0156
1520/6530 [=====>........................] - ETA: 0s - loss: 0.1466
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0156
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1467
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0157
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1474
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0156
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1484
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0156
2944/6530 [============>.................] - ETA: 0s - loss: 0.1470
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0157
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1461
6530/6530 [==============================] - 1s 168us/step - loss: 0.0156 - val_loss: 0.0171
Epoch 20/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0083
3696/6530 [===============>..............] - ETA: 0s - loss: 0.1469
 336/6530 [>.............................] - ETA: 1s - loss: 0.0137
4064/6530 [=================>............] - ETA: 0s - loss: 0.1476
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0150
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1476
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0153
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1481
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0150
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1477
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0148
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1474
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0150
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1477
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0148
6368/6530 [============================>.] - ETA: 0s - loss: 0.1471
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0150
6530/6530 [==============================] - 1s 142us/step - loss: 0.1470 - val_loss: 0.1396
Epoch 29/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1706
2912/6530 [============>.................] - ETA: 0s - loss: 0.0148
 368/6530 [>.............................] - ETA: 0s - loss: 0.1448
3232/6530 [=============>................] - ETA: 0s - loss: 0.0148
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1514
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0151
1104/6530 [====>.........................] - ETA: 0s - loss: 0.1504
3872/6530 [================>.............] - ETA: 0s - loss: 0.0151
1456/6530 [=====>........................] - ETA: 0s - loss: 0.1472
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0151
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1472
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0150
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1470
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0152
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1476
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0151
2912/6530 [============>.................] - ETA: 0s - loss: 0.1466
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0151
3232/6530 [=============>................] - ETA: 0s - loss: 0.1453
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0152
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1464
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0152
3984/6530 [=================>............] - ETA: 0s - loss: 0.1469
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0152
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1463
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1477
6530/6530 [==============================] - 1s 169us/step - loss: 0.0151 - val_loss: 0.0152
Epoch 21/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0075
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1473
 336/6530 [>.............................] - ETA: 0s - loss: 0.0116
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1469
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0134
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1473
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0137
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1468
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0136
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0136
6530/6530 [==============================] - 1s 145us/step - loss: 0.1464 - val_loss: 0.1392
Epoch 30/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1755
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0140
 400/6530 [>.............................] - ETA: 0s - loss: 0.1496
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0144
 784/6530 [==>...........................] - ETA: 0s - loss: 0.1498
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0146
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1503
2896/6530 [============>.................] - ETA: 0s - loss: 0.0146
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1474
3216/6530 [=============>................] - ETA: 0s - loss: 0.0145
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1469
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0145
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1468
3856/6530 [================>.............] - ETA: 0s - loss: 0.0146
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1475
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0146
2992/6530 [============>.................] - ETA: 0s - loss: 0.1456
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0144
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1457
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0145
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1462
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0145
3968/6530 [=================>............] - ETA: 0s - loss: 0.1466
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0145
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1463
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0146
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1470
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0147
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1470
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0147
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1471
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1468
6530/6530 [==============================] - 1s 170us/step - loss: 0.0147 - val_loss: 0.0176
Epoch 22/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0110
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1467
 336/6530 [>.............................] - ETA: 1s - loss: 0.0135
6512/6530 [============================>.] - ETA: 0s - loss: 0.1465
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0139
6530/6530 [==============================] - 1s 147us/step - loss: 0.1465 - val_loss: 0.1387
Epoch 31/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1809
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0141
 400/6530 [>.............................] - ETA: 0s - loss: 0.1477
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0143
 752/6530 [==>...........................] - ETA: 0s - loss: 0.1505
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0140
1104/6530 [====>.........................] - ETA: 0s - loss: 0.1500
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0142
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1465
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0140
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1455
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0142
2224/6530 [=========>....................] - ETA: 0s - loss: 0.1451
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0141
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1457
3136/6530 [=============>................] - ETA: 0s - loss: 0.0138
2976/6530 [============>.................] - ETA: 0s - loss: 0.1445
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0139
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1441
3760/6530 [================>.............] - ETA: 0s - loss: 0.0138
3696/6530 [===============>..............] - ETA: 0s - loss: 0.1449
4064/6530 [=================>............] - ETA: 0s - loss: 0.0139
4048/6530 [=================>............] - ETA: 0s - loss: 0.1452
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0138
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1449
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0139
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1458
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0138
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1456
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0139
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1456
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0141
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1460
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0141
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1456
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0143
6530/6530 [==============================] - 1s 144us/step - loss: 0.1455 - val_loss: 0.1382
Epoch 32/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1736
6530/6530 [==============================] - 1s 169us/step - loss: 0.0142 - val_loss: 0.0189
Epoch 23/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0130
 400/6530 [>.............................] - ETA: 0s - loss: 0.1450
 336/6530 [>.............................] - ETA: 1s - loss: 0.0134
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1472
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0146
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1474
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0144
1520/6530 [=====>........................] - ETA: 0s - loss: 0.1436
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0149
1904/6530 [=======>......................] - ETA: 0s - loss: 0.1441
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0148
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1444
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0145
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1461
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0146
2944/6530 [============>.................] - ETA: 0s - loss: 0.1445
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0147
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1439
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0146
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1450
3072/6530 [=============>................] - ETA: 0s - loss: 0.0143
4000/6530 [=================>............] - ETA: 0s - loss: 0.1457
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0143
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1453
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0143
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1467
4000/6530 [=================>............] - ETA: 0s - loss: 0.0143
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1466
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0142
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1466
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0142
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1464
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0142
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1464
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0141
6530/6530 [==============================] - 1s 146us/step - loss: 0.1462 - val_loss: 0.1378
Epoch 33/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1705
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0141
 400/6530 [>.............................] - ETA: 0s - loss: 0.1478
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0141
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1493
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0141
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1491
6464/6530 [============================>.] - ETA: 0s - loss: 0.0142
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1466
6530/6530 [==============================] - 1s 171us/step - loss: 0.0142 - val_loss: 0.0148
Epoch 24/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0122
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1466
 336/6530 [>.............................] - ETA: 1s - loss: 0.0131
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1468
 640/6530 [=>............................] - ETA: 0s - loss: 0.0149
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1471
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0145
2992/6530 [============>.................] - ETA: 0s - loss: 0.1452
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0145
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1446
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0143
3728/6530 [================>.............] - ETA: 0s - loss: 0.1456
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0143
4080/6530 [=================>............] - ETA: 0s - loss: 0.1460
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0141
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1461
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0142
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1466
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0140
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1463
3136/6530 [=============>................] - ETA: 0s - loss: 0.0139
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1460
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0140
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1463
3776/6530 [================>.............] - ETA: 0s - loss: 0.0140
6320/6530 [============================>.] - ETA: 0s - loss: 0.1461
4112/6530 [=================>............] - ETA: 0s - loss: 0.0139
6530/6530 [==============================] - 1s 142us/step - loss: 0.1459 - val_loss: 0.1371

4432/6530 [===================>..........] - ETA: 0s - loss: 0.0137Epoch 34/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1734
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0137
 400/6530 [>.............................] - ETA: 0s - loss: 0.1465
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0138
 784/6530 [==>...........................] - ETA: 0s - loss: 0.1473
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0138
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1476
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0137
1488/6530 [=====>........................] - ETA: 0s - loss: 0.1444
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0138
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1440
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1439
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0140
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1443
6530/6530 [==============================] - 1s 168us/step - loss: 0.0139 - val_loss: 0.0181
Epoch 25/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0118
2944/6530 [============>.................] - ETA: 0s - loss: 0.1432
 320/6530 [>.............................] - ETA: 1s - loss: 0.0111
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1425
 624/6530 [=>............................] - ETA: 0s - loss: 0.0130
3696/6530 [===============>..............] - ETA: 0s - loss: 0.1433
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0139
4048/6530 [=================>............] - ETA: 0s - loss: 0.1439
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0139
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1437
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0137
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1447
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0137
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1445
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0138
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1445
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0139
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1448
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0138
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1446
3088/6530 [=============>................] - ETA: 0s - loss: 0.0136
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0137
6530/6530 [==============================] - 1s 144us/step - loss: 0.1444 - val_loss: 0.1366
Epoch 35/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1643
3728/6530 [================>.............] - ETA: 0s - loss: 0.0137
 400/6530 [>.............................] - ETA: 0s - loss: 0.1450
4048/6530 [=================>............] - ETA: 0s - loss: 0.0138
 784/6530 [==>...........................] - ETA: 0s - loss: 0.1452
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0135
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1455
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0136
1520/6530 [=====>........................] - ETA: 0s - loss: 0.1422
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0136
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1426
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0137
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1432
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0136
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1439
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0137
2944/6530 [============>.................] - ETA: 0s - loss: 0.1426
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0137
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1421
6480/6530 [============================>.] - ETA: 0s - loss: 0.0136
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1431
6530/6530 [==============================] - 1s 170us/step - loss: 0.0136 - val_loss: 0.0178
Epoch 26/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0142
4048/6530 [=================>............] - ETA: 0s - loss: 0.1435
 320/6530 [>.............................] - ETA: 1s - loss: 0.0128
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1434
 624/6530 [=>............................] - ETA: 0s - loss: 0.0140
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1441
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0142
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1440
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0138
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1438
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0136
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1443
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0136
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1441
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0137
6530/6530 [==============================] - 1s 145us/step - loss: 0.1438 - val_loss: 0.1359
Epoch 36/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1665
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0139
 368/6530 [>.............................] - ETA: 0s - loss: 0.1420
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0138
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1487
3120/6530 [=============>................] - ETA: 0s - loss: 0.0136
1104/6530 [====>.........................] - ETA: 0s - loss: 0.1477
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0137
1456/6530 [=====>........................] - ETA: 0s - loss: 0.1442
3744/6530 [================>.............] - ETA: 0s - loss: 0.0134
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1444
4048/6530 [=================>............] - ETA: 0s - loss: 0.0135
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1434
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0134
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1438
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0135
2912/6530 [============>.................] - ETA: 0s - loss: 0.1429
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0135
3248/6530 [=============>................] - ETA: 0s - loss: 0.1420
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0134
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1432
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0134
3984/6530 [=================>............] - ETA: 0s - loss: 0.1439
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0134
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1434
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0135
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1445
6464/6530 [============================>.] - ETA: 0s - loss: 0.0136
6530/6530 [==============================] - 1s 171us/step - loss: 0.0136 - val_loss: 0.0163

5104/6530 [======================>.......] - ETA: 0s - loss: 0.1443Epoch 27/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0175
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1442
 320/6530 [>.............................] - ETA: 1s - loss: 0.0122
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1447
 640/6530 [=>............................] - ETA: 0s - loss: 0.0133
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1443
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0136
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0138
6530/6530 [==============================] - 1s 146us/step - loss: 0.1439 - val_loss: 0.1355
Epoch 37/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1679
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0135
 368/6530 [>.............................] - ETA: 0s - loss: 0.1424
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0135
 720/6530 [==>...........................] - ETA: 0s - loss: 0.1486
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0136
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1480
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0138
1456/6530 [=====>........................] - ETA: 0s - loss: 0.1441
2832/6530 [============>.................] - ETA: 0s - loss: 0.0136
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1435
3136/6530 [=============>................] - ETA: 0s - loss: 0.0135
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1433
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0137
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1439
3760/6530 [================>.............] - ETA: 0s - loss: 0.0136
2928/6530 [============>.................] - ETA: 0s - loss: 0.1428
4064/6530 [=================>............] - ETA: 0s - loss: 0.0136
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1419
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0134
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1431
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0134
4016/6530 [=================>............] - ETA: 0s - loss: 0.1436
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0133
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1434
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0134
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1444
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0134
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1442
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0134
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1439
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0135
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1444
6528/6530 [============================>.] - ETA: 0s - loss: 0.0134
6530/6530 [==============================] - 1s 169us/step - loss: 0.0134 - val_loss: 0.0156
Epoch 28/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0061
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1440
 320/6530 [>.............................] - ETA: 1s - loss: 0.0122
6530/6530 [==============================] - 1s 145us/step - loss: 0.1436 - val_loss: 0.1350
Epoch 38/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1679
 624/6530 [=>............................] - ETA: 0s - loss: 0.0126
 368/6530 [>.............................] - ETA: 0s - loss: 0.1434
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0131
 752/6530 [==>...........................] - ETA: 0s - loss: 0.1465
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0130
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1467
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0127
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1437
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0126
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1426
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0129
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1423
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0131
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1435
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0130
2976/6530 [============>.................] - ETA: 0s - loss: 0.1421
3120/6530 [=============>................] - ETA: 0s - loss: 0.0129
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1421
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0130
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1428
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0130
4032/6530 [=================>............] - ETA: 0s - loss: 0.1434
4016/6530 [=================>............] - ETA: 0s - loss: 0.0130
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1433
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0128
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1437
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0128
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1435
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0128
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1433
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0128
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1435
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0128
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1433
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0128
6530/6530 [==============================] - 1s 144us/step - loss: 0.1430 - val_loss: 0.1348
Epoch 39/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1562
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0129
 400/6530 [>.............................] - ETA: 0s - loss: 0.1445
6512/6530 [============================>.] - ETA: 0s - loss: 0.0128
6530/6530 [==============================] - 1s 170us/step - loss: 0.0129 - val_loss: 0.0176

 768/6530 [==>...........................] - ETA: 0s - loss: 0.1443
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1438
1488/6530 [=====>........................] - ETA: 0s - loss: 0.1414
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1426
# training | RMSE: 0.1259, MAE: 0.1000
worker 0  xfile  [4, 81, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3014747197109999}, 'layer_2_size': 79, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37619115972170725}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.432264982318203}, 'layer_4_size': 18, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31335263854200923}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.12587688040155523, 'rmse': 0.12587688040155523, 'mae': 0.10004312083355246, 'early_stop': True}
vggnet done  0

2192/6530 [=========>....................] - ETA: 0s - loss: 0.1418
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1428
2992/6530 [============>.................] - ETA: 0s - loss: 0.1411
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1415
3712/6530 [================>.............] - ETA: 0s - loss: 0.1423
4080/6530 [=================>............] - ETA: 0s - loss: 0.1427
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1430
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1431
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1429
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1429
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1431
6336/6530 [============================>.] - ETA: 0s - loss: 0.1427
6530/6530 [==============================] - 1s 141us/step - loss: 0.1426 - val_loss: 0.1339
Epoch 40/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1580
 384/6530 [>.............................] - ETA: 0s - loss: 0.1436
 784/6530 [==>...........................] - ETA: 0s - loss: 0.1456
1168/6530 [====>.........................] - ETA: 0s - loss: 0.1441
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1431
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1419
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1412
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1420
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1425
3104/6530 [=============>................] - ETA: 0s - loss: 0.1406
3408/6530 [==============>...............] - ETA: 0s - loss: 0.1417
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1422
3936/6530 [=================>............] - ETA: 0s - loss: 0.1426
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1423
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1426
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1430
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1431
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1432
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1434
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1431
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1433
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1430
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1431
6448/6530 [============================>.] - ETA: 0s - loss: 0.1428
6530/6530 [==============================] - 1s 192us/step - loss: 0.1428 - val_loss: 0.1334
Epoch 41/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1575
 272/6530 [>.............................] - ETA: 1s - loss: 0.1405
 512/6530 [=>............................] - ETA: 1s - loss: 0.1444
 720/6530 [==>...........................] - ETA: 1s - loss: 0.1460
 928/6530 [===>..........................] - ETA: 1s - loss: 0.1438
1136/6530 [====>.........................] - ETA: 1s - loss: 0.1452
1328/6530 [=====>........................] - ETA: 1s - loss: 0.1431
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1416
1744/6530 [=======>......................] - ETA: 1s - loss: 0.1417
1984/6530 [========>.....................] - ETA: 1s - loss: 0.1409
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1417
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1420
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1424
2960/6530 [============>.................] - ETA: 0s - loss: 0.1410
3184/6530 [=============>................] - ETA: 0s - loss: 0.1398
3408/6530 [==============>...............] - ETA: 0s - loss: 0.1411
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1414
3856/6530 [================>.............] - ETA: 0s - loss: 0.1418
4064/6530 [=================>............] - ETA: 0s - loss: 0.1420
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1416
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1420
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1424
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1426
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1428
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1425
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1421
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1425
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1422
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1423
6432/6530 [============================>.] - ETA: 0s - loss: 0.1421
6530/6530 [==============================] - 2s 243us/step - loss: 0.1420 - val_loss: 0.1329
Epoch 42/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1630
 224/6530 [>.............................] - ETA: 1s - loss: 0.1450
 432/6530 [>.............................] - ETA: 1s - loss: 0.1449
 656/6530 [==>...........................] - ETA: 1s - loss: 0.1472
 896/6530 [===>..........................] - ETA: 1s - loss: 0.1449
1120/6530 [====>.........................] - ETA: 1s - loss: 0.1458
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1438
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1419
1744/6530 [=======>......................] - ETA: 1s - loss: 0.1423
1968/6530 [========>.....................] - ETA: 1s - loss: 0.1417
2208/6530 [=========>....................] - ETA: 1s - loss: 0.1415
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1418
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1422
2864/6530 [============>.................] - ETA: 0s - loss: 0.1411
3088/6530 [=============>................] - ETA: 0s - loss: 0.1407
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1406
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1414
3744/6530 [================>.............] - ETA: 0s - loss: 0.1417
4000/6530 [=================>............] - ETA: 0s - loss: 0.1423
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1421
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1430
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1429
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1427
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1428
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1423
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1429
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1428
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1424
6528/6530 [============================>.] - ETA: 0s - loss: 0.1422
6530/6530 [==============================] - 2s 231us/step - loss: 0.1422 - val_loss: 0.1320
Epoch 43/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1614
 240/6530 [>.............................] - ETA: 1s - loss: 0.1419
 480/6530 [=>............................] - ETA: 1s - loss: 0.1480
 704/6530 [==>...........................] - ETA: 1s - loss: 0.1476
 960/6530 [===>..........................] - ETA: 1s - loss: 0.1450
1232/6530 [====>.........................] - ETA: 1s - loss: 0.1446
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1430
1744/6530 [=======>......................] - ETA: 0s - loss: 0.1426
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1418
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1412
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1416
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1422
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1415
3040/6530 [============>.................] - ETA: 0s - loss: 0.1402
3264/6530 [=============>................] - ETA: 0s - loss: 0.1405
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1416
3728/6530 [================>.............] - ETA: 0s - loss: 0.1418
3968/6530 [=================>............] - ETA: 0s - loss: 0.1423
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1421
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1422
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1426
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1428
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1428
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1425
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1424
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1425
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1425
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1424
6530/6530 [==============================] - 1s 225us/step - loss: 0.1421 - val_loss: 0.1316
Epoch 44/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1598
 304/6530 [>.............................] - ETA: 1s - loss: 0.1393
 592/6530 [=>............................] - ETA: 1s - loss: 0.1443
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1436
1168/6530 [====>.........................] - ETA: 0s - loss: 0.1429
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1419
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1401
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1389
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1396
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1405
2912/6530 [============>.................] - ETA: 0s - loss: 0.1394
3216/6530 [=============>................] - ETA: 0s - loss: 0.1388
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1402
3792/6530 [================>.............] - ETA: 0s - loss: 0.1403
4048/6530 [=================>............] - ETA: 0s - loss: 0.1410
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1408
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1414
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1415
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1417
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1414
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1413
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1416
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1416
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1413
6400/6530 [============================>.] - ETA: 0s - loss: 0.1409
6530/6530 [==============================] - 1s 203us/step - loss: 0.1411 - val_loss: 0.1306
Epoch 45/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1710
 224/6530 [>.............................] - ETA: 1s - loss: 0.1445
 416/6530 [>.............................] - ETA: 1s - loss: 0.1421
 608/6530 [=>............................] - ETA: 1s - loss: 0.1435
 800/6530 [==>...........................] - ETA: 1s - loss: 0.1441
 992/6530 [===>..........................] - ETA: 1s - loss: 0.1430
1168/6530 [====>.........................] - ETA: 1s - loss: 0.1428
1344/6530 [=====>........................] - ETA: 1s - loss: 0.1413
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1401
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1409
1920/6530 [=======>......................] - ETA: 1s - loss: 0.1398
2192/6530 [=========>....................] - ETA: 1s - loss: 0.1400
2496/6530 [==========>...................] - ETA: 1s - loss: 0.1403
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1402
3024/6530 [============>.................] - ETA: 0s - loss: 0.1388
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1391
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1400
3840/6530 [================>.............] - ETA: 0s - loss: 0.1404
4080/6530 [=================>............] - ETA: 0s - loss: 0.1407
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1403
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1410
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1411
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1409
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1410
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1413
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1407
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1414
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1409
6320/6530 [============================>.] - ETA: 0s - loss: 0.1408
6512/6530 [============================>.] - ETA: 0s - loss: 0.1408
6530/6530 [==============================] - 2s 240us/step - loss: 0.1408 - val_loss: 0.1303
Epoch 46/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1487
 240/6530 [>.............................] - ETA: 1s - loss: 0.1393
 448/6530 [=>............................] - ETA: 1s - loss: 0.1423
 672/6530 [==>...........................] - ETA: 1s - loss: 0.1441
 912/6530 [===>..........................] - ETA: 1s - loss: 0.1431
1136/6530 [====>.........................] - ETA: 1s - loss: 0.1440
1360/6530 [=====>........................] - ETA: 1s - loss: 0.1416
1568/6530 [======>.......................] - ETA: 1s - loss: 0.1404
1792/6530 [=======>......................] - ETA: 1s - loss: 0.1406
1984/6530 [========>.....................] - ETA: 1s - loss: 0.1400
2208/6530 [=========>....................] - ETA: 1s - loss: 0.1404
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1409
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1414
2912/6530 [============>.................] - ETA: 0s - loss: 0.1404
3152/6530 [=============>................] - ETA: 0s - loss: 0.1397
3408/6530 [==============>...............] - ETA: 0s - loss: 0.1407
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1410
3856/6530 [================>.............] - ETA: 0s - loss: 0.1411
4032/6530 [=================>............] - ETA: 0s - loss: 0.1413
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1407
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1415
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1418
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1417
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1415
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1411
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1417
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1412
6448/6530 [============================>.] - ETA: 0s - loss: 0.1409
6530/6530 [==============================] - 1s 222us/step - loss: 0.1409 - val_loss: 0.1300
Epoch 47/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1598
 304/6530 [>.............................] - ETA: 1s - loss: 0.1375
 576/6530 [=>............................] - ETA: 1s - loss: 0.1424
 848/6530 [==>...........................] - ETA: 1s - loss: 0.1434
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1445
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1413
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1406
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1393
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1398
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1403
2960/6530 [============>.................] - ETA: 0s - loss: 0.1394
3200/6530 [=============>................] - ETA: 0s - loss: 0.1384
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1395
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1400
3808/6530 [================>.............] - ETA: 0s - loss: 0.1399
4016/6530 [=================>............] - ETA: 0s - loss: 0.1405
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1403
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1404
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1410
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1410
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1411
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1408
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1412
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1406
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1410
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1407
6368/6530 [============================>.] - ETA: 0s - loss: 0.1403
6530/6530 [==============================] - 1s 219us/step - loss: 0.1403 - val_loss: 0.1291
Epoch 48/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1625
 272/6530 [>.............................] - ETA: 1s - loss: 0.1411
 528/6530 [=>............................] - ETA: 1s - loss: 0.1412
 816/6530 [==>...........................] - ETA: 1s - loss: 0.1414
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1425
1456/6530 [=====>........................] - ETA: 0s - loss: 0.1401
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1393
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1382
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1389
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1388
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1394
2896/6530 [============>.................] - ETA: 0s - loss: 0.1387
3136/6530 [=============>................] - ETA: 0s - loss: 0.1376
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1390
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1389
3872/6530 [================>.............] - ETA: 0s - loss: 0.1397
4112/6530 [=================>............] - ETA: 0s - loss: 0.1396
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1392
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1397
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1398
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1401
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1400
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1401
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1401
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1400
6400/6530 [============================>.] - ETA: 0s - loss: 0.1396
6530/6530 [==============================] - 1s 213us/step - loss: 0.1398 - val_loss: 0.1291
Epoch 49/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1520
 256/6530 [>.............................] - ETA: 1s - loss: 0.1392
 496/6530 [=>............................] - ETA: 1s - loss: 0.1424
 720/6530 [==>...........................] - ETA: 1s - loss: 0.1418
 960/6530 [===>..........................] - ETA: 1s - loss: 0.1404
1184/6530 [====>.........................] - ETA: 1s - loss: 0.1406
1424/6530 [=====>........................] - ETA: 1s - loss: 0.1396
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1385
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1377
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1381
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1388
2880/6530 [============>.................] - ETA: 0s - loss: 0.1373
3152/6530 [=============>................] - ETA: 0s - loss: 0.1365
3408/6530 [==============>...............] - ETA: 0s - loss: 0.1378
3712/6530 [================>.............] - ETA: 0s - loss: 0.1384
4016/6530 [=================>............] - ETA: 0s - loss: 0.1388
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1388
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1392
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1393
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1393
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1391
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1392
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1392
6448/6530 [============================>.] - ETA: 0s - loss: 0.1388
6530/6530 [==============================] - 1s 191us/step - loss: 0.1388 - val_loss: 0.1280
Epoch 50/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1640
 256/6530 [>.............................] - ETA: 1s - loss: 0.1414
 528/6530 [=>............................] - ETA: 1s - loss: 0.1410
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1411
 992/6530 [===>..........................] - ETA: 1s - loss: 0.1409
1216/6530 [====>.........................] - ETA: 1s - loss: 0.1394
1456/6530 [=====>........................] - ETA: 1s - loss: 0.1388
1648/6530 [======>.......................] - ETA: 1s - loss: 0.1373
1840/6530 [=======>......................] - ETA: 1s - loss: 0.1376
2048/6530 [========>.....................] - ETA: 1s - loss: 0.1365
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1365
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1376
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1376
3056/6530 [=============>................] - ETA: 0s - loss: 0.1362
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1369
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1375
3936/6530 [=================>............] - ETA: 0s - loss: 0.1381
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1377
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1387
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1383
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1386
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1385
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1384
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1385
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1386
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1382
6384/6530 [============================>.] - ETA: 0s - loss: 0.1380
6530/6530 [==============================] - 1s 219us/step - loss: 0.1382 - val_loss: 0.1277
Epoch 51/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1726
 288/6530 [>.............................] - ETA: 1s - loss: 0.1380
 576/6530 [=>............................] - ETA: 1s - loss: 0.1390
 880/6530 [===>..........................] - ETA: 1s - loss: 0.1390
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1391
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1389
1712/6530 [======>.......................] - ETA: 0s - loss: 0.1376
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1366
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1371
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1378
2832/6530 [============>.................] - ETA: 0s - loss: 0.1372
3088/6530 [=============>................] - ETA: 0s - loss: 0.1364
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1369
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1372
3904/6530 [================>.............] - ETA: 0s - loss: 0.1377
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1374
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1382
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1383
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1380
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1380
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1381
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1381
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1382
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1380
6530/6530 [==============================] - 1s 197us/step - loss: 0.1377 - val_loss: 0.1269
Epoch 52/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1697
 208/6530 [..............................] - ETA: 1s - loss: 0.1394
 400/6530 [>.............................] - ETA: 1s - loss: 0.1385
 592/6530 [=>............................] - ETA: 1s - loss: 0.1392
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1398
 976/6530 [===>..........................] - ETA: 1s - loss: 0.1399
1168/6530 [====>.........................] - ETA: 1s - loss: 0.1399
1376/6530 [=====>........................] - ETA: 1s - loss: 0.1378
1584/6530 [======>.......................] - ETA: 1s - loss: 0.1375
1776/6530 [=======>......................] - ETA: 1s - loss: 0.1371
2000/6530 [========>.....................] - ETA: 1s - loss: 0.1364
2224/6530 [=========>....................] - ETA: 1s - loss: 0.1367
2464/6530 [==========>...................] - ETA: 1s - loss: 0.1366
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1373
2928/6530 [============>.................] - ETA: 0s - loss: 0.1368
3184/6530 [=============>................] - ETA: 0s - loss: 0.1360
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1374
3776/6530 [================>.............] - ETA: 0s - loss: 0.1372
4048/6530 [=================>............] - ETA: 0s - loss: 0.1376
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1374
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1379
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1379
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1379
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1378
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1374
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1378
5968/6530 [==========================>...] - ETA: 0s - loss: 0.1378
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1377
6400/6530 [============================>.] - ETA: 0s - loss: 0.1373
6530/6530 [==============================] - 2s 238us/step - loss: 0.1374 - val_loss: 0.1271
Epoch 53/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1614
 224/6530 [>.............................] - ETA: 1s - loss: 0.1406
 416/6530 [>.............................] - ETA: 1s - loss: 0.1400
 576/6530 [=>............................] - ETA: 1s - loss: 0.1393
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1404
 928/6530 [===>..........................] - ETA: 1s - loss: 0.1400
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1416
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1393
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1383
1680/6530 [======>.......................] - ETA: 1s - loss: 0.1373
1904/6530 [=======>......................] - ETA: 1s - loss: 0.1366
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1362
2432/6530 [==========>...................] - ETA: 1s - loss: 0.1368
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1374
2912/6530 [============>.................] - ETA: 0s - loss: 0.1366
3152/6530 [=============>................] - ETA: 0s - loss: 0.1358
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1366
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1370
3888/6530 [================>.............] - ETA: 0s - loss: 0.1375
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1372
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1379
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1380
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1378
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1378
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1375
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1376
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1374
6352/6530 [============================>.] - ETA: 0s - loss: 0.1373
6530/6530 [==============================] - 1s 226us/step - loss: 0.1372 - val_loss: 0.1265
Epoch 54/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1569
 336/6530 [>.............................] - ETA: 1s - loss: 0.1337
 656/6530 [==>...........................] - ETA: 0s - loss: 0.1409
 944/6530 [===>..........................] - ETA: 0s - loss: 0.1376
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1378
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1373
1680/6530 [======>.......................] - ETA: 0s - loss: 0.1364
1936/6530 [=======>......................] - ETA: 0s - loss: 0.1361
2224/6530 [=========>....................] - ETA: 0s - loss: 0.1364
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1368
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1368
3040/6530 [============>.................] - ETA: 0s - loss: 0.1360
3264/6530 [=============>................] - ETA: 0s - loss: 0.1363
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1376
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1375
3904/6530 [================>.............] - ETA: 0s - loss: 0.1375
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1375
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1374
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1380
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1380
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1380
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1376
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1375
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1377
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1379
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1375
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1373
6480/6530 [============================>.] - ETA: 0s - loss: 0.1372
6530/6530 [==============================] - 1s 226us/step - loss: 0.1372 - val_loss: 0.1263
Epoch 55/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1589
 208/6530 [..............................] - ETA: 1s - loss: 0.1390
 384/6530 [>.............................] - ETA: 1s - loss: 0.1371
 576/6530 [=>............................] - ETA: 1s - loss: 0.1370
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1377
1008/6530 [===>..........................] - ETA: 1s - loss: 0.1391
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1386
1552/6530 [======>.......................] - ETA: 1s - loss: 0.1367
1824/6530 [=======>......................] - ETA: 1s - loss: 0.1375
2080/6530 [========>.....................] - ETA: 1s - loss: 0.1356
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1366
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1376
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1372
3056/6530 [=============>................] - ETA: 0s - loss: 0.1363
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1366
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1376
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1376
3856/6530 [================>.............] - ETA: 0s - loss: 0.1376
4048/6530 [=================>............] - ETA: 0s - loss: 0.1380
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1375
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1377
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1382
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1382
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1379
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1378
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1382
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1377
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1379
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1380
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1377
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1376
6530/6530 [==============================] - 2s 254us/step - loss: 0.1373 - val_loss: 0.1254
Epoch 56/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1653
 256/6530 [>.............................] - ETA: 1s - loss: 0.1405
 512/6530 [=>............................] - ETA: 1s - loss: 0.1410
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1403
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1411
1344/6530 [=====>........................] - ETA: 1s - loss: 0.1380
1648/6530 [======>.......................] - ETA: 0s - loss: 0.1377
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1367
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1359
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1370
2864/6530 [============>.................] - ETA: 0s - loss: 0.1364
3152/6530 [=============>................] - ETA: 0s - loss: 0.1353
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1371
3760/6530 [================>.............] - ETA: 0s - loss: 0.1367
4064/6530 [=================>............] - ETA: 0s - loss: 0.1372
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1370
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1376
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1376
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1375
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1377
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1375
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1375
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1374
6480/6530 [============================>.] - ETA: 0s - loss: 0.1370
6530/6530 [==============================] - 1s 190us/step - loss: 0.1371 - val_loss: 0.1257
Epoch 57/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1558
 256/6530 [>.............................] - ETA: 1s - loss: 0.1375
 544/6530 [=>............................] - ETA: 1s - loss: 0.1353
 816/6530 [==>...........................] - ETA: 1s - loss: 0.1377
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1402
1328/6530 [=====>........................] - ETA: 1s - loss: 0.1369
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1356
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1359
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1347
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1350
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1359
2848/6530 [============>.................] - ETA: 0s - loss: 0.1355
3152/6530 [=============>................] - ETA: 0s - loss: 0.1345
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1361
3760/6530 [================>.............] - ETA: 0s - loss: 0.1360
4064/6530 [=================>............] - ETA: 0s - loss: 0.1370
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1365
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1370
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1367
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1364
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1362
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1364
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1365
6416/6530 [============================>.] - ETA: 0s - loss: 0.1361
6530/6530 [==============================] - 1s 191us/step - loss: 0.1360 - val_loss: 0.1254
Epoch 58/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1714
 272/6530 [>.............................] - ETA: 1s - loss: 0.1392
 576/6530 [=>............................] - ETA: 1s - loss: 0.1382
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1370
1200/6530 [====>.........................] - ETA: 0s - loss: 0.1365
1488/6530 [=====>........................] - ETA: 0s - loss: 0.1353
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1348
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1339
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1343
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1350
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1348
2992/6530 [============>.................] - ETA: 0s - loss: 0.1343
3200/6530 [=============>................] - ETA: 0s - loss: 0.1335
3408/6530 [==============>...............] - ETA: 0s - loss: 0.1346
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1352
3856/6530 [================>.............] - ETA: 0s - loss: 0.1358
4128/6530 [=================>............] - ETA: 0s - loss: 0.1359
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1363
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1370
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1367
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1366
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1364
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1366
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1365
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1362
6448/6530 [============================>.] - ETA: 0s - loss: 0.1360
6530/6530 [==============================] - 1s 210us/step - loss: 0.1361 - val_loss: 0.1247
Epoch 59/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1567
 224/6530 [>.............................] - ETA: 1s - loss: 0.1379
 480/6530 [=>............................] - ETA: 1s - loss: 0.1382
 736/6530 [==>...........................] - ETA: 1s - loss: 0.1381
 960/6530 [===>..........................] - ETA: 1s - loss: 0.1371
1152/6530 [====>.........................] - ETA: 1s - loss: 0.1399
1344/6530 [=====>........................] - ETA: 1s - loss: 0.1376
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1369
1744/6530 [=======>......................] - ETA: 1s - loss: 0.1368
1968/6530 [========>.....................] - ETA: 1s - loss: 0.1358
2224/6530 [=========>....................] - ETA: 1s - loss: 0.1355
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1363
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1371
2928/6530 [============>.................] - ETA: 0s - loss: 0.1363
3200/6530 [=============>................] - ETA: 0s - loss: 0.1357
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1374
3744/6530 [================>.............] - ETA: 0s - loss: 0.1372
4048/6530 [=================>............] - ETA: 0s - loss: 0.1378
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1372
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1380
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1378
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1377
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1378
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1374
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1376
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1373
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1373
6496/6530 [============================>.] - ETA: 0s - loss: 0.1368
6530/6530 [==============================] - 1s 222us/step - loss: 0.1368 - val_loss: 0.1246
Epoch 60/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1626
 256/6530 [>.............................] - ETA: 1s - loss: 0.1374
 496/6530 [=>............................] - ETA: 1s - loss: 0.1393
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1378
 992/6530 [===>..........................] - ETA: 1s - loss: 0.1371
1216/6530 [====>.........................] - ETA: 1s - loss: 0.1365
1408/6530 [=====>........................] - ETA: 1s - loss: 0.1357
1616/6530 [======>.......................] - ETA: 1s - loss: 0.1358
1792/6530 [=======>......................] - ETA: 1s - loss: 0.1349
2000/6530 [========>.....................] - ETA: 1s - loss: 0.1337
2240/6530 [=========>....................] - ETA: 1s - loss: 0.1337
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1340
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1349
2896/6530 [============>.................] - ETA: 0s - loss: 0.1341
3152/6530 [=============>................] - ETA: 0s - loss: 0.1331
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1344
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1349
3888/6530 [================>.............] - ETA: 0s - loss: 0.1351
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1351
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1358
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1358
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1357
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1358
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1354
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1355
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1357
6528/6530 [============================>.] - ETA: 0s - loss: 0.1352
6530/6530 [==============================] - 1s 212us/step - loss: 0.1352 - val_loss: 0.1245
Epoch 61/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1669
 336/6530 [>.............................] - ETA: 0s - loss: 0.1369
 640/6530 [=>............................] - ETA: 0s - loss: 0.1404
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1393
1200/6530 [====>.........................] - ETA: 0s - loss: 0.1391
1392/6530 [=====>........................] - ETA: 0s - loss: 0.1387
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1377
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1369
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1362
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1366
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1366
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1370
2848/6530 [============>.................] - ETA: 0s - loss: 0.1360
3088/6530 [=============>................] - ETA: 0s - loss: 0.1353
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1357
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1365
3856/6530 [================>.............] - ETA: 0s - loss: 0.1367
4112/6530 [=================>............] - ETA: 0s - loss: 0.1367
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1363
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1369
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1366
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1365
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1364
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1366
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1366
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1365
6512/6530 [============================>.] - ETA: 0s - loss: 0.1363
6530/6530 [==============================] - 1s 215us/step - loss: 0.1363 - val_loss: 0.1237
Epoch 62/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1671
 240/6530 [>.............................] - ETA: 1s - loss: 0.1383
 496/6530 [=>............................] - ETA: 1s - loss: 0.1408
 720/6530 [==>...........................] - ETA: 1s - loss: 0.1402
 944/6530 [===>..........................] - ETA: 1s - loss: 0.1392
1168/6530 [====>.........................] - ETA: 1s - loss: 0.1390
1392/6530 [=====>........................] - ETA: 1s - loss: 0.1368
1632/6530 [======>.......................] - ETA: 1s - loss: 0.1368
1840/6530 [=======>......................] - ETA: 1s - loss: 0.1365
2048/6530 [========>.....................] - ETA: 1s - loss: 0.1356
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1350
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1359
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1359
3072/6530 [=============>................] - ETA: 0s - loss: 0.1349
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1354
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1361
3872/6530 [================>.............] - ETA: 0s - loss: 0.1364
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1362
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1359
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1365
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1365
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1363
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1363
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1361
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1360
6336/6530 [============================>.] - ETA: 0s - loss: 0.1359
6530/6530 [==============================] - 1s 211us/step - loss: 0.1358 - val_loss: 0.1237
Epoch 63/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1725
 272/6530 [>.............................] - ETA: 1s - loss: 0.1376
 544/6530 [=>............................] - ETA: 1s - loss: 0.1391
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1406
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1419
1360/6530 [=====>........................] - ETA: 0s - loss: 0.1390
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1383
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1375
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1361
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1366
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1372
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1375
2896/6530 [============>.................] - ETA: 0s - loss: 0.1370
3136/6530 [=============>................] - ETA: 0s - loss: 0.1354
3408/6530 [==============>...............] - ETA: 0s - loss: 0.1368
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1370
3872/6530 [================>.............] - ETA: 0s - loss: 0.1373
4096/6530 [=================>............] - ETA: 0s - loss: 0.1373
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1366
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1371
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1371
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1370
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1372
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1370
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1366
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1369
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1366
6352/6530 [============================>.] - ETA: 0s - loss: 0.1363
6530/6530 [==============================] - 1s 226us/step - loss: 0.1363 - val_loss: 0.1239
Epoch 64/81

  16/6530 [..............................] - ETA: 0s - loss: 0.1379
 336/6530 [>.............................] - ETA: 1s - loss: 0.1357
 624/6530 [=>............................] - ETA: 1s - loss: 0.1405
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1378
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1406
1344/6530 [=====>........................] - ETA: 1s - loss: 0.1378
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1371
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1370
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1345
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1350
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1359
2832/6530 [============>.................] - ETA: 0s - loss: 0.1353
3120/6530 [=============>................] - ETA: 0s - loss: 0.1343
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1354
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1357
3904/6530 [================>.............] - ETA: 0s - loss: 0.1360
4128/6530 [=================>............] - ETA: 0s - loss: 0.1359
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1355
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1363
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1363
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1362
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1363
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1357
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1360
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1356
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1355
6480/6530 [============================>.] - ETA: 0s - loss: 0.1352
6530/6530 [==============================] - 1s 218us/step - loss: 0.1353 - val_loss: 0.1232
Epoch 65/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1429
 208/6530 [..............................] - ETA: 1s - loss: 0.1368
 416/6530 [>.............................] - ETA: 1s - loss: 0.1355
 592/6530 [=>............................] - ETA: 1s - loss: 0.1365
 848/6530 [==>...........................] - ETA: 1s - loss: 0.1369
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1390
1392/6530 [=====>........................] - ETA: 1s - loss: 0.1357
1712/6530 [======>.......................] - ETA: 1s - loss: 0.1355
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1338
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1341
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1351
2880/6530 [============>.................] - ETA: 0s - loss: 0.1343
3120/6530 [=============>................] - ETA: 0s - loss: 0.1338
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1350
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1357
3904/6530 [================>.............] - ETA: 0s - loss: 0.1357
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1354
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1357
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1361
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1362
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1363
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1359
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1361
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1359
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1362
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1358
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1357
6464/6530 [============================>.] - ETA: 0s - loss: 0.1355
6530/6530 [==============================] - 1s 226us/step - loss: 0.1356 - val_loss: 0.1232
Epoch 66/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1569
 240/6530 [>.............................] - ETA: 1s - loss: 0.1351
 496/6530 [=>............................] - ETA: 1s - loss: 0.1371
 720/6530 [==>...........................] - ETA: 1s - loss: 0.1388
 928/6530 [===>..........................] - ETA: 1s - loss: 0.1383
1168/6530 [====>.........................] - ETA: 1s - loss: 0.1388
1424/6530 [=====>........................] - ETA: 1s - loss: 0.1374
1680/6530 [======>.......................] - ETA: 1s - loss: 0.1362
1920/6530 [=======>......................] - ETA: 1s - loss: 0.1347
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1342
2384/6530 [=========>....................] - ETA: 0s - loss: 0.1345
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1356
2864/6530 [============>.................] - ETA: 0s - loss: 0.1346
3104/6530 [=============>................] - ETA: 0s - loss: 0.1337
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1347
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1351
3984/6530 [=================>............] - ETA: 0s - loss: 0.1354
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1346
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1352
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1349
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1348
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1349
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1352
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1349
6400/6530 [============================>.] - ETA: 0s - loss: 0.1345
6530/6530 [==============================] - 1s 200us/step - loss: 0.1347 - val_loss: 0.1232
Epoch 67/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1641
 288/6530 [>.............................] - ETA: 1s - loss: 0.1324
 544/6530 [=>............................] - ETA: 1s - loss: 0.1330
 800/6530 [==>...........................] - ETA: 1s - loss: 0.1359
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1385
1344/6530 [=====>........................] - ETA: 1s - loss: 0.1358
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1351
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1351
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1332
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1342
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1348
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1353
3056/6530 [=============>................] - ETA: 0s - loss: 0.1340
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1344
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1353
3904/6530 [================>.............] - ETA: 0s - loss: 0.1354
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1352
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1357
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1356
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1359
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1357
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1352
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1354
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1352
6448/6530 [============================>.] - ETA: 0s - loss: 0.1348
6530/6530 [==============================] - 1s 197us/step - loss: 0.1349 - val_loss: 0.1231
Epoch 68/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1826
 304/6530 [>.............................] - ETA: 1s - loss: 0.1330
 608/6530 [=>............................] - ETA: 1s - loss: 0.1389
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1377
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1383
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1354
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1355
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1339
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1351
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1354
3088/6530 [=============>................] - ETA: 0s - loss: 0.1344
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1353
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1358
3936/6530 [=================>............] - ETA: 0s - loss: 0.1358
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1355
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1361
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1358
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1357
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1359
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1356
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1358
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1356
6480/6530 [============================>.] - ETA: 0s - loss: 0.1351
6530/6530 [==============================] - 1s 180us/step - loss: 0.1352 - val_loss: 0.1227
Epoch 69/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1534
 288/6530 [>.............................] - ETA: 1s - loss: 0.1349
 528/6530 [=>............................] - ETA: 1s - loss: 0.1376
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1375
 960/6530 [===>..........................] - ETA: 1s - loss: 0.1370
1168/6530 [====>.........................] - ETA: 1s - loss: 0.1386
1376/6530 [=====>........................] - ETA: 1s - loss: 0.1369
1600/6530 [======>.......................] - ETA: 1s - loss: 0.1371
1856/6530 [=======>......................] - ETA: 1s - loss: 0.1360
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1345
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1353
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1361
2928/6530 [============>.................] - ETA: 0s - loss: 0.1354
3168/6530 [=============>................] - ETA: 0s - loss: 0.1344
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1359
3744/6530 [================>.............] - ETA: 0s - loss: 0.1356
3984/6530 [=================>............] - ETA: 0s - loss: 0.1361
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1355
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1358
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1357
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1355
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1357
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1353
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1355
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1351
6384/6530 [============================>.] - ETA: 0s - loss: 0.1347
6530/6530 [==============================] - 1s 210us/step - loss: 0.1349 - val_loss: 0.1227
Epoch 70/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1573
 288/6530 [>.............................] - ETA: 1s - loss: 0.1363
 560/6530 [=>............................] - ETA: 1s - loss: 0.1362
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1372
1136/6530 [====>.........................] - ETA: 1s - loss: 0.1389
1424/6530 [=====>........................] - ETA: 0s - loss: 0.1372
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1348
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1332
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1334
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1343
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1341
2912/6530 [============>.................] - ETA: 0s - loss: 0.1336
3120/6530 [=============>................] - ETA: 0s - loss: 0.1329
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1337
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1346
3824/6530 [================>.............] - ETA: 0s - loss: 0.1347
4080/6530 [=================>............] - ETA: 0s - loss: 0.1350
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1342
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1352
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1350
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1350
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1345
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1349
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1344
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1345
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1344
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1340
6530/6530 [==============================] - 1s 221us/step - loss: 0.1338 - val_loss: 0.1228
Epoch 71/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1581
 256/6530 [>.............................] - ETA: 1s - loss: 0.1322
 496/6530 [=>............................] - ETA: 1s - loss: 0.1338
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1339
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1371
1360/6530 [=====>........................] - ETA: 1s - loss: 0.1344
1616/6530 [======>.......................] - ETA: 0s - loss: 0.1338
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1316
2160/6530 [========>.....................] - ETA: 0s - loss: 0.1307
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1313
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1325
2880/6530 [============>.................] - ETA: 0s - loss: 0.1321
3120/6530 [=============>................] - ETA: 0s - loss: 0.1315
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1324
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1329
3776/6530 [================>.............] - ETA: 0s - loss: 0.1332
3984/6530 [=================>............] - ETA: 0s - loss: 0.1340
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1335
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1341
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1342
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1345
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1345
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1343
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1345
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1342
6480/6530 [============================>.] - ETA: 0s - loss: 0.1339
6530/6530 [==============================] - 1s 205us/step - loss: 0.1340 - val_loss: 0.1225
Epoch 72/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1543
 240/6530 [>.............................] - ETA: 1s - loss: 0.1353
 416/6530 [>.............................] - ETA: 1s - loss: 0.1332
 592/6530 [=>............................] - ETA: 1s - loss: 0.1348
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1366
 960/6530 [===>..........................] - ETA: 1s - loss: 0.1355
1152/6530 [====>.........................] - ETA: 1s - loss: 0.1376
1328/6530 [=====>........................] - ETA: 1s - loss: 0.1360
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1348
1728/6530 [======>.......................] - ETA: 1s - loss: 0.1343
1920/6530 [=======>......................] - ETA: 1s - loss: 0.1335
2144/6530 [========>.....................] - ETA: 1s - loss: 0.1327
2368/6530 [=========>....................] - ETA: 1s - loss: 0.1329
2624/6530 [===========>..................] - ETA: 1s - loss: 0.1344
2832/6530 [============>.................] - ETA: 0s - loss: 0.1340
3040/6530 [============>.................] - ETA: 0s - loss: 0.1330
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1330
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1338
3808/6530 [================>.............] - ETA: 0s - loss: 0.1338
4048/6530 [=================>............] - ETA: 0s - loss: 0.1342
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1336
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1344
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1346
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1348
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1342
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1342
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1346
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1343
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1343
6530/6530 [==============================] - 2s 239us/step - loss: 0.1342 - val_loss: 0.1224
Epoch 73/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1461
 240/6530 [>.............................] - ETA: 1s - loss: 0.1377
 448/6530 [=>............................] - ETA: 1s - loss: 0.1375
 640/6530 [=>............................] - ETA: 1s - loss: 0.1369
 848/6530 [==>...........................] - ETA: 1s - loss: 0.1357
1040/6530 [===>..........................] - ETA: 1s - loss: 0.1373
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1356
1424/6530 [=====>........................] - ETA: 1s - loss: 0.1356
1600/6530 [======>.......................] - ETA: 1s - loss: 0.1355
1792/6530 [=======>......................] - ETA: 1s - loss: 0.1342
2032/6530 [========>.....................] - ETA: 1s - loss: 0.1329
2272/6530 [=========>....................] - ETA: 1s - loss: 0.1332
2496/6530 [==========>...................] - ETA: 1s - loss: 0.1343
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1350
2944/6530 [============>.................] - ETA: 0s - loss: 0.1343
3136/6530 [=============>................] - ETA: 0s - loss: 0.1331
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1339
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1343
3792/6530 [================>.............] - ETA: 0s - loss: 0.1342
4048/6530 [=================>............] - ETA: 0s - loss: 0.1349
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1345
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1349
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1347
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1349
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1347
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1341
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1342
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1342
6530/6530 [==============================] - 1s 228us/step - loss: 0.1340 - val_loss: 0.1225
Epoch 74/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1545
 272/6530 [>.............................] - ETA: 1s - loss: 0.1281
 496/6530 [=>............................] - ETA: 1s - loss: 0.1320
 688/6530 [==>...........................] - ETA: 1s - loss: 0.1328
 912/6530 [===>..........................] - ETA: 1s - loss: 0.1341
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1363
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1343
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1337
1792/6530 [=======>......................] - ETA: 1s - loss: 0.1336
2064/6530 [========>.....................] - ETA: 1s - loss: 0.1330
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1334
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1343
2912/6530 [============>.................] - ETA: 0s - loss: 0.1335
3216/6530 [=============>................] - ETA: 0s - loss: 0.1330
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1340
3840/6530 [================>.............] - ETA: 0s - loss: 0.1345
4128/6530 [=================>............] - ETA: 0s - loss: 0.1345
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1344
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1349
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1347
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1347
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1347
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1350
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1347
6464/6530 [============================>.] - ETA: 0s - loss: 0.1344
6530/6530 [==============================] - 1s 199us/step - loss: 0.1344 - val_loss: 0.1213
Epoch 75/81

  16/6530 [..............................] - ETA: 2s - loss: 0.1460
 256/6530 [>.............................] - ETA: 1s - loss: 0.1355
 512/6530 [=>............................] - ETA: 1s - loss: 0.1354
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1348
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1371
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1363
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1354
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1343
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1329
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1342
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1344
2992/6530 [============>.................] - ETA: 0s - loss: 0.1332
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1334
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1340
3840/6530 [================>.............] - ETA: 0s - loss: 0.1339
4112/6530 [=================>............] - ETA: 0s - loss: 0.1338
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1336
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1343
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1341
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1338
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1340
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1336
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1338
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1336
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1337
6528/6530 [============================>.] - ETA: 0s - loss: 0.1334
6530/6530 [==============================] - 1s 205us/step - loss: 0.1334 - val_loss: 0.1216
Epoch 76/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1518
 256/6530 [>.............................] - ETA: 1s - loss: 0.1335
 528/6530 [=>............................] - ETA: 1s - loss: 0.1313
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1340
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1370
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1355
1440/6530 [=====>........................] - ETA: 1s - loss: 0.1343
1680/6530 [======>.......................] - ETA: 1s - loss: 0.1331
1936/6530 [=======>......................] - ETA: 0s - loss: 0.1316
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1316
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1319
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1328
2912/6530 [============>.................] - ETA: 0s - loss: 0.1327
3152/6530 [=============>................] - ETA: 0s - loss: 0.1322
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1334
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1337
3824/6530 [================>.............] - ETA: 0s - loss: 0.1340
4048/6530 [=================>............] - ETA: 0s - loss: 0.1340
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1337
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1343
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1343
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1342
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1341
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1339
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1342
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1339
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1338
6464/6530 [============================>.] - ETA: 0s - loss: 0.1336
6530/6530 [==============================] - 1s 224us/step - loss: 0.1336 - val_loss: 0.1218
Epoch 77/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1668
 288/6530 [>.............................] - ETA: 1s - loss: 0.1327
 560/6530 [=>............................] - ETA: 1s - loss: 0.1322
 816/6530 [==>...........................] - ETA: 1s - loss: 0.1351
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1364
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1343
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1338
1968/6530 [========>.....................] - ETA: 0s - loss: 0.1328
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1326
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1330
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1334
2912/6530 [============>.................] - ETA: 0s - loss: 0.1329
3168/6530 [=============>................] - ETA: 0s - loss: 0.1322
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1332
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1334
3840/6530 [================>.............] - ETA: 0s - loss: 0.1336
4048/6530 [=================>............] - ETA: 0s - loss: 0.1336
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1331
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1340
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1339
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1339
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1341
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1337
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1336
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1334
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1336
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1332
6368/6530 [============================>.] - ETA: 0s - loss: 0.1330
6530/6530 [==============================] - 1s 225us/step - loss: 0.1330 - val_loss: 0.1213
Epoch 78/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1597
 304/6530 [>.............................] - ETA: 1s - loss: 0.1326
 592/6530 [=>............................] - ETA: 1s - loss: 0.1359
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1355
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1359
1552/6530 [======>.......................] - ETA: 0s - loss: 0.1342
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1334
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1321
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1327
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1333
2976/6530 [============>.................] - ETA: 0s - loss: 0.1323
3184/6530 [=============>................] - ETA: 0s - loss: 0.1318
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1329
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1329
3776/6530 [================>.............] - ETA: 0s - loss: 0.1330
3968/6530 [=================>............] - ETA: 0s - loss: 0.1333
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1329
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1330
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1332
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1333
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1332
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1332
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1327
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1329
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1326
6320/6530 [============================>.] - ETA: 0s - loss: 0.1326
6496/6530 [============================>.] - ETA: 0s - loss: 0.1325
6530/6530 [==============================] - 1s 219us/step - loss: 0.1326 - val_loss: 0.1212
Epoch 79/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1522
 224/6530 [>.............................] - ETA: 1s - loss: 0.1347
 448/6530 [=>............................] - ETA: 1s - loss: 0.1354
 656/6530 [==>...........................] - ETA: 1s - loss: 0.1371
 848/6530 [==>...........................] - ETA: 1s - loss: 0.1346
1040/6530 [===>..........................] - ETA: 1s - loss: 0.1367
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1358
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1352
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1340
1936/6530 [=======>......................] - ETA: 1s - loss: 0.1331
2144/6530 [========>.....................] - ETA: 1s - loss: 0.1325
2368/6530 [=========>....................] - ETA: 1s - loss: 0.1333
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1345
2832/6530 [============>.................] - ETA: 0s - loss: 0.1334
3072/6530 [=============>................] - ETA: 0s - loss: 0.1325
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1327
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1332
3872/6530 [================>.............] - ETA: 0s - loss: 0.1336
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1333
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1329
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1336
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1337
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1338
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1333
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1334
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1332
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1336
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1333
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1334
6480/6530 [============================>.] - ETA: 0s - loss: 0.1331
6530/6530 [==============================] - 2s 241us/step - loss: 0.1332 - val_loss: 0.1211
Epoch 80/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1506
 224/6530 [>.............................] - ETA: 1s - loss: 0.1368
 448/6530 [=>............................] - ETA: 1s - loss: 0.1372
 672/6530 [==>...........................] - ETA: 1s - loss: 0.1375
 880/6530 [===>..........................] - ETA: 1s - loss: 0.1345
1120/6530 [====>.........................] - ETA: 1s - loss: 0.1361
1408/6530 [=====>........................] - ETA: 1s - loss: 0.1341
1680/6530 [======>.......................] - ETA: 1s - loss: 0.1328
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1315
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1313
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1323
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1332
2880/6530 [============>.................] - ETA: 0s - loss: 0.1323
3136/6530 [=============>................] - ETA: 0s - loss: 0.1316
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1330
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1335
3824/6530 [================>.............] - ETA: 0s - loss: 0.1337
4064/6530 [=================>............] - ETA: 0s - loss: 0.1341
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1334
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1339
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1338
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1339
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1333
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1334
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1337
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1336
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1337
6512/6530 [============================>.] - ETA: 0s - loss: 0.1335
6530/6530 [==============================] - 1s 225us/step - loss: 0.1335 - val_loss: 0.1212
Epoch 81/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1502
 256/6530 [>.............................] - ETA: 1s - loss: 0.1361
 496/6530 [=>............................] - ETA: 1s - loss: 0.1374
 688/6530 [==>...........................] - ETA: 1s - loss: 0.1360
 880/6530 [===>..........................] - ETA: 1s - loss: 0.1345
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1377
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1366
1440/6530 [=====>........................] - ETA: 1s - loss: 0.1360
1648/6530 [======>.......................] - ETA: 1s - loss: 0.1353
1872/6530 [=======>......................] - ETA: 1s - loss: 0.1342
2096/6530 [========>.....................] - ETA: 1s - loss: 0.1322
2352/6530 [=========>....................] - ETA: 1s - loss: 0.1332
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1346
2848/6530 [============>.................] - ETA: 0s - loss: 0.1345
3056/6530 [=============>................] - ETA: 0s - loss: 0.1339
3248/6530 [=============>................] - ETA: 0s - loss: 0.1335
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1344
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1346
3872/6530 [================>.............] - ETA: 0s - loss: 0.1347
4128/6530 [=================>............] - ETA: 0s - loss: 0.1344
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1341
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1349
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1347
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1345
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1343
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1344
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1343
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1342
6416/6530 [============================>.] - ETA: 0s - loss: 0.1337
6530/6530 [==============================] - 2s 233us/step - loss: 0.1338 - val_loss: 0.1212

# training | RMSE: 0.1491, MAE: 0.1149
worker 2  xfile  [2, 81, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20537784995900182}, 'layer_2_size': 39, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3182253766971461}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.14914512669365873, 'rmse': 0.14914512669365873, 'mae': 0.11489294638663224, 'early_stop': False}
vggnet done  2
all of workers have been done
calculation finished
#1 epoch=81 loss={'loss': 0.20384992456194864, 'rmse': 0.20384992456194864, 'mae': 0.16019485835484962, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3683057751741984}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 67, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2672256338472979}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10409852991432521}, 'layer_4_size': 43, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#0 epoch=81 loss={'loss': 0.10782994300053479, 'rmse': 0.10782994300053479, 'mae': 0.08458231751992143, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24230766296308875}, 'layer_2_size': 81, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26425376474083073}, 'layer_4_size': 23, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#3 epoch=81 loss={'loss': 0.1303229629236031, 'rmse': 0.1303229629236031, 'mae': 0.10065355875744092, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27793970530039186}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#4 epoch=81 loss={'loss': 0.12587688040155523, 'rmse': 0.12587688040155523, 'mae': 0.10004312083355246, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3014747197109999}, 'layer_2_size': 79, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37619115972170725}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.432264982318203}, 'layer_4_size': 18, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31335263854200923}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#2 epoch=81 loss={'loss': 0.14914512669365873, 'rmse': 0.14914512669365873, 'mae': 0.11489294638663224, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.20537784995900182}, 'layer_2_size': 39, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3182253766971461}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
get a list [results] of length 206
get a list [loss] of length 5
get a list [val_loss] of length 5
length of indices is (0, 4, 3, 2, 1)
length of indices is 5
length of T is 5
206 total, best:

loss: 6.75%  | #81.0th iterations | run 0 
("{'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', "
 "'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': "
 "'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, "
 "'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', "
 "'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': "
 "'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, "
 "'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': "
 "0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', "
 "'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', "
 "'shuffle': False}")

loss: 6.86%  | #27.0th iterations | run 1 
("{'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', "
 "'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, "
 "'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, "
 "'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': "
 "'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', "
 "'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': "
 "'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': "
 "0.32305615123930476}, 'layer_5_size': 87, 'loss': 'mean_squared_error', "
 "'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': "
 'False}')

loss: 7.11%  | #27.0th iterations | run 0 
("{'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', "
 "'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': "
 "'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, "
 "'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, "
 "'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': "
 "{'name': 'dropout', 'rate': 0.22893106919985234}, 'layer_4_size': 9, "
 "'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, "
 "'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 4, "
 "'optimizer': 'adam', 'scaler': None, 'shuffle': False}")

loss: 7.51%  | #27.0th iterations | run 0 
("{'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', "
 "'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': "
 "'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 84, "
 "'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', "
 "'rate': 0.1859160703073981}, 'layer_3_size': 83, 'layer_4_activation': "
 "'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, "
 "'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': "
 "0.13856508401929069}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', "
 "'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', "
 "'shuffle': False}")

loss: 7.99%  | #9.0th iterations | run 5 
("{'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', "
 "'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': "
 "'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, "
 "'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, "
 "'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': "
 "{'name': 'dropout', 'rate': 0.22893106919985234}, 'layer_4_size': 9, "
 "'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, "
 "'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 4, "
 "'optimizer': 'adam', 'scaler': None, 'shuffle': False}")

saving...

568 seconds.
