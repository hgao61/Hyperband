## The CPU implementaion of parallelize hyperparameter tuning algorithm

This is the CPU simulation implementatino of the parallel hyperparamter 
tuning algorithm. 

It's using the individual CPU core as workers for training. And use the
asynchronously executing of threads through the ThreadPoolExecutor.

More details about this package can be found at:
https://docs.python.org/3/library/concurrent.futures.html

It deployes three CPU workers, and creates a pool of 20 randomly 
generated int number within the range from 1 to 100. This pool is called 
*cong_pool*, representing the combination of hyperparamter configurations generated by
random search method. The reason for using randomly generated int number 
as elements of each entry in *cong_pool*
is to mimic the different processing time for individual GPU task. 
The function *return__future_result* is taking an entry from the *cong_pool* or 
*rung_pool*, sleep for a period of time and return the updated entry back to *rung_pool*. 

Also, a pool called *rung_pool* is created, with MAX_RUNG number of elements. This is to mimic the rung pool mentioned in the paper, through where the freed GPU worker will scan from higher rank to lower rank for promotable configurations.

A configuration is considered as promotable configuration under this situation: the number of configurations in the #th rung in the *rung_pool* meets the user specified number: eta.

The free GPU worker scan through higher rank to lower rank in the rung pool first. If it can't find any qualified configurations, it will grab a new configuration from the *cong_pool*.


Whenever the free GPU worker detects the promotable configuration, it will increase the configuraion's rank by 1 first, and training it for the given iterations. 


