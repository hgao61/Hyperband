loading data...
None
[0, 1, 2]
s=4
T is of size 81
T=[{'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.16149723882932443}, 'layer_3_size': 70, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12254503281447732}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 50, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13742751467075745}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 26, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.48779807601499936}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.13672626873389904}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3395701537007203}, 'layer_2_size': 56, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18358446062662281}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12243765775546467}, 'layer_4_size': 60, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15123969364242107}, 'layer_2_size': 97, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 11, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2805800228674691}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 2, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2705959581751023}, 'layer_2_size': 51, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 44, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3638345383735373}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17662534013240705}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 8, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2813567573149851}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.31136432916598167}, 'layer_3_size': 2, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.24457064043811153}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2895023053176152}, 'layer_1_size': 88, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.17593740513486156}, 'layer_2_size': 83, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 23, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 65, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.29698258592896976}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.30204101148674845}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.45911546730391384}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11334948869959081}, 'layer_2_size': 23, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 97, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3514355690518264}, 'layer_5_size': 17, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18108261962734026}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10416314660244544}, 'layer_2_size': 69, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1235273523668929}, 'layer_3_size': 48, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4953388185253027}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.32066176829549653}, 'layer_1_size': 56, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22006884379319352}, 'layer_4_size': 2, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4828959430995342}, 'layer_5_size': 41, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1325178683063519}, 'layer_3_size': 15, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 27, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10757573622851334}, 'layer_1_size': 100, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.33288135222369186}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24656785081188726}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3384337246940693}, 'layer_3_size': 30, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 89, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3763355894571466}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3279647395358488}, 'layer_1_size': 16, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 89, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 22, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2369344727551148}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 20, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10970613529878435}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47789425011377495}, 'layer_1_size': 74, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4450129062456337}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 75, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17153197723016456}, 'layer_1_size': 34, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14708139271545076}, 'layer_2_size': 77, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 63, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39137249636293736}, 'layer_1_size': 23, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22200695121501193}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17235765624521715}, 'layer_3_size': 51, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22759252164997124}, 'layer_5_size': 80, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3958506948699859}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3985947438007549}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10855261526378511}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 22, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 98, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 48, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3619910331156264}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.286597451825205}, 'layer_4_size': 88, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 94, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12643692050394267}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3676760429632786}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 69, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3110843672502082}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2555820286847462}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 5, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14897819670033827}, 'layer_2_size': 31, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27118399118299463}, 'layer_3_size': 2, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 73, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 49, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.32004886003186006}, 'layer_1_size': 38, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4812453420169397}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 40, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10789032155064207}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 28, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4283350651365785}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3455095972091068}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2508346160127697}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 72, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2623268259265953}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 82, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30306473984005766}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43085673746807795}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3242093808736577}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41255552454864375}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 92, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39023019053840957}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4908483209465242}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.18134188180962785}, 'layer_4_size': 60, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 5, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 63, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.181589516080453}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.44647201048090357}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 2, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 34, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22542703942748094}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21353699840031415}, 'layer_1_size': 12, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1887525394629789}, 'layer_2_size': 64, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27407622708939067}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 98, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4222044144905175}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 2, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4322557475652903}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.36962354345532367}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 19, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.154058449532362}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 47, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16005469532790345}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 21, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 83, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41122132628080166}, 'layer_4_size': 93, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21837030191686663}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22624305720097207}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4326772368856566}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27774682114412147}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39114648997625634}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.40623525449208187}, 'layer_2_size': 3, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 93, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 6, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.29015795012512713}, 'layer_5_size': 10, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45809535997411455}, 'layer_1_size': 56, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4137789166294171}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4302478130034917}, 'layer_3_size': 59, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3709287657001251}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2905420164871645}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 42, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2799649795995304}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 71, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47194424194851814}, 'layer_2_size': 82, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.43826874021108175}, 'layer_4_size': 78, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10489365671261544}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15602519089682165}, 'layer_1_size': 47, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3662071301271337}, 'layer_2_size': 70, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4752384599654145}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48429916230485714}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28042922014676985}, 'layer_2_size': 32, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 81, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17246170518745202}, 'layer_1_size': 92, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1433120180375093}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21864295299191291}, 'layer_4_size': 37, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14309072567378933}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11896381784270203}, 'layer_3_size': 81, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1872715376547708}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38953016521482287}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 76, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35047528622460133}, 'layer_4_size': 78, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 9, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2577702797077809}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.16685390974228376}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41851260000555746}, 'layer_3_size': 81, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21760004908081332}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39118890592267064}, 'layer_2_size': 74, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 42, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2939361354953432}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 76, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21552202662751477}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19200815181849593}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21790253153759923}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3854686318955314}, 'layer_1_size': 80, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4486193665141055}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4306695586227791}, 'layer_1_size': 65, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.33496577595643284}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31657417017907896}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 50, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 98, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1666171614564424}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3510927047964897}, 'layer_1_size': 97, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47912796968769367}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22468178723709364}, 'layer_3_size': 47, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]
newly formed T structure is:[[0, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.16149723882932443}, 'layer_3_size': 70, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [1, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12254503281447732}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 50, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13742751467075745}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [2, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 26, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [3, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.48779807601499936}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.13672626873389904}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [4, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3395701537007203}, 'layer_2_size': 56, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18358446062662281}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12243765775546467}, 'layer_4_size': 60, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [5, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15123969364242107}, 'layer_2_size': 97, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 11, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2805800228674691}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 2, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [6, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2705959581751023}, 'layer_2_size': 51, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [7, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 44, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3638345383735373}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [8, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17662534013240705}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [9, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 8, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [10, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2813567573149851}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.31136432916598167}, 'layer_3_size': 2, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.24457064043811153}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [11, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2895023053176152}, 'layer_1_size': 88, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.17593740513486156}, 'layer_2_size': 83, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 23, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [12, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 65, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.29698258592896976}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.30204101148674845}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.45911546730391384}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [13, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11334948869959081}, 'layer_2_size': 23, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 97, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3514355690518264}, 'layer_5_size': 17, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [14, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18108261962734026}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [15, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10416314660244544}, 'layer_2_size': 69, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1235273523668929}, 'layer_3_size': 48, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4953388185253027}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [16, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.32066176829549653}, 'layer_1_size': 56, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22006884379319352}, 'layer_4_size': 2, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4828959430995342}, 'layer_5_size': 41, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [17, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1325178683063519}, 'layer_3_size': 15, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 27, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [18, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10757573622851334}, 'layer_1_size': 100, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.33288135222369186}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [19, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24656785081188726}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3384337246940693}, 'layer_3_size': 30, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [20, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 89, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [21, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3763355894571466}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [22, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3279647395358488}, 'layer_1_size': 16, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 89, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 22, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2369344727551148}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [23, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 20, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10970613529878435}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}], [24, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47789425011377495}, 'layer_1_size': 74, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4450129062456337}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 75, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [25, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17153197723016456}, 'layer_1_size': 34, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14708139271545076}, 'layer_2_size': 77, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 63, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [26, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39137249636293736}, 'layer_1_size': 23, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22200695121501193}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17235765624521715}, 'layer_3_size': 51, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22759252164997124}, 'layer_5_size': 80, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [27, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3958506948699859}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3985947438007549}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [28, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [29, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10855261526378511}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [30, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [31, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 22, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 98, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [32, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 48, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3619910331156264}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.286597451825205}, 'layer_4_size': 88, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [33, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 94, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12643692050394267}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [34, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3676760429632786}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 69, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [35, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3110843672502082}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2555820286847462}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [36, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 5, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [37, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14897819670033827}, 'layer_2_size': 31, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27118399118299463}, 'layer_3_size': 2, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 73, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 49, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [38, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.32004886003186006}, 'layer_1_size': 38, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4812453420169397}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 40, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10789032155064207}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [39, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 28, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4283350651365785}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3455095972091068}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [40, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2508346160127697}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 72, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [41, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2623268259265953}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 82, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30306473984005766}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [42, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43085673746807795}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3242093808736577}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [43, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41255552454864375}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 92, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [44, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39023019053840957}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [45, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4908483209465242}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.18134188180962785}, 'layer_4_size': 60, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [46, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 5, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [47, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 63, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [48, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.181589516080453}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.44647201048090357}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 2, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 34, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22542703942748094}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [49, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21353699840031415}, 'layer_1_size': 12, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1887525394629789}, 'layer_2_size': 64, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27407622708939067}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [50, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 98, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4222044144905175}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 2, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [51, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4322557475652903}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.36962354345532367}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [52, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 19, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.154058449532362}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [53, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 47, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16005469532790345}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [54, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 21, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 83, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41122132628080166}, 'layer_4_size': 93, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [55, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21837030191686663}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22624305720097207}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4326772368856566}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27774682114412147}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [56, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39114648997625634}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [57, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.40623525449208187}, 'layer_2_size': 3, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 93, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 6, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.29015795012512713}, 'layer_5_size': 10, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [58, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45809535997411455}, 'layer_1_size': 56, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4137789166294171}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4302478130034917}, 'layer_3_size': 59, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3709287657001251}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [59, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2905420164871645}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [60, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 42, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2799649795995304}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 71, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [61, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47194424194851814}, 'layer_2_size': 82, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.43826874021108175}, 'layer_4_size': 78, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10489365671261544}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [62, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15602519089682165}, 'layer_1_size': 47, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3662071301271337}, 'layer_2_size': 70, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [63, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4752384599654145}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48429916230485714}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [64, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28042922014676985}, 'layer_2_size': 32, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 81, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [65, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17246170518745202}, 'layer_1_size': 92, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1433120180375093}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21864295299191291}, 'layer_4_size': 37, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14309072567378933}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [66, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11896381784270203}, 'layer_3_size': 81, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1872715376547708}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [67, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38953016521482287}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 76, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35047528622460133}, 'layer_4_size': 78, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [68, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 9, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2577702797077809}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.16685390974228376}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [69, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [70, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41851260000555746}, 'layer_3_size': 81, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [71, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21760004908081332}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39118890592267064}, 'layer_2_size': 74, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 42, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2939361354953432}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [72, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 76, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21552202662751477}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19200815181849593}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [73, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21790253153759923}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [74, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3854686318955314}, 'layer_1_size': 80, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4486193665141055}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [75, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4306695586227791}, 'layer_1_size': 65, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [76, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [77, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.33496577595643284}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31657417017907896}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [78, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 50, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 98, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1666171614564424}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [79, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3510927047964897}, 'layer_1_size': 97, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47912796968769367}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22468178723709364}, 'layer_3_size': 47, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [80, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]] 

*** 81 configurations x 1.0 iterations each

1 | Thu Sep 27 22:59:11 2018 | lowest loss so far: inf (run -1)

{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  67 | activation: sigmoid | extras: dropout - rate: 12.3% 
layer 2 | size:  53 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f082f5fb240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:02 - loss: 0.4176
 864/6530 [==>...........................] - ETA: 6s - loss: 0.1664  
1856/6530 [=======>......................] - ETA: 2s - loss: 0.1185
2816/6530 [===========>..................] - ETA: 1s - loss: 0.1021{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  64 | activation: tanh    | extras: batchnorm 
layer 2 | size:  13 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f082f5fb240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:59 - loss: 1.2729
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0957
 336/6530 [>.............................] - ETA: 17s - loss: 0.7101 
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0897
 688/6530 [==>...........................] - ETA: 8s - loss: 0.4979 
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0855
1040/6530 [===>..........................] - ETA: 5s - loss: 0.3875
6528/6530 [============================>.] - ETA: 0s - loss: 0.0827
1392/6530 [=====>........................] - ETA: 4s - loss: 0.3302
6530/6530 [==============================] - 1s 197us/step - loss: 0.0827 - val_loss: 0.0683

1760/6530 [=======>......................] - ETA: 3s - loss: 0.2858
2096/6530 [========>.....................] - ETA: 2s - loss: 0.2612
2448/6530 [==========>...................] - ETA: 2s - loss: 0.2421
2576/6530 [==========>...................] - ETA: 2s - loss: 0.2360
2864/6530 [============>.................] - ETA: 1s - loss: 0.2227
3104/6530 [=============>................] - ETA: 1s - loss: 0.2130
3392/6530 [==============>...............] - ETA: 1s - loss: 0.2022
3680/6530 [===============>..............] - ETA: 1s - loss: 0.1927
3952/6530 [=================>............] - ETA: 1s - loss: 0.1864
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1805{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  83 | activation: tanh    | extras: batchnorm 
layer 2 | size:  26 | activation: relu    | extras: batchnorm 
layer 3 | size:  25 | activation: tanh    | extras: None 
layer 4 | size:  63 | activation: tanh    | extras: batchnorm 
layer 5 | size:  36 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f082f5fb240>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:53 - loss: 0.5618
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1756
 384/6530 [>.............................] - ETA: 19s - loss: 0.5586 
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1700
 768/6530 [==>...........................] - ETA: 9s - loss: 0.4998 
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1629
1184/6530 [====>.........................] - ETA: 5s - loss: 0.4348
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1573
1632/6530 [======>.......................] - ETA: 4s - loss: 0.3665
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1519
2080/6530 [========>.....................] - ETA: 3s - loss: 0.3001
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1474
2496/6530 [==========>...................] - ETA: 2s - loss: 0.2576
6416/6530 [============================>.] - ETA: 0s - loss: 0.1429
2912/6530 [============>.................] - ETA: 1s - loss: 0.2256
# training | RMSE: 0.2627, MAE: 0.2150
worker 1  xfile  [1, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12254503281447732}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 50, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13742751467075745}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.262704264602928, 'rmse': 0.262704264602928, 'mae': 0.2149827019257636, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  33 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  10 | activation: relu    | extras: dropout - rate: 48.8% 
layer 3 | size:  13 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f07ba30eb38>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 8s - loss: 0.6453
3296/6530 [==============>...............] - ETA: 1s - loss: 0.2032
5888/6530 [==========================>...] - ETA: 0s - loss: 0.4595
6530/6530 [==============================] - 2s 317us/step - loss: 0.1416 - val_loss: 0.0513

6530/6530 [==============================] - 0s 66us/step - loss: 0.4542 - val_loss: 0.2829

3712/6530 [================>.............] - ETA: 1s - loss: 0.1839
4096/6530 [=================>............] - ETA: 0s - loss: 0.1695
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1563
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1455
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1361
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1293
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1229
6528/6530 [============================>.] - ETA: 0s - loss: 0.1168
6530/6530 [==============================] - 2s 317us/step - loss: 0.1167 - val_loss: 0.0217

# training | RMSE: 0.3468, MAE: 0.2847
worker 1  xfile  [3, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.48779807601499936}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.13672626873389904}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.3467875106657467, 'rmse': 0.3467875106657467, 'mae': 0.2846748548932067, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  34 | activation: sigmoid | extras: None 
layer 2 | size:  56 | activation: sigmoid | extras: dropout - rate: 34.0% 
layer 3 | size:  28 | activation: tanh    | extras: dropout - rate: 18.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0778371630>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 11s - loss: 1.0792
3456/6530 [==============>...............] - ETA: 0s - loss: 0.4601 
6530/6530 [==============================] - 0s 54us/step - loss: 0.3659 - val_loss: 0.0647

# training | RMSE: 0.2219, MAE: 0.1756
worker 0  xfile  [0, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.16149723882932443}, 'layer_3_size': 70, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.22186077301511778, 'rmse': 0.22186077301511778, 'mae': 0.17557900975182777, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  64 | activation: sigmoid | extras: None 
layer 2 | size:  97 | activation: tanh    | extras: dropout - rate: 15.1% 
layer 3 | size:  11 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  32 | activation: sigmoid | extras: dropout - rate: 28.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f07b81fe0b8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 41s - loss: 1.6112
# training | RMSE: 0.2573, MAE: 0.2105
worker 1  xfile  [4, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3395701537007203}, 'layer_2_size': 56, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18358446062662281}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12243765775546467}, 'layer_4_size': 60, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.257289723707473, 'rmse': 0.257289723707473, 'mae': 0.2105048871965249, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  16 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f0778371470>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 57s - loss: 0.5671
1024/6530 [===>..........................] - ETA: 2s - loss: 1.3828 
 624/6530 [=>............................] - ETA: 1s - loss: 0.1749 
2112/6530 [========>.....................] - ETA: 1s - loss: 1.0771
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1179
3200/6530 [=============>................] - ETA: 0s - loss: 0.8308
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0977
4416/6530 [===================>..........] - ETA: 0s - loss: 0.6648
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0868
5632/6530 [========================>.....] - ETA: 0s - loss: 0.5617
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0794
4048/6530 [=================>............] - ETA: 0s - loss: 0.0745
6530/6530 [==============================] - 1s 116us/step - loss: 0.5083 - val_loss: 0.2331

4816/6530 [=====================>........] - ETA: 0s - loss: 0.0703
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0679
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0658
6530/6530 [==============================] - 1s 102us/step - loss: 0.0647 - val_loss: 0.0489

# training | RMSE: 0.2718, MAE: 0.2301
worker 0  xfile  [5, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15123969364242107}, 'layer_2_size': 97, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 11, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2805800228674691}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 2, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.27175185968145343, 'rmse': 0.27175185968145343, 'mae': 0.2301439723701545, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  81 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f07b818b0b8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 4s - loss: 1.4837
6530/6530 [==============================] - 0s 35us/step - loss: 0.9856 - val_loss: 0.5345

# training | RMSE: 0.1436, MAE: 0.1106
worker 2  xfile  [2, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 26, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.14363537723480127, 'rmse': 0.14363537723480127, 'mae': 0.11056388543489429, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  86 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  51 | activation: sigmoid | extras: dropout - rate: 27.1% 
layer 3 | size:  22 | activation: tanh    | extras: batchnorm 
layer 4 | size:  24 | activation: tanh    | extras: None 
layer 5 | size:  24 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f07ba30eb70>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:05 - loss: 0.9875
 832/6530 [==>...........................] - ETA: 4s - loss: 0.7727  
1536/6530 [======>.......................] - ETA: 2s - loss: 0.6804
2368/6530 [=========>....................] - ETA: 1s - loss: 0.5880
3136/6530 [=============>................] - ETA: 0s - loss: 0.5297
3904/6530 [================>.............] - ETA: 0s - loss: 0.4850
4672/6530 [====================>.........] - ETA: 0s - loss: 0.4478
5504/6530 [========================>.....] - ETA: 0s - loss: 0.4203
6272/6530 [===========================>..] - ETA: 0s - loss: 0.3972
6530/6530 [==============================] - 1s 176us/step - loss: 0.3907 - val_loss: 0.1899

# training | RMSE: 0.2210, MAE: 0.1798
worker 1  xfile  [7, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 44, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3638345383735373}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.22095463773543367, 'rmse': 0.22095463773543367, 'mae': 0.17984716632043246, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  43 | activation: relu    | extras: batchnorm 
layer 2 | size:  37 | activation: relu    | extras: batchnorm 
layer 3 | size:   8 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f047469c4e0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:34 - loss: 0.4601
 544/6530 [=>............................] - ETA: 5s - loss: 0.3823  
1088/6530 [===>..........................] - ETA: 2s - loss: 0.3183
1664/6530 [======>.......................] - ETA: 1s - loss: 0.2671
2272/6530 [=========>....................] - ETA: 1s - loss: 0.2250
# training | RMSE: 0.7273, MAE: 0.5989
worker 0  xfile  [8, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17662534013240705}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.7273343010408425, 'rmse': 0.7273343010408425, 'mae': 0.598906247181177, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  35 | activation: sigmoid | extras: dropout - rate: 28.1% 
layer 2 | size:  70 | activation: sigmoid | extras: batchnorm 
layer 3 | size:   2 | activation: relu    | extras: dropout - rate: 31.1% 
layer 4 | size:  12 | activation: sigmoid | extras: dropout - rate: 24.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f07887c1358>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:42 - loss: 1.0103
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1940
 608/6530 [=>............................] - ETA: 5s - loss: 0.8199  
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1678
1216/6530 [====>.........................] - ETA: 2s - loss: 0.6590
3968/6530 [=================>............] - ETA: 0s - loss: 0.1477
1792/6530 [=======>......................] - ETA: 1s - loss: 0.5435
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1320
2368/6530 [=========>....................] - ETA: 1s - loss: 0.4695
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1197
2976/6530 [============>.................] - ETA: 0s - loss: 0.4257
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1098
3520/6530 [===============>..............] - ETA: 0s - loss: 0.3947
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1015
4096/6530 [=================>............] - ETA: 0s - loss: 0.3708
4704/6530 [====================>.........] - ETA: 0s - loss: 0.3524
# training | RMSE: 0.2449, MAE: 0.1918
worker 2  xfile  [6, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2705959581751023}, 'layer_2_size': 51, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.244887105262517, 'rmse': 0.244887105262517, 'mae': 0.19176989211404036, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  88 | activation: tanh    | extras: dropout - rate: 29.0% 
layer 2 | size:  83 | activation: sigmoid | extras: dropout - rate: 17.6% 
layer 3 | size:  14 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0790bcb748>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 14s - loss: 1.3801
6530/6530 [==============================] - 1s 174us/step - loss: 0.0976 - val_loss: 0.0143

5280/6530 [=======================>......] - ETA: 0s - loss: 0.3378
3840/6530 [================>.............] - ETA: 0s - loss: 0.3992 
5888/6530 [==========================>...] - ETA: 0s - loss: 0.3251
6496/6530 [============================>.] - ETA: 0s - loss: 0.3149
6530/6530 [==============================] - 0s 65us/step - loss: 0.3489 - val_loss: 0.2232

6530/6530 [==============================] - 1s 175us/step - loss: 0.3144 - val_loss: 0.2057

# training | RMSE: 0.1167, MAE: 0.0906
worker 1  xfile  [9, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 8, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.11667032512754376, 'rmse': 0.11667032512754376, 'mae': 0.09059037300781238, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  17 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f04745cfc18>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 19s - loss: 0.6774
2880/6530 [============>.................] - ETA: 0s - loss: 0.3083 
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2471
# training | RMSE: 0.2623, MAE: 0.2209
worker 2  xfile  [11, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2895023053176152}, 'layer_1_size': 88, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.17593740513486156}, 'layer_2_size': 83, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 23, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.26234691295939616, 'rmse': 0.26234691295939616, 'mae': 0.22086147405691808, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  12 | activation: tanh    | extras: None 
layer 2 | size:  23 | activation: relu    | extras: dropout - rate: 11.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0474572a58>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 27s - loss: 0.7046
6530/6530 [==============================] - 0s 52us/step - loss: 0.2396 - val_loss: 0.1713

2048/6530 [========>.....................] - ETA: 0s - loss: 0.6909 
3904/6530 [================>.............] - ETA: 0s - loss: 0.6311
5952/6530 [==========================>...] - ETA: 0s - loss: 0.5102
6530/6530 [==============================] - 0s 74us/step - loss: 0.4799 - val_loss: 0.1652

# training | RMSE: 0.2550, MAE: 0.2086
worker 0  xfile  [10, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2813567573149851}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.31136432916598167}, 'layer_3_size': 2, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.24457064043811153}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2550249323189136, 'rmse': 0.2550249323189136, 'mae': 0.20864166800252704, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  48 | activation: sigmoid | extras: None 
layer 2 | size:  48 | activation: relu    | extras: batchnorm 
layer 3 | size:  37 | activation: sigmoid | extras: dropout - rate: 18.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f07881d66a0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:05 - loss: 0.6473
# training | RMSE: 0.2140, MAE: 0.1717
worker 1  xfile  [12, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 65, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.29698258592896976}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.30204101148674845}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.45911546730391384}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2139776830497447, 'rmse': 0.2139776830497447, 'mae': 0.1716508403516478, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  58 | activation: tanh    | extras: None 
layer 2 | size:  69 | activation: sigmoid | extras: dropout - rate: 10.4% 
layer 3 | size:  48 | activation: tanh    | extras: dropout - rate: 12.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0467737470>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 7s - loss: 0.4184
 336/6530 [>.............................] - ETA: 9s - loss: 0.4554  
 640/6530 [=>............................] - ETA: 5s - loss: 0.3456
6530/6530 [==============================] - 0s 58us/step - loss: 0.1475 - val_loss: 0.0427

# training | RMSE: 0.2125, MAE: 0.1644
worker 2  xfile  [13, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11334948869959081}, 'layer_2_size': 23, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 97, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3514355690518264}, 'layer_5_size': 17, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.21250770074622807, 'rmse': 0.21250770074622807, 'mae': 0.16442821736999788, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  56 | activation: tanh    | extras: dropout - rate: 32.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f04742cc208>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 5s - loss: 0.7269
 944/6530 [===>..........................] - ETA: 3s - loss: 0.2904
1264/6530 [====>.........................] - ETA: 2s - loss: 0.2603
6530/6530 [==============================] - 0s 46us/step - loss: 0.5909 - val_loss: 0.4580

1584/6530 [======>.......................] - ETA: 2s - loss: 0.2427
1904/6530 [=======>......................] - ETA: 1s - loss: 0.2294
2224/6530 [=========>....................] - ETA: 1s - loss: 0.2205
2560/6530 [==========>...................] - ETA: 1s - loss: 0.2129
2880/6530 [============>.................] - ETA: 1s - loss: 0.2077
3152/6530 [=============>................] - ETA: 1s - loss: 0.2049
3456/6530 [==============>...............] - ETA: 0s - loss: 0.2014
3744/6530 [================>.............] - ETA: 0s - loss: 0.1974
4064/6530 [=================>............] - ETA: 0s - loss: 0.1952
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1933
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1907
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1893
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1873
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1860
5968/6530 [==========================>...] - ETA: 0s - loss: 0.1853
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1841
6530/6530 [==============================] - 2s 246us/step - loss: 0.1834 - val_loss: 0.1983

# training | RMSE: 0.2050, MAE: 0.1631
worker 1  xfile  [15, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10416314660244544}, 'layer_2_size': 69, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1235273523668929}, 'layer_3_size': 48, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4953388185253027}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2050090941966866, 'rmse': 0.2050090941966866, 'mae': 0.16306016692450104, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  77 | activation: relu    | extras: None 
layer 2 | size:  99 | activation: relu    | extras: batchnorm 
layer 3 | size:  15 | activation: sigmoid | extras: dropout - rate: 13.3% 
layer 4 | size:  27 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f0467737358>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:54 - loss: 1.2709
 576/6530 [=>............................] - ETA: 6s - loss: 0.9297  
1120/6530 [====>.........................] - ETA: 3s - loss: 0.6044
1664/6530 [======>.......................] - ETA: 2s - loss: 0.4631
2240/6530 [=========>....................] - ETA: 1s - loss: 0.3759
# training | RMSE: 0.5392, MAE: 0.4707
worker 2  xfile  [16, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.32066176829549653}, 'layer_1_size': 56, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22006884379319352}, 'layer_4_size': 2, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4828959430995342}, 'layer_5_size': 41, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.5392046499925002, 'rmse': 0.5392046499925002, 'mae': 0.47073461649969095, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size: 100 | activation: relu    | extras: dropout - rate: 10.8% 
layer 2 | size:  45 | activation: relu    | extras: dropout - rate: 33.3% 
layer 3 | size:  32 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  68 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f04742cc080>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 16s - loss: 1.0624
2784/6530 [===========>..................] - ETA: 1s - loss: 0.3276
3840/6530 [================>.............] - ETA: 0s - loss: 0.6670 
3360/6530 [==============>...............] - ETA: 0s - loss: 0.2904
3904/6530 [================>.............] - ETA: 0s - loss: 0.2632
6530/6530 [==============================] - 1s 124us/step - loss: 0.5735 - val_loss: 0.2973

4448/6530 [===================>..........] - ETA: 0s - loss: 0.2425
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2249
# training | RMSE: 0.2372, MAE: 0.1960
worker 0  xfile  [14, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18108261962734026}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2371620384223731, 'rmse': 0.2371620384223731, 'mae': 0.19604312404621727, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  67 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0770298d30>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 5s - loss: 1.0162
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2102
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1989
6530/6530 [==============================] - 0s 48us/step - loss: 0.4598 - val_loss: 0.2457

6530/6530 [==============================] - 1s 192us/step - loss: 0.1901 - val_loss: 0.0291

# training | RMSE: 0.3549, MAE: 0.2861
worker 2  xfile  [18, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10757573622851334}, 'layer_1_size': 100, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.33288135222369186}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.35490751890116473, 'rmse': 0.35490751890116473, 'mae': 0.2861201387013932, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  70 | activation: relu    | extras: None 
layer 2 | size:  89 | activation: tanh    | extras: None 
layer 3 | size:  69 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f04741841d0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:16 - loss: 0.5411
 704/6530 [==>...........................] - ETA: 3s - loss: 0.2374  
1344/6530 [=====>........................] - ETA: 1s - loss: 0.1465
1952/6530 [=======>......................] - ETA: 1s - loss: 0.1148
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0957
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0828
# training | RMSE: 0.3069, MAE: 0.2523
worker 0  xfile  [19, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24656785081188726}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3384337246940693}, 'layer_3_size': 30, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.3069006180398186, 'rmse': 0.3069006180398186, 'mae': 0.25233237024874217, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  76 | activation: relu    | extras: None 
layer 2 | size:  49 | activation: sigmoid | extras: dropout - rate: 37.6% 
layer 3 | size:  40 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0478178f28>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:31 - loss: 0.6035
4096/6530 [=================>............] - ETA: 0s - loss: 0.0758
 368/6530 [>.............................] - ETA: 7s - loss: 0.5101  
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0700
 688/6530 [==>...........................] - ETA: 4s - loss: 0.3792
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0651
1088/6530 [===>..........................] - ETA: 2s - loss: 0.3234
6464/6530 [============================>.] - ETA: 0s - loss: 0.0610
# training | RMSE: 0.1632, MAE: 0.1286
worker 1  xfile  [17, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1325178683063519}, 'layer_3_size': 15, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 27, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.16318723193723106, 'rmse': 0.16318723193723106, 'mae': 0.12855008375065757, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  16 | activation: relu    | extras: dropout - rate: 32.8% 
layer 2 | size:  89 | activation: relu    | extras: None 
layer 3 | size:  22 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f04673c3390>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:49 - loss: 1.0353
1504/6530 [=====>........................] - ETA: 1s - loss: 0.2953
 432/6530 [>.............................] - ETA: 6s - loss: 0.4929  
6530/6530 [==============================] - 1s 138us/step - loss: 0.0607 - val_loss: 0.0561

1904/6530 [=======>......................] - ETA: 1s - loss: 0.2744
 832/6530 [==>...........................] - ETA: 3s - loss: 0.3980
2320/6530 [=========>....................] - ETA: 1s - loss: 0.2594
1216/6530 [====>.........................] - ETA: 2s - loss: 0.3498
2736/6530 [===========>..................] - ETA: 1s - loss: 0.2473
1584/6530 [======>.......................] - ETA: 1s - loss: 0.3193
3152/6530 [=============>................] - ETA: 0s - loss: 0.2399
1952/6530 [=======>......................] - ETA: 1s - loss: 0.2997
3568/6530 [===============>..............] - ETA: 0s - loss: 0.2326
2336/6530 [=========>....................] - ETA: 1s - loss: 0.2858
4000/6530 [=================>............] - ETA: 0s - loss: 0.2257
2704/6530 [===========>..................] - ETA: 1s - loss: 0.2748
4432/6530 [===================>..........] - ETA: 0s - loss: 0.2194
3056/6530 [=============>................] - ETA: 0s - loss: 0.2638
4848/6530 [=====================>........] - ETA: 0s - loss: 0.2148
3456/6530 [==============>...............] - ETA: 0s - loss: 0.2549
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2113
3872/6530 [================>.............] - ETA: 0s - loss: 0.2491
5648/6530 [========================>.....] - ETA: 0s - loss: 0.2083
4256/6530 [==================>...........] - ETA: 0s - loss: 0.2437
6032/6530 [==========================>...] - ETA: 0s - loss: 0.2059
4640/6530 [====================>.........] - ETA: 0s - loss: 0.2395
6448/6530 [============================>.] - ETA: 0s - loss: 0.2028
5040/6530 [======================>.......] - ETA: 0s - loss: 0.2344
5424/6530 [=======================>......] - ETA: 0s - loss: 0.2312
6530/6530 [==============================] - 1s 197us/step - loss: 0.2024 - val_loss: 0.1597

5824/6530 [=========================>....] - ETA: 0s - loss: 0.2281
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2255
# training | RMSE: 0.2331, MAE: 0.1920
worker 2  xfile  [20, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 89, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.23306661576592533, 'rmse': 0.23306661576592533, 'mae': 0.1920436948756081, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  20 | activation: sigmoid | extras: None 
layer 2 | size:  14 | activation: sigmoid | extras: None 
layer 3 | size:  84 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0474184208>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 8s - loss: 0.1590
6530/6530 [==============================] - 1s 207us/step - loss: 0.2225 - val_loss: 0.1649

6530/6530 [==============================] - 0s 68us/step - loss: 0.0786 - val_loss: 0.0670

# training | RMSE: 0.1939, MAE: 0.1558
worker 0  xfile  [21, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3763355894571466}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.1939094029789395, 'rmse': 0.1939094029789395, 'mae': 0.15582493974421957, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  74 | activation: relu    | extras: dropout - rate: 47.8% 
layer 2 | size:  36 | activation: tanh    | extras: dropout - rate: 44.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f047804d0f0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:11 - loss: 0.6952
1088/6530 [===>..........................] - ETA: 2s - loss: 0.2546  
2080/6530 [========>.....................] - ETA: 0s - loss: 0.2188
3136/6530 [=============>................] - ETA: 0s - loss: 0.2042
# training | RMSE: 0.2041, MAE: 0.1626
worker 1  xfile  [22, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3279647395358488}, 'layer_1_size': 16, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 89, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 22, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2369344727551148}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20414756886250035, 'rmse': 0.20414756886250035, 'mae': 0.16258614468183427, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  23 | activation: tanh    | extras: dropout - rate: 39.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f04673c3278>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 6s - loss: 0.5557
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1983
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1932
6530/6530 [==============================] - 0s 55us/step - loss: 0.1984 - val_loss: 0.0928

6336/6530 [============================>.] - ETA: 0s - loss: 0.1897
6530/6530 [==============================] - 1s 111us/step - loss: 0.1889 - val_loss: 0.1845

# training | RMSE: 0.2607, MAE: 0.2131
worker 2  xfile  [23, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 20, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10970613529878435}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.26074026624151153, 'rmse': 0.26074026624151153, 'mae': 0.2131434504079487, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  34 | activation: relu    | extras: dropout - rate: 17.2% 
layer 2 | size:  77 | activation: relu    | extras: dropout - rate: 14.7% 
layer 3 | size:  91 | activation: relu    | extras: batchnorm 
layer 4 | size:  63 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f044e9a8828>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:05 - loss: 1.2408
 960/6530 [===>..........................] - ETA: 4s - loss: 0.7478  
1920/6530 [=======>......................] - ETA: 1s - loss: 0.5658
2880/6530 [============>.................] - ETA: 1s - loss: 0.4608
3712/6530 [================>.............] - ETA: 0s - loss: 0.4008
4672/6530 [====================>.........] - ETA: 0s - loss: 0.3540
5760/6530 [=========================>....] - ETA: 0s - loss: 0.3139
6530/6530 [==============================] - 1s 166us/step - loss: 0.2935 - val_loss: 0.0906

# training | RMSE: 0.2205, MAE: 0.1818
worker 0  xfile  [24, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47789425011377495}, 'layer_1_size': 74, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4450129062456337}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 75, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.22050932338117418, 'rmse': 0.22050932338117418, 'mae': 0.1818300775966546, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  15 | activation: relu    | extras: batchnorm 
layer 2 | size:  29 | activation: tanh    | extras: batchnorm 
layer 3 | size:  56 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0456dbbe48>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:15 - loss: 1.4398
 240/6530 [>.............................] - ETA: 17s - loss: 0.5736 
 464/6530 [=>............................] - ETA: 9s - loss: 0.4198 
 688/6530 [==>...........................] - ETA: 6s - loss: 0.3452
 912/6530 [===>..........................] - ETA: 5s - loss: 0.2918
1136/6530 [====>.........................] - ETA: 4s - loss: 0.2562
1392/6530 [=====>........................] - ETA: 3s - loss: 0.2269
# training | RMSE: 0.3035, MAE: 0.2470
worker 1  xfile  [26, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39137249636293736}, 'layer_1_size': 23, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22200695121501193}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17235765624521715}, 'layer_3_size': 51, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22759252164997124}, 'layer_5_size': 80, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.303484738442385, 'rmse': 0.303484738442385, 'mae': 0.2470276698360866, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  31 | activation: tanh    | extras: dropout - rate: 39.6% 
layer 2 | size:  95 | activation: relu    | extras: batchnorm 
layer 3 | size:  98 | activation: tanh    | extras: batchnorm 
layer 4 | size:  12 | activation: relu    | extras: dropout - rate: 39.9% 
layer 5 | size:  52 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f0466b31c18>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:53 - loss: 5.2868
1600/6530 [======>.......................] - ETA: 3s - loss: 0.2114
 192/6530 [..............................] - ETA: 30s - loss: 3.9086 
1840/6530 [=======>......................] - ETA: 2s - loss: 0.1952
 384/6530 [>.............................] - ETA: 15s - loss: 3.0401
2096/6530 [========>.....................] - ETA: 2s - loss: 0.1807
 592/6530 [=>............................] - ETA: 10s - loss: 2.3348
2352/6530 [=========>....................] - ETA: 2s - loss: 0.1691
 800/6530 [==>...........................] - ETA: 7s - loss: 1.8434 
2592/6530 [==========>...................] - ETA: 1s - loss: 0.1604
1008/6530 [===>..........................] - ETA: 6s - loss: 1.5140
2848/6530 [============>.................] - ETA: 1s - loss: 0.1530
1200/6530 [====>.........................] - ETA: 5s - loss: 1.2998
# training | RMSE: 0.3045, MAE: 0.2418
worker 2  xfile  [25, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17153197723016456}, 'layer_1_size': 34, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14708139271545076}, 'layer_2_size': 77, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 63, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.3045042852810891, 'rmse': 0.3045042852810891, 'mae': 0.24182052712339197, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  88 | activation: sigmoid | extras: None 
layer 2 | size:  13 | activation: sigmoid | extras: None 
layer 3 | size:  13 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  17 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f044ed4b208>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:11 - loss: 1.0409
3088/6530 [=============>................] - ETA: 1s - loss: 0.1462
1392/6530 [=====>........................] - ETA: 4s - loss: 1.1465
 640/6530 [=>............................] - ETA: 6s - loss: 0.3452  
3344/6530 [==============>...............] - ETA: 1s - loss: 0.1398
1584/6530 [======>.......................] - ETA: 4s - loss: 1.0219
1312/6530 [=====>........................] - ETA: 2s - loss: 0.1985
3600/6530 [===============>..............] - ETA: 1s - loss: 0.1345
1776/6530 [=======>......................] - ETA: 3s - loss: 0.9229
1984/6530 [========>.....................] - ETA: 1s - loss: 0.1455
3872/6530 [================>.............] - ETA: 1s - loss: 0.1294
1984/6530 [========>.....................] - ETA: 3s - loss: 0.8394
2624/6530 [===========>..................] - ETA: 1s - loss: 0.1201
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1244
2176/6530 [========>.....................] - ETA: 2s - loss: 0.7766
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1035
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1206
2368/6530 [=========>....................] - ETA: 2s - loss: 0.7221
3904/6530 [================>.............] - ETA: 0s - loss: 0.0931
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1181
2544/6530 [==========>...................] - ETA: 2s - loss: 0.6787
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0848
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1145
2736/6530 [===========>..................] - ETA: 2s - loss: 0.6373
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0786
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1111
2928/6530 [============>.................] - ETA: 2s - loss: 0.6008
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0731
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1086
3120/6530 [=============>................] - ETA: 1s - loss: 0.5684
6496/6530 [============================>.] - ETA: 0s - loss: 0.0686
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1057
3312/6530 [==============>...............] - ETA: 1s - loss: 0.5402
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1032
3520/6530 [===============>..............] - ETA: 1s - loss: 0.5130
6530/6530 [==============================] - 1s 193us/step - loss: 0.0684 - val_loss: 0.0564

6176/6530 [===========================>..] - ETA: 0s - loss: 0.1009
3712/6530 [================>.............] - ETA: 1s - loss: 0.4902
6448/6530 [============================>.] - ETA: 0s - loss: 0.0988
3904/6530 [================>.............] - ETA: 1s - loss: 0.4690
4096/6530 [=================>............] - ETA: 1s - loss: 0.4503
6530/6530 [==============================] - 2s 322us/step - loss: 0.0981 - val_loss: 0.0573

4304/6530 [==================>...........] - ETA: 1s - loss: 0.4316
4512/6530 [===================>..........] - ETA: 0s - loss: 0.4152
4720/6530 [====================>.........] - ETA: 0s - loss: 0.3999
4944/6530 [=====================>........] - ETA: 0s - loss: 0.3849
5168/6530 [======================>.......] - ETA: 0s - loss: 0.3708
5360/6530 [=======================>......] - ETA: 0s - loss: 0.3597
5568/6530 [========================>.....] - ETA: 0s - loss: 0.3485
5776/6530 [=========================>....] - ETA: 0s - loss: 0.3380
5984/6530 [==========================>...] - ETA: 0s - loss: 0.3282
6208/6530 [===========================>..] - ETA: 0s - loss: 0.3181
6416/6530 [============================>.] - ETA: 0s - loss: 0.3096
6530/6530 [==============================] - 3s 412us/step - loss: 0.3051 - val_loss: 0.0358

# training | RMSE: 0.2327, MAE: 0.1853
worker 2  xfile  [29, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10855261526378511}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.23273625047446353, 'rmse': 0.23273625047446353, 'mae': 0.18534387846511508, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  43 | activation: relu    | extras: None 
layer 2 | size:  34 | activation: tanh    | extras: None 
layer 3 | size:  83 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f044e1f6780>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:37 - loss: 0.7724
 800/6530 [==>...........................] - ETA: 3s - loss: 0.3215  
1536/6530 [======>.......................] - ETA: 1s - loss: 0.2730
# training | RMSE: 0.2308, MAE: 0.1841
worker 0  xfile  [28, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2308171341444946, 'rmse': 0.2308171341444946, 'mae': 0.18410875991541242, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  22 | activation: relu    | extras: None 
layer 2 | size:  98 | activation: sigmoid | extras: None 
layer 3 | size:  93 | activation: relu    | extras: None 
layer 4 | size:  17 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f04780cbf98>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 20s - loss: 0.7500
2304/6530 [=========>....................] - ETA: 1s - loss: 0.2518
3840/6530 [================>.............] - ETA: 0s - loss: 0.3380 
2944/6530 [============>.................] - ETA: 0s - loss: 0.2362
3744/6530 [================>.............] - ETA: 0s - loss: 0.2251
6530/6530 [==============================] - 1s 85us/step - loss: 0.2193 - val_loss: 0.0589

4512/6530 [===================>..........] - ETA: 0s - loss: 0.2184
5312/6530 [=======================>......] - ETA: 0s - loss: 0.2131
6080/6530 [==========================>...] - ETA: 0s - loss: 0.2066
6530/6530 [==============================] - 1s 151us/step - loss: 0.2034 - val_loss: 0.2895

# training | RMSE: 0.1883, MAE: 0.1498
worker 1  xfile  [27, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3958506948699859}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3985947438007549}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.18826842162022026, 'rmse': 0.18826842162022026, 'mae': 0.14983851003486867, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  85 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  48 | activation: relu    | extras: batchnorm 
layer 3 | size:  60 | activation: tanh    | extras: dropout - rate: 36.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0466e9ad30>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 39s - loss: 0.9680
# training | RMSE: 0.2412, MAE: 0.1985
worker 0  xfile  [31, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 22, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 98, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2412170917797639, 'rmse': 0.2412170917797639, 'mae': 0.19852241213717106, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  94 | activation: tanh    | extras: batchnorm 
layer 2 | size:  15 | activation: tanh    | extras: batchnorm 
layer 3 | size:  85 | activation: relu    | extras: dropout - rate: 12.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0456b8b400>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 18s - loss: 1.4106
1920/6530 [=======>......................] - ETA: 2s - loss: 0.5354 
# training | RMSE: 0.3313, MAE: 0.2870
worker 2  xfile  [30, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.3313070790690605, 'rmse': 0.3313070790690605, 'mae': 0.28697057983195595, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  85 | activation: relu    | extras: dropout - rate: 36.8% 
layer 2 | size:  17 | activation: tanh    | extras: batchnorm 
layer 3 | size:  69 | activation: relu    | extras: None 
layer 4 | size:  37 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f044e1f66a0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:30 - loss: 3.7665
3840/6530 [================>.............] - ETA: 0s - loss: 0.8919 
3584/6530 [===============>..............] - ETA: 0s - loss: 0.4182
 288/6530 [>.............................] - ETA: 15s - loss: 1.6187 
5248/6530 [=======================>......] - ETA: 0s - loss: 0.3647
 576/6530 [=>............................] - ETA: 7s - loss: 0.9258 
6530/6530 [==============================] - 1s 142us/step - loss: 0.7106 - val_loss: 0.4116

 880/6530 [===>..........................] - ETA: 5s - loss: 0.6493
1200/6530 [====>.........................] - ETA: 3s - loss: 0.4986
6530/6530 [==============================] - 1s 167us/step - loss: 0.3387 - val_loss: 0.2483

1504/6530 [=====>........................] - ETA: 3s - loss: 0.4137
1808/6530 [=======>......................] - ETA: 2s - loss: 0.3564
2080/6530 [========>.....................] - ETA: 2s - loss: 0.3196
2384/6530 [=========>....................] - ETA: 1s - loss: 0.2872
2720/6530 [===========>..................] - ETA: 1s - loss: 0.2600
3024/6530 [============>.................] - ETA: 1s - loss: 0.2397
3344/6530 [==============>...............] - ETA: 1s - loss: 0.2232
3696/6530 [===============>..............] - ETA: 0s - loss: 0.2075
4048/6530 [=================>............] - ETA: 0s - loss: 0.1947
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1838
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1744
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1666
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1606
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1551
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1503
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1456
6530/6530 [==============================] - 2s 287us/step - loss: 0.1408 - val_loss: 0.0380

# training | RMSE: 0.3152, MAE: 0.2489
worker 1  xfile  [32, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 48, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3619910331156264}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.286597451825205}, 'layer_4_size': 88, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.3151910933317293, 'rmse': 0.3151910933317293, 'mae': 0.24890702316052463, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  13 | activation: relu    | extras: None 
layer 2 | size:   5 | activation: relu    | extras: None 
layer 3 | size:  38 | activation: relu    | extras: batchnorm 
layer 4 | size:  75 | activation: relu    | extras: None 
layer 5 | size:  80 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0466b39828>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:45 - loss: 1.0983
 608/6530 [=>............................] - ETA: 8s - loss: 0.4841  
# training | RMSE: 0.5176, MAE: 0.4098
worker 0  xfile  [33, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 94, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12643692050394267}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.517575323159229, 'rmse': 0.517575323159229, 'mae': 0.4097713398280512, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  25 | activation: relu    | extras: batchnorm 
layer 2 | size:  22 | activation: relu    | extras: batchnorm 
layer 3 | size:  90 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0455bf6588>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:24 - loss: 0.8792
1056/6530 [===>..........................] - ETA: 4s - loss: 0.3979
1024/6530 [===>..........................] - ETA: 4s - loss: 0.5511  
1600/6530 [======>.......................] - ETA: 2s - loss: 0.3389
1984/6530 [========>.....................] - ETA: 2s - loss: 0.4212
2080/6530 [========>.....................] - ETA: 2s - loss: 0.3149
2624/6530 [===========>..................] - ETA: 1s - loss: 0.2978
3008/6530 [============>.................] - ETA: 1s - loss: 0.3512
3200/6530 [=============>................] - ETA: 1s - loss: 0.2825
4032/6530 [=================>............] - ETA: 0s - loss: 0.3118
3712/6530 [================>.............] - ETA: 0s - loss: 0.2734
5056/6530 [======================>.......] - ETA: 0s - loss: 0.2858
4256/6530 [==================>...........] - ETA: 0s - loss: 0.2645
6016/6530 [==========================>...] - ETA: 0s - loss: 0.2668
4800/6530 [=====================>........] - ETA: 0s - loss: 0.2569
5312/6530 [=======================>......] - ETA: 0s - loss: 0.2514
6530/6530 [==============================] - 1s 196us/step - loss: 0.2592 - val_loss: 0.1759

5824/6530 [=========================>....] - ETA: 0s - loss: 0.2475
6368/6530 [============================>.] - ETA: 0s - loss: 0.2437
# training | RMSE: 0.1928, MAE: 0.1563
worker 2  xfile  [34, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3676760429632786}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 69, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.1928476890217273, 'rmse': 0.1928476890217273, 'mae': 0.15628298039273883, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  93 | activation: tanh    | extras: None 
layer 2 | size:  31 | activation: tanh    | extras: dropout - rate: 14.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f044d65d5c0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 25s - loss: 0.5545
3968/6530 [=================>............] - ETA: 0s - loss: 0.3062 
6530/6530 [==============================] - 2s 243us/step - loss: 0.2423 - val_loss: 0.2042

6530/6530 [==============================] - 1s 102us/step - loss: 0.2039 - val_loss: 0.0420

# training | RMSE: 0.2492, MAE: 0.1968
worker 1  xfile  [36, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 5, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.24921812716672342, 'rmse': 0.24921812716672342, 'mae': 0.1967781642087812, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  33 | activation: sigmoid | extras: dropout - rate: 25.1% 
layer 2 | size:  72 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f04660f1b00>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 27s - loss: 0.2796
2304/6530 [=========>....................] - ETA: 1s - loss: 0.2318 
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2178
6530/6530 [==============================] - 1s 117us/step - loss: 0.2118 - val_loss: 0.1812

# training | RMSE: 0.2101, MAE: 0.1652
worker 0  xfile  [35, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3110843672502082}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2555820286847462}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.21014472919974375, 'rmse': 0.21014472919974375, 'mae': 0.16518558920853382, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  38 | activation: sigmoid | extras: dropout - rate: 32.0% 
layer 2 | size:  28 | activation: sigmoid | extras: dropout - rate: 48.1% 
layer 3 | size:  40 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  93 | activation: tanh    | extras: dropout - rate: 10.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0455bf6358>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:54 - loss: 0.7293
 416/6530 [>.............................] - ETA: 13s - loss: 0.6643 
 864/6530 [==>...........................] - ETA: 6s - loss: 0.6089 
1376/6530 [=====>........................] - ETA: 3s - loss: 0.5188
1856/6530 [=======>......................] - ETA: 2s - loss: 0.4546
2400/6530 [==========>...................] - ETA: 1s - loss: 0.4124
2944/6530 [============>.................] - ETA: 1s - loss: 0.3819
# training | RMSE: 0.2034, MAE: 0.1623
worker 2  xfile  [37, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14897819670033827}, 'layer_2_size': 31, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27118399118299463}, 'layer_3_size': 2, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 73, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 49, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.20340037372450684, 'rmse': 0.20340037372450684, 'mae': 0.1623268265192807, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  84 | activation: relu    | extras: batchnorm 
layer 2 | size:  49 | activation: tanh    | extras: None 
layer 3 | size:  28 | activation: relu    | extras: None 
layer 4 | size:  20 | activation: tanh    | extras: dropout - rate: 42.8% 
layer 5 | size:  62 | activation: sigmoid | extras: dropout - rate: 34.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f044d65d3c8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 50s - loss: 0.2743
3424/6530 [==============>...............] - ETA: 1s - loss: 0.3630
2176/6530 [========>.....................] - ETA: 2s - loss: 0.2051 
3872/6530 [================>.............] - ETA: 0s - loss: 0.3475
4096/6530 [=================>............] - ETA: 0s - loss: 0.1841
4384/6530 [===================>..........] - ETA: 0s - loss: 0.3340
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1739
4864/6530 [=====================>........] - ETA: 0s - loss: 0.3240
5312/6530 [=======================>......] - ETA: 0s - loss: 0.3159
5792/6530 [=========================>....] - ETA: 0s - loss: 0.3090
6530/6530 [==============================] - 1s 198us/step - loss: 0.1707 - val_loss: 0.1347

6336/6530 [============================>.] - ETA: 0s - loss: 0.3022
6530/6530 [==============================] - 2s 254us/step - loss: 0.3004 - val_loss: 0.2384

# training | RMSE: 0.2253, MAE: 0.1829
worker 1  xfile  [40, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2508346160127697}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 72, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.22531471771968617, 'rmse': 0.22531471771968617, 'mae': 0.18286873657900923, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  68 | activation: relu    | extras: batchnorm 
layer 2 | size:  62 | activation: sigmoid | extras: dropout - rate: 26.2% 
layer 3 | size:  82 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0465515828>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:23 - loss: 0.4660
 768/6530 [==>...........................] - ETA: 6s - loss: 0.3644  
1472/6530 [=====>........................] - ETA: 3s - loss: 0.3803
2240/6530 [=========>....................] - ETA: 1s - loss: 0.3672
# training | RMSE: 0.1704, MAE: 0.1293
worker 2  xfile  [39, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 28, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4283350651365785}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3455095972091068}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.1704448432664166, 'rmse': 0.1704448432664166, 'mae': 0.12926143542610996, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  73 | activation: relu    | extras: None 
layer 2 | size:  79 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f044d4a2fd0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:01 - loss: 0.5990
3136/6530 [=============>................] - ETA: 1s - loss: 0.3504
1600/6530 [======>.......................] - ETA: 2s - loss: 0.2541  
3904/6530 [================>.............] - ETA: 0s - loss: 0.3416
3136/6530 [=============>................] - ETA: 0s - loss: 0.2156
4800/6530 [=====================>........] - ETA: 0s - loss: 0.3253
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1978
5696/6530 [=========================>....] - ETA: 0s - loss: 0.3135
6530/6530 [==============================] - 1s 138us/step - loss: 0.1870 - val_loss: 0.2293

# training | RMSE: 0.2911, MAE: 0.2355
worker 0  xfile  [38, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.32004886003186006}, 'layer_1_size': 38, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4812453420169397}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 40, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10789032155064207}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.29110163875175143, 'rmse': 0.29110163875175143, 'mae': 0.2354601352236099, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  17 | activation: relu    | extras: None 
layer 2 | size:   8 | activation: relu    | extras: dropout - rate: 41.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f04556ff710>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 14s - loss: 0.5445
6530/6530 [==============================] - 1s 205us/step - loss: 0.3011 - val_loss: 0.1750

6530/6530 [==============================] - 1s 112us/step - loss: 0.3528 - val_loss: 0.2877

# training | RMSE: 0.2735, MAE: 0.2290
worker 2  xfile  [42, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43085673746807795}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3242093808736577}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2734687318831778, 'rmse': 0.2734687318831778, 'mae': 0.22899223587561784, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  58 | activation: relu    | extras: dropout - rate: 39.0% 
layer 2 | size:   4 | activation: tanh    | extras: None 
layer 3 | size:  74 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f044d4a2f60>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 33s - loss: 1.0333
2176/6530 [========>.....................] - ETA: 1s - loss: 0.5880 
4096/6530 [=================>............] - ETA: 0s - loss: 0.4576
6528/6530 [============================>.] - ETA: 0s - loss: 0.3796
6530/6530 [==============================] - 1s 140us/step - loss: 0.3796 - val_loss: 0.2349

# training | RMSE: 0.2108, MAE: 0.1732
worker 1  xfile  [41, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2623268259265953}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 82, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30306473984005766}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2107744134554754, 'rmse': 0.2107744134554754, 'mae': 0.1732228241946067, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  64 | activation: tanh    | extras: None 
layer 2 | size:  38 | activation: relu    | extras: batchnorm 
layer 3 | size:   5 | activation: sigmoid | extras: None 
layer 4 | size:  87 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f0465187cf8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 20s - loss: 0.4578
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2649 
# training | RMSE: 0.3503, MAE: 0.2811
worker 0  xfile  [43, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41255552454864375}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 92, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.35026904849078955, 'rmse': 0.35026904849078955, 'mae': 0.2811338817115241, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:   7 | activation: relu    | extras: batchnorm 
layer 2 | size:  81 | activation: tanh    | extras: None 
layer 3 | size:  60 | activation: relu    | extras: dropout - rate: 49.1% 
layer 4 | size:  60 | activation: tanh    | extras: dropout - rate: 18.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f04550ba0b8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:19 - loss: 0.5812
1152/6530 [====>.........................] - ETA: 3s - loss: 0.1651  
2368/6530 [=========>....................] - ETA: 1s - loss: 0.1066
6530/6530 [==============================] - 1s 153us/step - loss: 0.2345 - val_loss: 0.1787

3584/6530 [===============>..............] - ETA: 0s - loss: 0.0858
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0739
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0675
6530/6530 [==============================] - 1s 181us/step - loss: 0.0650 - val_loss: 0.0338

# training | RMSE: 0.2889, MAE: 0.2330
worker 2  xfile  [44, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39023019053840957}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2888695316091966, 'rmse': 0.2888695316091966, 'mae': 0.23303205629967932, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  87 | activation: sigmoid | extras: None 
layer 2 | size:  17 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f044d3fd780>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:58 - loss: 0.5075
 448/6530 [=>............................] - ETA: 8s - loss: 0.0932  
 928/6530 [===>..........................] - ETA: 4s - loss: 0.0812
1456/6530 [=====>........................] - ETA: 2s - loss: 0.0715
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0648
2576/6530 [==========>...................] - ETA: 1s - loss: 0.0611
3120/6530 [=============>................] - ETA: 0s - loss: 0.0571
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0555
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0536
# training | RMSE: 0.2110, MAE: 0.1682
worker 1  xfile  [46, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 5, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.21097637185610413, 'rmse': 0.21097637185610413, 'mae': 0.16816580948800003, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:   9 | activation: sigmoid | extras: dropout - rate: 18.2% 
layer 2 | size:  88 | activation: sigmoid | extras: dropout - rate: 44.6% 
layer 3 | size:   2 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f04654ea550>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:48 - loss: 0.9439
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0527
 480/6530 [=>............................] - ETA: 9s - loss: 0.2788  
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0516
 928/6530 [===>..........................] - ETA: 4s - loss: 0.2460
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0508
1392/6530 [=====>........................] - ETA: 3s - loss: 0.2386
6384/6530 [============================>.] - ETA: 0s - loss: 0.0499
1824/6530 [=======>......................] - ETA: 2s - loss: 0.2349
2272/6530 [=========>....................] - ETA: 1s - loss: 0.2306
2672/6530 [===========>..................] - ETA: 1s - loss: 0.2283
# training | RMSE: 0.1789, MAE: 0.1439
worker 0  xfile  [45, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4908483209465242}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.18134188180962785}, 'layer_4_size': 60, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.1789475538016504, 'rmse': 0.1789475538016504, 'mae': 0.1438966485993309, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  12 | activation: sigmoid | extras: dropout - rate: 21.4% 
layer 2 | size:  64 | activation: sigmoid | extras: dropout - rate: 18.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f0454e161d0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 16s - loss: 0.5131
6530/6530 [==============================] - 1s 202us/step - loss: 0.0495 - val_loss: 0.0449

3104/6530 [=============>................] - ETA: 1s - loss: 0.2263
3472/6530 [==============>...............] - ETA: 0s - loss: 0.2259
6530/6530 [==============================] - 1s 124us/step - loss: 0.2710 - val_loss: 0.2159

3872/6530 [================>.............] - ETA: 0s - loss: 0.2262
4320/6530 [==================>...........] - ETA: 0s - loss: 0.2258
4768/6530 [====================>.........] - ETA: 0s - loss: 0.2253
5104/6530 [======================>.......] - ETA: 0s - loss: 0.2246
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2244
5984/6530 [==========================>...] - ETA: 0s - loss: 0.2230
6432/6530 [============================>.] - ETA: 0s - loss: 0.2223
6530/6530 [==============================] - 2s 247us/step - loss: 0.2224 - val_loss: 0.2141

# training | RMSE: 0.2679, MAE: 0.2202
worker 0  xfile  [49, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21353699840031415}, 'layer_1_size': 12, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1887525394629789}, 'layer_2_size': 64, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27407622708939067}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.2678976525823216, 'rmse': 0.2678976525823216, 'mae': 0.22015499629647464, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  98 | activation: sigmoid | extras: None 
layer 2 | size:  86 | activation: sigmoid | extras: None 
layer 3 | size:   5 | activation: tanh    | extras: dropout - rate: 42.2% 
layer 4 | size:  86 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0454b086d8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:33 - loss: 0.4323
 512/6530 [=>............................] - ETA: 9s - loss: 0.2096  
 928/6530 [===>..........................] - ETA: 5s - loss: 0.1564
1472/6530 [=====>........................] - ETA: 3s - loss: 0.1276
2080/6530 [========>.....................] - ETA: 2s - loss: 0.1116
2656/6530 [===========>..................] - ETA: 1s - loss: 0.1027
3168/6530 [=============>................] - ETA: 1s - loss: 0.0968
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0939
# training | RMSE: 0.2108, MAE: 0.1731
worker 2  xfile  [47, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 63, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.21083252016302698, 'rmse': 0.21083252016302698, 'mae': 0.17305093874271454, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  23 | activation: relu    | extras: dropout - rate: 43.2% 
layer 2 | size:   6 | activation: tanh    | extras: None 
layer 3 | size:  78 | activation: relu    | extras: batchnorm 
layer 4 | size:  11 | activation: relu    | extras: None 
layer 5 | size:  37 | activation: tanh    | extras: dropout - rate: 37.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f044c8e5320>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:05 - loss: 1.0329
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0906
 512/6530 [=>............................] - ETA: 11s - loss: 0.6869 
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0877
 928/6530 [===>..........................] - ETA: 6s - loss: 0.5552 
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0856
1376/6530 [=====>........................] - ETA: 4s - loss: 0.4784
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0843
1888/6530 [=======>......................] - ETA: 2s - loss: 0.4240
6528/6530 [============================>.] - ETA: 0s - loss: 0.0825
2432/6530 [==========>...................] - ETA: 1s - loss: 0.3866
2944/6530 [============>.................] - ETA: 1s - loss: 0.3638
6530/6530 [==============================] - 1s 229us/step - loss: 0.0825 - val_loss: 0.0692

3456/6530 [==============>...............] - ETA: 1s - loss: 0.3481
3968/6530 [=================>............] - ETA: 0s - loss: 0.3349
4480/6530 [===================>..........] - ETA: 0s - loss: 0.3247
4928/6530 [=====================>........] - ETA: 0s - loss: 0.3164
5376/6530 [=======================>......] - ETA: 0s - loss: 0.3097
5856/6530 [=========================>....] - ETA: 0s - loss: 0.3026
6432/6530 [============================>.] - ETA: 0s - loss: 0.2957
6530/6530 [==============================] - 2s 265us/step - loss: 0.2948 - val_loss: 0.2147

# training | RMSE: 0.2645, MAE: 0.2169
worker 1  xfile  [48, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.181589516080453}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.44647201048090357}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 2, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 34, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22542703942748094}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2644710794690549, 'rmse': 0.2644710794690549, 'mae': 0.216894650138128, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  19 | activation: tanh    | extras: batchnorm 
layer 2 | size:  36 | activation: tanh    | extras: batchnorm 
layer 3 | size:  27 | activation: sigmoid | extras: dropout - rate: 15.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f04647601d0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:39 - loss: 0.5436
 544/6530 [=>............................] - ETA: 12s - loss: 0.2622 
1024/6530 [===>..........................] - ETA: 6s - loss: 0.1696 
1504/6530 [=====>........................] - ETA: 4s - loss: 0.1299
1984/6530 [========>.....................] - ETA: 2s - loss: 0.1098
2464/6530 [==========>...................] - ETA: 2s - loss: 0.0973
2944/6530 [============>.................] - ETA: 1s - loss: 0.0892
3456/6530 [==============>...............] - ETA: 1s - loss: 0.0830
3936/6530 [=================>............] - ETA: 0s - loss: 0.0778
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0740
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0707
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0678
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0652
6464/6530 [============================>.] - ETA: 0s - loss: 0.0632
6530/6530 [==============================] - 2s 292us/step - loss: 0.0630 - val_loss: 0.0371

# training | RMSE: 0.2645, MAE: 0.2164
worker 0  xfile  [50, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 98, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4222044144905175}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 2, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.26445715514655926, 'rmse': 0.26445715514655926, 'mae': 0.21643900064868482, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  47 | activation: tanh    | extras: None 
layer 2 | size:  32 | activation: sigmoid | extras: dropout - rate: 16.0% 
layer 3 | size:  98 | activation: relu    | extras: batchnorm 
layer 4 | size:  71 | activation: sigmoid | extras: batchnorm 
layer 5 | size:  11 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f04548d4048>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 2:00 - loss: 0.4152
 768/6530 [==>...........................] - ETA: 9s - loss: 0.2950  
1536/6530 [======>.......................] - ETA: 4s - loss: 0.2548
2368/6530 [=========>....................] - ETA: 2s - loss: 0.2377
3136/6530 [=============>................] - ETA: 1s - loss: 0.2253
3904/6530 [================>.............] - ETA: 0s - loss: 0.2209
4672/6530 [====================>.........] - ETA: 0s - loss: 0.2151
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2117
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2084
# training | RMSE: 0.2615, MAE: 0.2107
worker 2  xfile  [51, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4322557475652903}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.36962354345532367}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.26154673708850235, 'rmse': 0.26154673708850235, 'mae': 0.21066859580108133, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  21 | activation: tanh    | extras: batchnorm 
layer 2 | size:  83 | activation: tanh    | extras: batchnorm 
layer 3 | size:  14 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f044c6da198>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:51 - loss: 0.9698
 832/6530 [==>...........................] - ETA: 7s - loss: 0.7297  
1728/6530 [======>.......................] - ETA: 3s - loss: 0.6409
6530/6530 [==============================] - 2s 272us/step - loss: 0.2077 - val_loss: 0.1648

2496/6530 [==========>...................] - ETA: 2s - loss: 0.5710
3328/6530 [==============>...............] - ETA: 1s - loss: 0.4964
4160/6530 [==================>...........] - ETA: 0s - loss: 0.4420
4928/6530 [=====================>........] - ETA: 0s - loss: 0.4048
5696/6530 [=========================>....] - ETA: 0s - loss: 0.3756
# training | RMSE: 0.1920, MAE: 0.1518
worker 1  xfile  [52, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 19, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.154058449532362}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.1919713274053179, 'rmse': 0.1919713274053179, 'mae': 0.1518490683436333, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  69 | activation: tanh    | extras: dropout - rate: 21.8% 
layer 2 | size:  63 | activation: relu    | extras: dropout - rate: 22.6% 
layer 3 | size:  28 | activation: sigmoid | extras: dropout - rate: 43.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f04646f4f98>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:38 - loss: 2.3724
6528/6530 [============================>.] - ETA: 0s - loss: 0.3512
 336/6530 [>.............................] - ETA: 16s - loss: 1.2182 
 640/6530 [=>............................] - ETA: 8s - loss: 0.7363 
 976/6530 [===>..........................] - ETA: 5s - loss: 0.5216
6530/6530 [==============================] - 2s 254us/step - loss: 0.3512 - val_loss: 0.2405

1312/6530 [=====>........................] - ETA: 4s - loss: 0.4128
1680/6530 [======>.......................] - ETA: 3s - loss: 0.3442
2016/6530 [========>.....................] - ETA: 2s - loss: 0.3015
2336/6530 [=========>....................] - ETA: 2s - loss: 0.2715
2656/6530 [===========>..................] - ETA: 1s - loss: 0.2492
2976/6530 [============>.................] - ETA: 1s - loss: 0.2299
3312/6530 [==============>...............] - ETA: 1s - loss: 0.2139
3664/6530 [===============>..............] - ETA: 1s - loss: 0.1997
4000/6530 [=================>............] - ETA: 0s - loss: 0.1881
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1768
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1677
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1594
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1521
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1461
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1416
6448/6530 [============================>.] - ETA: 0s - loss: 0.1370
6530/6530 [==============================] - 2s 301us/step - loss: 0.1357 - val_loss: 0.0368

# training | RMSE: 0.2091, MAE: 0.1648
worker 0  xfile  [53, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 47, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16005469532790345}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2091489217965739, 'rmse': 0.2091489217965739, 'mae': 0.16476422511124741, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  87 | activation: sigmoid | extras: None 
layer 2 | size:  23 | activation: relu    | extras: dropout - rate: 39.1% 
layer 3 | size:  86 | activation: tanh    | extras: batchnorm 
layer 4 | size:  17 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f04548f7208>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:19 - loss: 0.7245
 544/6530 [=>............................] - ETA: 11s - loss: 0.6712 
1056/6530 [===>..........................] - ETA: 5s - loss: 0.6134 
1568/6530 [======>.......................] - ETA: 3s - loss: 0.5419
2112/6530 [========>.....................] - ETA: 2s - loss: 0.4660
2688/6530 [===========>..................] - ETA: 1s - loss: 0.4157
3168/6530 [=============>................] - ETA: 1s - loss: 0.3861
3648/6530 [===============>..............] - ETA: 1s - loss: 0.3627
4160/6530 [==================>...........] - ETA: 0s - loss: 0.3440
4736/6530 [====================>.........] - ETA: 0s - loss: 0.3276
5312/6530 [=======================>......] - ETA: 0s - loss: 0.3132
5856/6530 [=========================>....] - ETA: 0s - loss: 0.3023
6336/6530 [============================>.] - ETA: 0s - loss: 0.2946
# training | RMSE: 0.2900, MAE: 0.2384
worker 2  xfile  [54, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 21, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 83, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41122132628080166}, 'layer_4_size': 93, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.28995406020407494, 'rmse': 0.28995406020407494, 'mae': 0.2383929929452457, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  27 | activation: sigmoid | extras: None 
layer 2 | size:   3 | activation: sigmoid | extras: dropout - rate: 40.6% 
layer 3 | size:  93 | activation: relu    | extras: batchnorm 
layer 4 | size:   6 | activation: relu    | extras: batchnorm 
layer 5 | size:  10 | activation: relu    | extras: dropout - rate: 29.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f044c0fd080>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 58s - loss: 0.7179
1664/6530 [======>.......................] - ETA: 3s - loss: 0.3347 
6530/6530 [==============================] - 2s 272us/step - loss: 0.2918 - val_loss: 0.2835

3328/6530 [==============>...............] - ETA: 1s - loss: 0.2290
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1747
6530/6530 [==============================] - 2s 231us/step - loss: 0.1548 - val_loss: 0.5448

# training | RMSE: 0.1848, MAE: 0.1506
worker 1  xfile  [55, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21837030191686663}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22624305720097207}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4326772368856566}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27774682114412147}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.18481128073241967, 'rmse': 0.18481128073241967, 'mae': 0.15063477814423132, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  56 | activation: sigmoid | extras: dropout - rate: 45.8% 
layer 2 | size:  85 | activation: sigmoid | extras: dropout - rate: 41.4% 
layer 3 | size:  59 | activation: relu    | extras: dropout - rate: 43.0% 
layer 4 | size:  59 | activation: relu    | extras: dropout - rate: 37.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f04655bc5c0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:11 - loss: 0.6433
 512/6530 [=>............................] - ETA: 11s - loss: 0.4630 
 960/6530 [===>..........................] - ETA: 6s - loss: 0.4091 
1536/6530 [======>.......................] - ETA: 3s - loss: 0.3790
2208/6530 [=========>....................] - ETA: 2s - loss: 0.3531
2784/6530 [===========>..................] - ETA: 1s - loss: 0.3386
3488/6530 [===============>..............] - ETA: 1s - loss: 0.3271
4224/6530 [==================>...........] - ETA: 0s - loss: 0.3169
4960/6530 [=====================>........] - ETA: 0s - loss: 0.3098
5696/6530 [=========================>....] - ETA: 0s - loss: 0.3045
6432/6530 [============================>.] - ETA: 0s - loss: 0.2988
6530/6530 [==============================] - 2s 246us/step - loss: 0.2982 - val_loss: 0.3391

# training | RMSE: 0.3533, MAE: 0.2912
worker 0  xfile  [56, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39114648997625634}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.35333056974750615, 'rmse': 0.35333056974750615, 'mae': 0.29123917205416394, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  93 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  67 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f044efadd30>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:47 - loss: 0.2491
1088/6530 [===>..........................] - ETA: 5s - loss: 0.0850  
# training | RMSE: 0.7328, MAE: 0.6818
worker 2  xfile  [57, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.40623525449208187}, 'layer_2_size': 3, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 93, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 6, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.29015795012512713}, 'layer_5_size': 10, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.7328137402804643, 'rmse': 0.7328137402804643, 'mae': 0.6818258761993052, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  25 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  42 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f043fb09208>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 52s - loss: 0.6266
2304/6530 [=========>....................] - ETA: 2s - loss: 0.0651
2688/6530 [===========>..................] - ETA: 1s - loss: 0.5030 
3456/6530 [==============>...............] - ETA: 1s - loss: 0.0575
4736/6530 [====================>.........] - ETA: 0s - loss: 0.4219
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0531
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0503
6530/6530 [==============================] - 1s 202us/step - loss: 0.3478 - val_loss: 0.0535

6530/6530 [==============================] - 1s 225us/step - loss: 0.0494 - val_loss: 0.0374

# training | RMSE: 0.4005, MAE: 0.3315
worker 1  xfile  [58, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45809535997411455}, 'layer_1_size': 56, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4137789166294171}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4302478130034917}, 'layer_3_size': 59, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3709287657001251}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.40045347118844715, 'rmse': 0.40045347118844715, 'mae': 0.33153141415583604, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  68 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  82 | activation: sigmoid | extras: dropout - rate: 47.2% 
layer 3 | size:  60 | activation: relu    | extras: batchnorm 
layer 4 | size:  78 | activation: tanh    | extras: dropout - rate: 43.8% 
layer 5 | size:  42 | activation: relu    | extras: dropout - rate: 10.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f04655bc940>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 2:05 - loss: 0.6789
 704/6530 [==>...........................] - ETA: 10s - loss: 0.6198 
# training | RMSE: 0.2314, MAE: 0.1923
worker 2  xfile  [60, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 42, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2799649795995304}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 71, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.23141975722246266, 'rmse': 0.23141975722246266, 'mae': 0.19228679821750141, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  47 | activation: relu    | extras: dropout - rate: 15.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f043f407d30>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:01 - loss: 0.5858
1408/6530 [=====>........................] - ETA: 4s - loss: 0.5336 
1024/6530 [===>..........................] - ETA: 5s - loss: 0.2315  
2112/6530 [========>.....................] - ETA: 2s - loss: 0.4526
2080/6530 [========>.....................] - ETA: 2s - loss: 0.1550
2880/6530 [============>.................] - ETA: 1s - loss: 0.4002
3200/6530 [=============>................] - ETA: 1s - loss: 0.1209
# training | RMSE: 0.1932, MAE: 0.1531
worker 0  xfile  [59, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2905420164871645}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.19316483715117394, 'rmse': 0.19316483715117394, 'mae': 0.15309115262132544, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  76 | activation: tanh    | extras: dropout - rate: 47.5% 
layer 2 | size:  37 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f044efadc18>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:27 - loss: 0.3192
3648/6530 [===============>..............] - ETA: 1s - loss: 0.3631
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1024
1728/6530 [======>.......................] - ETA: 2s - loss: 0.2549  
4416/6530 [===================>..........] - ETA: 0s - loss: 0.3392
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0909
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2291
5184/6530 [======================>.......] - ETA: 0s - loss: 0.3210
6432/6530 [============================>.] - ETA: 0s - loss: 0.0828
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2180
5952/6530 [==========================>...] - ETA: 0s - loss: 0.3045
6530/6530 [==============================] - 1s 204us/step - loss: 0.0823 - val_loss: 0.0438

6530/6530 [==============================] - 1s 181us/step - loss: 0.2121 - val_loss: 0.1687

6530/6530 [==============================] - 2s 285us/step - loss: 0.2953 - val_loss: 0.1903

# training | RMSE: 0.2044, MAE: 0.1612
worker 2  xfile  [62, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15602519089682165}, 'layer_1_size': 47, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3662071301271337}, 'layer_2_size': 70, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.20435136044142693, 'rmse': 0.20435136044142693, 'mae': 0.1611933739718669, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  92 | activation: relu    | extras: dropout - rate: 17.2% 
layer 2 | size:  58 | activation: relu    | extras: None 
layer 3 | size:  80 | activation: relu    | extras: dropout - rate: 14.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f043efd16d8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 42s - loss: 0.7256
2688/6530 [===========>..................] - ETA: 1s - loss: 0.3732 
4992/6530 [=====================>........] - ETA: 0s - loss: 0.3054
6530/6530 [==============================] - 1s 170us/step - loss: 0.2833 - val_loss: 0.2292

# training | RMSE: 0.2074, MAE: 0.1688
worker 0  xfile  [63, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4752384599654145}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48429916230485714}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2074411441120533, 'rmse': 0.2074411441120533, 'mae': 0.1687521005600519, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  23 | activation: tanh    | extras: batchnorm 
layer 2 | size:  32 | activation: tanh    | extras: dropout - rate: 28.0% 
layer 3 | size:  81 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f044eb88dd8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 55s - loss: 1.9008
1536/6530 [======>.......................] - ETA: 3s - loss: 1.3638 
3200/6530 [=============>................] - ETA: 1s - loss: 0.8949
5120/6530 [======================>.......] - ETA: 0s - loss: 0.5929
# training | RMSE: 0.2308, MAE: 0.1873
worker 1  xfile  [61, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47194424194851814}, 'layer_2_size': 82, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.43826874021108175}, 'layer_4_size': 78, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10489365671261544}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.23080586523079752, 'rmse': 0.23080586523079752, 'mae': 0.1872913610051805, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  46 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  44 | activation: sigmoid | extras: None 
layer 3 | size:  81 | activation: relu    | extras: dropout - rate: 11.9% 
layer 4 | size:  67 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f043fa5d7b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 29s - loss: 1.1611
4608/6530 [====================>.........] - ETA: 0s - loss: 0.3040 
6530/6530 [==============================] - 1s 220us/step - loss: 0.4756 - val_loss: 0.0492

6530/6530 [==============================] - 1s 220us/step - loss: 0.2351 - val_loss: 0.0550

# training | RMSE: 0.2778, MAE: 0.2293
worker 2  xfile  [65, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17246170518745202}, 'layer_1_size': 92, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1433120180375093}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21864295299191291}, 'layer_4_size': 37, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14309072567378933}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.27777213306138165, 'rmse': 0.27777213306138165, 'mae': 0.22928016020199254, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  34 | activation: relu    | extras: dropout - rate: 39.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f043ec3ae10>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:38 - loss: 1.0513
 464/6530 [=>............................] - ETA: 11s - loss: 0.2454 
 960/6530 [===>..........................] - ETA: 5s - loss: 0.1825 
1440/6530 [=====>........................] - ETA: 3s - loss: 0.1590
1792/6530 [=======>......................] - ETA: 2s - loss: 0.1465
2096/6530 [========>.....................] - ETA: 2s - loss: 0.1397
2480/6530 [==========>...................] - ETA: 1s - loss: 0.1348
2960/6530 [============>.................] - ETA: 1s - loss: 0.1256
3520/6530 [===============>..............] - ETA: 1s - loss: 0.1170
4032/6530 [=================>............] - ETA: 0s - loss: 0.1125
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1077
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1042
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1014
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0996
6530/6530 [==============================] - 2s 262us/step - loss: 0.0973 - val_loss: 0.0669

# training | RMSE: 0.2182, MAE: 0.1694
worker 0  xfile  [64, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28042922014676985}, 'layer_2_size': 32, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 81, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.21815476743434217, 'rmse': 0.21815476743434217, 'mae': 0.16942346606034064, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:   9 | activation: sigmoid | extras: None 
layer 2 | size:  15 | activation: relu    | extras: None 
layer 3 | size:  79 | activation: sigmoid | extras: None 
layer 4 | size:  16 | activation: tanh    | extras: dropout - rate: 25.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f044e310ba8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 24s - loss: 1.8934
3584/6530 [===============>..............] - ETA: 0s - loss: 0.5472 
6530/6530 [==============================] - 1s 189us/step - loss: 0.3287 - val_loss: 0.0619

# training | RMSE: 0.2338, MAE: 0.1907
worker 1  xfile  [66, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11896381784270203}, 'layer_3_size': 81, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1872715376547708}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.23382954582057633, 'rmse': 0.23382954582057633, 'mae': 0.19066452722236762, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  63 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  21 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f043f2b8898>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 55s - loss: 0.2411
1792/6530 [=======>......................] - ETA: 3s - loss: 0.1572 
3840/6530 [================>.............] - ETA: 0s - loss: 0.0995
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0806
6530/6530 [==============================] - 2s 233us/step - loss: 0.0769 - val_loss: 0.0420

# training | RMSE: 0.2507, MAE: 0.2046
worker 0  xfile  [68, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 9, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2577702797077809}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.16685390974228376}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2507227377000718, 'rmse': 0.2507227377000718, 'mae': 0.2046419747401127, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:   8 | activation: relu    | extras: batchnorm 
layer 2 | size:  45 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f044e3109e8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 7:04 - loss: 0.6575
 304/6530 [>.............................] - ETA: 22s - loss: 0.6600 
 576/6530 [=>............................] - ETA: 11s - loss: 0.5872
 864/6530 [==>...........................] - ETA: 7s - loss: 0.5207 
1168/6530 [====>.........................] - ETA: 5s - loss: 0.4449
1472/6530 [=====>........................] - ETA: 4s - loss: 0.3895
# training | RMSE: 0.2542, MAE: 0.2047
worker 2  xfile  [67, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38953016521482287}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 76, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35047528622460133}, 'layer_4_size': 78, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2541754265410221, 'rmse': 0.2541754265410221, 'mae': 0.20470905644759319, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  63 | activation: tanh    | extras: dropout - rate: 21.8% 
layer 2 | size:  74 | activation: sigmoid | extras: dropout - rate: 39.1% 
layer 3 | size:  42 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  88 | activation: tanh    | extras: dropout - rate: 29.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7f043ec3ac18>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:50 - loss: 1.0730
1792/6530 [=======>......................] - ETA: 3s - loss: 0.3524
 576/6530 [=>............................] - ETA: 12s - loss: 0.5759 
2112/6530 [========>.....................] - ETA: 2s - loss: 0.3263
1120/6530 [====>.........................] - ETA: 5s - loss: 0.4654 
2432/6530 [==========>...................] - ETA: 2s - loss: 0.3052
1696/6530 [======>.......................] - ETA: 3s - loss: 0.4240
2768/6530 [===========>..................] - ETA: 2s - loss: 0.2882
2208/6530 [=========>....................] - ETA: 2s - loss: 0.4013
# training | RMSE: 0.2022, MAE: 0.1621
worker 1  xfile  [69, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2022006761273695, 'rmse': 0.2022006761273695, 'mae': 0.16210904622469938, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  86 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f043f1b34a8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 23s - loss: 0.4714
3104/6530 [=============>................] - ETA: 1s - loss: 0.2725
2752/6530 [===========>..................] - ETA: 1s - loss: 0.3836
3408/6530 [==============>...............] - ETA: 1s - loss: 0.2635
3264/6530 [=============>................] - ETA: 1s - loss: 0.3715
3728/6530 [================>.............] - ETA: 1s - loss: 0.2540
3808/6530 [================>.............] - ETA: 1s - loss: 0.3632
6530/6530 [==============================] - 1s 176us/step - loss: 0.1074 - val_loss: 0.0587

4048/6530 [=================>............] - ETA: 1s - loss: 0.2464
4352/6530 [==================>...........] - ETA: 0s - loss: 0.3525
4400/6530 [===================>..........] - ETA: 0s - loss: 0.2380
4928/6530 [=====================>........] - ETA: 0s - loss: 0.3469
4720/6530 [====================>.........] - ETA: 0s - loss: 0.2322
5472/6530 [========================>.....] - ETA: 0s - loss: 0.3411
5056/6530 [======================>.......] - ETA: 0s - loss: 0.2260
6048/6530 [==========================>...] - ETA: 0s - loss: 0.3362
5360/6530 [=======================>......] - ETA: 0s - loss: 0.2210
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2165
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2124
6224/6530 [===========================>..] - ETA: 0s - loss: 0.2093
6530/6530 [==============================] - 2s 294us/step - loss: 0.3318 - val_loss: 0.2940

6530/6530 [==============================] - 2s 350us/step - loss: 0.2057 - val_loss: 0.1355

# training | RMSE: 0.2419, MAE: 0.1978
worker 1  xfile  [72, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 76, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21552202662751477}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19200815181849593}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.24193185725841768, 'rmse': 0.24193185725841768, 'mae': 0.19778273642676503, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  11 | activation: relu    | extras: None 
layer 2 | size:  81 | activation: tanh    | extras: None 
layer 3 | size:  58 | activation: sigmoid | extras: dropout - rate: 21.8% 
layer 4 | size:  59 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f0465582ac8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:45 - loss: 0.8759
 272/6530 [>.............................] - ETA: 24s - loss: 0.4185 
 528/6530 [=>............................] - ETA: 12s - loss: 0.3565
 816/6530 [==>...........................] - ETA: 8s - loss: 0.3327 
1104/6530 [====>.........................] - ETA: 5s - loss: 0.3153
1376/6530 [=====>........................] - ETA: 4s - loss: 0.3026
1696/6530 [======>.......................] - ETA: 3s - loss: 0.2927
1984/6530 [========>.....................] - ETA: 3s - loss: 0.2831
2272/6530 [=========>....................] - ETA: 2s - loss: 0.2774
2512/6530 [==========>...................] - ETA: 2s - loss: 0.2720
2848/6530 [============>.................] - ETA: 1s - loss: 0.2677
# training | RMSE: 0.3519, MAE: 0.2946
worker 2  xfile  [71, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21760004908081332}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39118890592267064}, 'layer_2_size': 74, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 42, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2939361354953432}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.3519220676269693, 'rmse': 0.3519220676269693, 'mae': 0.29462129491034694, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  80 | activation: tanh    | extras: dropout - rate: 38.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f043f523898>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:37 - loss: 0.9999
3200/6530 [=============>................] - ETA: 1s - loss: 0.2621
 464/6530 [=>............................] - ETA: 13s - loss: 0.7234 
3520/6530 [===============>..............] - ETA: 1s - loss: 0.2576
 928/6530 [===>..........................] - ETA: 6s - loss: 0.5248 
3872/6530 [================>.............] - ETA: 1s - loss: 0.2535
1408/6530 [=====>........................] - ETA: 4s - loss: 0.4191
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2519
1888/6530 [=======>......................] - ETA: 2s - loss: 0.3661
4544/6530 [===================>..........] - ETA: 0s - loss: 0.2480
2304/6530 [=========>....................] - ETA: 2s - loss: 0.3358
4896/6530 [=====================>........] - ETA: 0s - loss: 0.2450
2800/6530 [===========>..................] - ETA: 1s - loss: 0.3128
5200/6530 [======================>.......] - ETA: 0s - loss: 0.2433
3264/6530 [=============>................] - ETA: 1s - loss: 0.2962
5520/6530 [========================>.....] - ETA: 0s - loss: 0.2407
3744/6530 [================>.............] - ETA: 1s - loss: 0.2845
5856/6530 [=========================>....] - ETA: 0s - loss: 0.2375
4240/6530 [==================>...........] - ETA: 0s - loss: 0.2744
6224/6530 [===========================>..] - ETA: 0s - loss: 0.2352
4720/6530 [====================>.........] - ETA: 0s - loss: 0.2667
5168/6530 [======================>.......] - ETA: 0s - loss: 0.2613
5600/6530 [========================>.....] - ETA: 0s - loss: 0.2550
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2516
6352/6530 [============================>.] - ETA: 0s - loss: 0.2477
6530/6530 [==============================] - 2s 345us/step - loss: 0.2340 - val_loss: 0.3792

6530/6530 [==============================] - 2s 287us/step - loss: 0.2455 - val_loss: 0.1957

# training | RMSE: 0.1739, MAE: 0.1321
worker 0  xfile  [70, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41851260000555746}, 'layer_3_size': 81, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.17385276572357225, 'rmse': 0.17385276572357225, 'mae': 0.1321177492425397, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  65 | activation: sigmoid | extras: dropout - rate: 43.1% 
layer 2 | size:  34 | activation: tanh    | extras: batchnorm 
layer 3 | size: 100 | activation: relu    | extras: None 
layer 4 | size:  50 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f044df59470>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 1:01 - loss: 1.0765
1536/6530 [======>.......................] - ETA: 4s - loss: 0.6088  
2816/6530 [===========>..................] - ETA: 1s - loss: 0.5138
4352/6530 [==================>...........] - ETA: 0s - loss: 0.4616
5888/6530 [==========================>...] - ETA: 0s - loss: 0.4310
6530/6530 [==============================] - 2s 249us/step - loss: 0.4206 - val_loss: 0.3893

# training | RMSE: 0.2405, MAE: 0.1852
worker 2  xfile  [74, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3854686318955314}, 'layer_1_size': 80, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4486193665141055}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.24050408148065924, 'rmse': 0.24050408148065924, 'mae': 0.18521848814174435, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  97 | activation: tanh    | extras: None 
layer 2 | size:  41 | activation: sigmoid | extras: dropout - rate: 33.5% 
layer 3 | size:  95 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f044e200160>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 51s - loss: 1.6260
# training | RMSE: 0.4380, MAE: 0.3852
worker 1  xfile  [73, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21790253153759923}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.4379746480627341, 'rmse': 0.4379746480627341, 'mae': 0.3852437845054967, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  24 | activation: tanh    | extras: batchnorm 
layer 2 | size:  82 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0464a4f6a0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 7:15 - loss: 0.7510
1920/6530 [=======>......................] - ETA: 2s - loss: 0.5516 
 224/6530 [>.............................] - ETA: 31s - loss: 0.4326 
3712/6530 [================>.............] - ETA: 0s - loss: 0.4261
 496/6530 [=>............................] - ETA: 14s - loss: 0.3379
6144/6530 [===========================>..] - ETA: 0s - loss: 0.3535
 800/6530 [==>...........................] - ETA: 8s - loss: 0.2903 
1088/6530 [===>..........................] - ETA: 6s - loss: 0.2686
1408/6530 [=====>........................] - ETA: 4s - loss: 0.2534
6530/6530 [==============================] - 1s 205us/step - loss: 0.3458 - val_loss: 0.1702

1744/6530 [=======>......................] - ETA: 3s - loss: 0.2434
2112/6530 [========>.....................] - ETA: 2s - loss: 0.2339
2464/6530 [==========>...................] - ETA: 2s - loss: 0.2268
2768/6530 [===========>..................] - ETA: 2s - loss: 0.2213
3088/6530 [=============>................] - ETA: 1s - loss: 0.2164
3440/6530 [==============>...............] - ETA: 1s - loss: 0.2124
3760/6530 [================>.............] - ETA: 1s - loss: 0.2093
4064/6530 [=================>............] - ETA: 1s - loss: 0.2067
4384/6530 [===================>..........] - ETA: 0s - loss: 0.2045
4752/6530 [====================>.........] - ETA: 0s - loss: 0.2007
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1985
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1960
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1932
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1917
# training | RMSE: 0.4461, MAE: 0.3807
worker 0  xfile  [75, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4306695586227791}, 'layer_1_size': 65, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.44610730496168843, 'rmse': 0.44610730496168843, 'mae': 0.38074187686945965, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  77 | activation: sigmoid | extras: None 
layer 2 | size:  69 | activation: tanh    | extras: batchnorm 
layer 3 | size:  50 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  98 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f044f0687b8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 2:17 - loss: 0.8249
 768/6530 [==>...........................] - ETA: 10s - loss: 0.5293 
1600/6530 [======>.......................] - ETA: 4s - loss: 0.3538 
6530/6530 [==============================] - 2s 349us/step - loss: 0.1907 - val_loss: 0.1904

2432/6530 [==========>...................] - ETA: 2s - loss: 0.2895
3328/6530 [==============>...............] - ETA: 1s - loss: 0.2524
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2302
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2165
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2070
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1987
6530/6530 [==============================] - 2s 304us/step - loss: 0.1957 - val_loss: 0.1792

# training | RMSE: 0.2167, MAE: 0.1718
worker 2  xfile  [77, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.33496577595643284}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31657417017907896}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.21670035294038983, 'rmse': 0.21670035294038983, 'mae': 0.17179363874321937, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  97 | activation: sigmoid | extras: dropout - rate: 35.1% 
layer 2 | size:  22 | activation: sigmoid | extras: dropout - rate: 47.9% 
layer 3 | size:  47 | activation: tanh    | extras: dropout - rate: 22.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f043e357fd0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 52s - loss: 0.6468
2304/6530 [=========>....................] - ETA: 2s - loss: 0.2158 
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1608
# training | RMSE: 0.2192, MAE: 0.1723
worker 0  xfile  [78, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 50, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 98, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1666171614564424}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.21915488815869777, 'rmse': 0.21915488815869777, 'mae': 0.17232717587570753, 'early_stop': False}
vggnet done  0

6530/6530 [==============================] - 1s 204us/step - loss: 0.1368 - val_loss: 0.0724

# training | RMSE: 0.2730, MAE: 0.2239
worker 2  xfile  [79, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3510927047964897}, 'layer_1_size': 97, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47912796968769367}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22468178723709364}, 'layer_3_size': 47, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2730417044032003, 'rmse': 0.2730417044032003, 'mae': 0.2238982680321343, 'early_stop': False}
vggnet done  2

# training | RMSE: 0.2301, MAE: 0.1851
worker 1  xfile  [76, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.23014204611099864, 'rmse': 0.23014204611099864, 'mae': 0.1851259012870158, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  67 | activation: relu    | extras: batchnorm 
layer 2 | size:  84 | activation: sigmoid | extras: None 
layer 3 | size:  45 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  31 | activation: tanh    | extras: None 
layer 5 | size:  50 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f0464a4f2b0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 4:17 - loss: 1.3759
 352/6530 [>.............................] - ETA: 23s - loss: 0.4218 
 672/6530 [==>...........................] - ETA: 11s - loss: 0.3105
 960/6530 [===>..........................] - ETA: 8s - loss: 0.2524 
1344/6530 [=====>........................] - ETA: 5s - loss: 0.2083
1696/6530 [======>.......................] - ETA: 4s - loss: 0.1869
2048/6530 [========>.....................] - ETA: 3s - loss: 0.1700
2336/6530 [=========>....................] - ETA: 2s - loss: 0.1594
2688/6530 [===========>..................] - ETA: 2s - loss: 0.1495
3008/6530 [============>.................] - ETA: 2s - loss: 0.1407
3392/6530 [==============>...............] - ETA: 1s - loss: 0.1315
3744/6530 [================>.............] - ETA: 1s - loss: 0.1251
4032/6530 [=================>............] - ETA: 1s - loss: 0.1200
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1152
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1115
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1083
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1055
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1030
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1004
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0979
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0952
6528/6530 [============================>.] - ETA: 0s - loss: 0.0934
6530/6530 [==============================] - 3s 394us/step - loss: 0.0934 - val_loss: 0.0597

# training | RMSE: 0.2402, MAE: 0.1923
worker 1  xfile  [80, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.24016509045801826, 'rmse': 0.24016509045801826, 'mae': 0.19230435353119218, 'early_stop': False}
vggnet done  1
all of workers have been done
calculation finished
#1 epoch=1.0 loss={'loss': 0.262704264602928, 'rmse': 0.262704264602928, 'mae': 0.2149827019257636, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12254503281447732}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 50, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13742751467075745}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#3 epoch=1.0 loss={'loss': 0.3467875106657467, 'rmse': 0.3467875106657467, 'mae': 0.2846748548932067, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.48779807601499936}, 'layer_2_size': 10, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.13672626873389904}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#0 epoch=1.0 loss={'loss': 0.22186077301511778, 'rmse': 0.22186077301511778, 'mae': 0.17557900975182777, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.16149723882932443}, 'layer_3_size': 70, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#2 epoch=1.0 loss={'loss': 0.14363537723480127, 'rmse': 0.14363537723480127, 'mae': 0.11056388543489429, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 26, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#4 epoch=1.0 loss={'loss': 0.257289723707473, 'rmse': 0.257289723707473, 'mae': 0.2105048871965249, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3395701537007203}, 'layer_2_size': 56, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18358446062662281}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12243765775546467}, 'layer_4_size': 60, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#5 epoch=1.0 loss={'loss': 0.27175185968145343, 'rmse': 0.27175185968145343, 'mae': 0.2301439723701545, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15123969364242107}, 'layer_2_size': 97, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 11, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2805800228674691}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 2, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#7 epoch=1.0 loss={'loss': 0.22095463773543367, 'rmse': 0.22095463773543367, 'mae': 0.17984716632043246, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 44, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3638345383735373}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#8 epoch=1.0 loss={'loss': 0.7273343010408425, 'rmse': 0.7273343010408425, 'mae': 0.598906247181177, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 82, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17662534013240705}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#6 epoch=1.0 loss={'loss': 0.244887105262517, 'rmse': 0.244887105262517, 'mae': 0.19176989211404036, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2705959581751023}, 'layer_2_size': 51, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#9 epoch=1.0 loss={'loss': 0.11667032512754376, 'rmse': 0.11667032512754376, 'mae': 0.09059037300781238, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 8, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#11 epoch=1.0 loss={'loss': 0.26234691295939616, 'rmse': 0.26234691295939616, 'mae': 0.22086147405691808, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2895023053176152}, 'layer_1_size': 88, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.17593740513486156}, 'layer_2_size': 83, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 23, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#10 epoch=1.0 loss={'loss': 0.2550249323189136, 'rmse': 0.2550249323189136, 'mae': 0.20864166800252704, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2813567573149851}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.31136432916598167}, 'layer_3_size': 2, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.24457064043811153}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#12 epoch=1.0 loss={'loss': 0.2139776830497447, 'rmse': 0.2139776830497447, 'mae': 0.1716508403516478, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 65, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.29698258592896976}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.30204101148674845}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.45911546730391384}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#13 epoch=1.0 loss={'loss': 0.21250770074622807, 'rmse': 0.21250770074622807, 'mae': 0.16442821736999788, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11334948869959081}, 'layer_2_size': 23, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 97, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3514355690518264}, 'layer_5_size': 17, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#15 epoch=1.0 loss={'loss': 0.2050090941966866, 'rmse': 0.2050090941966866, 'mae': 0.16306016692450104, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10416314660244544}, 'layer_2_size': 69, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1235273523668929}, 'layer_3_size': 48, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4953388185253027}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#16 epoch=1.0 loss={'loss': 0.5392046499925002, 'rmse': 0.5392046499925002, 'mae': 0.47073461649969095, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.32066176829549653}, 'layer_1_size': 56, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22006884379319352}, 'layer_4_size': 2, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4828959430995342}, 'layer_5_size': 41, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#14 epoch=1.0 loss={'loss': 0.2371620384223731, 'rmse': 0.2371620384223731, 'mae': 0.19604312404621727, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 48, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18108261962734026}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#18 epoch=1.0 loss={'loss': 0.35490751890116473, 'rmse': 0.35490751890116473, 'mae': 0.2861201387013932, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10757573622851334}, 'layer_1_size': 100, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.33288135222369186}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#19 epoch=1.0 loss={'loss': 0.3069006180398186, 'rmse': 0.3069006180398186, 'mae': 0.25233237024874217, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24656785081188726}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3384337246940693}, 'layer_3_size': 30, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#17 epoch=1.0 loss={'loss': 0.16318723193723106, 'rmse': 0.16318723193723106, 'mae': 0.12855008375065757, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1325178683063519}, 'layer_3_size': 15, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 27, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#20 epoch=1.0 loss={'loss': 0.23306661576592533, 'rmse': 0.23306661576592533, 'mae': 0.1920436948756081, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 89, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 69, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 27, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 3, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#21 epoch=1.0 loss={'loss': 0.1939094029789395, 'rmse': 0.1939094029789395, 'mae': 0.15582493974421957, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3763355894571466}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#23 epoch=1.0 loss={'loss': 0.26074026624151153, 'rmse': 0.26074026624151153, 'mae': 0.2131434504079487, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 20, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10970613529878435}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}
#22 epoch=1.0 loss={'loss': 0.20414756886250035, 'rmse': 0.20414756886250035, 'mae': 0.16258614468183427, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3279647395358488}, 'layer_1_size': 16, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 89, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 22, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2369344727551148}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#26 epoch=1.0 loss={'loss': 0.303484738442385, 'rmse': 0.303484738442385, 'mae': 0.2470276698360866, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39137249636293736}, 'layer_1_size': 23, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22200695121501193}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17235765624521715}, 'layer_3_size': 51, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22759252164997124}, 'layer_5_size': 80, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#24 epoch=1.0 loss={'loss': 0.22050932338117418, 'rmse': 0.22050932338117418, 'mae': 0.1818300775966546, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47789425011377495}, 'layer_1_size': 74, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4450129062456337}, 'layer_2_size': 36, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 75, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#25 epoch=1.0 loss={'loss': 0.3045042852810891, 'rmse': 0.3045042852810891, 'mae': 0.24182052712339197, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17153197723016456}, 'layer_1_size': 34, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14708139271545076}, 'layer_2_size': 77, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 63, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#29 epoch=1.0 loss={'loss': 0.23273625047446353, 'rmse': 0.23273625047446353, 'mae': 0.18534387846511508, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10855261526378511}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#28 epoch=1.0 loss={'loss': 0.2308171341444946, 'rmse': 0.2308171341444946, 'mae': 0.18410875991541242, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#27 epoch=1.0 loss={'loss': 0.18826842162022026, 'rmse': 0.18826842162022026, 'mae': 0.14983851003486867, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3958506948699859}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3985947438007549}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#31 epoch=1.0 loss={'loss': 0.2412170917797639, 'rmse': 0.2412170917797639, 'mae': 0.19852241213717106, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 22, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 98, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#30 epoch=1.0 loss={'loss': 0.3313070790690605, 'rmse': 0.3313070790690605, 'mae': 0.28697057983195595, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#33 epoch=1.0 loss={'loss': 0.517575323159229, 'rmse': 0.517575323159229, 'mae': 0.4097713398280512, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 94, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12643692050394267}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
#32 epoch=1.0 loss={'loss': 0.3151910933317293, 'rmse': 0.3151910933317293, 'mae': 0.24890702316052463, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 48, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3619910331156264}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.286597451825205}, 'layer_4_size': 88, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#34 epoch=1.0 loss={'loss': 0.1928476890217273, 'rmse': 0.1928476890217273, 'mae': 0.15628298039273883, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3676760429632786}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 69, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#35 epoch=1.0 loss={'loss': 0.21014472919974375, 'rmse': 0.21014472919974375, 'mae': 0.16518558920853382, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3110843672502082}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2555820286847462}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#37 epoch=1.0 loss={'loss': 0.20340037372450684, 'rmse': 0.20340037372450684, 'mae': 0.1623268265192807, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14897819670033827}, 'layer_2_size': 31, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27118399118299463}, 'layer_3_size': 2, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 73, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 49, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#36 epoch=1.0 loss={'loss': 0.24921812716672342, 'rmse': 0.24921812716672342, 'mae': 0.1967781642087812, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 5, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#40 epoch=1.0 loss={'loss': 0.22531471771968617, 'rmse': 0.22531471771968617, 'mae': 0.18286873657900923, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2508346160127697}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 72, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#39 epoch=1.0 loss={'loss': 0.1704448432664166, 'rmse': 0.1704448432664166, 'mae': 0.12926143542610996, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 28, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4283350651365785}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3455095972091068}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#38 epoch=1.0 loss={'loss': 0.29110163875175143, 'rmse': 0.29110163875175143, 'mae': 0.2354601352236099, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.32004886003186006}, 'layer_1_size': 38, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4812453420169397}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 40, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10789032155064207}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
#42 epoch=1.0 loss={'loss': 0.2734687318831778, 'rmse': 0.2734687318831778, 'mae': 0.22899223587561784, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43085673746807795}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3242093808736577}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#43 epoch=1.0 loss={'loss': 0.35026904849078955, 'rmse': 0.35026904849078955, 'mae': 0.2811338817115241, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41255552454864375}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 92, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#41 epoch=1.0 loss={'loss': 0.2107744134554754, 'rmse': 0.2107744134554754, 'mae': 0.1732228241946067, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2623268259265953}, 'layer_2_size': 62, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 82, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30306473984005766}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#44 epoch=1.0 loss={'loss': 0.2888695316091966, 'rmse': 0.2888695316091966, 'mae': 0.23303205629967932, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39023019053840957}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#46 epoch=1.0 loss={'loss': 0.21097637185610413, 'rmse': 0.21097637185610413, 'mae': 0.16816580948800003, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 5, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#45 epoch=1.0 loss={'loss': 0.1789475538016504, 'rmse': 0.1789475538016504, 'mae': 0.1438966485993309, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 7, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4908483209465242}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.18134188180962785}, 'layer_4_size': 60, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#49 epoch=1.0 loss={'loss': 0.2678976525823216, 'rmse': 0.2678976525823216, 'mae': 0.22015499629647464, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21353699840031415}, 'layer_1_size': 12, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1887525394629789}, 'layer_2_size': 64, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27407622708939067}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#47 epoch=1.0 loss={'loss': 0.21083252016302698, 'rmse': 0.21083252016302698, 'mae': 0.17305093874271454, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 63, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#48 epoch=1.0 loss={'loss': 0.2644710794690549, 'rmse': 0.2644710794690549, 'mae': 0.216894650138128, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.181589516080453}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.44647201048090357}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 2, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 34, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22542703942748094}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#50 epoch=1.0 loss={'loss': 0.26445715514655926, 'rmse': 0.26445715514655926, 'mae': 0.21643900064868482, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 98, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4222044144905175}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 2, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#51 epoch=1.0 loss={'loss': 0.26154673708850235, 'rmse': 0.26154673708850235, 'mae': 0.21066859580108133, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4322557475652903}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 6, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.36962354345532367}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#52 epoch=1.0 loss={'loss': 0.1919713274053179, 'rmse': 0.1919713274053179, 'mae': 0.1518490683436333, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 19, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.154058449532362}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#53 epoch=1.0 loss={'loss': 0.2091489217965739, 'rmse': 0.2091489217965739, 'mae': 0.16476422511124741, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 47, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16005469532790345}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#54 epoch=1.0 loss={'loss': 0.28995406020407494, 'rmse': 0.28995406020407494, 'mae': 0.2383929929452457, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 21, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 83, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41122132628080166}, 'layer_4_size': 93, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#55 epoch=1.0 loss={'loss': 0.18481128073241967, 'rmse': 0.18481128073241967, 'mae': 0.15063477814423132, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21837030191686663}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22624305720097207}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4326772368856566}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27774682114412147}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#56 epoch=1.0 loss={'loss': 0.35333056974750615, 'rmse': 0.35333056974750615, 'mae': 0.29123917205416394, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39114648997625634}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#57 epoch=1.0 loss={'loss': 0.7328137402804643, 'rmse': 0.7328137402804643, 'mae': 0.6818258761993052, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 27, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.40623525449208187}, 'layer_2_size': 3, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 93, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 6, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.29015795012512713}, 'layer_5_size': 10, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#58 epoch=1.0 loss={'loss': 0.40045347118844715, 'rmse': 0.40045347118844715, 'mae': 0.33153141415583604, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45809535997411455}, 'layer_1_size': 56, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4137789166294171}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4302478130034917}, 'layer_3_size': 59, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3709287657001251}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#60 epoch=1.0 loss={'loss': 0.23141975722246266, 'rmse': 0.23141975722246266, 'mae': 0.19228679821750141, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 42, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2799649795995304}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 71, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#59 epoch=1.0 loss={'loss': 0.19316483715117394, 'rmse': 0.19316483715117394, 'mae': 0.15309115262132544, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2905420164871645}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#63 epoch=1.0 loss={'loss': 0.2074411441120533, 'rmse': 0.2074411441120533, 'mae': 0.1687521005600519, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4752384599654145}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48429916230485714}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#62 epoch=1.0 loss={'loss': 0.20435136044142693, 'rmse': 0.20435136044142693, 'mae': 0.1611933739718669, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15602519089682165}, 'layer_1_size': 47, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3662071301271337}, 'layer_2_size': 70, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#61 epoch=1.0 loss={'loss': 0.23080586523079752, 'rmse': 0.23080586523079752, 'mae': 0.1872913610051805, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47194424194851814}, 'layer_2_size': 82, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.43826874021108175}, 'layer_4_size': 78, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10489365671261544}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#65 epoch=1.0 loss={'loss': 0.27777213306138165, 'rmse': 0.27777213306138165, 'mae': 0.22928016020199254, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17246170518745202}, 'layer_1_size': 92, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1433120180375093}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21864295299191291}, 'layer_4_size': 37, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14309072567378933}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#64 epoch=1.0 loss={'loss': 0.21815476743434217, 'rmse': 0.21815476743434217, 'mae': 0.16942346606034064, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28042922014676985}, 'layer_2_size': 32, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 81, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#66 epoch=1.0 loss={'loss': 0.23382954582057633, 'rmse': 0.23382954582057633, 'mae': 0.19066452722236762, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11896381784270203}, 'layer_3_size': 81, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1872715376547708}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#68 epoch=1.0 loss={'loss': 0.2507227377000718, 'rmse': 0.2507227377000718, 'mae': 0.2046419747401127, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 9, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2577702797077809}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.16685390974228376}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#67 epoch=1.0 loss={'loss': 0.2541754265410221, 'rmse': 0.2541754265410221, 'mae': 0.20470905644759319, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38953016521482287}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 76, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35047528622460133}, 'layer_4_size': 78, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#69 epoch=1.0 loss={'loss': 0.2022006761273695, 'rmse': 0.2022006761273695, 'mae': 0.16210904622469938, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#72 epoch=1.0 loss={'loss': 0.24193185725841768, 'rmse': 0.24193185725841768, 'mae': 0.19778273642676503, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 76, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 71, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.21552202662751477}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19200815181849593}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#71 epoch=1.0 loss={'loss': 0.3519220676269693, 'rmse': 0.3519220676269693, 'mae': 0.29462129491034694, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21760004908081332}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39118890592267064}, 'layer_2_size': 74, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 42, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2939361354953432}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#70 epoch=1.0 loss={'loss': 0.17385276572357225, 'rmse': 0.17385276572357225, 'mae': 0.1321177492425397, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 8, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41851260000555746}, 'layer_3_size': 81, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#73 epoch=1.0 loss={'loss': 0.4379746480627341, 'rmse': 0.4379746480627341, 'mae': 0.3852437845054967, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 81, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21790253153759923}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#74 epoch=1.0 loss={'loss': 0.24050408148065924, 'rmse': 0.24050408148065924, 'mae': 0.18521848814174435, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3854686318955314}, 'layer_1_size': 80, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4486193665141055}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#75 epoch=1.0 loss={'loss': 0.44610730496168843, 'rmse': 0.44610730496168843, 'mae': 0.38074187686945965, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4306695586227791}, 'layer_1_size': 65, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#77 epoch=1.0 loss={'loss': 0.21670035294038983, 'rmse': 0.21670035294038983, 'mae': 0.17179363874321937, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.33496577595643284}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31657417017907896}, 'layer_4_size': 2, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#76 epoch=1.0 loss={'loss': 0.23014204611099864, 'rmse': 0.23014204611099864, 'mae': 0.1851259012870158, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#78 epoch=1.0 loss={'loss': 0.21915488815869777, 'rmse': 0.21915488815869777, 'mae': 0.17232717587570753, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 50, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 98, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1666171614564424}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#79 epoch=1.0 loss={'loss': 0.2730417044032003, 'rmse': 0.2730417044032003, 'mae': 0.2238982680321343, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3510927047964897}, 'layer_1_size': 97, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47912796968769367}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22468178723709364}, 'layer_3_size': 47, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#80 epoch=1.0 loss={'loss': 0.24016509045801826, 'rmse': 0.24016509045801826, 'mae': 0.19230435353119218, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 84, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
get a list [results] of length 81
get a list [loss] of length 81
get a list [val_loss] of length 81
