loading data...
/media/adamsmith/Storage2/DataSet/catsdogs/orig_test1
[0, 1, 2]
s=4
T is of size 81
T=[{'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46137102250149586}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 48, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3724017857680866}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16279580265125976}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3614654257953209}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 5, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12937626152858106}, 'layer_3_size': 7, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.43753802427907695}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12046716908374014}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1007813314397716}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2853750582989068}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43864034354968184}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 43, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 87, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 26, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.346233167432349}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32065776293655024}, 'layer_5_size': 90, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 22, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2168997762906385}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2612631090932722}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.19701135383320914}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 19, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 72, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36109801408773246}, 'layer_4_size': 41, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 21, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2072244149230852}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.42224742193516607}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30621154993011024}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4982396270218775}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 2, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3567934048743575}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.39305863317156176}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 23, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 59, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 36, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 48, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3896079512565712}, 'layer_1_size': 18, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 89, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28825810739112234}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35554818377183195}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1356978201261766}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2200949158370239}, 'layer_4_size': 64, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14949304504676364}, 'layer_1_size': 29, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23928116716847572}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3369672140250073}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21490099625368733}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 76, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2216825861213768}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 16, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.295266464493373}, 'layer_3_size': 21, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.398437043406736}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4781339033988239}, 'layer_3_size': 69, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 28, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42869787366229073}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24768611357533984}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42166682354193574}, 'layer_2_size': 92, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15642438405662276}, 'layer_3_size': 37, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 35, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4373613018863446}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 57, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 7, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.26702796872659207}, 'layer_1_size': 99, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2977242385713669}, 'layer_2_size': 25, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4584800667065043}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 94, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3359869337584227}, 'layer_1_size': 43, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22757371472251747}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 20, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4323655185229003}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37912495085969833}, 'layer_1_size': 59, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 46, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3013341732109067}, 'layer_3_size': 25, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45099737197957945}, 'layer_4_size': 11, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.37120010978460194}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22998319532744838}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27567507773101646}, 'layer_1_size': 47, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3923161703172813}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4528981391423986}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3037758680094471}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12281921476282998}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26588728380855603}, 'layer_4_size': 88, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2765503361921329}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21266595991325876}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 73, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3415595929356917}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3062275847875007}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27343847398789506}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 59, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15852722822896373}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27478419091375783}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 10, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 6, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12067248710353487}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11567062256385562}, 'layer_5_size': 14, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.48950206073258296}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22379305594810261}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3668652798941937}, 'layer_3_size': 82, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.36836965610871286}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.141270226702442}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15488674361554342}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.417044002812225}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20145156269732586}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4247982192376758}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 98, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13961529042578574}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45310255735518523}, 'layer_1_size': 94, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 84, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.481322482298282}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36004732452955024}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 57, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30882653488680895}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 73, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15910655064483598}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1444125637290366}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15512839148967222}, 'layer_4_size': 94, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.414201397850555}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41948648799435884}, 'layer_2_size': 68, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.44198961842359097}, 'layer_3_size': 4, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1343834459157337}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3490893391698009}, 'layer_3_size': 36, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14066260202130243}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41190397279469126}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3040401023406488}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 8, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12247234374565133}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 77, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36518771937381966}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 88, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2564204672293974}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1475303709604141}, 'layer_5_size': 26, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3333100225035752}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45470889922489743}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.47054583251857096}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1726832470496576}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.48005461741963973}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2854291176914812}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 94, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2659078945767359}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4292273429449166}, 'layer_1_size': 66, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2139108718857471}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35606983238878764}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23159184883153522}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.31074013935492373}, 'layer_2_size': 13, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11434659060086627}, 'layer_3_size': 79, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23000509299984168}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4566068568723328}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 84, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3432776851517856}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.185800721236857}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22896973214221594}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3985166197993746}, 'layer_1_size': 14, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13558220511915478}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 99, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 92, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.40646090272414603}, 'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 58, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 61, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22686791878390122}, 'layer_1_size': 64, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3627590148563128}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14519842067262304}, 'layer_2_size': 68, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2541417076702347}, 'layer_2_size': 45, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.13655975076858798}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20029354860657234}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2295313374218199}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22768606312641546}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30941827120558196}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15433139618584396}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12065696796480144}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 78, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 24, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20137771080882494}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 75, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17277741147530945}, 'layer_4_size': 6, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 37, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 53, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4318110609485434}, 'layer_4_size': 5, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4034406372087809}, 'layer_2_size': 3, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.39094249870900966}, 'layer_3_size': 54, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 43, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24622101941459051}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]
newly formed T structure is:[[0, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46137102250149586}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 48, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3724017857680866}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [1, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16279580265125976}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [2, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3614654257953209}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 5, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [3, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12937626152858106}, 'layer_3_size': 7, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.43753802427907695}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [4, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12046716908374014}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [5, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1007813314397716}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2853750582989068}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [6, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43864034354968184}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 43, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [7, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 87, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 26, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [8, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.346233167432349}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32065776293655024}, 'layer_5_size': 90, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [9, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [10, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 22, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [11, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2168997762906385}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2612631090932722}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.19701135383320914}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [12, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 19, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 72, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36109801408773246}, 'layer_4_size': 41, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [13, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 21, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [14, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2072244149230852}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.42224742193516607}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30621154993011024}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [15, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4982396270218775}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [16, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 2, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3567934048743575}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.39305863317156176}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 23, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [17, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 59, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 36, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 48, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [18, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3896079512565712}, 'layer_1_size': 18, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 89, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28825810739112234}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [19, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35554818377183195}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1356978201261766}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2200949158370239}, 'layer_4_size': 64, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [20, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14949304504676364}, 'layer_1_size': 29, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23928116716847572}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3369672140250073}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [21, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21490099625368733}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 76, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [22, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2216825861213768}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 16, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.295266464493373}, 'layer_3_size': 21, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.398437043406736}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [23, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4781339033988239}, 'layer_3_size': 69, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 28, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42869787366229073}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [24, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24768611357533984}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42166682354193574}, 'layer_2_size': 92, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15642438405662276}, 'layer_3_size': 37, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [25, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 35, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [26, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4373613018863446}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 57, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 7, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [27, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.26702796872659207}, 'layer_1_size': 99, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2977242385713669}, 'layer_2_size': 25, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4584800667065043}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [28, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 94, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [29, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3359869337584227}, 'layer_1_size': 43, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22757371472251747}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [30, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 20, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4323655185229003}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [31, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37912495085969833}, 'layer_1_size': 59, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 46, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3013341732109067}, 'layer_3_size': 25, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45099737197957945}, 'layer_4_size': 11, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.37120010978460194}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [32, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22998319532744838}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [33, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27567507773101646}, 'layer_1_size': 47, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3923161703172813}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [34, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4528981391423986}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [35, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3037758680094471}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [36, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12281921476282998}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26588728380855603}, 'layer_4_size': 88, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [37, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2765503361921329}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21266595991325876}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [38, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 73, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3415595929356917}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3062275847875007}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [39, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27343847398789506}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [40, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 59, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15852722822896373}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27478419091375783}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [41, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 10, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 6, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12067248710353487}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [42, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11567062256385562}, 'layer_5_size': 14, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [43, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.48950206073258296}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22379305594810261}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3668652798941937}, 'layer_3_size': 82, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [44, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.36836965610871286}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [45, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.141270226702442}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15488674361554342}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [46, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.417044002812225}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20145156269732586}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4247982192376758}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [47, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 98, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13961529042578574}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [48, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45310255735518523}, 'layer_1_size': 94, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 84, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.481322482298282}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [49, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36004732452955024}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 57, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30882653488680895}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 73, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [50, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15910655064483598}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1444125637290366}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15512839148967222}, 'layer_4_size': 94, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.414201397850555}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [51, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41948648799435884}, 'layer_2_size': 68, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.44198961842359097}, 'layer_3_size': 4, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [52, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1343834459157337}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [53, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [54, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3490893391698009}, 'layer_3_size': 36, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [55, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14066260202130243}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41190397279469126}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3040401023406488}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [56, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 8, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12247234374565133}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 77, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36518771937381966}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [57, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 88, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2564204672293974}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1475303709604141}, 'layer_5_size': 26, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [58, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3333100225035752}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45470889922489743}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.47054583251857096}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [59, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1726832470496576}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [60, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.48005461741963973}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2854291176914812}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [61, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 94, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [62, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2659078945767359}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [63, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4292273429449166}, 'layer_1_size': 66, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2139108718857471}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35606983238878764}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23159184883153522}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [64, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.31074013935492373}, 'layer_2_size': 13, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11434659060086627}, 'layer_3_size': 79, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23000509299984168}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [65, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4566068568723328}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [66, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 84, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3432776851517856}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [67, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.185800721236857}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22896973214221594}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [68, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3985166197993746}, 'layer_1_size': 14, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13558220511915478}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 99, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [69, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 92, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.40646090272414603}, 'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 58, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [70, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 61, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [71, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22686791878390122}, 'layer_1_size': 64, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3627590148563128}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [72, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14519842067262304}, 'layer_2_size': 68, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [73, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2541417076702347}, 'layer_2_size': 45, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.13655975076858798}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20029354860657234}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [74, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2295313374218199}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22768606312641546}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [75, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30941827120558196}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15433139618584396}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [76, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12065696796480144}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 78, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [77, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 24, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [78, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20137771080882494}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 75, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17277741147530945}, 'layer_4_size': 6, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [79, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 37, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 53, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4318110609485434}, 'layer_4_size': 5, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [80, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4034406372087809}, 'layer_2_size': 3, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.39094249870900966}, 'layer_3_size': 54, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 43, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24622101941459051}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]] 

*** 81 configurations x 1.0 iterations each

1 | Thu Sep 27 17:46:09 2018 | lowest loss so far: inf (run -1)

{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  67 | activation: sigmoid | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:32 - loss: 1.2740
 560/6530 [=>............................] - ETA: 9s - loss: 0.4634  
1152/6530 [====>.........................] - ETA: 4s - loss: 0.3244
1872/6530 [=======>......................] - ETA: 2s - loss: 0.2665
2592/6530 [==========>...................] - ETA: 1s - loss: 0.2369
3376/6530 [==============>...............] - ETA: 0s - loss: 0.2186
4128/6530 [=================>............] - ETA: 0s - loss: 0.2086
4896/6530 [=====================>........] - ETA: 0s - loss: 0.2029
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1986{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  85 | activation: relu    | extras: batchnorm 
layer 2 | size:  42 | activation: relu    | extras: dropout - rate: 36.1% 
layer 3 | size:  10 | activation: tanh    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 23s - loss: 0.5613
6320/6530 [============================>.] - ETA: 0s - loss: 0.1953
5120/6530 [======================>.......] - ETA: 0s - loss: 0.4111 
6530/6530 [==============================] - 1s 202us/step - loss: 0.1945 - val_loss: 0.1655

6530/6530 [==============================] - 1s 162us/step - loss: 0.3564 - val_loss: 0.0898
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:   9 | activation: tanh    | extras: dropout - rate: 46.1% 
layer 2 | size:  18 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  35 | activation: tanh    | extras: None 
layer 4 | size:  48 | activation: relu    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 24s - loss: 0.6910
5120/6530 [======================>.......] - ETA: 0s - loss: 0.6503 
6530/6530 [==============================] - 1s 168us/step - loss: 0.6192 - val_loss: 0.4477

# training | RMSE: 0.5213, MAE: 0.4429
worker 0  xfile  [0, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46137102250149586}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 48, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3724017857680866}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.5213052974043579, 'rmse': 0.5213052974043579, 'mae': 0.4428832164298356, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  33 | activation: relu    | extras: None 
layer 2 | size:   8 | activation: tanh    | extras: dropout - rate: 10.1% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 8s - loss: 0.3738
2688/6530 [===========>..................] - ETA: 0s - loss: 0.2748
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2502
6530/6530 [==============================] - 0s 48us/step - loss: 0.2410 - val_loss: 0.1907

# training | RMSE: 0.2981, MAE: 0.2480
worker 2  xfile  [2, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3614654257953209}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 5, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2981332846355835, 'rmse': 0.2981332846355835, 'mae': 0.24798093481462818, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  86 | activation: sigmoid | extras: None 
layer 2 | size:  92 | activation: tanh    | extras: batchnorm 
layer 3 | size:   7 | activation: tanh    | extras: dropout - rate: 12.9% 
layer 4 | size:  94 | activation: sigmoid | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:22 - loss: 1.1037
 224/6530 [>.............................] - ETA: 15s - loss: 0.6926 
 448/6530 [=>............................] - ETA: 8s - loss: 0.4458 
# training | RMSE: 0.2078, MAE: 0.1678
worker 1  xfile  [1, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16279580265125976}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2078054773852269, 'rmse': 0.2078054773852269, 'mae': 0.16775571880550813, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  86 | activation: tanh    | extras: batchnorm 
layer 2 | size:  12 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  54 | activation: tanh    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:36 - loss: 0.7631
 704/6530 [==>...........................] - ETA: 5s - loss: 0.3497
 608/6530 [=>............................] - ETA: 5s - loss: 0.4048  
 976/6530 [===>..........................] - ETA: 4s - loss: 0.2996
1248/6530 [====>.........................] - ETA: 2s - loss: 0.2987
1216/6530 [====>.........................] - ETA: 3s - loss: 0.2715
1856/6530 [=======>......................] - ETA: 1s - loss: 0.2321
1472/6530 [=====>........................] - ETA: 2s - loss: 0.2525
2464/6530 [==========>...................] - ETA: 1s - loss: 0.1876
1744/6530 [=======>......................] - ETA: 2s - loss: 0.2367
3072/6530 [=============>................] - ETA: 0s - loss: 0.1591
2016/6530 [========>.....................] - ETA: 2s - loss: 0.2261
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1397
2288/6530 [=========>....................] - ETA: 1s - loss: 0.2188
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1249
2576/6530 [==========>...................] - ETA: 1s - loss: 0.2133
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1136
2848/6530 [============>.................] - ETA: 1s - loss: 0.2073
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1049
3120/6530 [=============>................] - ETA: 1s - loss: 0.2025
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0977
3408/6530 [==============>...............] - ETA: 1s - loss: 0.1992
3696/6530 [===============>..............] - ETA: 0s - loss: 0.1965
6530/6530 [==============================] - 1s 168us/step - loss: 0.0929 - val_loss: 0.0456

3984/6530 [=================>............] - ETA: 0s - loss: 0.1943
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1921
# training | RMSE: 0.2407, MAE: 0.1938
worker 0  xfile  [5, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1007813314397716}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2853750582989068}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.24074956225177252, 'rmse': 0.24074956225177252, 'mae': 0.1938287190817625, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  30 | activation: relu    | extras: batchnorm 
layer 2 | size:  95 | activation: relu    | extras: None 
layer 3 | size:  29 | activation: tanh    | extras: dropout - rate: 43.9% 
layer 4 | size:  43 | activation: sigmoid | extras: None 
layer 5 | size:  73 | activation: relu    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 26s - loss: 0.7940
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1908
2432/6530 [==========>...................] - ETA: 0s - loss: 0.5470 
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1893
4352/6530 [==================>...........] - ETA: 0s - loss: 0.4155
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1871
6528/6530 [============================>.] - ETA: 0s - loss: 0.3491
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1860
6530/6530 [==============================] - 1s 112us/step - loss: 0.3491 - val_loss: 0.2026

5680/6530 [=========================>....] - ETA: 0s - loss: 0.1844
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1831
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1816
6530/6530 [==============================] - 2s 276us/step - loss: 0.1804 - val_loss: 0.2590

# training | RMSE: 0.2176, MAE: 0.1727
worker 1  xfile  [4, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12046716908374014}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2176223288376924, 'rmse': 0.2176223288376924, 'mae': 0.17271549730988808, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  83 | activation: tanh    | extras: None 
layer 2 | size:  87 | activation: tanh    | extras: batchnorm 
layer 3 | size: 100 | activation: relu    | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 18s - loss: 0.6425
2048/6530 [========>.....................] - ETA: 0s - loss: 0.3444 
4096/6530 [=================>............] - ETA: 0s - loss: 0.2179
6400/6530 [============================>.] - ETA: 0s - loss: 0.1636
6530/6530 [==============================] - 1s 87us/step - loss: 0.1614 - val_loss: 0.1642

# training | RMSE: 0.3113, MAE: 0.2569
worker 2  xfile  [3, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12937626152858106}, 'layer_3_size': 7, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.43753802427907695}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.31131487233761346, 'rmse': 0.31131487233761346, 'mae': 0.25692913404290013, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  11 | activation: sigmoid | extras: None 
layer 2 | size:  60 | activation: tanh    | extras: None 
layer 3 | size:  53 | activation: sigmoid | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 6s - loss: 0.4469
6530/6530 [==============================] - 0s 49us/step - loss: 0.1499 - val_loss: 0.0735

# training | RMSE: 0.2487, MAE: 0.2035
worker 0  xfile  [6, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43864034354968184}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 43, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2486649382756091, 'rmse': 0.2486649382756091, 'mae': 0.20345831830868583, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  40 | activation: relu    | extras: batchnorm 
layer 2 | size:  46 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  74 | activation: sigmoid | extras: None 
layer 4 | size:  13 | activation: tanh    | extras: dropout - rate: 34.6% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:18 - loss: 0.5133
 256/6530 [>.............................] - ETA: 13s - loss: 0.1203 
 544/6530 [=>............................] - ETA: 6s - loss: 0.0797 
 816/6530 [==>...........................] - ETA: 4s - loss: 0.0671
# training | RMSE: 0.3938, MAE: 0.3112
worker 1  xfile  [7, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 87, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 26, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.39376694094175757, 'rmse': 0.39376694094175757, 'mae': 0.3111650419045698, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  22 | activation: relu    | extras: None 
layer 2 | size:  37 | activation: sigmoid | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 1:49 - loss: 0.7146
1104/6530 [====>.........................] - ETA: 3s - loss: 0.0594
1392/6530 [=====>........................] - ETA: 2s - loss: 0.0532
 624/6530 [=>............................] - ETA: 3s - loss: 0.3696  
1184/6530 [====>.........................] - ETA: 1s - loss: 0.3037
1664/6530 [======>.......................] - ETA: 2s - loss: 0.0492
1776/6530 [=======>......................] - ETA: 1s - loss: 0.2744
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0468
2352/6530 [=========>....................] - ETA: 0s - loss: 0.2619
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0448
2944/6530 [============>.................] - ETA: 0s - loss: 0.2513
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0437
# training | RMSE: 0.2753, MAE: 0.2261
worker 2  xfile  [9, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2752707883807601, 'rmse': 0.2752707883807601, 'mae': 0.22606413814358106, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  83 | activation: tanh    | extras: dropout - rate: 21.7% 
layer 2 | size:  66 | activation: tanh    | extras: dropout - rate: 26.1% 
layer 3 | size:  52 | activation: tanh    | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 6s - loss: 0.5989
3520/6530 [===============>..............] - ETA: 0s - loss: 0.2451
2816/6530 [===========>..................] - ETA: 1s - loss: 0.0425
4096/6530 [=================>............] - ETA: 0s - loss: 0.2402
3104/6530 [=============>................] - ETA: 1s - loss: 0.0408
6530/6530 [==============================] - 0s 50us/step - loss: 0.1188 - val_loss: 0.0421

4656/6530 [====================>.........] - ETA: 0s - loss: 0.2358
3376/6530 [==============>...............] - ETA: 1s - loss: 0.0398
5232/6530 [=======================>......] - ETA: 0s - loss: 0.2329
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0391
5840/6530 [=========================>....] - ETA: 0s - loss: 0.2301
3920/6530 [=================>............] - ETA: 0s - loss: 0.0384
6400/6530 [============================>.] - ETA: 0s - loss: 0.2273
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0375
6530/6530 [==============================] - 1s 135us/step - loss: 0.2264 - val_loss: 0.1933

4496/6530 [===================>..........] - ETA: 0s - loss: 0.0369
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0362
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0355
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0350
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0343
# training | RMSE: 0.2047, MAE: 0.1668
worker 2  xfile  [11, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2168997762906385}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2612631090932722}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.19701135383320914}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.20469795272258126, 'rmse': 0.20469795272258126, 'mae': 0.16675269212269514, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  19 | activation: sigmoid | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 10s - loss: 0.6009
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0337
4736/6530 [====================>.........] - ETA: 0s - loss: 0.3450 
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0334
6530/6530 [==============================] - 0s 49us/step - loss: 0.2858 - val_loss: 0.1082

6528/6530 [============================>.] - ETA: 0s - loss: 0.0328
6530/6530 [==============================] - 2s 270us/step - loss: 0.0328 - val_loss: 0.0206

# training | RMSE: 0.3262, MAE: 0.2643
worker 2  xfile  [12, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 19, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 72, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36109801408773246}, 'layer_4_size': 41, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.3261617381256271, 'rmse': 0.3261617381256271, 'mae': 0.26428197292005295, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  56 | activation: sigmoid | extras: dropout - rate: 20.7% 
layer 2 | size:  90 | activation: relu    | extras: batchnorm 
layer 3 | size:  28 | activation: sigmoid | extras: dropout - rate: 42.2% 
layer 4 | size:  30 | activation: sigmoid | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 30s - loss: 0.6136
# training | RMSE: 0.2395, MAE: 0.1957
worker 1  xfile  [10, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 22, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2395484056286396, 'rmse': 0.2395484056286396, 'mae': 0.1956619780884545, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  28 | activation: tanh    | extras: batchnorm 
layer 2 | size:  53 | activation: relu    | extras: batchnorm 
layer 3 | size:  14 | activation: relu    | extras: batchnorm 
layer 4 | size:  21 | activation: sigmoid | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:46 - loss: 0.4243
1920/6530 [=======>......................] - ETA: 1s - loss: 0.4456 
 192/6530 [..............................] - ETA: 24s - loss: 0.3100 
3712/6530 [================>.............] - ETA: 0s - loss: 0.3466
 384/6530 [>.............................] - ETA: 12s - loss: 0.2367
5632/6530 [========================>.....] - ETA: 0s - loss: 0.3040
 592/6530 [=>............................] - ETA: 8s - loss: 0.1825 
 800/6530 [==>...........................] - ETA: 6s - loss: 0.1504
6530/6530 [==============================] - 1s 131us/step - loss: 0.2917 - val_loss: 0.2161

1008/6530 [===>..........................] - ETA: 5s - loss: 0.1300
1216/6530 [====>.........................] - ETA: 4s - loss: 0.1153
1440/6530 [=====>........................] - ETA: 3s - loss: 0.1046
# training | RMSE: 0.1393, MAE: 0.1074
worker 0  xfile  [8, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.346233167432349}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32065776293655024}, 'layer_5_size': 90, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.13932264852392157, 'rmse': 0.13932264852392157, 'mae': 0.107408333921119, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  32 | activation: relu    | extras: None 
layer 2 | size:  47 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  97 | activation: relu    | extras: batchnorm 
layer 4 | size:  76 | activation: tanh    | extras: dropout - rate: 49.8% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 13s - loss: 0.9847
1648/6530 [======>.......................] - ETA: 3s - loss: 0.0960
4608/6530 [====================>.........] - ETA: 0s - loss: 0.4679 
1872/6530 [=======>......................] - ETA: 2s - loss: 0.0890
2096/6530 [========>.....................] - ETA: 2s - loss: 0.0830
6530/6530 [==============================] - 1s 105us/step - loss: 0.4061 - val_loss: 0.2810

2336/6530 [=========>....................] - ETA: 2s - loss: 0.0788
2576/6530 [==========>...................] - ETA: 2s - loss: 0.0755
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0721
3024/6530 [============>.................] - ETA: 1s - loss: 0.0688
3232/6530 [=============>................] - ETA: 1s - loss: 0.0664
3456/6530 [==============>...............] - ETA: 1s - loss: 0.0647
3664/6530 [===============>..............] - ETA: 1s - loss: 0.0626
# training | RMSE: 0.2658, MAE: 0.2170
worker 2  xfile  [14, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2072244149230852}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.42224742193516607}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30621154993011024}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.26579671825271867, 'rmse': 0.26579671825271867, 'mae': 0.2170318496494003, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:   2 | activation: relu    | extras: None 
layer 2 | size:  23 | activation: sigmoid | extras: dropout - rate: 35.7% 
layer 3 | size:  83 | activation: tanh    | extras: dropout - rate: 39.3% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 31s - loss: 0.9807
3904/6530 [================>.............] - ETA: 1s - loss: 0.0609
2112/6530 [========>.....................] - ETA: 0s - loss: 0.3072 
4128/6530 [=================>............] - ETA: 0s - loss: 0.0594
4096/6530 [=================>............] - ETA: 0s - loss: 0.2732
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0578
6144/6530 [===========================>..] - ETA: 0s - loss: 0.2563
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0569
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0557
6530/6530 [==============================] - 1s 81us/step - loss: 0.2547 - val_loss: 0.1974

4992/6530 [=====================>........] - ETA: 0s - loss: 0.0547
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0536
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0529
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0518
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0509
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0500
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0494
# training | RMSE: 0.3272, MAE: 0.2614
worker 0  xfile  [15, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4982396270218775}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.32722652157685683, 'rmse': 0.32722652157685683, 'mae': 0.26140270345156535, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  59 | activation: tanh    | extras: batchnorm 
layer 2 | size:  42 | activation: sigmoid | extras: None 
layer 3 | size:  36 | activation: tanh    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 46s - loss: 0.7702
6496/6530 [============================>.] - ETA: 0s - loss: 0.0486
1472/6530 [=====>........................] - ETA: 1s - loss: 0.3959 
2880/6530 [============>.................] - ETA: 0s - loss: 0.2982
4288/6530 [==================>...........] - ETA: 0s - loss: 0.2555
6530/6530 [==============================] - 2s 364us/step - loss: 0.0485 - val_loss: 0.0208

5760/6530 [=========================>....] - ETA: 0s - loss: 0.2320
6530/6530 [==============================] - 1s 116us/step - loss: 0.2230 - val_loss: 0.1627

# training | RMSE: 0.2443, MAE: 0.2004
worker 2  xfile  [16, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 2, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3567934048743575}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.39305863317156176}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 23, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.24428250698385456, 'rmse': 0.24428250698385456, 'mae': 0.20041748158895178, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  18 | activation: relu    | extras: dropout - rate: 39.0% 
layer 2 | size:  29 | activation: sigmoid | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 17s - loss: 0.9879
3712/6530 [================>.............] - ETA: 0s - loss: 0.5441 
6530/6530 [==============================] - 0s 75us/step - loss: 0.3948 - val_loss: 0.1417

# training | RMSE: 0.2038, MAE: 0.1598
worker 0  xfile  [17, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 59, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 36, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 48, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2037554206580471, 'rmse': 0.2037554206580471, 'mae': 0.15978190911917906, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  33 | activation: sigmoid | extras: dropout - rate: 35.6% 
layer 2 | size:  41 | activation: sigmoid | extras: dropout - rate: 13.6% 
layer 3 | size:  86 | activation: tanh    | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 20s - loss: 0.5740
# training | RMSE: 0.1397, MAE: 0.1099
worker 1  xfile  [13, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 21, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.13974185169502987, 'rmse': 0.13974185169502987, 'mae': 0.10990660656205996, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  29 | activation: sigmoid | extras: dropout - rate: 14.9% 
layer 2 | size:  98 | activation: relu    | extras: dropout - rate: 23.9% 
layer 3 | size:  25 | activation: sigmoid | extras: dropout - rate: 33.7% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:01 - loss: 2.4952
2688/6530 [===========>..................] - ETA: 0s - loss: 0.2137 
 736/6530 [==>...........................] - ETA: 2s - loss: 0.2708  
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1449
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1744
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1377
6530/6530 [==============================] - 1s 91us/step - loss: 0.1333 - val_loss: 0.0701

3360/6530 [==============>...............] - ETA: 0s - loss: 0.1192
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1090
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1015
6464/6530 [============================>.] - ETA: 0s - loss: 0.0963
6530/6530 [==============================] - 1s 109us/step - loss: 0.0962 - val_loss: 0.0614

# training | RMSE: 0.3706, MAE: 0.3038
worker 2  xfile  [18, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3896079512565712}, 'layer_1_size': 18, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 89, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28825810739112234}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.3705673448022455, 'rmse': 0.3705673448022455, 'mae': 0.30383211105582847, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  67 | activation: tanh    | extras: batchnorm 
layer 2 | size:  15 | activation: relu    | extras: dropout - rate: 21.5% 
layer 3 | size:  15 | activation: sigmoid | extras: None 
layer 4 | size:  76 | activation: tanh    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:03 - loss: 0.3005
1024/6530 [===>..........................] - ETA: 3s - loss: 0.2237  
2048/6530 [========>.....................] - ETA: 1s - loss: 0.2081
3136/6530 [=============>................] - ETA: 0s - loss: 0.1968
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1884
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1840
# training | RMSE: 0.2677, MAE: 0.2200
worker 0  xfile  [19, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35554818377183195}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1356978201261766}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2200949158370239}, 'layer_4_size': 64, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2676917397116646, 'rmse': 0.2676917397116646, 'mae': 0.2199846300958158, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  24 | activation: sigmoid | extras: dropout - rate: 22.2% 
layer 2 | size:  16 | activation: sigmoid | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:09 - loss: 0.6862
6400/6530 [============================>.] - ETA: 0s - loss: 0.1800
1120/6530 [====>.........................] - ETA: 1s - loss: 0.2083  
6530/6530 [==============================] - 1s 156us/step - loss: 0.1796 - val_loss: 0.2640

2176/6530 [========>.....................] - ETA: 0s - loss: 0.1528
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1297
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1154
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1057
6530/6530 [==============================] - 1s 109us/step - loss: 0.1001 - val_loss: 0.0583

# training | RMSE: 0.2486, MAE: 0.2030
worker 1  xfile  [20, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14949304504676364}, 'layer_1_size': 29, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23928116716847572}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3369672140250073}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.248641424948513, 'rmse': 0.248641424948513, 'mae': 0.2030236359262989, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  17 | activation: relu    | extras: batchnorm 
layer 2 | size:  42 | activation: tanh    | extras: None 
layer 3 | size:  69 | activation: sigmoid | extras: dropout - rate: 47.8% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 47s - loss: 0.6791
1472/6530 [=====>........................] - ETA: 1s - loss: 0.1895 
2880/6530 [============>.................] - ETA: 0s - loss: 0.1630
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1488
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1442
6530/6530 [==============================] - 1s 116us/step - loss: 0.1414 - val_loss: 0.1684

# training | RMSE: 0.3176, MAE: 0.2571
worker 2  xfile  [21, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21490099625368733}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 76, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.3176435891539007, 'rmse': 0.3176435891539007, 'mae': 0.25712204918131587, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  40 | activation: relu    | extras: dropout - rate: 24.8% 
layer 2 | size:  92 | activation: sigmoid | extras: dropout - rate: 42.2% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 20s - loss: 1.4026
3840/6530 [================>.............] - ETA: 0s - loss: 0.5699 
6530/6530 [==============================] - 1s 84us/step - loss: 0.4487 - val_loss: 0.2208

# training | RMSE: 0.2006, MAE: 0.1658
worker 1  xfile  [23, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4781339033988239}, 'layer_3_size': 69, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 28, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42869787366229073}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.20061122857558972, 'rmse': 0.20061122857558972, 'mae': 0.16575131399339893, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  42 | activation: tanh    | extras: dropout - rate: 43.7% 
layer 2 | size:  75 | activation: sigmoid | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 7s - loss: 0.5494
6530/6530 [==============================] - 0s 64us/step - loss: 0.2426 - val_loss: 0.1937

# training | RMSE: 0.2411, MAE: 0.1955
worker 0  xfile  [22, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2216825861213768}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 16, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.295266464493373}, 'layer_3_size': 21, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.398437043406736}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.24108196079894773, 'rmse': 0.24108196079894773, 'mae': 0.19545963539816708, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  28 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  21 | activation: tanh    | extras: batchnorm 
layer 3 | size:  16 | activation: sigmoid | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:04 - loss: 1.3020
1088/6530 [===>..........................] - ETA: 3s - loss: 1.1654  
2176/6530 [========>.....................] - ETA: 1s - loss: 1.0574
3328/6530 [==============>...............] - ETA: 0s - loss: 0.9527
4480/6530 [===================>..........] - ETA: 0s - loss: 0.8510
5568/6530 [========================>.....] - ETA: 0s - loss: 0.7527
# training | RMSE: 0.2777, MAE: 0.2231
worker 2  xfile  [24, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24768611357533984}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42166682354193574}, 'layer_2_size': 92, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15642438405662276}, 'layer_3_size': 37, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.2776976623146451, 'rmse': 0.2776976623146451, 'mae': 0.2231434172367511, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  99 | activation: tanh    | extras: dropout - rate: 26.7% 
layer 2 | size:  25 | activation: relu    | extras: dropout - rate: 29.8% 
layer 3 | size:  46 | activation: tanh    | extras: None 
layer 4 | size:  80 | activation: tanh    | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:34 - loss: 0.5601
 864/6530 [==>...........................] - ETA: 3s - loss: 0.2012  
1728/6530 [======>.......................] - ETA: 1s - loss: 0.1450
6530/6530 [==============================] - 1s 158us/step - loss: 0.6741 - val_loss: 0.1658

2560/6530 [==========>...................] - ETA: 0s - loss: 0.1228
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1075
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0965
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0882
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0823
6530/6530 [==============================] - 1s 143us/step - loss: 0.0796 - val_loss: 0.0755

# training | RMSE: 0.2381, MAE: 0.1942
worker 1  xfile  [26, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4373613018863446}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 57, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 7, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2380722077292145, 'rmse': 0.2380722077292145, 'mae': 0.194189389182616, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  94 | activation: tanh    | extras: batchnorm 
layer 2 | size:  41 | activation: tanh    | extras: None 
layer 3 | size:  56 | activation: tanh    | extras: None 
layer 4 | size:   8 | activation: sigmoid | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:16 - loss: 0.6091
 208/6530 [..............................] - ETA: 20s - loss: 0.6398 
 416/6530 [>.............................] - ETA: 10s - loss: 0.6127
 656/6530 [==>...........................] - ETA: 7s - loss: 0.5566 
# training | RMSE: 0.2014, MAE: 0.1643
worker 0  xfile  [25, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 35, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.20136367167878805, 'rmse': 0.20136367167878805, 'mae': 0.16433426940027476, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  43 | activation: tanh    | extras: dropout - rate: 33.6% 
layer 2 | size:  74 | activation: sigmoid | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:59 - loss: 0.8048
 928/6530 [===>..........................] - ETA: 5s - loss: 0.4897
 448/6530 [=>............................] - ETA: 6s - loss: 0.3723  
1216/6530 [====>.........................] - ETA: 3s - loss: 0.4257
 896/6530 [===>..........................] - ETA: 3s - loss: 0.2930
1488/6530 [=====>........................] - ETA: 3s - loss: 0.3891
1360/6530 [=====>........................] - ETA: 2s - loss: 0.2495
1792/6530 [=======>......................] - ETA: 2s - loss: 0.3564
1808/6530 [=======>......................] - ETA: 1s - loss: 0.2292
2080/6530 [========>.....................] - ETA: 2s - loss: 0.3360
2240/6530 [=========>....................] - ETA: 1s - loss: 0.2160
2384/6530 [=========>....................] - ETA: 1s - loss: 0.3178
2704/6530 [===========>..................] - ETA: 1s - loss: 0.2083
2672/6530 [===========>..................] - ETA: 1s - loss: 0.3044
3120/6530 [=============>................] - ETA: 0s - loss: 0.2008
2960/6530 [============>.................] - ETA: 1s - loss: 0.2918
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1974
3248/6530 [=============>................] - ETA: 1s - loss: 0.2805
4000/6530 [=================>............] - ETA: 0s - loss: 0.1940
3552/6530 [===============>..............] - ETA: 1s - loss: 0.2718
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1914
3856/6530 [================>.............] - ETA: 0s - loss: 0.2647
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1892
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2574
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1875
4448/6530 [===================>..........] - ETA: 0s - loss: 0.2523
# training | RMSE: 0.2750, MAE: 0.2134
worker 2  xfile  [27, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.26702796872659207}, 'layer_1_size': 99, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2977242385713669}, 'layer_2_size': 25, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4584800667065043}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2749783108340743, 'rmse': 0.2749783108340743, 'mae': 0.21343433310423243, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  50 | activation: relu    | extras: batchnorm 
layer 2 | size:  50 | activation: relu    | extras: None 
layer 3 | size:  20 | activation: tanh    | extras: None 
layer 4 | size:  17 | activation: relu    | extras: dropout - rate: 43.2% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 29s - loss: 0.2721
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1853
4752/6530 [====================>.........] - ETA: 0s - loss: 0.2473
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0633 
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1840
5040/6530 [======================>.......] - ETA: 0s - loss: 0.2432
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0466
5344/6530 [=======================>......] - ETA: 0s - loss: 0.2393
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2354
6530/6530 [==============================] - 1s 197us/step - loss: 0.1827 - val_loss: 0.1942

6530/6530 [==============================] - 1s 120us/step - loss: 0.0424 - val_loss: 0.0422

5936/6530 [==========================>...] - ETA: 0s - loss: 0.2322
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2291
6464/6530 [============================>.] - ETA: 0s - loss: 0.2265
6530/6530 [==============================] - 2s 296us/step - loss: 0.2257 - val_loss: 0.1662

# training | RMSE: 0.2054, MAE: 0.1628
worker 1  xfile  [28, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 94, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.20544606392230783, 'rmse': 0.20544606392230783, 'mae': 0.16284022414953728, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  47 | activation: sigmoid | extras: dropout - rate: 27.6% 
layer 2 | size:  61 | activation: tanh    | extras: None 
layer 3 | size:  17 | activation: sigmoid | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:20 - loss: 0.5913
 368/6530 [>.............................] - ETA: 9s - loss: 0.3793  
# training | RMSE: 0.1983, MAE: 0.1577
worker 2  xfile  [30, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 20, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4323655185229003}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.19834485731888304, 'rmse': 0.19834485731888304, 'mae': 0.15766272813020035, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  59 | activation: sigmoid | extras: dropout - rate: 37.9% 
layer 2 | size:  46 | activation: relu    | extras: batchnorm 
layer 3 | size:  25 | activation: tanh    | extras: dropout - rate: 30.1% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:09 - loss: 0.6320
 720/6530 [==>...........................] - ETA: 4s - loss: 0.2380
 960/6530 [===>..........................] - ETA: 4s - loss: 0.5415  
1088/6530 [===>..........................] - ETA: 3s - loss: 0.1862
1984/6530 [========>.....................] - ETA: 1s - loss: 0.4221
1488/6530 [=====>........................] - ETA: 2s - loss: 0.1542
3008/6530 [============>.................] - ETA: 0s - loss: 0.3051
1904/6530 [=======>......................] - ETA: 1s - loss: 0.1358
4096/6530 [=================>............] - ETA: 0s - loss: 0.2430
2336/6530 [=========>....................] - ETA: 1s - loss: 0.1242
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2056
2784/6530 [===========>..................] - ETA: 1s - loss: 0.1152
6336/6530 [============================>.] - ETA: 0s - loss: 0.1832
3248/6530 [=============>................] - ETA: 0s - loss: 0.1084
3712/6530 [================>.............] - ETA: 0s - loss: 0.1038
6530/6530 [==============================] - 1s 168us/step - loss: 0.1796 - val_loss: 0.1190

4160/6530 [==================>...........] - ETA: 0s - loss: 0.0999
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0965
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0938
# training | RMSE: 0.2328, MAE: 0.1933
worker 0  xfile  [29, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3359869337584227}, 'layer_1_size': 43, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22757371472251747}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.23279724061555593, 'rmse': 0.23279724061555593, 'mae': 0.19333303073546862, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  42 | activation: tanh    | extras: dropout - rate: 23.0% 
layer 2 | size:  43 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  90 | activation: tanh    | extras: batchnorm 
layer 4 | size:  10 | activation: sigmoid | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:30 - loss: 0.7056
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0923
 896/6530 [===>..........................] - ETA: 5s - loss: 0.6420  
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0903
1728/6530 [======>.......................] - ETA: 2s - loss: 0.5498
6384/6530 [============================>.] - ETA: 0s - loss: 0.0888
2496/6530 [==========>...................] - ETA: 1s - loss: 0.4777
6530/6530 [==============================] - 1s 208us/step - loss: 0.0882 - val_loss: 0.0688

3264/6530 [=============>................] - ETA: 1s - loss: 0.4198
3968/6530 [=================>............] - ETA: 0s - loss: 0.3839
4672/6530 [====================>.........] - ETA: 0s - loss: 0.3581
5376/6530 [=======================>......] - ETA: 0s - loss: 0.3367
6080/6530 [==========================>...] - ETA: 0s - loss: 0.3196
6530/6530 [==============================] - 1s 225us/step - loss: 0.3109 - val_loss: 0.1946

# training | RMSE: 0.3422, MAE: 0.2784
worker 2  xfile  [31, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37912495085969833}, 'layer_1_size': 59, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 46, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3013341732109067}, 'layer_3_size': 25, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45099737197957945}, 'layer_4_size': 11, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.37120010978460194}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.34221938523464707, 'rmse': 0.34221938523464707, 'mae': 0.27842457482413063, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  85 | activation: tanh    | extras: None 
layer 2 | size:  29 | activation: relu    | extras: dropout - rate: 45.3% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:48 - loss: 1.9592
 928/6530 [===>..........................] - ETA: 3s - loss: 0.4826  
1824/6530 [=======>......................] - ETA: 1s - loss: 0.3671
2816/6530 [===========>..................] - ETA: 0s - loss: 0.3155
3872/6530 [================>.............] - ETA: 0s - loss: 0.2850
5088/6530 [======================>.......] - ETA: 0s - loss: 0.2628
6240/6530 [===========================>..] - ETA: 0s - loss: 0.2484
6530/6530 [==============================] - 1s 145us/step - loss: 0.2455 - val_loss: 0.1877

# training | RMSE: 0.2638, MAE: 0.2159
worker 1  xfile  [33, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27567507773101646}, 'layer_1_size': 47, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3923161703172813}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2637933417625938, 'rmse': 0.2637933417625938, 'mae': 0.2159149415588516, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  57 | activation: relu    | extras: batchnorm 
layer 2 | size:  24 | activation: sigmoid | extras: None 
layer 3 | size:  55 | activation: tanh    | extras: dropout - rate: 30.4% 
layer 4 | size:  30 | activation: relu    | extras: None 
layer 5 | size:   8 | activation: tanh    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:08 - loss: 0.7962
 512/6530 [=>............................] - ETA: 8s - loss: 0.6041  
# training | RMSE: 0.2544, MAE: 0.2031
worker 0  xfile  [32, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22998319532744838}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2543814374962411, 'rmse': 0.2543814374962411, 'mae': 0.20307087373716662, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  24 | activation: tanh    | extras: dropout - rate: 12.3% 
layer 2 | size:  30 | activation: sigmoid | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 16s - loss: 1.5986
 992/6530 [===>..........................] - ETA: 4s - loss: 0.4413
1568/6530 [======>.......................] - ETA: 2s - loss: 0.3480
6530/6530 [==============================] - 1s 120us/step - loss: 1.2113 - val_loss: 0.9346

2176/6530 [========>.....................] - ETA: 1s - loss: 0.3023
2816/6530 [===========>..................] - ETA: 1s - loss: 0.2712
3424/6530 [==============>...............] - ETA: 0s - loss: 0.2511
4000/6530 [=================>............] - ETA: 0s - loss: 0.2392
4544/6530 [===================>..........] - ETA: 0s - loss: 0.2296
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2216
5728/6530 [=========================>....] - ETA: 0s - loss: 0.2153
6240/6530 [===========================>..] - ETA: 0s - loss: 0.2097
6530/6530 [==============================] - 1s 204us/step - loss: 0.2073 - val_loss: 0.1502

# training | RMSE: 0.2442, MAE: 0.1903
worker 2  xfile  [34, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4528981391423986}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2442260938583066, 'rmse': 0.2442260938583066, 'mae': 0.19034127012947782, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  83 | activation: tanh    | extras: batchnorm 
layer 2 | size:  29 | activation: sigmoid | extras: dropout - rate: 27.7% 
layer 3 | size:  24 | activation: sigmoid | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:13 - loss: 0.7902
1088/6530 [===>..........................] - ETA: 3s - loss: 0.3705  
2112/6530 [========>.....................] - ETA: 1s - loss: 0.2376
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1691
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1366
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1176
6530/6530 [==============================] - 1s 170us/step - loss: 0.1120 - val_loss: 0.0381

# training | RMSE: 1.0083, MAE: 0.9094
worker 0  xfile  [36, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12281921476282998}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26588728380855603}, 'layer_4_size': 88, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 1.0082865005381851, 'rmse': 1.0082865005381851, 'mae': 0.9094152931862048, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  73 | activation: tanh    | extras: batchnorm 
layer 2 | size:  79 | activation: relu    | extras: dropout - rate: 34.2% 
layer 3 | size:  46 | activation: sigmoid | extras: None 
layer 4 | size:  76 | activation: tanh    | extras: dropout - rate: 30.6% 
layer 5 | size:  35 | activation: sigmoid | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:33 - loss: 0.6699
 512/6530 [=>............................] - ETA: 9s - loss: 0.1356  
 992/6530 [===>..........................] - ETA: 4s - loss: 0.0963
1504/6530 [=====>........................] - ETA: 3s - loss: 0.0814
2048/6530 [========>.....................] - ETA: 2s - loss: 0.0719
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0654
3264/6530 [=============>................] - ETA: 1s - loss: 0.0620
# training | RMSE: 0.1910, MAE: 0.1473
worker 1  xfile  [35, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3037758680094471}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.19103737849627397, 'rmse': 0.19103737849627397, 'mae': 0.14729833645362836, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  53 | activation: sigmoid | extras: None 
layer 2 | size:  13 | activation: tanh    | extras: dropout - rate: 27.3% 
layer 3 | size:   3 | activation: sigmoid | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:02 - loss: 0.5443
3872/6530 [================>.............] - ETA: 0s - loss: 0.0591
1600/6530 [======>.......................] - ETA: 2s - loss: 0.4958  
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0564
3136/6530 [=============>................] - ETA: 0s - loss: 0.4294
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0548
4736/6530 [====================>.........] - ETA: 0s - loss: 0.3567
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0529
6272/6530 [===========================>..] - ETA: 0s - loss: 0.3012
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0511
6530/6530 [==============================] - 1s 138us/step - loss: 0.2937 - val_loss: 0.1013

6530/6530 [==============================] - 1s 226us/step - loss: 0.0503 - val_loss: 0.0335

# training | RMSE: 0.1937, MAE: 0.1568
worker 2  xfile  [37, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2765503361921329}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21266595991325876}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.19373277352908388, 'rmse': 0.19373277352908388, 'mae': 0.15678136986219904, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  59 | activation: relu    | extras: batchnorm 
layer 2 | size:  80 | activation: sigmoid | extras: None 
layer 3 | size:  95 | activation: sigmoid | extras: dropout - rate: 15.9% 
layer 4 | size:  30 | activation: sigmoid | extras: dropout - rate: 27.5% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:53 - loss: 0.4228
 512/6530 [=>............................] - ETA: 10s - loss: 0.2802 
 928/6530 [===>..........................] - ETA: 5s - loss: 0.2665 
1440/6530 [=====>........................] - ETA: 3s - loss: 0.2453
1856/6530 [=======>......................] - ETA: 2s - loss: 0.2314
2304/6530 [=========>....................] - ETA: 2s - loss: 0.2194
2752/6530 [===========>..................] - ETA: 1s - loss: 0.2131
3296/6530 [==============>...............] - ETA: 1s - loss: 0.2074
3712/6530 [================>.............] - ETA: 0s - loss: 0.2039
4128/6530 [=================>............] - ETA: 0s - loss: 0.2007
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1987
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1967
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1948
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1923
6368/6530 [============================>.] - ETA: 0s - loss: 0.1889
6530/6530 [==============================] - 2s 263us/step - loss: 0.1879 - val_loss: 0.1408

# training | RMSE: 0.1753, MAE: 0.1381
worker 0  xfile  [38, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 73, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3415595929356917}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3062275847875007}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.1753157197157378, 'rmse': 0.1753157197157378, 'mae': 0.13813036491817915, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  15 | activation: tanh    | extras: batchnorm 
layer 2 | size:  90 | activation: relu    | extras: None 
layer 3 | size:  49 | activation: sigmoid | extras: batchnorm 
layer 4 | size:   8 | activation: tanh    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:28 - loss: 0.7047
1088/6530 [===>..........................] - ETA: 4s - loss: 0.5375  
2176/6530 [========>.....................] - ETA: 1s - loss: 0.4621
3328/6530 [==============>...............] - ETA: 0s - loss: 0.4017
# training | RMSE: 0.3160, MAE: 0.2552
worker 1  xfile  [39, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27343847398789506}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.3159519474238808, 'rmse': 0.3159519474238808, 'mae': 0.25516729157581797, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:   3 | activation: relu    | extras: batchnorm 
layer 2 | size:  10 | activation: tanh    | extras: batchnorm 
layer 3 | size:   6 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  36 | activation: tanh    | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:36 - loss: 1.1431
4352/6530 [==================>...........] - ETA: 0s - loss: 0.3550
 448/6530 [=>............................] - ETA: 15s - loss: 0.9592 
5504/6530 [========================>.....] - ETA: 0s - loss: 0.3116
 832/6530 [==>...........................] - ETA: 8s - loss: 0.8033 
6464/6530 [============================>.] - ETA: 0s - loss: 0.2848
1216/6530 [====>.........................] - ETA: 5s - loss: 0.6964
1600/6530 [======>.......................] - ETA: 3s - loss: 0.6213
6530/6530 [==============================] - 1s 201us/step - loss: 0.2834 - val_loss: 0.2600

1952/6530 [=======>......................] - ETA: 3s - loss: 0.5686
2336/6530 [=========>....................] - ETA: 2s - loss: 0.5210
2688/6530 [===========>..................] - ETA: 2s - loss: 0.4882
3040/6530 [============>.................] - ETA: 1s - loss: 0.4647
3360/6530 [==============>...............] - ETA: 1s - loss: 0.4457
3712/6530 [================>.............] - ETA: 1s - loss: 0.4275
4064/6530 [=================>............] - ETA: 0s - loss: 0.4128
4480/6530 [===================>..........] - ETA: 0s - loss: 0.3979
4864/6530 [=====================>........] - ETA: 0s - loss: 0.3863
# training | RMSE: 0.1802, MAE: 0.1381
worker 2  xfile  [40, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 59, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15852722822896373}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27478419091375783}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.180235331294135, 'rmse': 0.180235331294135, 'mae': 0.13806467083935228, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  17 | activation: sigmoid | extras: dropout - rate: 49.0% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 15s - loss: 0.7125
5248/6530 [=======================>......] - ETA: 0s - loss: 0.3757
5632/6530 [========================>.....] - ETA: 0s - loss: 0.3659
6530/6530 [==============================] - 1s 113us/step - loss: 0.5223 - val_loss: 0.3468

6016/6530 [==========================>...] - ETA: 0s - loss: 0.3582
6368/6530 [============================>.] - ETA: 0s - loss: 0.3515
6530/6530 [==============================] - 2s 324us/step - loss: 0.3486 - val_loss: 0.2181

# training | RMSE: 0.3109, MAE: 0.2546
worker 0  xfile  [42, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11567062256385562}, 'layer_5_size': 14, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.3109327882976752, 'rmse': 0.3109327882976752, 'mae': 0.25462066702640845, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  90 | activation: sigmoid | extras: None 
layer 2 | size:  67 | activation: tanh    | extras: dropout - rate: 36.8% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:27 - loss: 0.8008
 480/6530 [=>............................] - ETA: 8s - loss: 0.2722  
 944/6530 [===>..........................] - ETA: 4s - loss: 0.2334
1408/6530 [=====>........................] - ETA: 2s - loss: 0.2171
1888/6530 [=======>......................] - ETA: 2s - loss: 0.2028
2352/6530 [=========>....................] - ETA: 1s - loss: 0.1977
2800/6530 [===========>..................] - ETA: 1s - loss: 0.1936
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1887
3872/6530 [================>.............] - ETA: 0s - loss: 0.1859
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1838
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1828
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1817
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1823
6432/6530 [============================>.] - ETA: 0s - loss: 0.1822
6530/6530 [==============================] - 1s 222us/step - loss: 0.1827 - val_loss: 0.2531

# training | RMSE: 0.2767, MAE: 0.2238
worker 1  xfile  [41, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 10, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 6, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12067248710353487}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.27669793463743686, 'rmse': 0.27669793463743686, 'mae': 0.22377471439099428, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  60 | activation: relu    | extras: dropout - rate: 41.7% 
layer 2 | size:  63 | activation: relu    | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 29s - loss: 0.7377
3712/6530 [================>.............] - ETA: 0s - loss: 0.4232 
6530/6530 [==============================] - 1s 117us/step - loss: 0.3310 - val_loss: 0.1788

# training | RMSE: 0.4101, MAE: 0.3394
worker 2  xfile  [43, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.48950206073258296}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22379305594810261}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3668652798941937}, 'layer_3_size': 82, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.4101313467400663, 'rmse': 0.4101313467400663, 'mae': 0.3393899675077224, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  64 | activation: tanh    | extras: batchnorm 
layer 2 | size:  31 | activation: sigmoid | extras: None 
layer 3 | size:  78 | activation: tanh    | extras: dropout - rate: 14.1% 
layer 4 | size:  44 | activation: relu    | extras: dropout - rate: 15.5% 
layer 5 | size:  63 | activation: relu    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:42 - loss: 0.7502
 288/6530 [>.............................] - ETA: 22s - loss: 0.3154 
 592/6530 [=>............................] - ETA: 10s - loss: 0.2583
 896/6530 [===>..........................] - ETA: 7s - loss: 0.2353 
1184/6530 [====>.........................] - ETA: 5s - loss: 0.2214
1440/6530 [=====>........................] - ETA: 4s - loss: 0.2130
1760/6530 [=======>......................] - ETA: 3s - loss: 0.2050
2080/6530 [========>.....................] - ETA: 2s - loss: 0.2003
2368/6530 [=========>....................] - ETA: 2s - loss: 0.1979
2640/6530 [===========>..................] - ETA: 2s - loss: 0.1977
2912/6530 [============>.................] - ETA: 1s - loss: 0.1952
3184/6530 [=============>................] - ETA: 1s - loss: 0.1923
3472/6530 [==============>...............] - ETA: 1s - loss: 0.1888
3792/6530 [================>.............] - ETA: 1s - loss: 0.1855
4112/6530 [=================>............] - ETA: 1s - loss: 0.1832
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1808
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1800
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1784
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1775
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1772
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1757
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1751
6448/6530 [============================>.] - ETA: 0s - loss: 0.1742
6530/6530 [==============================] - 2s 352us/step - loss: 0.1739 - val_loss: 0.1393

# training | RMSE: 0.2197, MAE: 0.1767
worker 1  xfile  [46, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.417044002812225}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20145156269732586}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4247982192376758}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.21968116208844302, 'rmse': 0.21968116208844302, 'mae': 0.17673271151690936, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  94 | activation: tanh    | extras: dropout - rate: 45.3% 
layer 2 | size:  22 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  84 | activation: tanh    | extras: None 
layer 4 | size:   8 | activation: sigmoid | extras: dropout - rate: 48.1% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:19 - loss: 0.3775
 256/6530 [>.............................] - ETA: 20s - loss: 0.4354 
 512/6530 [=>............................] - ETA: 10s - loss: 0.3677
 784/6530 [==>...........................] - ETA: 6s - loss: 0.3182 
1072/6530 [===>..........................] - ETA: 5s - loss: 0.2738
1376/6530 [=====>........................] - ETA: 3s - loss: 0.2292
1680/6530 [======>.......................] - ETA: 3s - loss: 0.1965
1984/6530 [========>.....................] - ETA: 2s - loss: 0.1744
# training | RMSE: 0.2936, MAE: 0.2517
worker 0  xfile  [44, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.36836965610871286}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2935959107965114, 'rmse': 0.2935959107965114, 'mae': 0.25166467855578467, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  43 | activation: tanh    | extras: None 
layer 2 | size:  98 | activation: tanh    | extras: None 
layer 3 | size:  57 | activation: tanh    | extras: batchnorm 
layer 4 | size:  14 | activation: sigmoid | extras: batchnorm 
layer 5 | size:  89 | activation: sigmoid | extras: dropout - rate: 14.0% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 52s - loss: 0.7394
2304/6530 [=========>....................] - ETA: 2s - loss: 0.1565
1920/6530 [=======>......................] - ETA: 2s - loss: 0.3597 
2624/6530 [===========>..................] - ETA: 1s - loss: 0.1442
3712/6530 [================>.............] - ETA: 0s - loss: 0.2123
2944/6530 [============>.................] - ETA: 1s - loss: 0.1338
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1612
3248/6530 [=============>................] - ETA: 1s - loss: 0.1254
3552/6530 [===============>..............] - ETA: 1s - loss: 0.1183
3856/6530 [================>.............] - ETA: 1s - loss: 0.1132
6530/6530 [==============================] - 1s 212us/step - loss: 0.1426 - val_loss: 0.0456

4128/6530 [=================>............] - ETA: 0s - loss: 0.1089
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1044
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1009
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0976
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0950
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0921
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0896
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0876
6432/6530 [============================>.] - ETA: 0s - loss: 0.0855
6530/6530 [==============================] - 2s 318us/step - loss: 0.0851 - val_loss: 0.0420

# training | RMSE: 0.1696, MAE: 0.1376
worker 2  xfile  [45, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.141270226702442}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15488674361554342}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.16959948056839336, 'rmse': 0.16959948056839336, 'mae': 0.1376389057694634, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  19 | activation: relu    | extras: dropout - rate: 36.0% 
layer 2 | size:  57 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  29 | activation: relu    | extras: dropout - rate: 30.9% 
layer 4 | size:  73 | activation: relu    | extras: None 
layer 5 | size:  80 | activation: relu    | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:07 - loss: 2.3023
 512/6530 [=>............................] - ETA: 11s - loss: 0.6231 
1024/6530 [===>..........................] - ETA: 5s - loss: 0.4899 
1568/6530 [======>.......................] - ETA: 3s - loss: 0.4241
2176/6530 [========>.....................] - ETA: 2s - loss: 0.3851
2784/6530 [===========>..................] - ETA: 1s - loss: 0.3611
3296/6530 [==============>...............] - ETA: 1s - loss: 0.3440
3840/6530 [================>.............] - ETA: 0s - loss: 0.3309
4416/6530 [===================>..........] - ETA: 0s - loss: 0.3219
4928/6530 [=====================>........] - ETA: 0s - loss: 0.3144
5408/6530 [=======================>......] - ETA: 0s - loss: 0.3065
5856/6530 [=========================>....] - ETA: 0s - loss: 0.3022
6336/6530 [============================>.] - ETA: 0s - loss: 0.2975
6530/6530 [==============================] - 2s 260us/step - loss: 0.2954 - val_loss: 0.2643

# training | RMSE: 0.2097, MAE: 0.1690
worker 0  xfile  [47, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 98, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13961529042578574}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20969891644359584, 'rmse': 0.20969891644359584, 'mae': 0.1690193139125977, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  91 | activation: relu    | extras: batchnorm 
layer 2 | size:  43 | activation: relu    | extras: dropout - rate: 15.9% 
layer 3 | size:  33 | activation: sigmoid | extras: dropout - rate: 14.4% 
layer 4 | size:  94 | activation: sigmoid | extras: dropout - rate: 15.5% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 27s - loss: 1.0942
3584/6530 [===============>..............] - ETA: 0s - loss: 0.5447 
6530/6530 [==============================] - 1s 202us/step - loss: 0.4267 - val_loss: 0.2091

# training | RMSE: 0.2039, MAE: 0.1640
worker 1  xfile  [48, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45310255735518523}, 'layer_1_size': 94, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 84, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.481322482298282}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20391820223005305, 'rmse': 0.20391820223005305, 'mae': 0.16400883792167562, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  46 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  68 | activation: tanh    | extras: dropout - rate: 41.9% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:23 - loss: 0.8065
1024/6530 [===>..........................] - ETA: 4s - loss: 0.6397  
1984/6530 [========>.....................] - ETA: 2s - loss: 0.5402
3008/6530 [============>.................] - ETA: 1s - loss: 0.4473
4160/6530 [==================>...........] - ETA: 0s - loss: 0.3848
5312/6530 [=======================>......] - ETA: 0s - loss: 0.3447
6464/6530 [============================>.] - ETA: 0s - loss: 0.3159
6530/6530 [==============================] - 1s 192us/step - loss: 0.3144 - val_loss: 0.1866

# training | RMSE: 0.2311, MAE: 0.1848
worker 1  xfile  [51, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41948648799435884}, 'layer_2_size': 68, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.44198961842359097}, 'layer_3_size': 4, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.23113546677643101, 'rmse': 0.23113546677643101, 'mae': 0.18475985942791553, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  36 | activation: relu    | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 16s - loss: 0.7321
# training | RMSE: 0.3108, MAE: 0.2593
worker 2  xfile  [49, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36004732452955024}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 57, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30882653488680895}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 73, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.3107786912930951, 'rmse': 0.3107786912930951, 'mae': 0.25927009927510586, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:   4 | activation: tanh    | extras: None 
layer 2 | size:  36 | activation: relu    | extras: batchnorm 
layer 3 | size:  53 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  65 | activation: tanh    | extras: dropout - rate: 13.4% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 7:36 - loss: 0.8337
6144/6530 [===========================>..] - ETA: 0s - loss: 0.6714 
 224/6530 [>.............................] - ETA: 32s - loss: 0.5564 
# training | RMSE: 0.2579, MAE: 0.2118
worker 0  xfile  [50, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15910655064483598}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1444125637290366}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15512839148967222}, 'layer_4_size': 94, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.414201397850555}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2579428318688393, 'rmse': 0.2579428318688393, 'mae': 0.2117959890610457, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  93 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  27 | activation: relu    | extras: None 
layer 3 | size:  73 | activation: relu    | extras: None 
layer 4 | size:  16 | activation: relu    | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:29 - loss: 0.9652
 432/6530 [>.............................] - ETA: 17s - loss: 0.4527
 416/6530 [>.............................] - ETA: 15s - loss: 0.4296 
6530/6530 [==============================] - 1s 132us/step - loss: 0.6689 - val_loss: 0.6240

 624/6530 [=>............................] - ETA: 12s - loss: 0.3937
 864/6530 [==>...........................] - ETA: 7s - loss: 0.3273 
 848/6530 [==>...........................] - ETA: 8s - loss: 0.3653 
1344/6530 [=====>........................] - ETA: 4s - loss: 0.2840
1088/6530 [===>..........................] - ETA: 6s - loss: 0.3442
1824/6530 [=======>......................] - ETA: 3s - loss: 0.2648
1344/6530 [=====>........................] - ETA: 5s - loss: 0.3288
2304/6530 [=========>....................] - ETA: 2s - loss: 0.2500
1584/6530 [======>.......................] - ETA: 4s - loss: 0.3151
2784/6530 [===========>..................] - ETA: 1s - loss: 0.2397
1808/6530 [=======>......................] - ETA: 4s - loss: 0.3076
3296/6530 [==============>...............] - ETA: 1s - loss: 0.2290
2064/6530 [========>.....................] - ETA: 3s - loss: 0.2988
3840/6530 [================>.............] - ETA: 1s - loss: 0.2202
2320/6530 [=========>....................] - ETA: 2s - loss: 0.2902
4384/6530 [===================>..........] - ETA: 0s - loss: 0.2144
2592/6530 [==========>...................] - ETA: 2s - loss: 0.2851
4896/6530 [=====================>........] - ETA: 0s - loss: 0.2088
2832/6530 [============>.................] - ETA: 2s - loss: 0.2810
5408/6530 [=======================>......] - ETA: 0s - loss: 0.2036
3088/6530 [=============>................] - ETA: 1s - loss: 0.2743
5920/6530 [==========================>...] - ETA: 0s - loss: 0.2000
3344/6530 [==============>...............] - ETA: 1s - loss: 0.2708
6368/6530 [============================>.] - ETA: 0s - loss: 0.1980
3568/6530 [===============>..............] - ETA: 1s - loss: 0.2669
3792/6530 [================>.............] - ETA: 1s - loss: 0.2641
4032/6530 [=================>............] - ETA: 1s - loss: 0.2613
6530/6530 [==============================] - 2s 286us/step - loss: 0.1972 - val_loss: 0.1737

4304/6530 [==================>...........] - ETA: 1s - loss: 0.2581
4576/6530 [====================>.........] - ETA: 0s - loss: 0.2554
4816/6530 [=====================>........] - ETA: 0s - loss: 0.2535
5056/6530 [======================>.......] - ETA: 0s - loss: 0.2519
5280/6530 [=======================>......] - ETA: 0s - loss: 0.2495
5488/6530 [========================>.....] - ETA: 0s - loss: 0.2470
5680/6530 [=========================>....] - ETA: 0s - loss: 0.2459
# training | RMSE: 0.6753, MAE: 0.6172
worker 1  xfile  [54, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3490893391698009}, 'layer_3_size': 36, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.6753201401192288, 'rmse': 0.6753201401192288, 'mae': 0.6171908303650028, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  37 | activation: relu    | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:12 - loss: 0.5913
5856/6530 [=========================>....] - ETA: 0s - loss: 0.2454
2048/6530 [========>.....................] - ETA: 1s - loss: 0.4809  
6064/6530 [==========================>...] - ETA: 0s - loss: 0.2445
4160/6530 [==================>...........] - ETA: 0s - loss: 0.3856
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2432
6208/6530 [===========================>..] - ETA: 0s - loss: 0.3046
6480/6530 [============================>.] - ETA: 0s - loss: 0.2414
6530/6530 [==============================] - 1s 153us/step - loss: 0.2931 - val_loss: 0.0707

6530/6530 [==============================] - 3s 428us/step - loss: 0.2413 - val_loss: 0.2674

# training | RMSE: 0.2121, MAE: 0.1705
worker 0  xfile  [53, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2120810121184814, 'rmse': 0.2120810121184814, 'mae': 0.17049012075196396, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:   8 | activation: sigmoid | extras: None 
layer 2 | size:  38 | activation: relu    | extras: dropout - rate: 12.2% 
layer 3 | size:  77 | activation: sigmoid | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:35 - loss: 0.8553
1472/6530 [=====>........................] - ETA: 3s - loss: 0.2473  
2816/6530 [===========>..................] - ETA: 1s - loss: 0.1693
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1366
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1190
6530/6530 [==============================] - 1s 199us/step - loss: 0.1140 - val_loss: 0.0688

# training | RMSE: 0.2705, MAE: 0.2250
worker 1  xfile  [55, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14066260202130243}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41190397279469126}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3040401023406488}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2704632257213804, 'rmse': 0.2704632257213804, 'mae': 0.22503025290120945, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  88 | activation: relu    | extras: batchnorm 
layer 2 | size:  34 | activation: sigmoid | extras: dropout - rate: 25.6% 
layer 3 | size:  40 | activation: relu    | extras: None 
layer 4 | size:  95 | activation: relu    | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:10 - loss: 0.4865
 208/6530 [..............................] - ETA: 29s - loss: 0.4952 
 384/6530 [>.............................] - ETA: 16s - loss: 0.3257
 592/6530 [=>............................] - ETA: 10s - loss: 0.2381
 816/6530 [==>...........................] - ETA: 7s - loss: 0.1862 
1072/6530 [===>..........................] - ETA: 5s - loss: 0.1518
1376/6530 [=====>........................] - ETA: 4s - loss: 0.1288
1664/6530 [======>.......................] - ETA: 3s - loss: 0.1131
1952/6530 [=======>......................] - ETA: 3s - loss: 0.1023
2256/6530 [=========>....................] - ETA: 2s - loss: 0.0933
2528/6530 [==========>...................] - ETA: 2s - loss: 0.0874
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0827
3056/6530 [=============>................] - ETA: 1s - loss: 0.0785
3312/6530 [==============>...............] - ETA: 1s - loss: 0.0748
3552/6530 [===============>..............] - ETA: 1s - loss: 0.0720
3824/6530 [================>.............] - ETA: 1s - loss: 0.0687
4112/6530 [=================>............] - ETA: 1s - loss: 0.0663
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0641
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0619
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0603
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0593
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0578
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0569
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0558
# training | RMSE: 0.3246, MAE: 0.2622
worker 2  xfile  [52, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1343834459157337}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.32463416582226196, 'rmse': 0.32463416582226196, 'mae': 0.26219289415643315, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  19 | activation: tanh    | extras: dropout - rate: 33.3% 
layer 2 | size:  90 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  39 | activation: tanh    | extras: batchnorm 
layer 4 | size:  46 | activation: relu    | extras: dropout - rate: 45.5% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 8:25 - loss: 1.3706
6432/6530 [============================>.] - ETA: 0s - loss: 0.0547
 240/6530 [>.............................] - ETA: 33s - loss: 1.1212 
 448/6530 [=>............................] - ETA: 18s - loss: 0.8141
 640/6530 [=>............................] - ETA: 12s - loss: 0.7072
 816/6530 [==>...........................] - ETA: 10s - loss: 0.6195
6530/6530 [==============================] - 2s 354us/step - loss: 0.0543 - val_loss: 0.0236

1008/6530 [===>..........................] - ETA: 8s - loss: 0.5745 
1216/6530 [====>.........................] - ETA: 6s - loss: 0.5242
1392/6530 [=====>........................] - ETA: 5s - loss: 0.4868
1568/6530 [======>.......................] - ETA: 5s - loss: 0.4557
1744/6530 [=======>......................] - ETA: 4s - loss: 0.4295
1920/6530 [=======>......................] - ETA: 4s - loss: 0.4087
2112/6530 [========>.....................] - ETA: 3s - loss: 0.3886
2320/6530 [=========>....................] - ETA: 3s - loss: 0.3719
2528/6530 [==========>...................] - ETA: 3s - loss: 0.3599
# training | RMSE: 0.2635, MAE: 0.2158
worker 0  xfile  [56, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 8, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12247234374565133}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 77, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36518771937381966}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2634821436838789, 'rmse': 0.2634821436838789, 'mae': 0.21584384724829317, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  64 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  46 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  24 | activation: relu    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:54 - loss: 1.3693
2736/6530 [===========>..................] - ETA: 2s - loss: 0.3473
1088/6530 [===>..........................] - ETA: 5s - loss: 0.2756  
2960/6530 [============>.................] - ETA: 2s - loss: 0.3318
2112/6530 [========>.....................] - ETA: 2s - loss: 0.1903
3184/6530 [=============>................] - ETA: 2s - loss: 0.3195
3200/6530 [=============>................] - ETA: 1s - loss: 0.1565
3376/6530 [==============>...............] - ETA: 1s - loss: 0.3078
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1388
3584/6530 [===============>..............] - ETA: 1s - loss: 0.2975
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1277
3776/6530 [================>.............] - ETA: 1s - loss: 0.2904
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1185
3968/6530 [=================>............] - ETA: 1s - loss: 0.2839
4176/6530 [==================>...........] - ETA: 1s - loss: 0.2766
4368/6530 [===================>..........] - ETA: 1s - loss: 0.2700
4560/6530 [===================>..........] - ETA: 1s - loss: 0.2637
6530/6530 [==============================] - 2s 249us/step - loss: 0.1151 - val_loss: 0.1111

4752/6530 [====================>.........] - ETA: 0s - loss: 0.2586
4944/6530 [=====================>........] - ETA: 0s - loss: 0.2534
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2486
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2454
5392/6530 [=======================>......] - ETA: 0s - loss: 0.2419
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2387
5696/6530 [=========================>....] - ETA: 0s - loss: 0.2353
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2313
6096/6530 [===========================>..] - ETA: 0s - loss: 0.2268
6304/6530 [===========================>..] - ETA: 0s - loss: 0.2226
6512/6530 [============================>.] - ETA: 0s - loss: 0.2189
6530/6530 [==============================] - 3s 491us/step - loss: 0.2185 - val_loss: 0.0837

# training | RMSE: 0.1459, MAE: 0.1108
worker 1  xfile  [57, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 88, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2564204672293974}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1475303709604141}, 'layer_5_size': 26, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.14594122454315286, 'rmse': 0.14594122454315286, 'mae': 0.1108003592824194, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  61 | activation: tanh    | extras: None 
layer 2 | size:  78 | activation: tanh    | extras: dropout - rate: 48.0% 
layer 3 | size:  14 | activation: sigmoid | extras: None 
layer 4 | size:  90 | activation: relu    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 41s - loss: 0.5630
2944/6530 [============>.................] - ETA: 1s - loss: 0.2269 
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1424
6530/6530 [==============================] - 1s 163us/step - loss: 0.1265 - val_loss: 0.0479

# training | RMSE: 0.2153, MAE: 0.1659
worker 1  xfile  [60, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.48005461741963973}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2854291176914812}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.21529017938792638, 'rmse': 0.21529017938792638, 'mae': 0.16592337791273654, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  55 | activation: tanh    | extras: dropout - rate: 26.6% 
layer 2 | size:  51 | activation: relu    | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:27 - loss: 0.6426
# training | RMSE: 0.3248, MAE: 0.2582
worker 0  xfile  [59, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1726832470496576}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.3248368013912588, 'rmse': 0.3248368013912588, 'mae': 0.25821976419889786, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  90 | activation: tanh    | extras: batchnorm 
layer 2 | size:  77 | activation: tanh    | extras: None 
layer 3 | size:  94 | activation: tanh    | extras: batchnorm 
layer 4 | size:  33 | activation: sigmoid | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 9:41 - loss: 0.4841
1088/6530 [===>..........................] - ETA: 4s - loss: 0.4033  
# training | RMSE: 0.2803, MAE: 0.2264
worker 2  xfile  [58, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3333100225035752}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45470889922489743}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.47054583251857096}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.28027294821113763, 'rmse': 0.28027294821113763, 'mae': 0.2264347500331414, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  66 | activation: sigmoid | extras: dropout - rate: 42.9% 
layer 2 | size:  53 | activation: tanh    | extras: dropout - rate: 21.4% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:25 - loss: 0.4567
 176/6530 [..............................] - ETA: 53s - loss: 0.2743 
2368/6530 [=========>....................] - ETA: 1s - loss: 0.3075
1536/6530 [======>.......................] - ETA: 2s - loss: 0.2498  
 368/6530 [>.............................] - ETA: 25s - loss: 0.2387
3712/6530 [================>.............] - ETA: 0s - loss: 0.2731
3072/6530 [=============>................] - ETA: 1s - loss: 0.1886
 592/6530 [=>............................] - ETA: 15s - loss: 0.2243
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2533
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1536
 816/6530 [==>...........................] - ETA: 11s - loss: 0.2144
1056/6530 [===>..........................] - ETA: 8s - loss: 0.2080 
1280/6530 [====>.........................] - ETA: 7s - loss: 0.2012
6530/6530 [==============================] - 1s 190us/step - loss: 0.2399 - val_loss: 0.1726

1488/6530 [=====>........................] - ETA: 6s - loss: 0.1965
6530/6530 [==============================] - 1s 179us/step - loss: 0.1337 - val_loss: 0.0707

1728/6530 [======>.......................] - ETA: 5s - loss: 0.1928
1952/6530 [=======>......................] - ETA: 4s - loss: 0.1902
2160/6530 [========>.....................] - ETA: 3s - loss: 0.1872
2384/6530 [=========>....................] - ETA: 3s - loss: 0.1854
2592/6530 [==========>...................] - ETA: 3s - loss: 0.1840
2816/6530 [===========>..................] - ETA: 2s - loss: 0.1807
3056/6530 [=============>................] - ETA: 2s - loss: 0.1779
3328/6530 [==============>...............] - ETA: 2s - loss: 0.1774
3600/6530 [===============>..............] - ETA: 1s - loss: 0.1767
3872/6530 [================>.............] - ETA: 1s - loss: 0.1752
4128/6530 [=================>............] - ETA: 1s - loss: 0.1741
4384/6530 [===================>..........] - ETA: 1s - loss: 0.1730
4624/6530 [====================>.........] - ETA: 1s - loss: 0.1725
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1716
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1702
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1689
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1683
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1669
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1664
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1654
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1647
6368/6530 [============================>.] - ETA: 0s - loss: 0.1638
6530/6530 [==============================] - 3s 484us/step - loss: 0.1632 - val_loss: 0.1207

# training | RMSE: 0.2099, MAE: 0.1719
worker 1  xfile  [62, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2659078945767359}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.20991413542883217, 'rmse': 0.20991413542883217, 'mae': 0.17189798455886315, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  63 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  13 | activation: sigmoid | extras: dropout - rate: 31.1% 
layer 3 | size:  79 | activation: tanh    | extras: dropout - rate: 11.4% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:33 - loss: 0.6487
1216/6530 [====>.........................] - ETA: 4s - loss: 0.2384  
2432/6530 [==========>...................] - ETA: 1s - loss: 0.2106
3840/6530 [================>.............] - ETA: 0s - loss: 0.2001
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1957
6400/6530 [============================>.] - ETA: 0s - loss: 0.1921
6530/6530 [==============================] - 1s 204us/step - loss: 0.1915 - val_loss: 0.1716

# training | RMSE: 0.2663, MAE: 0.2177
worker 2  xfile  [63, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4292273429449166}, 'layer_1_size': 66, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2139108718857471}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35606983238878764}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23159184883153522}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.26629792460083224, 'rmse': 0.26629792460083224, 'mae': 0.21765917425285916, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  76 | activation: tanh    | extras: None 
layer 2 | size:  27 | activation: sigmoid | extras: dropout - rate: 45.7% 
layer 3 | size:  37 | activation: relu    | extras: batchnorm 
layer 4 | size:  13 | activation: relu    | extras: None 
layer 5 | size:  34 | activation: tanh    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 4:14 - loss: 0.9862
 544/6530 [=>............................] - ETA: 14s - loss: 0.7105 
1024/6530 [===>..........................] - ETA: 7s - loss: 0.5816 
1536/6530 [======>.......................] - ETA: 4s - loss: 0.4705
2048/6530 [========>.....................] - ETA: 3s - loss: 0.4044
2528/6530 [==========>...................] - ETA: 2s - loss: 0.3565
3008/6530 [============>.................] - ETA: 1s - loss: 0.3217
3552/6530 [===============>..............] - ETA: 1s - loss: 0.2900
4032/6530 [=================>............] - ETA: 1s - loss: 0.2711
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2545
4960/6530 [=====================>........] - ETA: 0s - loss: 0.2408
5440/6530 [=======================>......] - ETA: 0s - loss: 0.2278
5824/6530 [=========================>....] - ETA: 0s - loss: 0.2191
6176/6530 [===========================>..] - ETA: 0s - loss: 0.2124
6530/6530 [==============================] - 2s 328us/step - loss: 0.2061 - val_loss: 0.0602

# training | RMSE: 0.2226, MAE: 0.1691
worker 1  xfile  [64, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.31074013935492373}, 'layer_2_size': 13, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11434659060086627}, 'layer_3_size': 79, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23000509299984168}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2225711948715962, 'rmse': 0.2225711948715962, 'mae': 0.16914670656004113, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  43 | activation: tanh    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:30 - loss: 0.6319
 432/6530 [>.............................] - ETA: 12s - loss: 0.6062 
# training | RMSE: 0.1476, MAE: 0.1174
worker 0  xfile  [61, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 94, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.14762964730500439, 'rmse': 0.14762964730500439, 'mae': 0.11742299351203137, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  29 | activation: tanh    | extras: batchnorm 
layer 2 | size:  84 | activation: tanh    | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 26s - loss: 0.5580
 848/6530 [==>...........................] - ETA: 6s - loss: 0.4827 
4864/6530 [=====================>........] - ETA: 0s - loss: 0.4502 
1344/6530 [=====>........................] - ETA: 3s - loss: 0.3743
1888/6530 [=======>......................] - ETA: 2s - loss: 0.3140
2368/6530 [=========>....................] - ETA: 1s - loss: 0.2839
6530/6530 [==============================] - 1s 199us/step - loss: 0.4083 - val_loss: 0.2198

2848/6530 [============>.................] - ETA: 1s - loss: 0.2650
3344/6530 [==============>...............] - ETA: 1s - loss: 0.2493
3888/6530 [================>.............] - ETA: 0s - loss: 0.2373
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2275
5088/6530 [======================>.......] - ETA: 0s - loss: 0.2208
5664/6530 [=========================>....] - ETA: 0s - loss: 0.2145
6224/6530 [===========================>..] - ETA: 0s - loss: 0.2100
6530/6530 [==============================] - 2s 243us/step - loss: 0.2075 - val_loss: 0.1625

# training | RMSE: 0.2430, MAE: 0.1981
worker 2  xfile  [65, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4566068568723328}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.24296525150287773, 'rmse': 0.24296525150287773, 'mae': 0.19810667837575133, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  14 | activation: sigmoid | extras: dropout - rate: 39.9% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 22s - loss: 0.4622
6530/6530 [==============================] - 1s 167us/step - loss: 0.3574 - val_loss: 0.2638

# training | RMSE: 0.4720, MAE: 0.4242
worker 0  xfile  [66, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 84, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3432776851517856}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.4719869749699302, 'rmse': 0.4719869749699302, 'mae': 0.42419478387881177, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  98 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  92 | activation: tanh    | extras: None 
layer 3 | size:  43 | activation: sigmoid | extras: dropout - rate: 40.6% 
layer 4 | size:  66 | activation: sigmoid | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 27s - loss: 0.8284
3328/6530 [==============>...............] - ETA: 1s - loss: 0.2685 
6400/6530 [============================>.] - ETA: 0s - loss: 0.2304
6530/6530 [==============================] - 1s 220us/step - loss: 0.2290 - val_loss: 0.1499

# training | RMSE: 0.2048, MAE: 0.1599
worker 1  xfile  [67, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.185800721236857}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22896973214221594}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.20478534527040634, 'rmse': 0.20478534527040634, 'mae': 0.1598698618862215, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  62 | activation: tanh    | extras: batchnorm 
layer 2 | size:  95 | activation: tanh    | extras: batchnorm 
layer 3 | size:  61 | activation: tanh    | extras: None 
layer 4 | size:  54 | activation: sigmoid | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:56 - loss: 1.0525
 896/6530 [===>..........................] - ETA: 7s - loss: 0.3686  
1664/6530 [======>.......................] - ETA: 3s - loss: 0.2802
# training | RMSE: 0.4963, MAE: 0.4114
worker 2  xfile  [68, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3985166197993746}, 'layer_1_size': 14, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13558220511915478}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 99, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.496260189060283, 'rmse': 0.496260189060283, 'mae': 0.41139444396456676, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  64 | activation: sigmoid | extras: dropout - rate: 22.7% 
layer 2 | size:  17 | activation: sigmoid | extras: dropout - rate: 36.3% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 58s - loss: 2.0756
2560/6530 [==========>...................] - ETA: 2s - loss: 0.2377
3200/6530 [=============>................] - ETA: 1s - loss: 0.7073 
3584/6530 [===============>..............] - ETA: 1s - loss: 0.2159
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1999
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1884
6530/6530 [==============================] - 1s 214us/step - loss: 0.3942 - val_loss: 0.0727

6530/6530 [==============================] - 2s 254us/step - loss: 0.1815 - val_loss: 0.2356

# training | RMSE: 0.2728, MAE: 0.2236
worker 2  xfile  [71, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22686791878390122}, 'layer_1_size': 64, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3627590148563128}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.27284003470117973, 'rmse': 0.27284003470117973, 'mae': 0.22364721821195194, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  16 | activation: tanh    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:16 - loss: 0.9192
1184/6530 [====>.........................] - ETA: 4s - loss: 0.9380  
2336/6530 [=========>....................] - ETA: 1s - loss: 0.8275
3424/6530 [==============>...............] - ETA: 1s - loss: 0.7425
4480/6530 [===================>..........] - ETA: 0s - loss: 0.6686
5600/6530 [========================>.....] - ETA: 0s - loss: 0.5928
# training | RMSE: 0.1880, MAE: 0.1468
worker 0  xfile  [69, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 92, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.40646090272414603}, 'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 58, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1880249668405121, 'rmse': 0.1880249668405121, 'mae': 0.14675030698725167, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  25 | activation: relu    | extras: batchnorm 
layer 2 | size:  68 | activation: sigmoid | extras: dropout - rate: 14.5% 
layer 3 | size:  96 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  92 | activation: relu    | extras: batchnorm 
layer 5 | size:  62 | activation: sigmoid | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 10:03 - loss: 0.5628
 160/6530 [..............................] - ETA: 1:01 - loss: 0.3819 
6530/6530 [==============================] - 1s 215us/step - loss: 0.5428 - val_loss: 0.2208

 320/6530 [>.............................] - ETA: 30s - loss: 0.3247 
 480/6530 [=>............................] - ETA: 20s - loss: 0.2965
 640/6530 [=>............................] - ETA: 15s - loss: 0.2824
 816/6530 [==>...........................] - ETA: 12s - loss: 0.2685
 992/6530 [===>..........................] - ETA: 10s - loss: 0.2588
1152/6530 [====>.........................] - ETA: 8s - loss: 0.2510 
1312/6530 [=====>........................] - ETA: 7s - loss: 0.2462
1488/6530 [=====>........................] - ETA: 6s - loss: 0.2399
1680/6530 [======>.......................] - ETA: 5s - loss: 0.2350
1888/6530 [=======>......................] - ETA: 5s - loss: 0.2322
2064/6530 [========>.....................] - ETA: 4s - loss: 0.2293
2256/6530 [=========>....................] - ETA: 4s - loss: 0.2271
2448/6530 [==========>...................] - ETA: 3s - loss: 0.2244
# training | RMSE: 0.3005, MAE: 0.2431
worker 1  xfile  [70, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 61, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.30051245143428473, 'rmse': 0.30051245143428473, 'mae': 0.24314916655702776, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  54 | activation: relu    | extras: dropout - rate: 23.0% 
layer 2 | size:  67 | activation: tanh    | extras: dropout - rate: 22.8% 
layer 3 | size:  78 | activation: relu    | extras: batchnorm 
layer 4 | size:  43 | activation: sigmoid | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 4:51 - loss: 0.8168
2640/6530 [===========>..................] - ETA: 3s - loss: 0.2237
 544/6530 [=>............................] - ETA: 16s - loss: 0.4535 
2816/6530 [===========>..................] - ETA: 3s - loss: 0.2217
1024/6530 [===>..........................] - ETA: 8s - loss: 0.2992 
2992/6530 [============>.................] - ETA: 2s - loss: 0.2194
1472/6530 [=====>........................] - ETA: 5s - loss: 0.2268
3184/6530 [=============>................] - ETA: 2s - loss: 0.2180
1920/6530 [=======>......................] - ETA: 3s - loss: 0.1876
3376/6530 [==============>...............] - ETA: 2s - loss: 0.2176
2400/6530 [==========>...................] - ETA: 2s - loss: 0.1607
3568/6530 [===============>..............] - ETA: 2s - loss: 0.2161
2848/6530 [============>.................] - ETA: 2s - loss: 0.1434
3728/6530 [================>.............] - ETA: 1s - loss: 0.2148
3264/6530 [=============>................] - ETA: 1s - loss: 0.1305
3904/6530 [================>.............] - ETA: 1s - loss: 0.2134
3680/6530 [===============>..............] - ETA: 1s - loss: 0.1210
4064/6530 [=================>............] - ETA: 1s - loss: 0.2126
4096/6530 [=================>............] - ETA: 1s - loss: 0.1131
4240/6530 [==================>...........] - ETA: 1s - loss: 0.2120
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1055
4432/6530 [===================>..........] - ETA: 1s - loss: 0.2110
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0993
4624/6530 [====================>.........] - ETA: 1s - loss: 0.2098
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0935
4816/6530 [=====================>........] - ETA: 1s - loss: 0.2081
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0895
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2072
6496/6530 [============================>.] - ETA: 0s - loss: 0.0860
5168/6530 [======================>.......] - ETA: 0s - loss: 0.2065
5344/6530 [=======================>......] - ETA: 0s - loss: 0.2060
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2051
5664/6530 [=========================>....] - ETA: 0s - loss: 0.2047
6530/6530 [==============================] - 2s 359us/step - loss: 0.0857 - val_loss: 0.0300

5856/6530 [=========================>....] - ETA: 0s - loss: 0.2040
6032/6530 [==========================>...] - ETA: 0s - loss: 0.2029
# training | RMSE: 0.2765, MAE: 0.2182
worker 2  xfile  [73, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2541417076702347}, 'layer_2_size': 45, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.13655975076858798}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20029354860657234}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2765450666336905, 'rmse': 0.2765450666336905, 'mae': 0.2181956313566634, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  82 | activation: sigmoid | extras: dropout - rate: 30.9% 
layer 2 | size:  13 | activation: sigmoid | extras: None 
layer 3 | size:  66 | activation: tanh    | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:37 - loss: 0.7361
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2027
1728/6530 [======>.......................] - ETA: 2s - loss: 0.3740  
6384/6530 [============================>.] - ETA: 0s - loss: 0.2021
3392/6530 [==============>...............] - ETA: 0s - loss: 0.2968
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2681
6528/6530 [============================>.] - ETA: 0s - loss: 0.2564
6530/6530 [==============================] - 1s 201us/step - loss: 0.2564 - val_loss: 0.2112

6530/6530 [==============================] - 4s 563us/step - loss: 0.2017 - val_loss: 0.1909

# training | RMSE: 0.1660, MAE: 0.1273
worker 1  xfile  [74, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2295313374218199}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22768606312641546}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1660141277151106, 'rmse': 0.1660141277151106, 'mae': 0.12730388944560697, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  38 | activation: sigmoid | extras: dropout - rate: 12.1% 
layer 2 | size:  85 | activation: relu    | extras: None 
layer 3 | size:  80 | activation: tanh    | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:42 - loss: 0.6808
1216/6530 [====>.........................] - ETA: 4s - loss: 0.3515  
2432/6530 [==========>...................] - ETA: 1s - loss: 0.2826
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2605
4608/6530 [====================>.........] - ETA: 0s - loss: 0.2482
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2418
6464/6530 [============================>.] - ETA: 0s - loss: 0.2349
6530/6530 [==============================] - 1s 229us/step - loss: 0.2351 - val_loss: 0.1919

# training | RMSE: 0.2607, MAE: 0.2137
worker 2  xfile  [75, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30941827120558196}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15433139618584396}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.26066088485598515, 'rmse': 0.26066088485598515, 'mae': 0.21372310645981651, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  45 | activation: tanh    | extras: batchnorm 
layer 2 | size:  24 | activation: relu    | extras: batchnorm 
layer 3 | size:  60 | activation: relu    | extras: batchnorm 
layer 4 | size:  96 | activation: relu    | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 11:21 - loss: 2.5491
 144/6530 [..............................] - ETA: 1:16 - loss: 1.9091 
 240/6530 [>.............................] - ETA: 46s - loss: 1.5388 
 336/6530 [>.............................] - ETA: 33s - loss: 1.2875
 432/6530 [>.............................] - ETA: 26s - loss: 1.1377
# training | RMSE: 0.2230, MAE: 0.1845
worker 0  xfile  [72, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14519842067262304}, 'layer_2_size': 68, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.22300206790857433, 'rmse': 0.22300206790857433, 'mae': 0.1844843259901606, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  45 | activation: sigmoid | extras: dropout - rate: 20.1% 
layer 2 | size:  17 | activation: tanh    | extras: batchnorm 
layer 3 | size:  75 | activation: tanh    | extras: batchnorm 
layer 4 | size:   6 | activation: sigmoid | extras: dropout - rate: 17.3% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 2:44 - loss: 0.6370
 560/6530 [=>............................] - ETA: 20s - loss: 1.0072
 832/6530 [==>...........................] - ETA: 11s - loss: 0.5756 
 704/6530 [==>...........................] - ETA: 16s - loss: 0.9130
1536/6530 [======>.......................] - ETA: 5s - loss: 0.5498 
 848/6530 [==>...........................] - ETA: 13s - loss: 0.8397
2304/6530 [=========>....................] - ETA: 3s - loss: 0.5088
1008/6530 [===>..........................] - ETA: 11s - loss: 0.7736
3072/6530 [=============>................] - ETA: 2s - loss: 0.4739
1168/6530 [====>.........................] - ETA: 9s - loss: 0.7255 
3840/6530 [================>.............] - ETA: 1s - loss: 0.4478
1344/6530 [=====>........................] - ETA: 8s - loss: 0.6845
4672/6530 [====================>.........] - ETA: 0s - loss: 0.4178
1504/6530 [=====>........................] - ETA: 7s - loss: 0.6571
5440/6530 [=======================>......] - ETA: 0s - loss: 0.3899
1680/6530 [======>.......................] - ETA: 6s - loss: 0.6297
6272/6530 [===========================>..] - ETA: 0s - loss: 0.3625
1824/6530 [=======>......................] - ETA: 6s - loss: 0.6090
1984/6530 [========>.....................] - ETA: 5s - loss: 0.5895
2128/6530 [========>.....................] - ETA: 5s - loss: 0.5738
2288/6530 [=========>....................] - ETA: 4s - loss: 0.5594
6530/6530 [==============================] - 2s 346us/step - loss: 0.3555 - val_loss: 0.1106

2448/6530 [==========>...................] - ETA: 4s - loss: 0.5453
2608/6530 [==========>...................] - ETA: 3s - loss: 0.5306
2768/6530 [===========>..................] - ETA: 3s - loss: 0.5178
2944/6530 [============>.................] - ETA: 3s - loss: 0.5047
3104/6530 [=============>................] - ETA: 3s - loss: 0.4940
3264/6530 [=============>................] - ETA: 2s - loss: 0.4847
3408/6530 [==============>...............] - ETA: 2s - loss: 0.4768
3552/6530 [===============>..............] - ETA: 2s - loss: 0.4703
3696/6530 [===============>..............] - ETA: 2s - loss: 0.4645
# training | RMSE: 0.2365, MAE: 0.1928
worker 1  xfile  [76, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12065696796480144}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 78, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.2365161466792876, 'rmse': 0.2365161466792876, 'mae': 0.19282922458388238, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  37 | activation: relu    | extras: batchnorm 
layer 2 | size:  53 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  28 | activation: sigmoid | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 2:11 - loss: 0.8795
3856/6530 [================>.............] - ETA: 2s - loss: 0.4577
 896/6530 [===>..........................] - ETA: 8s - loss: 0.7482  
4016/6530 [=================>............] - ETA: 1s - loss: 0.4512
1728/6530 [======>.......................] - ETA: 3s - loss: 0.6020
4192/6530 [==================>...........] - ETA: 1s - loss: 0.4444
2624/6530 [===========>..................] - ETA: 2s - loss: 0.4780
4368/6530 [===================>..........] - ETA: 1s - loss: 0.4386
3520/6530 [===============>..............] - ETA: 1s - loss: 0.3924
4512/6530 [===================>..........] - ETA: 1s - loss: 0.4333
4224/6530 [==================>...........] - ETA: 0s - loss: 0.3484
4672/6530 [====================>.........] - ETA: 1s - loss: 0.4276
4992/6530 [=====================>........] - ETA: 0s - loss: 0.3147
4832/6530 [=====================>........] - ETA: 1s - loss: 0.4237
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2883
4992/6530 [=====================>........] - ETA: 1s - loss: 0.4192
5152/6530 [======================>.......] - ETA: 0s - loss: 0.4140
5328/6530 [=======================>......] - ETA: 0s - loss: 0.4096
5504/6530 [========================>.....] - ETA: 0s - loss: 0.4045
6530/6530 [==============================] - 2s 288us/step - loss: 0.2687 - val_loss: 0.1478

5680/6530 [=========================>....] - ETA: 0s - loss: 0.4001
5856/6530 [=========================>....] - ETA: 0s - loss: 0.3955
6016/6530 [==========================>...] - ETA: 0s - loss: 0.3924
6176/6530 [===========================>..] - ETA: 0s - loss: 0.3888
6320/6530 [============================>.] - ETA: 0s - loss: 0.3856
6464/6530 [============================>.] - ETA: 0s - loss: 0.3829
# training | RMSE: 0.1882, MAE: 0.1470
worker 1  xfile  [79, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 37, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 53, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4318110609485434}, 'layer_4_size': 5, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.18823167687833256, 'rmse': 0.18823167687833256, 'mae': 0.14699113496420532, 'early_stop': False}
vggnet done  1

6530/6530 [==============================] - 4s 639us/step - loss: 0.3818 - val_loss: 0.2748

# training | RMSE: 0.3298, MAE: 0.2672
worker 0  xfile  [78, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20137771080882494}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 75, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17277741147530945}, 'layer_4_size': 6, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.3298014354210567, 'rmse': 0.3298014354210567, 'mae': 0.26723718881172237, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  71 | activation: relu    | extras: batchnorm 
layer 2 | size:   3 | activation: relu    | extras: dropout - rate: 40.3% 
layer 3 | size:  54 | activation: relu    | extras: dropout - rate: 39.1% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 9:03 - loss: 1.4555
 224/6530 [>.............................] - ETA: 39s - loss: 0.9637 
 448/6530 [=>............................] - ETA: 19s - loss: 0.8490
 672/6530 [==>...........................] - ETA: 13s - loss: 0.7455
 928/6530 [===>..........................] - ETA: 9s - loss: 0.6589 
# training | RMSE: 0.3353, MAE: 0.2643
worker 2  xfile  [77, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 24, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.33534713333683513, 'rmse': 0.33534713333683513, 'mae': 0.26429300853155946, 'early_stop': False}
vggnet done  2

1168/6530 [====>.........................] - ETA: 7s - loss: 0.5953
1472/6530 [=====>........................] - ETA: 5s - loss: 0.5378
1776/6530 [=======>......................] - ETA: 4s - loss: 0.4929
2064/6530 [========>.....................] - ETA: 3s - loss: 0.4573
2384/6530 [=========>....................] - ETA: 3s - loss: 0.4277
2688/6530 [===========>..................] - ETA: 2s - loss: 0.4041
2944/6530 [============>.................] - ETA: 2s - loss: 0.3866
3248/6530 [=============>................] - ETA: 1s - loss: 0.3705
3504/6530 [===============>..............] - ETA: 1s - loss: 0.3595
3760/6530 [================>.............] - ETA: 1s - loss: 0.3494
3984/6530 [=================>............] - ETA: 1s - loss: 0.3419
4192/6530 [==================>...........] - ETA: 1s - loss: 0.3349
4368/6530 [===================>..........] - ETA: 1s - loss: 0.3293
4512/6530 [===================>..........] - ETA: 1s - loss: 0.3264
4704/6530 [====================>.........] - ETA: 0s - loss: 0.3212
4896/6530 [=====================>........] - ETA: 0s - loss: 0.3169
5072/6530 [======================>.......] - ETA: 0s - loss: 0.3126
5232/6530 [=======================>......] - ETA: 0s - loss: 0.3097
5424/6530 [=======================>......] - ETA: 0s - loss: 0.3067
5600/6530 [========================>.....] - ETA: 0s - loss: 0.3029
5776/6530 [=========================>....] - ETA: 0s - loss: 0.2999
5936/6530 [==========================>...] - ETA: 0s - loss: 0.2970
6096/6530 [===========================>..] - ETA: 0s - loss: 0.2946
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2926
6448/6530 [============================>.] - ETA: 0s - loss: 0.2900
6530/6530 [==============================] - 3s 485us/step - loss: 0.2888 - val_loss: 0.1906

# training | RMSE: 0.2371, MAE: 0.1931
worker 0  xfile  [80, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4034406372087809}, 'layer_2_size': 3, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.39094249870900966}, 'layer_3_size': 54, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 43, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24622101941459051}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2371169876731363, 'rmse': 0.2371169876731363, 'mae': 0.19308613931965654, 'early_stop': False}
vggnet done  0
all of workers have been done
result is equal to None
calculation finished
#2 epoch=1.0 loss={'loss': 0.45433069745331084, 'rmse': 0.45433069745331084, 'mae': 0.3566936310324071, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#0 epoch=1.0 loss={'loss': 0.27387411146519486, 'rmse': 0.27387411146519486, 'mae': 0.21506428209384285, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24508931426614333}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 54, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#1 epoch=1.0 loss={'loss': 0.20561758402975297, 'rmse': 0.20561758402975297, 'mae': 0.16350082769938348, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4688037995543516}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#3 epoch=1.0 loss={'loss': 0.23290918479771722, 'rmse': 0.23290918479771722, 'mae': 0.1857776743985883, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1830453648773626}, 'layer_1_size': 89, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36110923197292344}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 9, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1782147798722572}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#4 epoch=1.0 loss={'loss': 0.21969950026056517, 'rmse': 0.21969950026056517, 'mae': 0.18072384169099978, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2090886416592586}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#5 epoch=1.0 loss={'loss': 0.20585709302544397, 'rmse': 0.20585709302544397, 'mae': 0.16542987848086604, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22759357904567784}, 'layer_2_size': 57, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40909023674512535}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#6 epoch=1.0 loss={'loss': 0.2578200870767448, 'rmse': 0.2578200870767448, 'mae': 0.2112159819868957, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3706207963655883}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4962179305960813}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20464899395687103}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#7 epoch=1.0 loss={'loss': 0.949085503471072, 'rmse': 0.949085503471072, 'mae': 0.8570311591795735, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35500839960594033}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4901778376958633}, 'layer_5_size': 79, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#9 epoch=1.0 loss={'loss': 0.19444634507967123, 'rmse': 0.19444634507967123, 'mae': 0.15512155739259711, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#11 epoch=1.0 loss={'loss': 0.26077592574063185, 'rmse': 0.26077592574063185, 'mae': 0.2136343914831388, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3835566305063546}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23192250650061058}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 18, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#8 epoch=1.0 loss={'loss': 0.1909963451123023, 'rmse': 0.1909963451123023, 'mae': 0.1502959830543466, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38664454623440137}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3089910751745466}, 'layer_3_size': 58, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#10 epoch=1.0 loss={'loss': 0.1919746888041542, 'rmse': 0.1919746888041542, 'mae': 0.15558377889969358, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4499329686752288}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 91, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#13 epoch=1.0 loss={'loss': 1.0217624080335945, 'rmse': 1.0217624080335945, 'mae': 0.9142615280815155, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20176026945598938}, 'layer_1_size': 6, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 16, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11281343145256294}, 'layer_4_size': 31, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14994890442267358}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#12 epoch=1.0 loss={'loss': 0.2222211860747114, 'rmse': 0.2222211860747114, 'mae': 0.18397093119156083, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22029350655847801}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20479056109862254}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#14 epoch=1.0 loss={'loss': 0.21351072176423466, 'rmse': 0.21351072176423466, 'mae': 0.17557865500180073, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38217105892249426}, 'layer_2_size': 6, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30502556147053894}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#17 epoch=1.0 loss={'loss': 0.17119929375756285, 'rmse': 0.17119929375756285, 'mae': 0.1327715363584132, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1302302520515125}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#15 epoch=1.0 loss={'loss': 0.24112061864512194, 'rmse': 0.24112061864512194, 'mae': 0.19197719169117355, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.147847915715405}, 'layer_4_size': 4, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#16 epoch=1.0 loss={'loss': 0.26940377375396285, 'rmse': 0.26940377375396285, 'mae': 0.2187831493624938, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1694124464538399}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46390761362282207}, 'layer_2_size': 99, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3161741413669089}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 12, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#18 epoch=1.0 loss={'loss': 0.23074099910571452, 'rmse': 0.23074099910571452, 'mae': 0.1843176113189077, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#19 epoch=1.0 loss={'loss': 0.2604627506378063, 'rmse': 0.2604627506378063, 'mae': 0.2131277219790262, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13811544797959066}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37009762923770795}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#20 epoch=1.0 loss={'loss': 0.22556560846911952, 'rmse': 0.22556560846911952, 'mae': 0.1755384187017281, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1379856625934365}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#23 epoch=1.0 loss={'loss': 0.20572730164821268, 'rmse': 0.20572730164821268, 'mae': 0.16338492578280436, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3541750557794504}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#21 epoch=1.0 loss={'loss': 0.2034654510549401, 'rmse': 0.2034654510549401, 'mae': 0.1618458420688361, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 39, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#22 epoch=1.0 loss={'loss': 0.36489891389570295, 'rmse': 0.36489891389570295, 'mae': 0.29869774767619084, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42750165664118256}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40234224544075115}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#24 epoch=1.0 loss={'loss': 0.22456140137291297, 'rmse': 0.22456140137291297, 'mae': 0.1721402673947596, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1121895844612825}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10065942982829018}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4642438706544345}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.461764616219107}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#25 epoch=1.0 loss={'loss': 0.3162097691032418, 'rmse': 0.3162097691032418, 'mae': 0.25650126072206186, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38392317341419036}, 'layer_1_size': 22, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 63, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2124436695855239}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
#26 epoch=1.0 loss={'loss': 0.24861559583080184, 'rmse': 0.24861559583080184, 'mae': 0.1988478443698782, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3634488551903946}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19465049393506934}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
#28 epoch=1.0 loss={'loss': 0.26161854527564565, 'rmse': 0.26161854527564565, 'mae': 0.21448844567244854, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10568616291913142}, 'layer_1_size': 71, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 98, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4282996641394471}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#29 epoch=1.0 loss={'loss': 0.17961634533654597, 'rmse': 0.17961634533654597, 'mae': 0.1430496255213611, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#27 epoch=1.0 loss={'loss': 0.19949572928557258, 'rmse': 0.19949572928557258, 'mae': 0.16180943177068846, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 56, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4720637156453483}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 22, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#32 epoch=1.0 loss={'loss': 0.40912709249982054, 'rmse': 0.40912709249982054, 'mae': 0.36616288798160707, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21905772316395972}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#30 epoch=1.0 loss={'loss': 0.20607960070134793, 'rmse': 0.20607960070134793, 'mae': 0.16857823547177542, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3002688216560363}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3995628521870124}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#31 epoch=1.0 loss={'loss': 0.2186529963665769, 'rmse': 0.2186529963665769, 'mae': 0.18000033289616155, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4599107245569436}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35545272342653067}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#33 epoch=1.0 loss={'loss': 0.2208821917541853, 'rmse': 0.2208821917541853, 'mae': 0.18211561591937103, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30225186523979475}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#36 epoch=1.0 loss={'loss': 0.27619233596833614, 'rmse': 0.27619233596833614, 'mae': 0.22264391000807218, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1896035259027695}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#34 epoch=1.0 loss={'loss': 0.2239501787605658, 'rmse': 0.2239501787605658, 'mae': 0.1838465328409698, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1611382916869748}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11824700906257526}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#35 epoch=1.0 loss={'loss': 0.18051610373449958, 'rmse': 0.18051610373449958, 'mae': 0.14203059368956775, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27147461663629957}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17359862039199497}, 'layer_4_size': 5, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1662718832327227}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#37 epoch=1.0 loss={'loss': 0.35393217494499135, 'rmse': 0.35393217494499135, 'mae': 0.3048353891689616, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3897060985256203}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1594077157195362}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#38 epoch=1.0 loss={'loss': 0.2636935583241373, 'rmse': 0.2636935583241373, 'mae': 0.21619528611876082, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13464396181250873}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#40 epoch=1.0 loss={'loss': 0.2568829692744277, 'rmse': 0.2568829692744277, 'mae': 0.21009799179161445, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1400293556848392}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#41 epoch=1.0 loss={'loss': 0.3300947462553907, 'rmse': 0.3300947462553907, 'mae': 0.2635282983290291, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14432071788110462}, 'layer_2_size': 44, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1877390820355882}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#39 epoch=1.0 loss={'loss': 0.2089536230764239, 'rmse': 0.2089536230764239, 'mae': 0.16189222572593726, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38547274004859367}, 'layer_2_size': 64, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23627536327943913}, 'layer_4_size': 39, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#43 epoch=1.0 loss={'loss': 0.22265978297811298, 'rmse': 0.22265978297811298, 'mae': 0.17660320548313363, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 89, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.19275846177406197}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22571997875034966}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#44 epoch=1.0 loss={'loss': 0.3961354766333239, 'rmse': 0.3961354766333239, 'mae': 0.31777668429719624, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12535511325541668}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24701655990175164}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#42 epoch=1.0 loss={'loss': 0.19775734818493118, 'rmse': 0.19775734818493118, 'mae': 0.15663902402952182, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.49815714481728435}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10719070900249368}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#45 epoch=1.0 loss={'loss': 0.20912731593057068, 'rmse': 0.20912731593057068, 'mae': 0.16269988557082302, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22493601036887703}, 'layer_2_size': 80, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 12, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.29306150019762184}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3819503917456204}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#46 epoch=1.0 loss={'loss': 0.27730545442607407, 'rmse': 0.27730545442607407, 'mae': 0.2344025305719126, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.26829488673006174}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12943210900540214}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 53, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.29719529485172025}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#47 epoch=1.0 loss={'loss': 0.26163937912677593, 'rmse': 0.26163937912677593, 'mae': 0.21443773372940597, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2685498972279782}, 'layer_1_size': 18, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3211616980545791}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#49 epoch=1.0 loss={'loss': 0.36182716144373767, 'rmse': 0.36182716144373767, 'mae': 0.3113284885310492, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4672537943168995}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#50 epoch=1.0 loss={'loss': 0.26314833637935375, 'rmse': 0.26314833637935375, 'mae': 0.21216221158082782, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 5, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 80, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#48 epoch=1.0 loss={'loss': 0.24096973998426338, 'rmse': 0.24096973998426338, 'mae': 0.19058701011776563, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 20, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3030003037194491}, 'layer_3_size': 19, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2656113327891144}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#51 epoch=1.0 loss={'loss': 0.20600833902811574, 'rmse': 0.20600833902811574, 'mae': 0.16429474930678592, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 16, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46901823491831196}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4790271524690606}, 'layer_3_size': 74, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#52 epoch=1.0 loss={'loss': 0.3241659066406219, 'rmse': 0.3241659066406219, 'mae': 0.258401360555338, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4757643375911187}, 'layer_4_size': 64, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#53 epoch=1.0 loss={'loss': 0.295241484219604, 'rmse': 0.295241484219604, 'mae': 0.23951361857756065, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#54 epoch=1.0 loss={'loss': 0.2483081832157355, 'rmse': 0.2483081832157355, 'mae': 0.19526265831485062, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#2 epoch=1.0 loss={'loss': 0.2981332846355835, 'rmse': 0.2981332846355835, 'mae': 0.24798093481462818, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3614654257953209}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 5, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#1 epoch=1.0 loss={'loss': 0.2078054773852269, 'rmse': 0.2078054773852269, 'mae': 0.16775571880550813, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.16279580265125976}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#0 epoch=1.0 loss={'loss': 0.5213052974043579, 'rmse': 0.5213052974043579, 'mae': 0.4428832164298356, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46137102250149586}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 35, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 48, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3724017857680866}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#5 epoch=1.0 loss={'loss': 0.24074956225177252, 'rmse': 0.24074956225177252, 'mae': 0.1938287190817625, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1007813314397716}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2853750582989068}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#4 epoch=1.0 loss={'loss': 0.2176223288376924, 'rmse': 0.2176223288376924, 'mae': 0.17271549730988808, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12046716908374014}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#6 epoch=1.0 loss={'loss': 0.2486649382756091, 'rmse': 0.2486649382756091, 'mae': 0.20345831830868583, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43864034354968184}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 43, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#3 epoch=1.0 loss={'loss': 0.31131487233761346, 'rmse': 0.31131487233761346, 'mae': 0.25692913404290013, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12937626152858106}, 'layer_3_size': 7, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.43753802427907695}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#7 epoch=1.0 loss={'loss': 0.39376694094175757, 'rmse': 0.39376694094175757, 'mae': 0.3111650419045698, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 87, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 26, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#9 epoch=1.0 loss={'loss': 0.2752707883807601, 'rmse': 0.2752707883807601, 'mae': 0.22606413814358106, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#11 epoch=1.0 loss={'loss': 0.20469795272258126, 'rmse': 0.20469795272258126, 'mae': 0.16675269212269514, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2168997762906385}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2612631090932722}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 52, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.19701135383320914}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#10 epoch=1.0 loss={'loss': 0.2395484056286396, 'rmse': 0.2395484056286396, 'mae': 0.1956619780884545, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 22, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#12 epoch=1.0 loss={'loss': 0.3261617381256271, 'rmse': 0.3261617381256271, 'mae': 0.26428197292005295, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 19, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 72, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36109801408773246}, 'layer_4_size': 41, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#8 epoch=1.0 loss={'loss': 0.13932264852392157, 'rmse': 0.13932264852392157, 'mae': 0.107408333921119, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.346233167432349}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32065776293655024}, 'layer_5_size': 90, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#14 epoch=1.0 loss={'loss': 0.26579671825271867, 'rmse': 0.26579671825271867, 'mae': 0.2170318496494003, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2072244149230852}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.42224742193516607}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30621154993011024}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#15 epoch=1.0 loss={'loss': 0.32722652157685683, 'rmse': 0.32722652157685683, 'mae': 0.26140270345156535, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4982396270218775}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#16 epoch=1.0 loss={'loss': 0.24428250698385456, 'rmse': 0.24428250698385456, 'mae': 0.20041748158895178, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 2, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3567934048743575}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.39305863317156176}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 23, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#17 epoch=1.0 loss={'loss': 0.2037554206580471, 'rmse': 0.2037554206580471, 'mae': 0.15978190911917906, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 59, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 36, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 48, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#13 epoch=1.0 loss={'loss': 0.13974185169502987, 'rmse': 0.13974185169502987, 'mae': 0.10990660656205996, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 21, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 21, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#18 epoch=1.0 loss={'loss': 0.3705673448022455, 'rmse': 0.3705673448022455, 'mae': 0.30383211105582847, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3896079512565712}, 'layer_1_size': 18, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 89, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28825810739112234}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#19 epoch=1.0 loss={'loss': 0.2676917397116646, 'rmse': 0.2676917397116646, 'mae': 0.2199846300958158, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35554818377183195}, 'layer_1_size': 33, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1356978201261766}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2200949158370239}, 'layer_4_size': 64, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#20 epoch=1.0 loss={'loss': 0.248641424948513, 'rmse': 0.248641424948513, 'mae': 0.2030236359262989, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.14949304504676364}, 'layer_1_size': 29, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23928116716847572}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3369672140250073}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#21 epoch=1.0 loss={'loss': 0.3176435891539007, 'rmse': 0.3176435891539007, 'mae': 0.25712204918131587, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21490099625368733}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 76, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#22 epoch=1.0 loss={'loss': 0.24108196079894773, 'rmse': 0.24108196079894773, 'mae': 0.19545963539816708, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2216825861213768}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 16, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.295266464493373}, 'layer_3_size': 21, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.398437043406736}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#23 epoch=1.0 loss={'loss': 0.20061122857558972, 'rmse': 0.20061122857558972, 'mae': 0.16575131399339893, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4781339033988239}, 'layer_3_size': 69, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 28, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42869787366229073}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#24 epoch=1.0 loss={'loss': 0.2776976623146451, 'rmse': 0.2776976623146451, 'mae': 0.2231434172367511, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24768611357533984}, 'layer_1_size': 40, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.42166682354193574}, 'layer_2_size': 92, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15642438405662276}, 'layer_3_size': 37, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#26 epoch=1.0 loss={'loss': 0.2380722077292145, 'rmse': 0.2380722077292145, 'mae': 0.194189389182616, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4373613018863446}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 57, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 7, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#25 epoch=1.0 loss={'loss': 0.20136367167878805, 'rmse': 0.20136367167878805, 'mae': 0.16433426940027476, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 21, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 35, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#27 epoch=1.0 loss={'loss': 0.2749783108340743, 'rmse': 0.2749783108340743, 'mae': 0.21343433310423243, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.26702796872659207}, 'layer_1_size': 99, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2977242385713669}, 'layer_2_size': 25, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4584800667065043}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#30 epoch=1.0 loss={'loss': 0.19834485731888304, 'rmse': 0.19834485731888304, 'mae': 0.15766272813020035, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 20, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4323655185229003}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#29 epoch=1.0 loss={'loss': 0.23279724061555593, 'rmse': 0.23279724061555593, 'mae': 0.19333303073546862, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3359869337584227}, 'layer_1_size': 43, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22757371472251747}, 'layer_4_size': 15, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#28 epoch=1.0 loss={'loss': 0.20544606392230783, 'rmse': 0.20544606392230783, 'mae': 0.16284022414953728, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 94, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#31 epoch=1.0 loss={'loss': 0.34221938523464707, 'rmse': 0.34221938523464707, 'mae': 0.27842457482413063, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37912495085969833}, 'layer_1_size': 59, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 46, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3013341732109067}, 'layer_3_size': 25, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45099737197957945}, 'layer_4_size': 11, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.37120010978460194}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#33 epoch=1.0 loss={'loss': 0.2637933417625938, 'rmse': 0.2637933417625938, 'mae': 0.2159149415588516, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27567507773101646}, 'layer_1_size': 47, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 61, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3923161703172813}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#32 epoch=1.0 loss={'loss': 0.2543814374962411, 'rmse': 0.2543814374962411, 'mae': 0.20307087373716662, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22998319532744838}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 43, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#34 epoch=1.0 loss={'loss': 0.2442260938583066, 'rmse': 0.2442260938583066, 'mae': 0.19034127012947782, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4528981391423986}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 85, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#36 epoch=1.0 loss={'loss': 1.0082865005381851, 'rmse': 1.0082865005381851, 'mae': 0.9094152931862048, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12281921476282998}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 30, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 25, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.26588728380855603}, 'layer_4_size': 88, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#35 epoch=1.0 loss={'loss': 0.19103737849627397, 'rmse': 0.19103737849627397, 'mae': 0.14729833645362836, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3037758680094471}, 'layer_3_size': 55, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#37 epoch=1.0 loss={'loss': 0.19373277352908388, 'rmse': 0.19373277352908388, 'mae': 0.15678136986219904, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2765503361921329}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21266595991325876}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#39 epoch=1.0 loss={'loss': 0.3159519474238808, 'rmse': 0.3159519474238808, 'mae': 0.25516729157581797, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.27343847398789506}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 81, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#38 epoch=1.0 loss={'loss': 0.1753157197157378, 'rmse': 0.1753157197157378, 'mae': 0.13813036491817915, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 73, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3415595929356917}, 'layer_2_size': 79, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3062275847875007}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#40 epoch=1.0 loss={'loss': 0.180235331294135, 'rmse': 0.180235331294135, 'mae': 0.13806467083935228, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 59, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.15852722822896373}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27478419091375783}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#42 epoch=1.0 loss={'loss': 0.3109327882976752, 'rmse': 0.3109327882976752, 'mae': 0.25462066702640845, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11567062256385562}, 'layer_5_size': 14, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#43 epoch=1.0 loss={'loss': 0.4101313467400663, 'rmse': 0.4101313467400663, 'mae': 0.3393899675077224, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.48950206073258296}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22379305594810261}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3668652798941937}, 'layer_3_size': 82, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#41 epoch=1.0 loss={'loss': 0.27669793463743686, 'rmse': 0.27669793463743686, 'mae': 0.22377471439099428, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 10, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 6, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12067248710353487}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#44 epoch=1.0 loss={'loss': 0.2935959107965114, 'rmse': 0.2935959107965114, 'mae': 0.25166467855578467, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.36836965610871286}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#46 epoch=1.0 loss={'loss': 0.21968116208844302, 'rmse': 0.21968116208844302, 'mae': 0.17673271151690936, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.417044002812225}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20145156269732586}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4247982192376758}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#45 epoch=1.0 loss={'loss': 0.16959948056839336, 'rmse': 0.16959948056839336, 'mae': 0.1376389057694634, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.141270226702442}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15488674361554342}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#47 epoch=1.0 loss={'loss': 0.20969891644359584, 'rmse': 0.20969891644359584, 'mae': 0.1690193139125977, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 43, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 98, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.13961529042578574}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#48 epoch=1.0 loss={'loss': 0.20391820223005305, 'rmse': 0.20391820223005305, 'mae': 0.16400883792167562, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45310255735518523}, 'layer_1_size': 94, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 84, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.481322482298282}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#49 epoch=1.0 loss={'loss': 0.3107786912930951, 'rmse': 0.3107786912930951, 'mae': 0.25927009927510586, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36004732452955024}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 57, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30882653488680895}, 'layer_3_size': 29, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 73, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#50 epoch=1.0 loss={'loss': 0.2579428318688393, 'rmse': 0.2579428318688393, 'mae': 0.2117959890610457, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 91, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15910655064483598}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1444125637290366}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15512839148967222}, 'layer_4_size': 94, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.414201397850555}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#51 epoch=1.0 loss={'loss': 0.23113546677643101, 'rmse': 0.23113546677643101, 'mae': 0.18475985942791553, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41948648799435884}, 'layer_2_size': 68, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.44198961842359097}, 'layer_3_size': 4, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#54 epoch=1.0 loss={'loss': 0.6753201401192288, 'rmse': 0.6753201401192288, 'mae': 0.6171908303650028, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3490893391698009}, 'layer_3_size': 36, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#53 epoch=1.0 loss={'loss': 0.2120810121184814, 'rmse': 0.2120810121184814, 'mae': 0.17049012075196396, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#55 epoch=1.0 loss={'loss': 0.2704632257213804, 'rmse': 0.2704632257213804, 'mae': 0.22503025290120945, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14066260202130243}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.41190397279469126}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3040401023406488}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#52 epoch=1.0 loss={'loss': 0.32463416582226196, 'rmse': 0.32463416582226196, 'mae': 0.26219289415643315, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 36, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1343834459157337}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#56 epoch=1.0 loss={'loss': 0.2634821436838789, 'rmse': 0.2634821436838789, 'mae': 0.21584384724829317, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 8, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12247234374565133}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 77, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36518771937381966}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#57 epoch=1.0 loss={'loss': 0.14594122454315286, 'rmse': 0.14594122454315286, 'mae': 0.1108003592824194, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 88, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2564204672293974}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 40, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 95, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1475303709604141}, 'layer_5_size': 26, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#59 epoch=1.0 loss={'loss': 0.3248368013912588, 'rmse': 0.3248368013912588, 'mae': 0.25821976419889786, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1726832470496576}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#60 epoch=1.0 loss={'loss': 0.21529017938792638, 'rmse': 0.21529017938792638, 'mae': 0.16592337791273654, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 61, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.48005461741963973}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2854291176914812}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#58 epoch=1.0 loss={'loss': 0.28027294821113763, 'rmse': 0.28027294821113763, 'mae': 0.2264347500331414, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3333100225035752}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45470889922489743}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.47054583251857096}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#62 epoch=1.0 loss={'loss': 0.20991413542883217, 'rmse': 0.20991413542883217, 'mae': 0.17189798455886315, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2659078945767359}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#63 epoch=1.0 loss={'loss': 0.26629792460083224, 'rmse': 0.26629792460083224, 'mae': 0.21765917425285916, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4292273429449166}, 'layer_1_size': 66, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2139108718857471}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35606983238878764}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23159184883153522}, 'layer_4_size': 69, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#61 epoch=1.0 loss={'loss': 0.14762964730500439, 'rmse': 0.14762964730500439, 'mae': 0.11742299351203137, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 90, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 94, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 29, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#64 epoch=1.0 loss={'loss': 0.2225711948715962, 'rmse': 0.2225711948715962, 'mae': 0.16914670656004113, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 63, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.31074013935492373}, 'layer_2_size': 13, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.11434659060086627}, 'layer_3_size': 79, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23000509299984168}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#65 epoch=1.0 loss={'loss': 0.24296525150287773, 'rmse': 0.24296525150287773, 'mae': 0.19810667837575133, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4566068568723328}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#66 epoch=1.0 loss={'loss': 0.4719869749699302, 'rmse': 0.4719869749699302, 'mae': 0.42419478387881177, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 84, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3432776851517856}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#67 epoch=1.0 loss={'loss': 0.20478534527040634, 'rmse': 0.20478534527040634, 'mae': 0.1598698618862215, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 43, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.185800721236857}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22896973214221594}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#68 epoch=1.0 loss={'loss': 0.496260189060283, 'rmse': 0.496260189060283, 'mae': 0.41139444396456676, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3985166197993746}, 'layer_1_size': 14, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13558220511915478}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 99, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 10, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#69 epoch=1.0 loss={'loss': 0.1880249668405121, 'rmse': 0.1880249668405121, 'mae': 0.14675030698725167, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 92, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.40646090272414603}, 'layer_3_size': 43, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 58, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#71 epoch=1.0 loss={'loss': 0.27284003470117973, 'rmse': 0.27284003470117973, 'mae': 0.22364721821195194, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22686791878390122}, 'layer_1_size': 64, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3627590148563128}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 95, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#70 epoch=1.0 loss={'loss': 0.30051245143428473, 'rmse': 0.30051245143428473, 'mae': 0.24314916655702776, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 61, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#73 epoch=1.0 loss={'loss': 0.2765450666336905, 'rmse': 0.2765450666336905, 'mae': 0.2181956313566634, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2541417076702347}, 'layer_2_size': 45, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.13655975076858798}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20029354860657234}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#74 epoch=1.0 loss={'loss': 0.1660141277151106, 'rmse': 0.1660141277151106, 'mae': 0.12730388944560697, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2295313374218199}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22768606312641546}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#75 epoch=1.0 loss={'loss': 0.26066088485598515, 'rmse': 0.26066088485598515, 'mae': 0.21372310645981651, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30941827120558196}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15433139618584396}, 'layer_4_size': 30, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#72 epoch=1.0 loss={'loss': 0.22300206790857433, 'rmse': 0.22300206790857433, 'mae': 0.1844843259901606, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14519842067262304}, 'layer_2_size': 68, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#76 epoch=1.0 loss={'loss': 0.2365161466792876, 'rmse': 0.2365161466792876, 'mae': 0.19282922458388238, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12065696796480144}, 'layer_1_size': 38, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 78, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
#78 epoch=1.0 loss={'loss': 0.3298014354210567, 'rmse': 0.3298014354210567, 'mae': 0.26723718881172237, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20137771080882494}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 75, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17277741147530945}, 'layer_4_size': 6, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#79 epoch=1.0 loss={'loss': 0.18823167687833256, 'rmse': 0.18823167687833256, 'mae': 0.14699113496420532, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 37, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 53, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 28, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4318110609485434}, 'layer_4_size': 5, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#77 epoch=1.0 loss={'loss': 0.33534713333683513, 'rmse': 0.33534713333683513, 'mae': 0.26429300853155946, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 24, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#80 epoch=1.0 loss={'loss': 0.2371169876731363, 'rmse': 0.2371169876731363, 'mae': 0.19308613931965654, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4034406372087809}, 'layer_2_size': 3, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.39094249870900966}, 'layer_3_size': 54, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 43, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24622101941459051}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
get a list [results] of length 136
get a list [loss] of length 136
get a list [val_loss] of length 136
length of indices is [ 67  72 112 118 128 101  15  94  28  95  36 124 133  10  91  11  92   8
  44  83  29  78  81  22  71 103  64 122  85   2  21   5  51  31  56  41
  45 102 116 108  14 114  59  32 100   4  33  13 119  42 130  35  24  20
  18 106  84   3 131 135  80  65  58  50  77  16 120  89  70  54  26  75
  60  88  39   6 105  19 129   9  27  47  49 111  38  87  68 117  74  17
 109 125   1  82  63  34 127  98  46  79 115  99  53  55 126 104  96  61
  93  25  76  52 110 113  66  69 132  40 134  86  37  48  23  73  62  43
  30  97   0 121 123  57 107   7  90  12]
length of indices is 136
length of T is 81
