loading data...
/media/adamsmith/Storage2/DataSet/catsdogs/orig_test1
[0, 1, 2]
s=4
T is of size 81
T=[{'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24508931426614333}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 54, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4688037995543516}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1830453648773626}, 'layer_1_size': 89, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36110923197292344}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 9, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1782147798722572}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2090886416592586}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22759357904567784}, 'layer_2_size': 57, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40909023674512535}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3706207963655883}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4962179305960813}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20464899395687103}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35500839960594033}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4901778376958633}, 'layer_5_size': 79, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38664454623440137}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3089910751745466}, 'layer_3_size': 58, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4499329686752288}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 91, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3835566305063546}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23192250650061058}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 18, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22029350655847801}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20479056109862254}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20176026945598938}, 'layer_1_size': 6, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 16, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11281343145256294}, 'layer_4_size': 31, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14994890442267358}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38217105892249426}, 'layer_2_size': 6, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30502556147053894}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.147847915715405}, 'layer_4_size': 4, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1694124464538399}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46390761362282207}, 'layer_2_size': 99, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3161741413669089}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 12, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1302302520515125}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13811544797959066}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37009762923770795}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1379856625934365}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 39, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42750165664118256}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40234224544075115}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3541750557794504}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1121895844612825}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10065942982829018}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4642438706544345}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.461764616219107}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38392317341419036}, 'layer_1_size': 22, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 63, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2124436695855239}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3634488551903946}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19465049393506934}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 56, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4720637156453483}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 22, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10568616291913142}, 'layer_1_size': 71, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 98, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4282996641394471}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3002688216560363}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3995628521870124}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4599107245569436}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35545272342653067}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21905772316395972}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30225186523979475}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1611382916869748}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11824700906257526}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27147461663629957}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17359862039199497}, 'layer_4_size': 5, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1662718832327227}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1896035259027695}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3897060985256203}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1594077157195362}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13464396181250873}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38547274004859367}, 'layer_2_size': 64, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23627536327943913}, 'layer_4_size': 39, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1400293556848392}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14432071788110462}, 'layer_2_size': 44, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1877390820355882}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.49815714481728435}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10719070900249368}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 89, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.19275846177406197}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22571997875034966}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12535511325541668}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24701655990175164}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22493601036887703}, 'layer_2_size': 80, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 12, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.29306150019762184}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3819503917456204}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.26829488673006174}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12943210900540214}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 53, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.29719529485172025}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2685498972279782}, 'layer_1_size': 18, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3211616980545791}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 20, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3030003037194491}, 'layer_3_size': 19, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2656113327891144}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4672537943168995}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 5, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 80, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 16, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46901823491831196}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4790271524690606}, 'layer_3_size': 74, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4757643375911187}, 'layer_4_size': 64, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4747820652699525}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18990815560638744}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.46535857208814924}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3785606602059729}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3619774967433266}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 78, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4641015318908026}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3499918539180671}, 'layer_2_size': 37, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 77, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3338500118517678}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21733077990734936}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 59, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4152912975829045}, 'layer_2_size': 52, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 56, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4164250803634669}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35456697442197815}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3456005888407692}, 'layer_1_size': 9, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37175830388274134}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22649751744615698}, 'layer_4_size': 48, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4902284534495991}, 'layer_2_size': 44, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35875723960368033}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42650755712741795}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18061994196690004}, 'layer_1_size': 74, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4529665353191762}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 37, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4601707524689007}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2907473276287246}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 94, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 57, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25266814823112443}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 23, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12242982296970335}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 82, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.418888951595746}, 'layer_4_size': 92, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23718605775698545}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2272457618830385}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10452094708784201}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.30862273470417034}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 45, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 57, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41506802225004635}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1357948231428067}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17180747868109397}, 'layer_3_size': 30, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12845280205534437}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31034787669558805}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2707532519413747}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1864451741191601}, 'layer_1_size': 48, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35581645225868186}, 'layer_4_size': 7, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]
newly formed T structure is:[[0, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24508931426614333}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 54, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [1, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4688037995543516}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [2, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [3, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1830453648773626}, 'layer_1_size': 89, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36110923197292344}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 9, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1782147798722572}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [4, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2090886416592586}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [5, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22759357904567784}, 'layer_2_size': 57, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40909023674512535}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [6, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3706207963655883}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4962179305960813}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20464899395687103}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [7, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35500839960594033}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4901778376958633}, 'layer_5_size': 79, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [8, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38664454623440137}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3089910751745466}, 'layer_3_size': 58, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [9, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [10, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4499329686752288}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 91, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [11, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3835566305063546}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23192250650061058}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 18, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [12, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22029350655847801}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20479056109862254}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [13, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20176026945598938}, 'layer_1_size': 6, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 16, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11281343145256294}, 'layer_4_size': 31, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14994890442267358}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [14, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38217105892249426}, 'layer_2_size': 6, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30502556147053894}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [15, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.147847915715405}, 'layer_4_size': 4, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [16, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1694124464538399}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46390761362282207}, 'layer_2_size': 99, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3161741413669089}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 12, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [17, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1302302520515125}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [18, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [19, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13811544797959066}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37009762923770795}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [20, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1379856625934365}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [21, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 39, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [22, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42750165664118256}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40234224544075115}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [23, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3541750557794504}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [24, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1121895844612825}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10065942982829018}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4642438706544345}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.461764616219107}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [25, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38392317341419036}, 'layer_1_size': 22, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 63, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2124436695855239}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [26, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3634488551903946}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19465049393506934}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [27, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 56, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4720637156453483}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 22, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [28, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10568616291913142}, 'layer_1_size': 71, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 98, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4282996641394471}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [29, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [30, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3002688216560363}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3995628521870124}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [31, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4599107245569436}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35545272342653067}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [32, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21905772316395972}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [33, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30225186523979475}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [34, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1611382916869748}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11824700906257526}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [35, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27147461663629957}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17359862039199497}, 'layer_4_size': 5, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1662718832327227}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [36, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1896035259027695}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [37, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3897060985256203}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1594077157195362}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [38, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13464396181250873}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [39, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38547274004859367}, 'layer_2_size': 64, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23627536327943913}, 'layer_4_size': 39, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [40, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1400293556848392}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [41, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14432071788110462}, 'layer_2_size': 44, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1877390820355882}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [42, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.49815714481728435}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10719070900249368}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [43, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 89, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.19275846177406197}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22571997875034966}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [44, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12535511325541668}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24701655990175164}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [45, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22493601036887703}, 'layer_2_size': 80, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 12, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.29306150019762184}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3819503917456204}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [46, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.26829488673006174}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12943210900540214}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 53, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.29719529485172025}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [47, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2685498972279782}, 'layer_1_size': 18, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3211616980545791}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [48, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 20, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3030003037194491}, 'layer_3_size': 19, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2656113327891144}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [49, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4672537943168995}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [50, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 5, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 80, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [51, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 16, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46901823491831196}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4790271524690606}, 'layer_3_size': 74, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [52, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4757643375911187}, 'layer_4_size': 64, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [53, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [54, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [55, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4747820652699525}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [56, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18990815560638744}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.46535857208814924}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [57, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3785606602059729}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3619774967433266}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 78, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [58, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4641015318908026}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3499918539180671}, 'layer_2_size': 37, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 77, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [59, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3338500118517678}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21733077990734936}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [60, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 59, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [61, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4152912975829045}, 'layer_2_size': 52, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 56, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4164250803634669}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35456697442197815}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [62, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3456005888407692}, 'layer_1_size': 9, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37175830388274134}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22649751744615698}, 'layer_4_size': 48, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [63, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4902284534495991}, 'layer_2_size': 44, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35875723960368033}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42650755712741795}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [64, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18061994196690004}, 'layer_1_size': 74, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4529665353191762}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 37, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [65, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4601707524689007}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2907473276287246}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [66, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 94, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 57, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25266814823112443}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [67, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [68, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [69, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [70, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 23, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [71, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12242982296970335}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 82, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [72, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.418888951595746}, 'layer_4_size': 92, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23718605775698545}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [73, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2272457618830385}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [74, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10452094708784201}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [75, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.30862273470417034}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [76, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 45, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 57, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41506802225004635}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [77, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [78, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1357948231428067}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17180747868109397}, 'layer_3_size': 30, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12845280205534437}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31034787669558805}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [79, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2707532519413747}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [80, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1864451741191601}, 'layer_1_size': 48, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35581645225868186}, 'layer_4_size': 7, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]] 

*** 81 configurations x 1.0 iterations each

1 | Thu Sep 27 17:39:50 2018 | lowest loss so far: inf (run -1)

{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  79 | activation: tanh    | extras: dropout - rate: 24.5% 
layer 2 | size:  60 | activation: sigmoid | extras: None 
layer 3 | size:  92 | activation: relu    | extras: None 
layer 4 | size:  54 | activation: tanh    | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:01 - loss: 0.5045
 400/6530 [>.............................] - ETA: 14s - loss: 0.2543 
 848/6530 [==>...........................] - ETA: 6s - loss: 0.2253 
1296/6530 [====>.........................] - ETA: 4s - loss: 0.2167{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  63 | activation: sigmoid | extras: None 
layer 2 | size:  74 | activation: tanh    | extras: None 
layer 3 | size:  17 | activation: tanh    | extras: batchnorm 
layer 4 | size:  54 | activation: relu    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 24s - loss: 0.5507
1744/6530 [=======>......................] - ETA: 2s - loss: 0.2088
5632/6530 [========================>.....] - ETA: 0s - loss: 0.3485 
2192/6530 [=========>....................] - ETA: 2s - loss: 0.2043
6530/6530 [==============================] - 1s 166us/step - loss: 0.3206 - val_loss: 0.2107

2624/6530 [===========>..................] - ETA: 1s - loss: 0.2015
3040/6530 [============>.................] - ETA: 1s - loss: 0.1978
3472/6530 [==============>...............] - ETA: 1s - loss: 0.1947
3952/6530 [=================>............] - ETA: 0s - loss: 0.1942
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1939
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1932
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1918{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  64 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  73 | activation: tanh    | extras: batchnorm 
layer 3 | size:  47 | activation: tanh    | extras: dropout - rate: 46.9% 
layer 4 | size:  46 | activation: relu    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:57 - loss: 1.6924
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1910
 272/6530 [>.............................] - ETA: 24s - loss: 1.0913 
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1901
 528/6530 [=>............................] - ETA: 12s - loss: 0.7189
6530/6530 [==============================] - 2s 257us/step - loss: 0.1893 - val_loss: 0.2167

 784/6530 [==>...........................] - ETA: 8s - loss: 0.5772 
1040/6530 [===>..........................] - ETA: 6s - loss: 0.4958
1296/6530 [====>.........................] - ETA: 5s - loss: 0.4457
1552/6530 [======>.......................] - ETA: 4s - loss: 0.4092
1808/6530 [=======>......................] - ETA: 3s - loss: 0.3845
2048/6530 [========>.....................] - ETA: 3s - loss: 0.3605
2288/6530 [=========>....................] - ETA: 2s - loss: 0.3403
# training | RMSE: 0.4543, MAE: 0.3567
worker 2  xfile  [2, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.4543286547741806, 'rmse': 0.4543286547741806, 'mae': 0.35669211230058157, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  89 | activation: relu    | extras: dropout - rate: 18.3% 
layer 2 | size:  65 | activation: tanh    | extras: batchnorm 
layer 3 | size:  17 | activation: tanh    | extras: dropout - rate: 36.1% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:14 - loss: 2.0586
2512/6530 [==========>...................] - ETA: 2s - loss: 0.3252
 352/6530 [>.............................] - ETA: 6s - loss: 0.9806  
2768/6530 [===========>..................] - ETA: 2s - loss: 0.3074
 704/6530 [==>...........................] - ETA: 3s - loss: 0.7236
3024/6530 [============>.................] - ETA: 1s - loss: 0.2955
1056/6530 [===>..........................] - ETA: 2s - loss: 0.5749
3296/6530 [==============>...............] - ETA: 1s - loss: 0.2815
1392/6530 [=====>........................] - ETA: 1s - loss: 0.4848
3520/6530 [===============>..............] - ETA: 1s - loss: 0.2719
1744/6530 [=======>......................] - ETA: 1s - loss: 0.4255
3776/6530 [================>.............] - ETA: 1s - loss: 0.2623
2112/6530 [========>.....................] - ETA: 1s - loss: 0.3789
4032/6530 [=================>............] - ETA: 1s - loss: 0.2540
2480/6530 [==========>...................] - ETA: 1s - loss: 0.3422
4272/6530 [==================>...........] - ETA: 1s - loss: 0.2468
2800/6530 [===========>..................] - ETA: 0s - loss: 0.3177
4544/6530 [===================>..........] - ETA: 0s - loss: 0.2397
# training | RMSE: 0.2739, MAE: 0.2151
worker 0  xfile  [0, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24508931426614333}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 54, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.27387409347056146, 'rmse': 0.27387409347056146, 'mae': 0.2150642661475466, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  89 | activation: tanh    | extras: dropout - rate: 20.9% 
layer 2 | size:  62 | activation: sigmoid | extras: None 
layer 3 | size:  73 | activation: relu    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 1:35 - loss: 0.6361
3152/6530 [=============>................] - ETA: 0s - loss: 0.2980
4784/6530 [====================>.........] - ETA: 0s - loss: 0.2337
 416/6530 [>.............................] - ETA: 4s - loss: 0.2268  
3504/6530 [===============>..............] - ETA: 0s - loss: 0.2779
5040/6530 [======================>.......] - ETA: 0s - loss: 0.2281
 784/6530 [==>...........................] - ETA: 2s - loss: 0.2082
3856/6530 [================>.............] - ETA: 0s - loss: 0.2633
5280/6530 [=======================>......] - ETA: 0s - loss: 0.2228
1168/6530 [====>.........................] - ETA: 1s - loss: 0.1983
4208/6530 [==================>...........] - ETA: 0s - loss: 0.2493
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2177
1568/6530 [======>.......................] - ETA: 1s - loss: 0.1922
4560/6530 [===================>..........] - ETA: 0s - loss: 0.2374
5792/6530 [=========================>....] - ETA: 0s - loss: 0.2131
1968/6530 [========>.....................] - ETA: 1s - loss: 0.1927
4912/6530 [=====================>........] - ETA: 0s - loss: 0.2272
6048/6530 [==========================>...] - ETA: 0s - loss: 0.2086
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1890
5264/6530 [=======================>......] - ETA: 0s - loss: 0.2172
6304/6530 [===========================>..] - ETA: 0s - loss: 0.2050
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1887
5616/6530 [========================>.....] - ETA: 0s - loss: 0.2082
3152/6530 [=============>................] - ETA: 0s - loss: 0.1847
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1998
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1850
6530/6530 [==============================] - 2s 373us/step - loss: 0.2017 - val_loss: 0.0438

6352/6530 [============================>.] - ETA: 0s - loss: 0.1929
3984/6530 [=================>............] - ETA: 0s - loss: 0.1841
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1836
6530/6530 [==============================] - 1s 206us/step - loss: 0.1896 - val_loss: 0.0576

4800/6530 [=====================>........] - ETA: 0s - loss: 0.1843
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1837
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1828
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1825
6464/6530 [============================>.] - ETA: 0s - loss: 0.1818
6530/6530 [==============================] - 1s 170us/step - loss: 0.1815 - val_loss: 0.1814

# training | RMSE: 0.2329, MAE: 0.1858
worker 2  xfile  [3, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1830453648773626}, 'layer_1_size': 89, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36110923197292344}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 9, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1782147798722572}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.23290934947965453, 'rmse': 0.23290934947965453, 'mae': 0.18577780718835804, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  27 | activation: relu    | extras: dropout - rate: 37.1% 
layer 2 | size:  77 | activation: relu    | extras: dropout - rate: 49.6% 
layer 3 | size:  48 | activation: sigmoid | extras: dropout - rate: 20.5% 
layer 4 | size:  13 | activation: sigmoid | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 26s - loss: 0.8034
1536/6530 [======>.......................] - ETA: 1s - loss: 0.6158 
3008/6530 [============>.................] - ETA: 0s - loss: 0.4580
4608/6530 [====================>.........] - ETA: 0s - loss: 0.3725
6336/6530 [============================>.] - ETA: 0s - loss: 0.3283
# training | RMSE: 0.2056, MAE: 0.1635
worker 1  xfile  [1, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4688037995543516}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20561757252853263, 'rmse': 0.20561757252853263, 'mae': 0.16350081531521782, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  31 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  57 | activation: relu    | extras: dropout - rate: 22.8% 
layer 3 | size:   4 | activation: sigmoid | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 9s - loss: 0.1653
6530/6530 [==============================] - 1s 80us/step - loss: 0.3247 - val_loss: 0.2091

5632/6530 [========================>.....] - ETA: 0s - loss: 0.0936
# training | RMSE: 0.2197, MAE: 0.1807
worker 0  xfile  [4, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2090886416592586}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.21969953191170108, 'rmse': 0.21969953191170108, 'mae': 0.18072387192689346, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  90 | activation: sigmoid | extras: None 
layer 2 | size:  93 | activation: sigmoid | extras: None 
layer 3 | size:  74 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  65 | activation: relu    | extras: dropout - rate: 35.5% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 16s - loss: 0.6286
6530/6530 [==============================] - 1s 77us/step - loss: 0.0890 - val_loss: 0.0429

2688/6530 [===========>..................] - ETA: 0s - loss: 0.3134 
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2429
6530/6530 [==============================] - 0s 75us/step - loss: 0.2300 - val_loss: 0.8486

# training | RMSE: 0.2059, MAE: 0.1654
worker 1  xfile  [5, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22759357904567784}, 'layer_2_size': 57, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40909023674512535}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.20585709256676668, 'rmse': 0.20585709256676668, 'mae': 0.16542987784648275, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  83 | activation: relu    | extras: batchnorm 
layer 2 | size:  29 | activation: tanh    | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 14s - loss: 0.7322
3200/6530 [=============>................] - ETA: 0s - loss: 0.3726 
6144/6530 [===========================>..] - ETA: 0s - loss: 0.2660
6530/6530 [==============================] - 0s 69us/step - loss: 0.2584 - val_loss: 0.2234

# training | RMSE: 0.9491, MAE: 0.8571
worker 0  xfile  [7, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35500839960594033}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4901778376958633}, 'layer_5_size': 79, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.9491090722125206, 'rmse': 0.9491090722125206, 'mae': 0.8570562131568096, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  87 | activation: tanh    | extras: dropout - rate: 45.0% 
layer 2 | size:   8 | activation: relu    | extras: None 
layer 3 | size:  33 | activation: tanh    | extras: None 
layer 4 | size:  91 | activation: sigmoid | extras: None 
layer 5 | size:  22 | activation: relu    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:08 - loss: 1.2537
 544/6530 [=>............................] - ETA: 4s - loss: 0.3591  
1184/6530 [====>.........................] - ETA: 1s - loss: 0.2220
1760/6530 [=======>......................] - ETA: 1s - loss: 0.1707
# training | RMSE: 0.2578, MAE: 0.2112
worker 2  xfile  [6, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3706207963655883}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4962179305960813}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20464899395687103}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.25782008734150774, 'rmse': 0.25782008734150774, 'mae': 0.2112159822516024, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  49 | activation: tanh    | extras: dropout - rate: 38.7% 
layer 2 | size:  90 | activation: tanh    | extras: batchnorm 
layer 3 | size:  58 | activation: relu    | extras: dropout - rate: 30.9% 
layer 4 | size:  23 | activation: sigmoid | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 51s - loss: 0.5467
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1438
1152/6530 [====>.........................] - ETA: 2s - loss: 0.3822 
2912/6530 [============>.................] - ETA: 0s - loss: 0.1247
2176/6530 [========>.....................] - ETA: 1s - loss: 0.2574
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1112
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1852
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0991
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1479
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0909
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1273
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0848
6432/6530 [============================>.] - ETA: 0s - loss: 0.0801
6530/6530 [==============================] - 1s 131us/step - loss: 0.1164 - val_loss: 0.0408

# training | RMSE: 0.2837, MAE: 0.2231
worker 1  xfile  [9, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.28367759506808204, 'rmse': 0.28367759506808204, 'mae': 0.2230702004208703, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  24 | activation: sigmoid | extras: dropout - rate: 38.4% 
layer 2 | size:  27 | activation: sigmoid | extras: dropout - rate: 23.2% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 5s - loss: 0.5801
6530/6530 [==============================] - 1s 138us/step - loss: 0.0793 - val_loss: 0.0390

6530/6530 [==============================] - 0s 45us/step - loss: 0.1273 - val_loss: 0.0669

# training | RMSE: 0.1974, MAE: 0.1584
worker 2  xfile  [8, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38664454623440137}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3089910751745466}, 'layer_3_size': 58, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.1974287844932267, 'rmse': 0.1974287844932267, 'mae': 0.1584050566620434, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  13 | activation: sigmoid | extras: None 
layer 2 | size:  94 | activation: relu    | extras: dropout - rate: 22.0% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 1:23 - loss: 0.8244
 656/6530 [==>...........................] - ETA: 2s - loss: 0.2124  
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1868
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1784
# training | RMSE: 0.2600, MAE: 0.2130
worker 1  xfile  [11, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3835566305063546}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23192250650061058}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 18, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.26000546178130773, 'rmse': 0.26000546178130773, 'mae': 0.21304755851384433, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:   6 | activation: sigmoid | extras: dropout - rate: 20.2% 
layer 2 | size:  16 | activation: relu    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 13s - loss: 1.7500
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1744
3968/6530 [=================>............] - ETA: 0s - loss: 1.4023 
3216/6530 [=============>................] - ETA: 0s - loss: 0.1686
# training | RMSE: 0.1920, MAE: 0.1556
worker 0  xfile  [10, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4499329686752288}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 91, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.1919746879913534, 'rmse': 0.1919746879913534, 'mae': 0.15558377740957746, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  63 | activation: tanh    | extras: None 
layer 2 | size:   6 | activation: tanh    | extras: dropout - rate: 38.2% 
layer 3 | size:  99 | activation: sigmoid | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:03 - loss: 0.0792
3872/6530 [================>.............] - ETA: 0s - loss: 0.1674
6530/6530 [==============================] - 0s 61us/step - loss: 1.1965 - val_loss: 0.5552

 928/6530 [===>..........................] - ETA: 2s - loss: 0.0691  
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1664
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0656
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1650
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0583
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1630
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0548
6320/6530 [============================>.] - ETA: 0s - loss: 0.1623
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0529
6530/6530 [==============================] - 1s 121us/step - loss: 0.1615 - val_loss: 0.1820

5248/6530 [=======================>......] - ETA: 0s - loss: 0.0511
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0501
6530/6530 [==============================] - 1s 115us/step - loss: 0.0498 - val_loss: 0.0461

# training | RMSE: 0.6818, MAE: 0.5681
worker 1  xfile  [13, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20176026945598938}, 'layer_1_size': 6, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 16, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11281343145256294}, 'layer_4_size': 31, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14994890442267358}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.6818443282152749, 'rmse': 0.6818443282152749, 'mae': 0.5680725337738203, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:   3 | activation: tanh    | extras: batchnorm 
layer 2 | size:  15 | activation: tanh    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:36 - loss: 1.0596
 304/6530 [>.............................] - ETA: 8s - loss: 0.7562  
# training | RMSE: 0.2135, MAE: 0.1756
worker 0  xfile  [14, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38217105892249426}, 'layer_2_size': 6, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30502556147053894}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.21351072125456003, 'rmse': 0.21351072125456003, 'mae': 0.1755786546001768, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  58 | activation: tanh    | extras: None 
layer 2 | size:   8 | activation: tanh    | extras: None 
layer 3 | size:  80 | activation: tanh    | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 1:34 - loss: 1.0736
 640/6530 [=>............................] - ETA: 4s - loss: 0.6278
 496/6530 [=>............................] - ETA: 3s - loss: 0.3367  
1024/6530 [===>..........................] - ETA: 2s - loss: 0.5185
1024/6530 [===>..........................] - ETA: 1s - loss: 0.2590
1424/6530 [=====>........................] - ETA: 2s - loss: 0.4417
1600/6530 [======>.......................] - ETA: 1s - loss: 0.2291
1808/6530 [=======>......................] - ETA: 1s - loss: 0.3874
2160/6530 [========>.....................] - ETA: 0s - loss: 0.2115
2208/6530 [=========>....................] - ETA: 1s - loss: 0.3389
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1998
2576/6530 [==========>...................] - ETA: 1s - loss: 0.3045
# training | RMSE: 0.2168, MAE: 0.1787
worker 2  xfile  [12, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22029350655847801}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20479056109862254}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.21679185928995987, 'rmse': 0.21679185928995987, 'mae': 0.17874792915653753, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  27 | activation: sigmoid | extras: dropout - rate: 16.9% 
layer 2 | size:  99 | activation: relu    | extras: dropout - rate: 46.4% 
layer 3 | size:  57 | activation: relu    | extras: dropout - rate: 31.6% 
layer 4 | size:  87 | activation: relu    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:46 - loss: 0.4938
3248/6530 [=============>................] - ETA: 0s - loss: 0.1911
2944/6530 [============>.................] - ETA: 0s - loss: 0.2772
 416/6530 [>.............................] - ETA: 6s - loss: 0.1799  
3776/6530 [================>.............] - ETA: 0s - loss: 0.1836
3328/6530 [==============>...............] - ETA: 0s - loss: 0.2553
 832/6530 [==>...........................] - ETA: 3s - loss: 0.1298
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1781
3728/6530 [================>.............] - ETA: 0s - loss: 0.2357
1232/6530 [====>.........................] - ETA: 2s - loss: 0.1150
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1728
4128/6530 [=================>............] - ETA: 0s - loss: 0.2200
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1684
1648/6530 [======>.......................] - ETA: 1s - loss: 0.1059
4528/6530 [===================>..........] - ETA: 0s - loss: 0.2070
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1649
2048/6530 [========>.....................] - ETA: 1s - loss: 0.1012
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1953
6496/6530 [============================>.] - ETA: 0s - loss: 0.1614
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0979
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1868
2848/6530 [============>.................] - ETA: 0s - loss: 0.0947
6530/6530 [==============================] - 1s 138us/step - loss: 0.1612 - val_loss: 0.1406

5664/6530 [=========================>....] - ETA: 0s - loss: 0.1780
3264/6530 [=============>................] - ETA: 0s - loss: 0.0925
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1701
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0908
6448/6530 [============================>.] - ETA: 0s - loss: 0.1634
4064/6530 [=================>............] - ETA: 0s - loss: 0.0897
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0880
6530/6530 [==============================] - 1s 206us/step - loss: 0.1621 - val_loss: 0.0564

4848/6530 [=====================>........] - ETA: 0s - loss: 0.0866
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0853
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0838
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0824
6480/6530 [============================>.] - ETA: 0s - loss: 0.0810
6530/6530 [==============================] - 1s 205us/step - loss: 0.0808 - val_loss: 0.0524

# training | RMSE: 0.1712, MAE: 0.1328
worker 0  xfile  [17, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1302302520515125}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.17119905505966707, 'rmse': 0.17119905505966707, 'mae': 0.1327713425132463, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  39 | activation: tanh    | extras: None 
layer 2 | size:  34 | activation: tanh    | extras: None 
layer 3 | size:  48 | activation: sigmoid | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 12s - loss: 1.7042
3712/6530 [================>.............] - ETA: 0s - loss: 0.1771 
6530/6530 [==============================] - 0s 57us/step - loss: 0.1169 - val_loss: 0.0567

# training | RMSE: 0.2355, MAE: 0.1899
worker 1  xfile  [15, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.147847915715405}, 'layer_4_size': 4, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.23545384583319995, 'rmse': 0.23545384583319995, 'mae': 0.18990396966307255, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  19 | activation: sigmoid | extras: dropout - rate: 13.8% 
layer 2 | size:  32 | activation: sigmoid | extras: None 
layer 3 | size:  79 | activation: relu    | extras: dropout - rate: 37.0% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 7s - loss: 0.7278
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2631
6530/6530 [==============================] - 0s 61us/step - loss: 0.2540 - val_loss: 0.2111

# training | RMSE: 0.2307, MAE: 0.1843
worker 0  xfile  [18, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.23074098849234811, 'rmse': 0.23074098849234811, 'mae': 0.18431760350321444, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  48 | activation: tanh    | extras: None 
layer 2 | size:  50 | activation: relu    | extras: None 
layer 3 | size:  39 | activation: sigmoid | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:17 - loss: 0.8323
 288/6530 [>.............................] - ETA: 8s - loss: 0.4642  
# training | RMSE: 0.2295, MAE: 0.1873
worker 2  xfile  [16, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1694124464538399}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46390761362282207}, 'layer_2_size': 99, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3161741413669089}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 12, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.22949216187804347, 'rmse': 0.22949216187804347, 'mae': 0.18726315030046395, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  77 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  29 | activation: sigmoid | extras: dropout - rate: 13.8% 
layer 3 | size:  68 | activation: tanh    | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 22s - loss: 0.4005
 576/6530 [=>............................] - ETA: 4s - loss: 0.2993
2048/6530 [========>.....................] - ETA: 1s - loss: 0.2415 
 896/6530 [===>..........................] - ETA: 3s - loss: 0.2196
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2256
1264/6530 [====>.........................] - ETA: 2s - loss: 0.1747
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1479
6530/6530 [==============================] - 1s 102us/step - loss: 0.2156 - val_loss: 0.1633

2112/6530 [========>.....................] - ETA: 1s - loss: 0.1325
2576/6530 [==========>...................] - ETA: 1s - loss: 0.1215
3024/6530 [============>.................] - ETA: 0s - loss: 0.1121
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1054
3888/6530 [================>.............] - ETA: 0s - loss: 0.1004
# training | RMSE: 0.2598, MAE: 0.2128
worker 1  xfile  [19, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13811544797959066}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37009762923770795}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.25975309956282994, 'rmse': 0.25975309956282994, 'mae': 0.21283009270751835, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  27 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  28 | activation: tanh    | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:30 - loss: 0.6958
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0963
 736/6530 [==>...........................] - ETA: 3s - loss: 0.6427  
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0919
1472/6530 [=====>........................] - ETA: 1s - loss: 0.5037
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0879
2208/6530 [=========>....................] - ETA: 1s - loss: 0.3983
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0844
2944/6530 [============>.................] - ETA: 0s - loss: 0.3414
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0812
3712/6530 [================>.............] - ETA: 0s - loss: 0.3063
6368/6530 [============================>.] - ETA: 0s - loss: 0.0787
4448/6530 [===================>..........] - ETA: 0s - loss: 0.2848
# training | RMSE: 0.2027, MAE: 0.1632
worker 2  xfile  [20, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1379856625934365}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.20270912087066512, 'rmse': 0.20270912087066512, 'mae': 0.16315214681022633, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  77 | activation: sigmoid | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 29s - loss: 1.5109
5216/6530 [======================>.......] - ETA: 0s - loss: 0.2686
6530/6530 [==============================] - 1s 190us/step - loss: 0.0776 - val_loss: 0.0428

2560/6530 [==========>...................] - ETA: 0s - loss: 0.5565 
5984/6530 [==========================>...] - ETA: 0s - loss: 0.2560
5184/6530 [======================>.......] - ETA: 0s - loss: 0.3760
6530/6530 [==============================] - 1s 147us/step - loss: 0.2486 - val_loss: 0.2870

6530/6530 [==============================] - 0s 70us/step - loss: 0.3341 - val_loss: 0.2110

# training | RMSE: 0.2035, MAE: 0.1618
worker 0  xfile  [21, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 39, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.20346544999046282, 'rmse': 0.20346544999046282, 'mae': 0.16184584204145264, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  32 | activation: relu    | extras: dropout - rate: 11.2% 
layer 2 | size:  72 | activation: tanh    | extras: dropout - rate: 10.1% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:58 - loss: 0.3834
 320/6530 [>.............................] - ETA: 9s - loss: 0.2839  
 720/6530 [==>...........................] - ETA: 4s - loss: 0.1757
1152/6530 [====>.........................] - ETA: 2s - loss: 0.1419
1600/6530 [======>.......................] - ETA: 1s - loss: 0.1239
1984/6530 [========>.....................] - ETA: 1s - loss: 0.1131
2320/6530 [=========>....................] - ETA: 1s - loss: 0.1058
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0984
3152/6530 [=============>................] - ETA: 0s - loss: 0.0924
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0884
3840/6530 [================>.............] - ETA: 0s - loss: 0.0845
# training | RMSE: 0.2473, MAE: 0.2058
worker 2  xfile  [23, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3541750557794504}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.24725493957239894, 'rmse': 0.24725493957239894, 'mae': 0.20583366576658968, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  22 | activation: sigmoid | extras: dropout - rate: 38.4% 
layer 2 | size:  63 | activation: tanh    | extras: batchnorm 
layer 3 | size:   7 | activation: relu    | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:06 - loss: 0.8319
# training | RMSE: 0.3531, MAE: 0.2872
worker 1  xfile  [22, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42750165664118256}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40234224544075115}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.3531300567975264, 'rmse': 0.3531300567975264, 'mae': 0.2871508458247739, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  81 | activation: tanh    | extras: None 
layer 2 | size:  60 | activation: relu    | extras: batchnorm 
layer 3 | size:  76 | activation: tanh    | extras: dropout - rate: 36.3% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:11 - loss: 0.8562
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0818
1024/6530 [===>..........................] - ETA: 3s - loss: 0.6809  
 512/6530 [=>............................] - ETA: 8s - loss: 0.5785  
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0786
2176/6530 [========>.....................] - ETA: 1s - loss: 0.5541
1056/6530 [===>..........................] - ETA: 3s - loss: 0.4661
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0756
3200/6530 [=============>................] - ETA: 0s - loss: 0.4893
1600/6530 [======>.......................] - ETA: 2s - loss: 0.4121
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0727
4288/6530 [==================>...........] - ETA: 0s - loss: 0.4499
2144/6530 [========>.....................] - ETA: 1s - loss: 0.3730
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0705
5440/6530 [=======================>......] - ETA: 0s - loss: 0.4199
2720/6530 [===========>..................] - ETA: 1s - loss: 0.3415
6528/6530 [============================>.] - ETA: 0s - loss: 0.0683
3296/6530 [==============>...............] - ETA: 0s - loss: 0.3200
6530/6530 [==============================] - 1s 202us/step - loss: 0.0683 - val_loss: 0.0514

3840/6530 [================>.............] - ETA: 0s - loss: 0.3040
6530/6530 [==============================] - 1s 160us/step - loss: 0.3992 - val_loss: 0.3291

4384/6530 [===================>..........] - ETA: 0s - loss: 0.2909
4928/6530 [=====================>........] - ETA: 0s - loss: 0.2799
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2705
6112/6530 [===========================>..] - ETA: 0s - loss: 0.2621
6530/6530 [==============================] - 1s 202us/step - loss: 0.2565 - val_loss: 0.4576

# training | RMSE: 0.2244, MAE: 0.1731
worker 0  xfile  [24, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1121895844612825}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10065942982829018}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4642438706544345}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.461764616219107}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2243559377916526, 'rmse': 0.2243559377916526, 'mae': 0.17309467787698474, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  71 | activation: sigmoid | extras: dropout - rate: 10.6% 
layer 2 | size:  34 | activation: sigmoid | extras: None 
layer 3 | size:  27 | activation: relu    | extras: None 
layer 4 | size:  98 | activation: relu    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 38s - loss: 0.7057
1472/6530 [=====>........................] - ETA: 1s - loss: 0.6102 
# training | RMSE: 0.3903, MAE: 0.3307
worker 2  xfile  [25, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38392317341419036}, 'layer_1_size': 22, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 63, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2124436695855239}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.3903116200201268, 'rmse': 0.3903116200201268, 'mae': 0.33068239551121204, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  56 | activation: relu    | extras: None 
layer 2 | size:  34 | activation: sigmoid | extras: dropout - rate: 47.2% 
layer 3 | size:  22 | activation: sigmoid | extras: None 
layer 4 | size:  82 | activation: sigmoid | extras: None 
layer 5 | size:  54 | activation: tanh    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:03 - loss: 0.1584
2752/6530 [===========>..................] - ETA: 0s - loss: 0.4558
 400/6530 [>.............................] - ETA: 7s - loss: 0.3705  
4224/6530 [==================>...........] - ETA: 0s - loss: 0.3716
 816/6530 [==>...........................] - ETA: 3s - loss: 0.2145
5824/6530 [=========================>....] - ETA: 0s - loss: 0.3287
1216/6530 [====>.........................] - ETA: 2s - loss: 0.1663
1632/6530 [======>.......................] - ETA: 1s - loss: 0.1402
6530/6530 [==============================] - 1s 103us/step - loss: 0.3160 - val_loss: 0.2119

2064/6530 [========>.....................] - ETA: 1s - loss: 0.1231
2496/6530 [==========>...................] - ETA: 1s - loss: 0.1119
2912/6530 [============>.................] - ETA: 1s - loss: 0.1033
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0979
3776/6530 [================>.............] - ETA: 0s - loss: 0.0921
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0882
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0849
# training | RMSE: 0.5711, MAE: 0.4697
worker 1  xfile  [26, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3634488551903946}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19465049393506934}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.5711244479712618, 'rmse': 0.5711244479712618, 'mae': 0.469717675532414, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  76 | activation: relu    | extras: batchnorm 
layer 2 | size:  47 | activation: sigmoid | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 26s - loss: 1.0441
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0820
3200/6530 [=============>................] - ETA: 0s - loss: 0.5032 
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0794
6400/6530 [============================>.] - ETA: 0s - loss: 0.3305
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0771
6530/6530 [==============================] - 1s 106us/step - loss: 0.3264 - val_loss: 0.1720

6224/6530 [===========================>..] - ETA: 0s - loss: 0.0750
6530/6530 [==============================] - 1s 206us/step - loss: 0.0736 - val_loss: 0.0404

# training | RMSE: 0.2616, MAE: 0.2145
worker 0  xfile  [28, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10568616291913142}, 'layer_1_size': 71, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 98, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4282996641394471}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.26161854499445625, 'rmse': 0.26161854499445625, 'mae': 0.21448844514303517, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  88 | activation: sigmoid | extras: None 
layer 2 | size:  64 | activation: relu    | extras: dropout - rate: 30.0% 
layer 3 | size:  60 | activation: sigmoid | extras: dropout - rate: 40.0% 
layer 4 | size:  86 | activation: relu    | extras: None 
layer 5 | size:   7 | activation: sigmoid | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 50s - loss: 0.2945
1024/6530 [===>..........................] - ETA: 2s - loss: 0.2528 
1856/6530 [=======>......................] - ETA: 1s - loss: 0.2394
2688/6530 [===========>..................] - ETA: 0s - loss: 0.2326
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2277
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2241
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2204
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2164
# training | RMSE: 0.1992, MAE: 0.1574
worker 2  xfile  [27, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 56, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4720637156453483}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 22, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.1991623138559677, 'rmse': 0.1991623138559677, 'mae': 0.1573751921328089, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  78 | activation: tanh    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 19s - loss: 0.6104
6530/6530 [==============================] - 1s 145us/step - loss: 0.2157 - val_loss: 0.1674

3712/6530 [================>.............] - ETA: 0s - loss: 0.4991 
6530/6530 [==============================] - 1s 85us/step - loss: 0.4073 - val_loss: 0.1800

# training | RMSE: 0.2151, MAE: 0.1674
worker 1  xfile  [29, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.21509306168577047, 'rmse': 0.21509306168577047, 'mae': 0.16740652994448652, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  50 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  67 | activation: sigmoid | extras: dropout - rate: 46.0% 
layer 3 | size:  38 | activation: tanh    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:20 - loss: 0.7489
 576/6530 [=>............................] - ETA: 7s - loss: 0.3250  
1216/6530 [====>.........................] - ETA: 3s - loss: 0.2544
1888/6530 [=======>......................] - ETA: 2s - loss: 0.2269
2592/6530 [==========>...................] - ETA: 1s - loss: 0.2122
3264/6530 [=============>................] - ETA: 0s - loss: 0.2053
3872/6530 [================>.............] - ETA: 0s - loss: 0.2009
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1967
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1932
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1908
6400/6530 [============================>.] - ETA: 0s - loss: 0.1894
6530/6530 [==============================] - 1s 200us/step - loss: 0.1888 - val_loss: 0.1671

# training | RMSE: 0.2061, MAE: 0.1686
worker 0  xfile  [30, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3002688216560363}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3995628521870124}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2060796034447526, 'rmse': 0.2060796034447526, 'mae': 0.16857823779024092, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  34 | activation: sigmoid | extras: None 
layer 2 | size:  33 | activation: relu    | extras: None 
layer 3 | size: 100 | activation: relu    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 22s - loss: 0.3317
2816/6530 [===========>..................] - ETA: 0s - loss: 0.2344 
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1976
# training | RMSE: 0.4226, MAE: 0.3799
worker 2  xfile  [32, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21905772316395972}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.42258579865153756, 'rmse': 0.42258579865153756, 'mae': 0.37991677743523167, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  18 | activation: sigmoid | extras: dropout - rate: 16.1% 
layer 2 | size:  38 | activation: tanh    | extras: dropout - rate: 11.8% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:10 - loss: 1.2371
6530/6530 [==============================] - 1s 94us/step - loss: 0.1928 - val_loss: 0.1754

 464/6530 [=>............................] - ETA: 6s - loss: 0.3631  
 912/6530 [===>..........................] - ETA: 3s - loss: 0.2634
1360/6530 [=====>........................] - ETA: 2s - loss: 0.2232
1824/6530 [=======>......................] - ETA: 1s - loss: 0.2006
2240/6530 [=========>....................] - ETA: 1s - loss: 0.1818
2624/6530 [===========>..................] - ETA: 1s - loss: 0.1720
3040/6530 [============>.................] - ETA: 0s - loss: 0.1595
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1519
3776/6530 [================>.............] - ETA: 0s - loss: 0.1435
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1364
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1316
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1273
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1236
# training | RMSE: 0.2137, MAE: 0.1728
worker 0  xfile  [33, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30225186523979475}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.21374204951603012, 'rmse': 0.21374204951603012, 'mae': 0.1727959411543234, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  46 | activation: relu    | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 38s - loss: 0.9360
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1197
1856/6530 [=======>......................] - ETA: 1s - loss: 0.3874 
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1156
4032/6530 [=================>............] - ETA: 0s - loss: 0.3086
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1120
6336/6530 [============================>.] - ETA: 0s - loss: 0.2736
6530/6530 [==============================] - 1s 92us/step - loss: 0.2714 - val_loss: 0.2083

6530/6530 [==============================] - 1s 218us/step - loss: 0.1089 - val_loss: 0.0453

# training | RMSE: 0.2172, MAE: 0.1658
worker 1  xfile  [31, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4599107245569436}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35545272342653067}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.21716925310583174, 'rmse': 0.21716925310583174, 'mae': 0.16582386323470605, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  45 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  66 | activation: tanh    | extras: None 
layer 3 | size:  84 | activation: relu    | extras: dropout - rate: 27.1% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:58 - loss: 0.5436
 640/6530 [=>............................] - ETA: 5s - loss: 0.1179  
1312/6530 [=====>........................] - ETA: 2s - loss: 0.0820
2016/6530 [========>.....................] - ETA: 1s - loss: 0.0685
2784/6530 [===========>..................] - ETA: 1s - loss: 0.0625
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0581
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0552
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0529
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0505
6432/6530 [============================>.] - ETA: 0s - loss: 0.0492
6530/6530 [==============================] - 1s 174us/step - loss: 0.0489 - val_loss: 0.0363

# training | RMSE: 0.2596, MAE: 0.2102
worker 0  xfile  [36, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1896035259027695}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.25956618684888605, 'rmse': 0.25956618684888605, 'mae': 0.21024065760943952, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  65 | activation: tanh    | extras: None 
layer 2 | size:  68 | activation: tanh    | extras: dropout - rate: 39.0% 
layer 3 | size:  53 | activation: relu    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 11s - loss: 0.6101
5376/6530 [=======================>......] - ETA: 0s - loss: 0.4853 
6530/6530 [==============================] - 1s 93us/step - loss: 0.4478 - val_loss: 0.1936

# training | RMSE: 0.2106, MAE: 0.1717
worker 2  xfile  [34, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1611382916869748}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11824700906257526}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2106487770789065, 'rmse': 0.2106487770789065, 'mae': 0.17174444292021088, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  46 | activation: sigmoid | extras: dropout - rate: 13.5% 
layer 2 | size:  44 | activation: tanh    | extras: None 
layer 3 | size:  49 | activation: sigmoid | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 11s - loss: 2.2158
6530/6530 [==============================] - 1s 92us/step - loss: 0.1763 - val_loss: 0.0680

# training | RMSE: 0.4399, MAE: 0.3907
worker 0  xfile  [37, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3897060985256203}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1594077157195362}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.4399105842568171, 'rmse': 0.4399105842568171, 'mae': 0.3907269723848202, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  32 | activation: sigmoid | extras: None 
layer 2 | size:  73 | activation: tanh    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 23s - loss: 0.6657
2944/6530 [============>.................] - ETA: 0s - loss: 0.1974 
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1278
6530/6530 [==============================] - 1s 96us/step - loss: 0.1251 - val_loss: 0.0655

# training | RMSE: 0.2630, MAE: 0.2158
worker 2  xfile  [38, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13464396181250873}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.26301125042611845, 'rmse': 0.26301125042611845, 'mae': 0.21576703259648733, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  50 | activation: relu    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 22s - loss: 0.4191
# training | RMSE: 0.1793, MAE: 0.1399
worker 1  xfile  [35, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27147461663629957}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17359862039199497}, 'layer_4_size': 5, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1662718832327227}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.17929080352332544, 'rmse': 0.17929080352332544, 'mae': 0.13994754931949208, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  79 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  64 | activation: sigmoid | extras: dropout - rate: 38.5% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 33s - loss: 0.6881
3840/6530 [================>.............] - ETA: 0s - loss: 0.3085 
2304/6530 [=========>....................] - ETA: 1s - loss: 0.3305 
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1948
6530/6530 [==============================] - 1s 93us/step - loss: 0.2830 - val_loss: 0.2354

6528/6530 [============================>.] - ETA: 0s - loss: 0.1483
6530/6530 [==============================] - 1s 139us/step - loss: 0.1483 - val_loss: 0.0404

# training | RMSE: 0.2573, MAE: 0.2104
worker 0  xfile  [40, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1400293556848392}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.25731351794861373, 'rmse': 0.25731351794861373, 'mae': 0.21042277887338626, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:   9 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  37 | activation: sigmoid | extras: dropout - rate: 49.8% 
layer 3 | size:  72 | activation: tanh    | extras: dropout - rate: 10.7% 
layer 4 | size:  94 | activation: sigmoid | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:14 - loss: 0.7660
 224/6530 [>.............................] - ETA: 23s - loss: 0.3354 
 416/6530 [>.............................] - ETA: 12s - loss: 0.2381
 608/6530 [=>............................] - ETA: 9s - loss: 0.2011 
# training | RMSE: 0.2986, MAE: 0.2393
worker 2  xfile  [41, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14432071788110462}, 'layer_2_size': 44, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1877390820355882}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2986457174363848, 'rmse': 0.2986457174363848, 'mae': 0.23929983373635164, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  48 | activation: sigmoid | extras: None 
layer 2 | size:  89 | activation: sigmoid | extras: batchnorm 
layer 3 | size:   8 | activation: relu    | extras: dropout - rate: 19.3% 
layer 4 | size:  26 | activation: relu    | extras: None 
layer 5 | size:  56 | activation: sigmoid | extras: dropout - rate: 22.6% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:22 - loss: 0.3048
 816/6530 [==>...........................] - ETA: 6s - loss: 0.1725
1024/6530 [===>..........................] - ETA: 4s - loss: 0.2311  
1056/6530 [===>..........................] - ETA: 5s - loss: 0.1524
1984/6530 [========>.....................] - ETA: 2s - loss: 0.2034
1328/6530 [=====>........................] - ETA: 4s - loss: 0.1355
3008/6530 [============>.................] - ETA: 1s - loss: 0.1901
1600/6530 [======>.......................] - ETA: 3s - loss: 0.1250
4032/6530 [=================>............] - ETA: 0s - loss: 0.1838
1872/6530 [=======>......................] - ETA: 2s - loss: 0.1168
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1795
2144/6530 [========>.....................] - ETA: 2s - loss: 0.1111
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1760
2416/6530 [==========>...................] - ETA: 2s - loss: 0.1054
2656/6530 [===========>..................] - ETA: 1s - loss: 0.1022
2912/6530 [============>.................] - ETA: 1s - loss: 0.0988
6530/6530 [==============================] - 1s 194us/step - loss: 0.1739 - val_loss: 0.1450

3152/6530 [=============>................] - ETA: 1s - loss: 0.0960
# training | RMSE: 0.1990, MAE: 0.1616
worker 1  xfile  [39, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38547274004859367}, 'layer_2_size': 64, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23627536327943913}, 'layer_4_size': 39, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.19896847865022876, 'rmse': 0.19896847865022876, 'mae': 0.1615563602533491, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  93 | activation: relu    | extras: batchnorm 
layer 2 | size:  20 | activation: relu    | extras: batchnorm 
layer 3 | size:  53 | activation: relu    | extras: batchnorm 
layer 4 | size:  97 | activation: tanh    | extras: dropout - rate: 12.5% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 47s - loss: 0.9620
3408/6530 [==============>...............] - ETA: 1s - loss: 0.0934
1792/6530 [=======>......................] - ETA: 2s - loss: 0.3365 
3680/6530 [===============>..............] - ETA: 1s - loss: 0.0911
3456/6530 [==============>...............] - ETA: 0s - loss: 0.2258
3936/6530 [=================>............] - ETA: 1s - loss: 0.0894
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1769
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0873
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0859
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0839
6530/6530 [==============================] - 1s 194us/step - loss: 0.1520 - val_loss: 0.1390

4976/6530 [=====================>........] - ETA: 0s - loss: 0.0824
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0813
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0801
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0793
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0782
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0774
6528/6530 [============================>.] - ETA: 0s - loss: 0.0761
6530/6530 [==============================] - 2s 346us/step - loss: 0.0761 - val_loss: 0.0412

# training | RMSE: 0.1783, MAE: 0.1382
worker 2  xfile  [43, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 89, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.19275846177406197}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22571997875034966}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.1782591048032154, 'rmse': 0.1782591048032154, 'mae': 0.1381513415795584, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  82 | activation: relu    | extras: batchnorm 
layer 2 | size:  80 | activation: relu    | extras: dropout - rate: 22.5% 
layer 3 | size:  12 | activation: tanh    | extras: batchnorm 
layer 4 | size:  27 | activation: tanh    | extras: dropout - rate: 29.3% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:33 - loss: 0.7949
# training | RMSE: 0.3642, MAE: 0.2910
worker 1  xfile  [44, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12535511325541668}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24701655990175164}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.3642046349641409, 'rmse': 0.3642046349641409, 'mae': 0.29104827226896623, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  58 | activation: relu    | extras: batchnorm 
layer 2 | size:  85 | activation: sigmoid | extras: dropout - rate: 26.8% 
layer 3 | size:  89 | activation: tanh    | extras: dropout - rate: 12.9% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:52 - loss: 0.6771
 704/6530 [==>...........................] - ETA: 8s - loss: 0.6774  
 544/6530 [=>............................] - ETA: 9s - loss: 0.3026  
1408/6530 [=====>........................] - ETA: 3s - loss: 0.5900
1056/6530 [===>..........................] - ETA: 4s - loss: 0.2507
2176/6530 [========>.....................] - ETA: 2s - loss: 0.5155
1664/6530 [======>.......................] - ETA: 2s - loss: 0.2135
3072/6530 [=============>................] - ETA: 1s - loss: 0.4452
2304/6530 [=========>....................] - ETA: 1s - loss: 0.1897
4032/6530 [=================>............] - ETA: 0s - loss: 0.3900
2944/6530 [============>.................] - ETA: 1s - loss: 0.1702
4864/6530 [=====================>........] - ETA: 0s - loss: 0.3581
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1545
5760/6530 [=========================>....] - ETA: 0s - loss: 0.3307
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1431
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1321
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1255
6530/6530 [==============================] - 1s 222us/step - loss: 0.3131 - val_loss: 0.1643

6208/6530 [===========================>..] - ETA: 0s - loss: 0.1188
6530/6530 [==============================] - 2s 231us/step - loss: 0.1158 - val_loss: 0.0357

# training | RMSE: 0.2038, MAE: 0.1639
worker 0  xfile  [42, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.49815714481728435}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10719070900249368}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.20378773802725852, 'rmse': 0.20378773802725852, 'mae': 0.16391134147448197, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  18 | activation: sigmoid | extras: dropout - rate: 26.9% 
layer 2 | size:  91 | activation: relu    | extras: batchnorm 
layer 3 | size:  79 | activation: tanh    | extras: dropout - rate: 32.1% 
layer 4 | size:  70 | activation: sigmoid | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 24s - loss: 0.9142
3584/6530 [===============>..............] - ETA: 0s - loss: 0.5705 
6530/6530 [==============================] - 1s 182us/step - loss: 0.4162 - val_loss: 0.2117

# training | RMSE: 0.2020, MAE: 0.1579
worker 2  xfile  [45, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22493601036887703}, 'layer_2_size': 80, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 12, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.29306150019762184}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3819503917456204}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20198308432802278, 'rmse': 0.20198308432802278, 'mae': 0.15792037786909427, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  20 | activation: tanh    | extras: None 
layer 2 | size:  47 | activation: tanh    | extras: None 
layer 3 | size:  19 | activation: tanh    | extras: dropout - rate: 30.3% 
layer 4 | size:  80 | activation: tanh    | extras: dropout - rate: 26.6% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:39 - loss: 1.0238
 256/6530 [>.............................] - ETA: 18s - loss: 0.6912 
 512/6530 [=>............................] - ETA: 9s - loss: 0.5845 
 800/6530 [==>...........................] - ETA: 6s - loss: 0.4997
1088/6530 [===>..........................] - ETA: 4s - loss: 0.4428
1392/6530 [=====>........................] - ETA: 3s - loss: 0.4046
1680/6530 [======>.......................] - ETA: 2s - loss: 0.3740
1904/6530 [=======>......................] - ETA: 2s - loss: 0.3552
# training | RMSE: 0.1874, MAE: 0.1491
worker 1  xfile  [46, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.26829488673006174}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12943210900540214}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 53, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.29719529485172025}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.1873793892758982, 'rmse': 0.1873793892758982, 'mae': 0.1490816601208161, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  60 | activation: tanh    | extras: batchnorm 
layer 2 | size:  23 | activation: sigmoid | extras: dropout - rate: 46.7% 
layer 3 | size:  98 | activation: tanh    | extras: None 
layer 4 | size:  68 | activation: tanh    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:21 - loss: 0.7277
2160/6530 [========>.....................] - ETA: 2s - loss: 0.3402
1024/6530 [===>..........................] - ETA: 4s - loss: 0.1359  
2464/6530 [==========>...................] - ETA: 1s - loss: 0.3231
2048/6530 [========>.....................] - ETA: 1s - loss: 0.1081
2800/6530 [===========>..................] - ETA: 1s - loss: 0.3090
3200/6530 [=============>................] - ETA: 1s - loss: 0.0947
3136/6530 [=============>................] - ETA: 1s - loss: 0.2980
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0880
3472/6530 [==============>...............] - ETA: 1s - loss: 0.2877
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0834
3808/6530 [================>.............] - ETA: 0s - loss: 0.2779
4144/6530 [==================>...........] - ETA: 0s - loss: 0.2692
4496/6530 [===================>..........] - ETA: 0s - loss: 0.2620
6530/6530 [==============================] - 1s 187us/step - loss: 0.0799 - val_loss: 0.1003

4864/6530 [=====================>........] - ETA: 0s - loss: 0.2564
5216/6530 [======================>.......] - ETA: 0s - loss: 0.2510
# training | RMSE: 0.2612, MAE: 0.2140
worker 0  xfile  [47, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2685498972279782}, 'layer_1_size': 18, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3211616980545791}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2611802974309499, 'rmse': 0.2611802974309499, 'mae': 0.2139983703435906, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:   5 | activation: tanh    | extras: batchnorm 
layer 2 | size:  86 | activation: sigmoid | extras: None 
layer 3 | size:  68 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  80 | activation: sigmoid | extras: None 
layer 5 | size:  39 | activation: relu    | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 43s - loss: 0.5586
5552/6530 [========================>.....] - ETA: 0s - loss: 0.2464
1792/6530 [=======>......................] - ETA: 2s - loss: 0.0895 
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2427
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0725
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2387
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0662
6528/6530 [============================>.] - ETA: 0s - loss: 0.2349
6530/6530 [==============================] - 2s 289us/step - loss: 0.2349 - val_loss: 0.1691

6530/6530 [==============================] - 1s 182us/step - loss: 0.0637 - val_loss: 0.0433

# training | RMSE: 0.3149, MAE: 0.2605
worker 1  xfile  [49, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4672537943168995}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.3148656731231956, 'rmse': 0.3148656731231956, 'mae': 0.2604850915209686, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  16 | activation: relu    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:15 - loss: 0.6861
1248/6530 [====>.........................] - ETA: 3s - loss: 0.4329  
2432/6530 [==========>...................] - ETA: 1s - loss: 0.3185
3648/6530 [===============>..............] - ETA: 0s - loss: 0.2689
4768/6530 [====================>.........] - ETA: 0s - loss: 0.2454
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2318
6530/6530 [==============================] - 1s 164us/step - loss: 0.2243 - val_loss: 0.1692

# training | RMSE: 0.2114, MAE: 0.1727
worker 0  xfile  [50, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 5, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 80, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.21135285916853164, 'rmse': 0.21135285916853164, 'mae': 0.17273781895591306, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  28 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  15 | activation: relu    | extras: None 
layer 3 | size:  64 | activation: relu    | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:20 - loss: 3.9915
1024/6530 [===>..........................] - ETA: 4s - loss: 1.7154  
2112/6530 [========>.....................] - ETA: 1s - loss: 0.9884
3264/6530 [=============>................] - ETA: 0s - loss: 0.6943
4544/6530 [===================>..........] - ETA: 0s - loss: 0.5350
5824/6530 [=========================>....] - ETA: 0s - loss: 0.4424
6530/6530 [==============================] - 1s 183us/step - loss: 0.4058 - val_loss: 0.0951

# training | RMSE: 0.2060, MAE: 0.1643
worker 2  xfile  [48, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 20, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3030003037194491}, 'layer_3_size': 19, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2656113327891144}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.20597552244888598, 'rmse': 0.20597552244888598, 'mae': 0.16428900708799365, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  60 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  24 | activation: sigmoid | extras: None 
layer 3 | size:  79 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  31 | activation: sigmoid | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 24s - loss: 0.7173
3072/6530 [=============>................] - ETA: 1s - loss: 0.5649 
5376/6530 [=======================>......] - ETA: 0s - loss: 0.4437
6530/6530 [==============================] - 1s 189us/step - loss: 0.3990 - val_loss: 0.1833

# training | RMSE: 0.2066, MAE: 0.1658
worker 1  xfile  [51, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 16, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46901823491831196}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4790271524690606}, 'layer_3_size': 74, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.20661695716245254, 'rmse': 0.20661695716245254, 'mae': 0.16583470699326064, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  24 | activation: relu    | extras: batchnorm 
layer 2 | size:  45 | activation: sigmoid | extras: None 
layer 3 | size:  86 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  60 | activation: sigmoid | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 26s - loss: 0.6037
3328/6530 [==============>...............] - ETA: 1s - loss: 0.3847 
6400/6530 [============================>.] - ETA: 0s - loss: 0.2976
6530/6530 [==============================] - 1s 204us/step - loss: 0.2949 - val_loss: 0.1709

# training | RMSE: 0.3142, MAE: 0.2505
worker 0  xfile  [52, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4757643375911187}, 'layer_4_size': 64, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.3141647283586018, 'rmse': 0.3141647283586018, 'mae': 0.25046183756982243, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  71 | activation: tanh    | extras: batchnorm 
layer 2 | size:  67 | activation: relu    | extras: batchnorm 
layer 3 | size:  23 | activation: tanh    | extras: batchnorm 
layer 4 | size:  64 | activation: tanh    | extras: dropout - rate: 47.5% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:54 - loss: 0.7115
 288/6530 [>.............................] - ETA: 26s - loss: 0.6677 
 576/6530 [=>............................] - ETA: 13s - loss: 0.6299
 832/6530 [==>...........................] - ETA: 9s - loss: 0.5925 
1152/6530 [====>.........................] - ETA: 6s - loss: 0.5298
1472/6530 [=====>........................] - ETA: 4s - loss: 0.4678
# training | RMSE: 0.2301, MAE: 0.1835
worker 2  xfile  [53, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2300660162577983, 'rmse': 0.2300660162577983, 'mae': 0.18346853018504522, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  81 | activation: sigmoid | extras: dropout - rate: 19.0% 
layer 2 | size:  13 | activation: relu    | extras: None 
layer 3 | size:  37 | activation: relu    | extras: batchnorm 
layer 4 | size:   8 | activation: sigmoid | extras: batchnorm 
layer 5 | size:  52 | activation: tanh    | extras: dropout - rate: 46.5% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 30s - loss: 0.7272
1824/6530 [=======>......................] - ETA: 3s - loss: 0.4105
3072/6530 [=============>................] - ETA: 1s - loss: 0.6858 
2208/6530 [=========>....................] - ETA: 2s - loss: 0.3671
5632/6530 [========================>.....] - ETA: 0s - loss: 0.5857
2624/6530 [===========>..................] - ETA: 2s - loss: 0.3351
3008/6530 [============>.................] - ETA: 1s - loss: 0.3114
3360/6530 [==============>...............] - ETA: 1s - loss: 0.2934
6530/6530 [==============================] - 2s 230us/step - loss: 0.5405 - val_loss: 0.2736

3648/6530 [===============>..............] - ETA: 1s - loss: 0.2812
3968/6530 [=================>............] - ETA: 1s - loss: 0.2695
4256/6530 [==================>...........] - ETA: 0s - loss: 0.2606
4576/6530 [====================>.........] - ETA: 0s - loss: 0.2522
4896/6530 [=====================>........] - ETA: 0s - loss: 0.2444
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2366
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2314
5856/6530 [=========================>....] - ETA: 0s - loss: 0.2256
6176/6530 [===========================>..] - ETA: 0s - loss: 0.2207
6496/6530 [============================>.] - ETA: 0s - loss: 0.2158
6530/6530 [==============================] - 2s 370us/step - loss: 0.2153 - val_loss: 0.1122

# training | RMSE: 0.2089, MAE: 0.1646
worker 1  xfile  [54, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20891350383084417, 'rmse': 0.20891350383084417, 'mae': 0.16460017251841608, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  17 | activation: tanh    | extras: dropout - rate: 37.9% 
layer 2 | size:  49 | activation: sigmoid | extras: dropout - rate: 36.2% 
layer 3 | size:  56 | activation: relu    | extras: batchnorm 
layer 4 | size:  78 | activation: sigmoid | extras: batchnorm 
layer 5 | size:  74 | activation: tanh    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 2:05 - loss: 0.9008
 704/6530 [==>...........................] - ETA: 10s - loss: 0.8108 
1408/6530 [=====>........................] - ETA: 4s - loss: 0.7033 
2112/6530 [========>.....................] - ETA: 2s - loss: 0.6138
2816/6530 [===========>..................] - ETA: 1s - loss: 0.5461
3520/6530 [===============>..............] - ETA: 1s - loss: 0.5001
4096/6530 [=================>............] - ETA: 0s - loss: 0.4715
4672/6530 [====================>.........] - ETA: 0s - loss: 0.4494
5312/6530 [=======================>......] - ETA: 0s - loss: 0.4293
5952/6530 [==========================>...] - ETA: 0s - loss: 0.4128
6530/6530 [==============================] - 2s 294us/step - loss: 0.4003 - val_loss: 0.2136

# training | RMSE: 0.3429, MAE: 0.2829
worker 2  xfile  [56, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18990815560638744}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.46535857208814924}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.34291324794420336, 'rmse': 0.34291324794420336, 'mae': 0.2828651814165786, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  15 | activation: relu    | extras: dropout - rate: 46.4% 
layer 2 | size:  37 | activation: tanh    | extras: dropout - rate: 35.0% 
layer 3 | size:  77 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  45 | activation: tanh    | extras: None 
layer 5 | size:  73 | activation: sigmoid | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 51s - loss: 1.2581
1408/6530 [=====>........................] - ETA: 3s - loss: 0.5838 
2688/6530 [===========>..................] - ETA: 1s - loss: 0.3773
4096/6530 [=================>............] - ETA: 0s - loss: 0.2779
# training | RMSE: 0.1432, MAE: 0.1067
worker 0  xfile  [55, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4747820652699525}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.14317449904334884, 'rmse': 0.14317449904334884, 'mae': 0.10668078621231573, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  18 | activation: sigmoid | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:25 - loss: 0.2558
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2155
 928/6530 [===>..........................] - ETA: 4s - loss: 0.1071  
2016/6530 [========>.....................] - ETA: 1s - loss: 0.0889
3200/6530 [=============>................] - ETA: 0s - loss: 0.0816
6530/6530 [==============================] - 1s 213us/step - loss: 0.2019 - val_loss: 0.0606

4384/6530 [===================>..........] - ETA: 0s - loss: 0.0774
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0741
6530/6530 [==============================] - 1s 172us/step - loss: 0.0718 - val_loss: 0.0547

# training | RMSE: 0.2645, MAE: 0.2152
worker 1  xfile  [57, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3785606602059729}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3619774967433266}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 78, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2645071336305211, 'rmse': 0.2645071336305211, 'mae': 0.21523519419102868, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  59 | activation: sigmoid | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:36 - loss: 0.4883
 768/6530 [==>...........................] - ETA: 6s - loss: 0.2772  
1632/6530 [======>.......................] - ETA: 2s - loss: 0.2262
2432/6530 [==========>...................] - ETA: 1s - loss: 0.2060
3264/6530 [=============>................] - ETA: 0s - loss: 0.1963
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1893
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1840
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1799
6530/6530 [==============================] - 1s 195us/step - loss: 0.1784 - val_loss: 0.1612

# training | RMSE: 0.2542, MAE: 0.2072
worker 2  xfile  [58, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4641015318908026}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3499918539180671}, 'layer_2_size': 37, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 77, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.25424201720517553, 'rmse': 0.25424201720517553, 'mae': 0.20723728488497514, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  35 | activation: relu    | extras: None 
layer 2 | size:  52 | activation: sigmoid | extras: dropout - rate: 41.5% 
layer 3 | size:  56 | activation: tanh    | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:01 - loss: 0.5955
 544/6530 [=>............................] - ETA: 10s - loss: 0.1783 
1120/6530 [====>.........................] - ETA: 4s - loss: 0.1273 
1632/6530 [======>.......................] - ETA: 3s - loss: 0.1120
# training | RMSE: 0.2367, MAE: 0.1931
worker 0  xfile  [59, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3338500118517678}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21733077990734936}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.23668808423300206, 'rmse': 0.23668808423300206, 'mae': 0.19312209428356736, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:   9 | activation: relu    | extras: dropout - rate: 34.6% 
layer 2 | size:  66 | activation: relu    | extras: dropout - rate: 37.2% 
layer 3 | size:  68 | activation: tanh    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 21s - loss: 1.8748
2112/6530 [========>.....................] - ETA: 2s - loss: 0.1032
5632/6530 [========================>.....] - ETA: 0s - loss: 0.4684 
2720/6530 [===========>..................] - ETA: 1s - loss: 0.0996
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0953
4000/6530 [=================>............] - ETA: 0s - loss: 0.0919
6530/6530 [==============================] - 1s 163us/step - loss: 0.4366 - val_loss: 0.1101

4704/6530 [====================>.........] - ETA: 0s - loss: 0.0885
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0852
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0836
6530/6530 [==============================] - 2s 239us/step - loss: 0.0830 - val_loss: 0.0854

# training | RMSE: 0.2034, MAE: 0.1615
worker 1  xfile  [60, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 59, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.20335280093209196, 'rmse': 0.20335280093209196, 'mae': 0.16148849226626008, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  73 | activation: relu    | extras: batchnorm 
layer 2 | size:  44 | activation: tanh    | extras: dropout - rate: 49.0% 
layer 3 | size:  49 | activation: sigmoid | extras: dropout - rate: 35.9% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:41 - loss: 0.7967
 896/6530 [===>..........................] - ETA: 6s - loss: 0.5527  
1728/6530 [======>.......................] - ETA: 3s - loss: 0.3898
2688/6530 [===========>..................] - ETA: 1s - loss: 0.3177
3712/6530 [================>.............] - ETA: 0s - loss: 0.2728
4928/6530 [=====================>........] - ETA: 0s - loss: 0.2425
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2250
6530/6530 [==============================] - 1s 227us/step - loss: 0.2164 - val_loss: 0.1275

# training | RMSE: 0.3272, MAE: 0.2643
worker 0  xfile  [62, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3456005888407692}, 'layer_1_size': 9, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37175830388274134}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22649751744615698}, 'layer_4_size': 48, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.32716804545500916, 'rmse': 0.32716804545500916, 'mae': 0.2642828322692294, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  74 | activation: sigmoid | extras: dropout - rate: 18.1% 
layer 2 | size:  95 | activation: tanh    | extras: batchnorm 
layer 3 | size:  64 | activation: relu    | extras: batchnorm 
layer 4 | size:  42 | activation: relu    | extras: dropout - rate: 45.3% 
layer 5 | size:  37 | activation: sigmoid | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 31s - loss: 0.1695
2304/6530 [=========>....................] - ETA: 2s - loss: 0.1110 
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0917
6530/6530 [==============================] - 2s 241us/step - loss: 0.0871 - val_loss: 0.0507

# training | RMSE: 0.2902, MAE: 0.2346
worker 2  xfile  [61, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4152912975829045}, 'layer_2_size': 52, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 56, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4164250803634669}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35456697442197815}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.29021305370703127, 'rmse': 0.29021305370703127, 'mae': 0.2346492555781693, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  41 | activation: tanh    | extras: dropout - rate: 46.0% 
layer 2 | size:   3 | activation: sigmoid | extras: dropout - rate: 29.1% 
layer 3 | size:  76 | activation: tanh    | extras: batchnorm 
layer 4 | size:  43 | activation: relu    | extras: batchnorm 
layer 5 | size:  12 | activation: tanh    | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 2:01 - loss: 1.2532
 768/6530 [==>...........................] - ETA: 9s - loss: 0.3793  
1600/6530 [======>.......................] - ETA: 4s - loss: 0.2473
2304/6530 [=========>....................] - ETA: 2s - loss: 0.2103
3136/6530 [=============>................] - ETA: 1s - loss: 0.1800
3904/6530 [================>.............] - ETA: 0s - loss: 0.1642
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1528
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1449
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1389
6530/6530 [==============================] - 2s 282us/step - loss: 0.1336 - val_loss: 0.1211

# training | RMSE: 0.2237, MAE: 0.1847
worker 0  xfile  [64, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18061994196690004}, 'layer_1_size': 74, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4529665353191762}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 37, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2237314151815987, 'rmse': 0.2237314151815987, 'mae': 0.1847365464564546, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  70 | activation: sigmoid | extras: None 
layer 2 | size:  86 | activation: tanh    | extras: None 
layer 3 | size:  70 | activation: relu    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 20s - loss: 0.7634
# training | RMSE: 0.1623, MAE: 0.1219
worker 1  xfile  [63, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4902284534495991}, 'layer_2_size': 44, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35875723960368033}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42650755712741795}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.16232704461387193, 'rmse': 0.16232704461387193, 'mae': 0.12194025645398769, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  94 | activation: sigmoid | extras: None 
layer 2 | size:  57 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  42 | activation: relu    | extras: None 
layer 4 | size:  22 | activation: sigmoid | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 7:35 - loss: 0.6612
4864/6530 [=====================>........] - ETA: 0s - loss: 0.3807 
 208/6530 [..............................] - ETA: 35s - loss: 0.6122 
 384/6530 [>.............................] - ETA: 19s - loss: 0.5288
6530/6530 [==============================] - 1s 156us/step - loss: 0.3434 - val_loss: 0.2609

 592/6530 [=>............................] - ETA: 12s - loss: 0.4408
 768/6530 [==>...........................] - ETA: 9s - loss: 0.3953 
1008/6530 [===>..........................] - ETA: 7s - loss: 0.3548
1232/6530 [====>.........................] - ETA: 6s - loss: 0.3233
1440/6530 [=====>........................] - ETA: 5s - loss: 0.3020
1616/6530 [======>.......................] - ETA: 4s - loss: 0.2861
1792/6530 [=======>......................] - ETA: 4s - loss: 0.2745
1968/6530 [========>.....................] - ETA: 3s - loss: 0.2665
2144/6530 [========>.....................] - ETA: 3s - loss: 0.2577
2304/6530 [=========>....................] - ETA: 3s - loss: 0.2519
2480/6530 [==========>...................] - ETA: 2s - loss: 0.2468
2688/6530 [===========>..................] - ETA: 2s - loss: 0.2409
2928/6530 [============>.................] - ETA: 2s - loss: 0.2339
3184/6530 [=============>................] - ETA: 2s - loss: 0.2275
3440/6530 [==============>...............] - ETA: 1s - loss: 0.2237
3680/6530 [===============>..............] - ETA: 1s - loss: 0.2201
3888/6530 [================>.............] - ETA: 1s - loss: 0.2176
4128/6530 [=================>............] - ETA: 1s - loss: 0.2151
4368/6530 [===================>..........] - ETA: 1s - loss: 0.2122
4592/6530 [====================>.........] - ETA: 0s - loss: 0.2106
4816/6530 [=====================>........] - ETA: 0s - loss: 0.2083
5040/6530 [======================>.......] - ETA: 0s - loss: 0.2062
5264/6530 [=======================>......] - ETA: 0s - loss: 0.2040
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2015
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1995
# training | RMSE: 0.3575, MAE: 0.2979
worker 2  xfile  [65, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4601707524689007}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2907473276287246}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.3574599491160073, 'rmse': 0.3574599491160073, 'mae': 0.2978756068625836, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  49 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  13 | activation: relu    | extras: None 
layer 3 | size:  15 | activation: relu    | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 28s - loss: 1.7223
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1983
4096/6530 [=================>............] - ETA: 0s - loss: 0.9024 
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1963
6464/6530 [============================>.] - ETA: 0s - loss: 0.1946
6530/6530 [==============================] - 1s 212us/step - loss: 0.6839 - val_loss: 0.2364

6530/6530 [==============================] - 3s 443us/step - loss: 0.1942 - val_loss: 0.1629

# training | RMSE: 0.3178, MAE: 0.2598
worker 0  xfile  [67, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.3177988421301995, 'rmse': 0.3177988421301995, 'mae': 0.25976808931042483, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  86 | activation: tanh    | extras: batchnorm 
layer 2 | size:   2 | activation: relu    | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:53 - loss: 0.5261
 256/6530 [>.............................] - ETA: 26s - loss: 0.5181 
 528/6530 [=>............................] - ETA: 12s - loss: 0.4927
 736/6530 [==>...........................] - ETA: 9s - loss: 0.4676 
 896/6530 [===>..........................] - ETA: 7s - loss: 0.4470
1056/6530 [===>..........................] - ETA: 6s - loss: 0.4217
1264/6530 [====>.........................] - ETA: 5s - loss: 0.3942
1472/6530 [=====>........................] - ETA: 4s - loss: 0.3737
1744/6530 [=======>......................] - ETA: 3s - loss: 0.3520
2000/6530 [========>.....................] - ETA: 3s - loss: 0.3371
2288/6530 [=========>....................] - ETA: 2s - loss: 0.3221
2560/6530 [==========>...................] - ETA: 2s - loss: 0.3112
2848/6530 [============>.................] - ETA: 2s - loss: 0.2985
3120/6530 [=============>................] - ETA: 1s - loss: 0.2891
3408/6530 [==============>...............] - ETA: 1s - loss: 0.2828
3696/6530 [===============>..............] - ETA: 1s - loss: 0.2766
3984/6530 [=================>............] - ETA: 1s - loss: 0.2713
4256/6530 [==================>...........] - ETA: 1s - loss: 0.2659
4528/6530 [===================>..........] - ETA: 0s - loss: 0.2618
4832/6530 [=====================>........] - ETA: 0s - loss: 0.2569
5136/6530 [======================>.......] - ETA: 0s - loss: 0.2528
5392/6530 [=======================>......] - ETA: 0s - loss: 0.2496
5664/6530 [=========================>....] - ETA: 0s - loss: 0.2459
5936/6530 [==========================>...] - ETA: 0s - loss: 0.2428
6160/6530 [===========================>..] - ETA: 0s - loss: 0.2403
6432/6530 [============================>.] - ETA: 0s - loss: 0.2379
6530/6530 [==============================] - 3s 384us/step - loss: 0.2367 - val_loss: 0.1623

# training | RMSE: 0.4745, MAE: 0.3802
worker 2  xfile  [68, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.4744696734086108, 'rmse': 0.4744696734086108, 'mae': 0.3802001349966286, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  15 | activation: relu    | extras: None 
layer 2 | size:  46 | activation: tanh    | extras: None 
layer 3 | size:  60 | activation: relu    | extras: None 
layer 4 | size:  69 | activation: tanh    | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:48 - loss: 0.5814
1216/6530 [====>.........................] - ETA: 4s - loss: 0.3754  
2496/6530 [==========>...................] - ETA: 1s - loss: 0.2198
3712/6530 [================>.............] - ETA: 0s - loss: 0.1693
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1421
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1222
6530/6530 [==============================] - 1s 225us/step - loss: 0.1192 - val_loss: 0.0864

# training | RMSE: 0.2037, MAE: 0.1610
worker 1  xfile  [66, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 94, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 57, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25266814823112443}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.20365438822815202, 'rmse': 0.20365438822815202, 'mae': 0.16100806931523134, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  50 | activation: tanh    | extras: batchnorm 
layer 2 | size:  85 | activation: tanh    | extras: dropout - rate: 12.2% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 1:00 - loss: 0.6023
2560/6530 [==========>...................] - ETA: 1s - loss: 0.4526  
5120/6530 [======================>.......] - ETA: 0s - loss: 0.3044
6530/6530 [==============================] - 1s 226us/step - loss: 0.2542 - val_loss: 0.4880

# training | RMSE: 0.2057, MAE: 0.1566
worker 0  xfile  [69, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20572302202762432, 'rmse': 0.20572302202762432, 'mae': 0.15664679981274668, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  54 | activation: sigmoid | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:38 - loss: 0.2155
 464/6530 [=>............................] - ETA: 11s - loss: 0.1908 
 992/6530 [===>..........................] - ETA: 5s - loss: 0.1768 
1392/6530 [=====>........................] - ETA: 3s - loss: 0.1722
1616/6530 [======>.......................] - ETA: 3s - loss: 0.1729
2096/6530 [========>.....................] - ETA: 2s - loss: 0.1689
2544/6530 [==========>...................] - ETA: 1s - loss: 0.1686
3056/6530 [=============>................] - ETA: 1s - loss: 0.1680
3616/6530 [===============>..............] - ETA: 1s - loss: 0.1677
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1671
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1673
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1668
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1667
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1652
# training | RMSE: 0.2912, MAE: 0.2430
worker 2  xfile  [70, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 23, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.29124649424022536, 'rmse': 0.29124649424022536, 'mae': 0.242955806443754, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  86 | activation: tanh    | extras: None 
layer 2 | size:  46 | activation: sigmoid | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 25s - loss: 1.6350
6530/6530 [==============================] - 2s 278us/step - loss: 0.1644 - val_loss: 0.1629

5888/6530 [==========================>...] - ETA: 0s - loss: 0.6411 
6530/6530 [==============================] - 1s 190us/step - loss: 0.6078 - val_loss: 0.2640

# training | RMSE: 0.6807, MAE: 0.5455
worker 1  xfile  [71, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12242982296970335}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 82, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.6807162272600225, 'rmse': 0.6807162272600225, 'mae': 0.5454749109615311, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  10 | activation: sigmoid | extras: None 
layer 2 | size:  90 | activation: relu    | extras: batchnorm 
layer 3 | size:   6 | activation: sigmoid | extras: dropout - rate: 10.5% 
layer 4 | size:  72 | activation: relu    | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 32s - loss: 1.2714
3072/6530 [=============>................] - ETA: 1s - loss: 0.7381 
6400/6530 [============================>.] - ETA: 0s - loss: 0.5225
6530/6530 [==============================] - 2s 243us/step - loss: 0.5169 - val_loss: 0.1998

# training | RMSE: 0.3269, MAE: 0.2649
worker 2  xfile  [73, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2272457618830385}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.32687561676520427, 'rmse': 0.32687561676520427, 'mae': 0.2648661308919742, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  27 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  14 | activation: tanh    | extras: None 
layer 3 | size:  36 | activation: tanh    | extras: None 
layer 4 | size:  16 | activation: tanh    | extras: dropout - rate: 30.9% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 2:11 - loss: 0.7124
 768/6530 [==>...........................] - ETA: 10s - loss: 0.6341 
1536/6530 [======>.......................] - ETA: 4s - loss: 0.4947 
2368/6530 [=========>....................] - ETA: 2s - loss: 0.3982
3264/6530 [=============>................] - ETA: 1s - loss: 0.3390
4032/6530 [=================>............] - ETA: 0s - loss: 0.3077
4672/6530 [====================>.........] - ETA: 0s - loss: 0.2885
# training | RMSE: 0.2035, MAE: 0.1623
worker 0  xfile  [72, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.418888951595746}, 'layer_4_size': 92, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23718605775698545}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.20352997171507106, 'rmse': 0.20352997171507106, 'mae': 0.1622608579117386, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  45 | activation: sigmoid | extras: None 
layer 2 | size:  57 | activation: relu    | extras: batchnorm 
layer 3 | size:  70 | activation: relu    | extras: batchnorm 
layer 4 | size:  76 | activation: tanh    | extras: dropout - rate: 41.5% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 1:06 - loss: 0.8656
1280/6530 [====>.........................] - ETA: 5s - loss: 0.5367  
5440/6530 [=======================>......] - ETA: 0s - loss: 0.2700
2688/6530 [===========>..................] - ETA: 2s - loss: 0.3726
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2549
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2711
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2191
# training | RMSE: 0.2419, MAE: 0.1955
worker 1  xfile  [74, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10452094708784201}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.24186474056502708, 'rmse': 0.24186474056502708, 'mae': 0.19546946885387528, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  36 | activation: relu    | extras: None 
layer 2 | size:  93 | activation: relu    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:26 - loss: 0.4001
6530/6530 [==============================] - 2s 291us/step - loss: 0.2494 - val_loss: 0.2812

 896/6530 [===>..........................] - ETA: 6s - loss: 0.2861  
1824/6530 [=======>......................] - ETA: 2s - loss: 0.2560
2720/6530 [===========>..................] - ETA: 1s - loss: 0.2410
6530/6530 [==============================] - 2s 266us/step - loss: 0.1972 - val_loss: 0.0639

3616/6530 [===============>..............] - ETA: 0s - loss: 0.2300
4576/6530 [====================>.........] - ETA: 0s - loss: 0.2204
5472/6530 [========================>.....] - ETA: 0s - loss: 0.2126
6304/6530 [===========================>..] - ETA: 0s - loss: 0.2081
6530/6530 [==============================] - 2s 236us/step - loss: 0.2064 - val_loss: 0.1642

# training | RMSE: 0.3492, MAE: 0.2829
worker 2  xfile  [75, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.30862273470417034}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.3492147642043146, 'rmse': 0.3492147642043146, 'mae': 0.2829079164433214, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  34 | activation: sigmoid | extras: dropout - rate: 13.6% 
layer 2 | size:  65 | activation: sigmoid | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:46 - loss: 0.6484
1344/6530 [=====>........................] - ETA: 4s - loss: 0.3184  
2432/6530 [==========>...................] - ETA: 1s - loss: 0.2737
3328/6530 [==============>...............] - ETA: 1s - loss: 0.2570
# training | RMSE: 0.2518, MAE: 0.2005
worker 0  xfile  [76, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 45, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 57, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41506802225004635}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2517823700316604, 'rmse': 0.2517823700316604, 'mae': 0.20053507118834304, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  67 | activation: relu    | extras: batchnorm 
layer 2 | size:  98 | activation: sigmoid | extras: dropout - rate: 27.1% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 7:15 - loss: 0.3979
4288/6530 [==================>...........] - ETA: 0s - loss: 0.2478
 256/6530 [>.............................] - ETA: 27s - loss: 0.2558 
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2420
 528/6530 [=>............................] - ETA: 13s - loss: 0.2192
 816/6530 [==>...........................] - ETA: 8s - loss: 0.2073 
1120/6530 [====>.........................] - ETA: 6s - loss: 0.1963
1472/6530 [=====>........................] - ETA: 4s - loss: 0.1865
# training | RMSE: 0.2102, MAE: 0.1643
worker 1  xfile  [77, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.21021882967028724, 'rmse': 0.21021882967028724, 'mae': 0.1642584948026162, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  48 | activation: sigmoid | extras: dropout - rate: 18.6% 
layer 2 | size:  41 | activation: tanh    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 50s - loss: 0.6190
6530/6530 [==============================] - 2s 234us/step - loss: 0.2389 - val_loss: 0.2122

1776/6530 [=======>......................] - ETA: 3s - loss: 0.1796
3968/6530 [=================>............] - ETA: 0s - loss: 0.0873 
2128/6530 [========>.....................] - ETA: 2s - loss: 0.1749
2496/6530 [==========>...................] - ETA: 2s - loss: 0.1722
2864/6530 [============>.................] - ETA: 1s - loss: 0.1681
3200/6530 [=============>................] - ETA: 1s - loss: 0.1645
6530/6530 [==============================] - 1s 190us/step - loss: 0.0755 - val_loss: 0.0551

3504/6530 [===============>..............] - ETA: 1s - loss: 0.1636
3840/6530 [================>.............] - ETA: 1s - loss: 0.1615
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1600
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1594
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1578
# training | RMSE: 0.2617, MAE: 0.2145
worker 2  xfile  [78, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1357948231428067}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17180747868109397}, 'layer_3_size': 30, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12845280205534437}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31034787669558805}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2617033390371281, 'rmse': 0.2617033390371281, 'mae': 0.21446677369677455, 'early_stop': False}
vggnet done  2

5248/6530 [=======================>......] - ETA: 0s - loss: 0.1561
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1546
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1538
6352/6530 [============================>.] - ETA: 0s - loss: 0.1527
# training | RMSE: 0.2335, MAE: 0.1904
worker 1  xfile  [80, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1864451741191601}, 'layer_1_size': 48, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35581645225868186}, 'layer_4_size': 7, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2334902768783353, 'rmse': 0.2334902768783353, 'mae': 0.19044892829860888, 'early_stop': False}
vggnet done  1

6530/6530 [==============================] - 2s 343us/step - loss: 0.1520 - val_loss: 0.1402

# training | RMSE: 0.1743, MAE: 0.1348
worker 0  xfile  [79, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2707532519413747}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.17429835399877033, 'rmse': 0.17429835399877033, 'mae': 0.13481227719754837, 'early_stop': False}
vggnet done  0
all of workers have been done
result is equal to None
calculation finished
#2 epoch=1.0 loss={'loss': 0.4543286547741806, 'rmse': 0.4543286547741806, 'mae': 0.35669211230058157, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#0 epoch=1.0 loss={'loss': 0.27387409347056146, 'rmse': 0.27387409347056146, 'mae': 0.2150642661475466, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24508931426614333}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 54, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#1 epoch=1.0 loss={'loss': 0.20561757252853263, 'rmse': 0.20561757252853263, 'mae': 0.16350081531521782, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4688037995543516}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#3 epoch=1.0 loss={'loss': 0.23290934947965453, 'rmse': 0.23290934947965453, 'mae': 0.18577780718835804, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1830453648773626}, 'layer_1_size': 89, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36110923197292344}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 9, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1782147798722572}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#4 epoch=1.0 loss={'loss': 0.21969953191170108, 'rmse': 0.21969953191170108, 'mae': 0.18072387192689346, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2090886416592586}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#6 epoch=1.0 loss={'loss': 0.25782008734150774, 'rmse': 0.25782008734150774, 'mae': 0.2112159822516024, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3706207963655883}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4962179305960813}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20464899395687103}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#5 epoch=1.0 loss={'loss': 0.20585709256676668, 'rmse': 0.20585709256676668, 'mae': 0.16542987784648275, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22759357904567784}, 'layer_2_size': 57, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40909023674512535}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#7 epoch=1.0 loss={'loss': 0.9491090722125206, 'rmse': 0.9491090722125206, 'mae': 0.8570562131568096, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35500839960594033}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4901778376958633}, 'layer_5_size': 79, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#9 epoch=1.0 loss={'loss': 0.28367759506808204, 'rmse': 0.28367759506808204, 'mae': 0.2230702004208703, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#8 epoch=1.0 loss={'loss': 0.1974287844932267, 'rmse': 0.1974287844932267, 'mae': 0.1584050566620434, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38664454623440137}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3089910751745466}, 'layer_3_size': 58, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#11 epoch=1.0 loss={'loss': 0.26000546178130773, 'rmse': 0.26000546178130773, 'mae': 0.21304755851384433, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3835566305063546}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23192250650061058}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 18, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#10 epoch=1.0 loss={'loss': 0.1919746879913534, 'rmse': 0.1919746879913534, 'mae': 0.15558377740957746, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4499329686752288}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 91, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#13 epoch=1.0 loss={'loss': 0.6818443282152749, 'rmse': 0.6818443282152749, 'mae': 0.5680725337738203, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20176026945598938}, 'layer_1_size': 6, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 16, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11281343145256294}, 'layer_4_size': 31, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14994890442267358}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#12 epoch=1.0 loss={'loss': 0.21679185928995987, 'rmse': 0.21679185928995987, 'mae': 0.17874792915653753, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22029350655847801}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20479056109862254}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#14 epoch=1.0 loss={'loss': 0.21351072125456003, 'rmse': 0.21351072125456003, 'mae': 0.1755786546001768, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38217105892249426}, 'layer_2_size': 6, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30502556147053894}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#17 epoch=1.0 loss={'loss': 0.17119905505966707, 'rmse': 0.17119905505966707, 'mae': 0.1327713425132463, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1302302520515125}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#15 epoch=1.0 loss={'loss': 0.23545384583319995, 'rmse': 0.23545384583319995, 'mae': 0.18990396966307255, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.147847915715405}, 'layer_4_size': 4, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#16 epoch=1.0 loss={'loss': 0.22949216187804347, 'rmse': 0.22949216187804347, 'mae': 0.18726315030046395, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1694124464538399}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46390761362282207}, 'layer_2_size': 99, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3161741413669089}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 12, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#18 epoch=1.0 loss={'loss': 0.23074098849234811, 'rmse': 0.23074098849234811, 'mae': 0.18431760350321444, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#19 epoch=1.0 loss={'loss': 0.25975309956282994, 'rmse': 0.25975309956282994, 'mae': 0.21283009270751835, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13811544797959066}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37009762923770795}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#20 epoch=1.0 loss={'loss': 0.20270912087066512, 'rmse': 0.20270912087066512, 'mae': 0.16315214681022633, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1379856625934365}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#21 epoch=1.0 loss={'loss': 0.20346544999046282, 'rmse': 0.20346544999046282, 'mae': 0.16184584204145264, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 39, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#23 epoch=1.0 loss={'loss': 0.24725493957239894, 'rmse': 0.24725493957239894, 'mae': 0.20583366576658968, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3541750557794504}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#22 epoch=1.0 loss={'loss': 0.3531300567975264, 'rmse': 0.3531300567975264, 'mae': 0.2871508458247739, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42750165664118256}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40234224544075115}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#25 epoch=1.0 loss={'loss': 0.3903116200201268, 'rmse': 0.3903116200201268, 'mae': 0.33068239551121204, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38392317341419036}, 'layer_1_size': 22, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 63, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2124436695855239}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
#24 epoch=1.0 loss={'loss': 0.2243559377916526, 'rmse': 0.2243559377916526, 'mae': 0.17309467787698474, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1121895844612825}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10065942982829018}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4642438706544345}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.461764616219107}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#26 epoch=1.0 loss={'loss': 0.5711244479712618, 'rmse': 0.5711244479712618, 'mae': 0.469717675532414, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3634488551903946}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19465049393506934}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
#28 epoch=1.0 loss={'loss': 0.26161854499445625, 'rmse': 0.26161854499445625, 'mae': 0.21448844514303517, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10568616291913142}, 'layer_1_size': 71, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 98, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4282996641394471}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#29 epoch=1.0 loss={'loss': 0.21509306168577047, 'rmse': 0.21509306168577047, 'mae': 0.16740652994448652, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#27 epoch=1.0 loss={'loss': 0.1991623138559677, 'rmse': 0.1991623138559677, 'mae': 0.1573751921328089, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 56, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4720637156453483}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 22, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#30 epoch=1.0 loss={'loss': 0.2060796034447526, 'rmse': 0.2060796034447526, 'mae': 0.16857823779024092, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3002688216560363}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3995628521870124}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#32 epoch=1.0 loss={'loss': 0.42258579865153756, 'rmse': 0.42258579865153756, 'mae': 0.37991677743523167, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21905772316395972}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#31 epoch=1.0 loss={'loss': 0.21716925310583174, 'rmse': 0.21716925310583174, 'mae': 0.16582386323470605, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4599107245569436}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35545272342653067}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#33 epoch=1.0 loss={'loss': 0.21374204951603012, 'rmse': 0.21374204951603012, 'mae': 0.1727959411543234, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30225186523979475}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#36 epoch=1.0 loss={'loss': 0.25956618684888605, 'rmse': 0.25956618684888605, 'mae': 0.21024065760943952, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1896035259027695}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#34 epoch=1.0 loss={'loss': 0.2106487770789065, 'rmse': 0.2106487770789065, 'mae': 0.17174444292021088, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1611382916869748}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11824700906257526}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#35 epoch=1.0 loss={'loss': 0.17929080352332544, 'rmse': 0.17929080352332544, 'mae': 0.13994754931949208, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27147461663629957}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17359862039199497}, 'layer_4_size': 5, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1662718832327227}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#37 epoch=1.0 loss={'loss': 0.4399105842568171, 'rmse': 0.4399105842568171, 'mae': 0.3907269723848202, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3897060985256203}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1594077157195362}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#38 epoch=1.0 loss={'loss': 0.26301125042611845, 'rmse': 0.26301125042611845, 'mae': 0.21576703259648733, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13464396181250873}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#40 epoch=1.0 loss={'loss': 0.25731351794861373, 'rmse': 0.25731351794861373, 'mae': 0.21042277887338626, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1400293556848392}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#41 epoch=1.0 loss={'loss': 0.2986457174363848, 'rmse': 0.2986457174363848, 'mae': 0.23929983373635164, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14432071788110462}, 'layer_2_size': 44, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1877390820355882}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#39 epoch=1.0 loss={'loss': 0.19896847865022876, 'rmse': 0.19896847865022876, 'mae': 0.1615563602533491, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38547274004859367}, 'layer_2_size': 64, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23627536327943913}, 'layer_4_size': 39, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#43 epoch=1.0 loss={'loss': 0.1782591048032154, 'rmse': 0.1782591048032154, 'mae': 0.1381513415795584, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 89, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.19275846177406197}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22571997875034966}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#44 epoch=1.0 loss={'loss': 0.3642046349641409, 'rmse': 0.3642046349641409, 'mae': 0.29104827226896623, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12535511325541668}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24701655990175164}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#42 epoch=1.0 loss={'loss': 0.20378773802725852, 'rmse': 0.20378773802725852, 'mae': 0.16391134147448197, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.49815714481728435}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10719070900249368}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#45 epoch=1.0 loss={'loss': 0.20198308432802278, 'rmse': 0.20198308432802278, 'mae': 0.15792037786909427, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22493601036887703}, 'layer_2_size': 80, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 12, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.29306150019762184}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3819503917456204}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#46 epoch=1.0 loss={'loss': 0.1873793892758982, 'rmse': 0.1873793892758982, 'mae': 0.1490816601208161, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.26829488673006174}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12943210900540214}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 53, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.29719529485172025}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#47 epoch=1.0 loss={'loss': 0.2611802974309499, 'rmse': 0.2611802974309499, 'mae': 0.2139983703435906, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2685498972279782}, 'layer_1_size': 18, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3211616980545791}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#49 epoch=1.0 loss={'loss': 0.3148656731231956, 'rmse': 0.3148656731231956, 'mae': 0.2604850915209686, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4672537943168995}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#50 epoch=1.0 loss={'loss': 0.21135285916853164, 'rmse': 0.21135285916853164, 'mae': 0.17273781895591306, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 5, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 80, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#48 epoch=1.0 loss={'loss': 0.20597552244888598, 'rmse': 0.20597552244888598, 'mae': 0.16428900708799365, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 20, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3030003037194491}, 'layer_3_size': 19, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2656113327891144}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#51 epoch=1.0 loss={'loss': 0.20661695716245254, 'rmse': 0.20661695716245254, 'mae': 0.16583470699326064, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 16, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46901823491831196}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4790271524690606}, 'layer_3_size': 74, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#52 epoch=1.0 loss={'loss': 0.3141647283586018, 'rmse': 0.3141647283586018, 'mae': 0.25046183756982243, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4757643375911187}, 'layer_4_size': 64, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#53 epoch=1.0 loss={'loss': 0.2300660162577983, 'rmse': 0.2300660162577983, 'mae': 0.18346853018504522, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#54 epoch=1.0 loss={'loss': 0.20891350383084417, 'rmse': 0.20891350383084417, 'mae': 0.16460017251841608, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#56 epoch=1.0 loss={'loss': 0.34291324794420336, 'rmse': 0.34291324794420336, 'mae': 0.2828651814165786, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18990815560638744}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.46535857208814924}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#55 epoch=1.0 loss={'loss': 0.14317449904334884, 'rmse': 0.14317449904334884, 'mae': 0.10668078621231573, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4747820652699525}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#57 epoch=1.0 loss={'loss': 0.2645071336305211, 'rmse': 0.2645071336305211, 'mae': 0.21523519419102868, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3785606602059729}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3619774967433266}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 78, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#58 epoch=1.0 loss={'loss': 0.25424201720517553, 'rmse': 0.25424201720517553, 'mae': 0.20723728488497514, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4641015318908026}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3499918539180671}, 'layer_2_size': 37, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 77, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#59 epoch=1.0 loss={'loss': 0.23668808423300206, 'rmse': 0.23668808423300206, 'mae': 0.19312209428356736, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3338500118517678}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21733077990734936}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#60 epoch=1.0 loss={'loss': 0.20335280093209196, 'rmse': 0.20335280093209196, 'mae': 0.16148849226626008, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 59, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#62 epoch=1.0 loss={'loss': 0.32716804545500916, 'rmse': 0.32716804545500916, 'mae': 0.2642828322692294, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3456005888407692}, 'layer_1_size': 9, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37175830388274134}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22649751744615698}, 'layer_4_size': 48, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#61 epoch=1.0 loss={'loss': 0.29021305370703127, 'rmse': 0.29021305370703127, 'mae': 0.2346492555781693, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4152912975829045}, 'layer_2_size': 52, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 56, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4164250803634669}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35456697442197815}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
#63 epoch=1.0 loss={'loss': 0.16232704461387193, 'rmse': 0.16232704461387193, 'mae': 0.12194025645398769, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4902284534495991}, 'layer_2_size': 44, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35875723960368033}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42650755712741795}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#64 epoch=1.0 loss={'loss': 0.2237314151815987, 'rmse': 0.2237314151815987, 'mae': 0.1847365464564546, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18061994196690004}, 'layer_1_size': 74, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4529665353191762}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 37, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#65 epoch=1.0 loss={'loss': 0.3574599491160073, 'rmse': 0.3574599491160073, 'mae': 0.2978756068625836, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4601707524689007}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2907473276287246}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#67 epoch=1.0 loss={'loss': 0.3177988421301995, 'rmse': 0.3177988421301995, 'mae': 0.25976808931042483, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#68 epoch=1.0 loss={'loss': 0.4744696734086108, 'rmse': 0.4744696734086108, 'mae': 0.3802001349966286, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#66 epoch=1.0 loss={'loss': 0.20365438822815202, 'rmse': 0.20365438822815202, 'mae': 0.16100806931523134, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 94, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 57, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25266814823112443}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#69 epoch=1.0 loss={'loss': 0.20572302202762432, 'rmse': 0.20572302202762432, 'mae': 0.15664679981274668, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#70 epoch=1.0 loss={'loss': 0.29124649424022536, 'rmse': 0.29124649424022536, 'mae': 0.242955806443754, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 23, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#71 epoch=1.0 loss={'loss': 0.6807162272600225, 'rmse': 0.6807162272600225, 'mae': 0.5454749109615311, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12242982296970335}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 82, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#73 epoch=1.0 loss={'loss': 0.32687561676520427, 'rmse': 0.32687561676520427, 'mae': 0.2648661308919742, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2272457618830385}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#72 epoch=1.0 loss={'loss': 0.20352997171507106, 'rmse': 0.20352997171507106, 'mae': 0.1622608579117386, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.418888951595746}, 'layer_4_size': 92, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23718605775698545}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#74 epoch=1.0 loss={'loss': 0.24186474056502708, 'rmse': 0.24186474056502708, 'mae': 0.19546946885387528, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10452094708784201}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#75 epoch=1.0 loss={'loss': 0.3492147642043146, 'rmse': 0.3492147642043146, 'mae': 0.2829079164433214, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.30862273470417034}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#76 epoch=1.0 loss={'loss': 0.2517823700316604, 'rmse': 0.2517823700316604, 'mae': 0.20053507118834304, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 45, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 57, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41506802225004635}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#77 epoch=1.0 loss={'loss': 0.21021882967028724, 'rmse': 0.21021882967028724, 'mae': 0.1642584948026162, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#78 epoch=1.0 loss={'loss': 0.2617033390371281, 'rmse': 0.2617033390371281, 'mae': 0.21446677369677455, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1357948231428067}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17180747868109397}, 'layer_3_size': 30, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12845280205534437}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31034787669558805}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#80 epoch=1.0 loss={'loss': 0.2334902768783353, 'rmse': 0.2334902768783353, 'mae': 0.19044892829860888, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1864451741191601}, 'layer_1_size': 48, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35581645225868186}, 'layer_4_size': 7, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#79 epoch=1.0 loss={'loss': 0.17429835399877033, 'rmse': 0.17429835399877033, 'mae': 0.13481227719754837, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2707532519413747}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
get a list [results] of length 81
get a list [loss] of length 81
get a list [val_loss] of length 81
length of indices is [56 63 15 80 42 36 46 11  9 41 29 45 20 60 21 73 68 44  2 69  6 50 30 51
 54 77 35 49 14 33 28 13 32  4 64 25 17 53 18  3 79 16 59 74 22 76 58 39
  5 34 19 10 47 27 78 38 57  1  8 62 70 40 52 48 66 72 61 55 75 23 65 43
 24 31 37  0 67 26 71 12  7]
length of indices is 81
length of T is 81
newly formed T structure is:[[0, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24508931426614333}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 54, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [1, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4688037995543516}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [2, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [3, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1830453648773626}, 'layer_1_size': 89, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36110923197292344}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 9, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1782147798722572}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [4, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2090886416592586}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [5, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22759357904567784}, 'layer_2_size': 57, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40909023674512535}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [6, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3706207963655883}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4962179305960813}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20464899395687103}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [7, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35500839960594033}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4901778376958633}, 'layer_5_size': 79, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [8, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38664454623440137}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3089910751745466}, 'layer_3_size': 58, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [9, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [10, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4499329686752288}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 91, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [11, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3835566305063546}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23192250650061058}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 18, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [12, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22029350655847801}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20479056109862254}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [13, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20176026945598938}, 'layer_1_size': 6, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 16, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11281343145256294}, 'layer_4_size': 31, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14994890442267358}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [14, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38217105892249426}, 'layer_2_size': 6, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30502556147053894}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [15, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.147847915715405}, 'layer_4_size': 4, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [16, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1694124464538399}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46390761362282207}, 'layer_2_size': 99, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3161741413669089}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 12, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [17, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1302302520515125}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [18, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [19, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13811544797959066}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37009762923770795}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [20, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1379856625934365}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [21, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 39, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [22, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42750165664118256}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40234224544075115}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [23, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3541750557794504}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [24, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1121895844612825}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10065942982829018}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4642438706544345}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.461764616219107}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [25, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38392317341419036}, 'layer_1_size': 22, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 63, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2124436695855239}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [26, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3634488551903946}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19465049393506934}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [27, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 56, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4720637156453483}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 22, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [28, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10568616291913142}, 'layer_1_size': 71, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 98, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4282996641394471}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [29, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [30, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3002688216560363}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3995628521870124}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [31, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4599107245569436}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35545272342653067}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [32, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21905772316395972}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [33, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30225186523979475}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [34, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1611382916869748}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11824700906257526}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [35, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27147461663629957}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17359862039199497}, 'layer_4_size': 5, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1662718832327227}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [36, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1896035259027695}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [37, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3897060985256203}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1594077157195362}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [38, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13464396181250873}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [39, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38547274004859367}, 'layer_2_size': 64, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23627536327943913}, 'layer_4_size': 39, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [40, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1400293556848392}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [41, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14432071788110462}, 'layer_2_size': 44, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1877390820355882}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [42, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.49815714481728435}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10719070900249368}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [43, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 89, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.19275846177406197}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22571997875034966}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [44, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12535511325541668}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24701655990175164}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [45, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22493601036887703}, 'layer_2_size': 80, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 12, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.29306150019762184}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3819503917456204}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [46, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.26829488673006174}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12943210900540214}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 53, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.29719529485172025}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [47, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2685498972279782}, 'layer_1_size': 18, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3211616980545791}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [48, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 20, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3030003037194491}, 'layer_3_size': 19, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2656113327891144}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [49, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4672537943168995}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [50, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 5, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 80, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [51, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 16, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46901823491831196}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4790271524690606}, 'layer_3_size': 74, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [52, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4757643375911187}, 'layer_4_size': 64, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [53, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [54, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [55, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 71, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4747820652699525}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [56, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18990815560638744}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.46535857208814924}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [57, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3785606602059729}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3619774967433266}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 78, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [58, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4641015318908026}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3499918539180671}, 'layer_2_size': 37, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 77, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 73, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [59, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 3, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3338500118517678}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.21733077990734936}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [60, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 59, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [61, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4152912975829045}, 'layer_2_size': 52, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 56, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4164250803634669}, 'layer_4_size': 47, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35456697442197815}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [62, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3456005888407692}, 'layer_1_size': 9, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37175830388274134}, 'layer_2_size': 66, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22649751744615698}, 'layer_4_size': 48, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [63, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4902284534495991}, 'layer_2_size': 44, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35875723960368033}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42650755712741795}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [64, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18061994196690004}, 'layer_1_size': 74, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4529665353191762}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 37, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [65, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4601707524689007}, 'layer_1_size': 41, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2907473276287246}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 12, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [66, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 94, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 57, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25266814823112443}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [67, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [68, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [69, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [70, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 23, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [71, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12242982296970335}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 82, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [72, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 54, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 71, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.418888951595746}, 'layer_4_size': 92, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23718605775698545}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [73, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2272457618830385}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [74, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10452094708784201}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [75, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.30862273470417034}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [76, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 45, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 57, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41506802225004635}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [77, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [78, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1357948231428067}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17180747868109397}, 'layer_3_size': 30, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12845280205534437}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.31034787669558805}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [79, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2707532519413747}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [80, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1864451741191601}, 'layer_1_size': 48, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35581645225868186}, 'layer_4_size': 7, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [0, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.18990815560638744}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 37, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 8, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.46535857208814924}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [1, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4902284534495991}, 'layer_2_size': 44, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35875723960368033}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42650755712741795}, 'layer_4_size': 95, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 95, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [2, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.147847915715405}, 'layer_4_size': 4, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [3, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1864451741191601}, 'layer_1_size': 48, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35581645225868186}, 'layer_4_size': 7, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [4, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.49815714481728435}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10719070900249368}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [5, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1896035259027695}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [6, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.26829488673006174}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12943210900540214}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 53, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.29719529485172025}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [7, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3835566305063546}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23192250650061058}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 18, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [8, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [9, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14432071788110462}, 'layer_2_size': 44, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1877390820355882}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [10, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [11, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22493601036887703}, 'layer_2_size': 80, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 12, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.29306150019762184}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3819503917456204}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [12, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1379856625934365}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [13, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 59, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [14, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 39, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [15, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 86, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2272457618830385}, 'layer_3_size': 13, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [16, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [17, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12535511325541668}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24701655990175164}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [18, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [19, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 2, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 45, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [20, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3706207963655883}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4962179305960813}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20464899395687103}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [21, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 5, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 80, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [22, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3002688216560363}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3995628521870124}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [23, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 16, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46901823491831196}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4790271524690606}, 'layer_3_size': 74, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [24, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [25, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [26, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27147461663629957}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17359862039199497}, 'layer_4_size': 5, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1662718832327227}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]] 

*** 27.0 configurations x 3.0 iterations each

80 | Thu Sep 27 17:40:43 2018 | lowest loss so far: 0.1432 (run 55)

{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  79 | activation: tanh    | extras: dropout - rate: 24.5% 
layer 2 | size:  60 | activation: sigmoid | extras: None 
layer 3 | size:  92 | activation: relu    | extras: None 
layer 4 | size:  54 | activation: tanh    | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:43 - loss: 0.5045
 368/6530 [>.............................] - ETA: 14s - loss: 0.2591 
 736/6530 [==>...........................] - ETA: 7s - loss: 0.2328 {'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  63 | activation: sigmoid | extras: None 
layer 2 | size:  74 | activation: tanh    | extras: None 
layer 3 | size:  17 | activation: tanh    | extras: batchnorm 
layer 4 | size:  54 | activation: relu    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 22s - loss: 0.5507
1120/6530 [====>.........................] - ETA: 4s - loss: 0.2178
5120/6530 [======================>.......] - ETA: 0s - loss: 0.3683 
1536/6530 [======>.......................] - ETA: 3s - loss: 0.2106
6530/6530 [==============================] - 1s 157us/step - loss: 0.3206 - val_loss: 0.2107

2000/6530 [========>.....................] - ETA: 2s - loss: 0.2062
2464/6530 [==========>...................] - ETA: 1s - loss: 0.2031
2928/6530 [============>.................] - ETA: 1s - loss: 0.1987
3360/6530 [==============>...............] - ETA: 1s - loss: 0.1954
3712/6530 [================>.............] - ETA: 0s - loss: 0.1946
4128/6530 [=================>............] - ETA: 0s - loss: 0.1945
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1939{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  64 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  73 | activation: tanh    | extras: batchnorm 
layer 3 | size:  47 | activation: tanh    | extras: dropout - rate: 46.9% 
layer 4 | size:  46 | activation: relu    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:57 - loss: 1.6924
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1928
 272/6530 [>.............................] - ETA: 24s - loss: 1.0913 
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1920
 528/6530 [=>............................] - ETA: 12s - loss: 0.7189
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1909
 784/6530 [==>...........................] - ETA: 8s - loss: 0.5772 
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1903
1056/6530 [===>..........................] - ETA: 6s - loss: 0.4932
6530/6530 [==============================] - 2s 258us/step - loss: 0.1893 - val_loss: 0.2167

1328/6530 [=====>........................] - ETA: 5s - loss: 0.4388
1584/6530 [======>.......................] - ETA: 4s - loss: 0.4072
1840/6530 [=======>......................] - ETA: 3s - loss: 0.3803
2096/6530 [========>.....................] - ETA: 3s - loss: 0.3572
2336/6530 [=========>....................] - ETA: 2s - loss: 0.3373
2608/6530 [==========>...................] - ETA: 2s - loss: 0.3194
# training | RMSE: 0.4543, MAE: 0.3567
worker 2  xfile  [2, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.45433069745331084, 'rmse': 0.45433069745331084, 'mae': 0.3566936310324071, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  89 | activation: relu    | extras: dropout - rate: 18.3% 
layer 2 | size:  65 | activation: tanh    | extras: batchnorm 
layer 3 | size:  17 | activation: tanh    | extras: dropout - rate: 36.1% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:18 - loss: 2.0586
2880/6530 [============>.................] - ETA: 2s - loss: 0.3017
 352/6530 [>.............................] - ETA: 6s - loss: 0.9806  
3136/6530 [=============>................] - ETA: 1s - loss: 0.2897
 688/6530 [==>...........................] - ETA: 3s - loss: 0.7319
3360/6530 [==============>...............] - ETA: 1s - loss: 0.2783
1024/6530 [===>..........................] - ETA: 2s - loss: 0.5864
3616/6530 [===============>..............] - ETA: 1s - loss: 0.2682
1376/6530 [=====>........................] - ETA: 2s - loss: 0.4877
3872/6530 [================>.............] - ETA: 1s - loss: 0.2598
1712/6530 [======>.......................] - ETA: 1s - loss: 0.4295
4128/6530 [=================>............] - ETA: 1s - loss: 0.2513
2016/6530 [========>.....................] - ETA: 1s - loss: 0.3901
4384/6530 [===================>..........] - ETA: 0s - loss: 0.2431
2368/6530 [=========>....................] - ETA: 1s - loss: 0.3510
4640/6530 [====================>.........] - ETA: 0s - loss: 0.2370
2688/6530 [===========>..................] - ETA: 1s - loss: 0.3254
4896/6530 [=====================>........] - ETA: 0s - loss: 0.2309
3056/6530 [=============>................] - ETA: 0s - loss: 0.3029
5168/6530 [======================>.......] - ETA: 0s - loss: 0.2252
3392/6530 [==============>...............] - ETA: 0s - loss: 0.2832
# training | RMSE: 0.2739, MAE: 0.2151
worker 0  xfile  [0, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24508931426614333}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 54, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.27387411146519486, 'rmse': 0.27387411146519486, 'mae': 0.21506428209384285, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  89 | activation: tanh    | extras: dropout - rate: 20.9% 
layer 2 | size:  62 | activation: sigmoid | extras: None 
layer 3 | size:  73 | activation: relu    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 1:37 - loss: 0.6361
5440/6530 [=======================>......] - ETA: 0s - loss: 0.2198
3744/6530 [================>.............] - ETA: 0s - loss: 0.2680
 416/6530 [>.............................] - ETA: 4s - loss: 0.2268  
5696/6530 [=========================>....] - ETA: 0s - loss: 0.2149
4096/6530 [=================>............] - ETA: 0s - loss: 0.2533
 832/6530 [==>...........................] - ETA: 2s - loss: 0.2072
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2100
4448/6530 [===================>..........] - ETA: 0s - loss: 0.2403
1232/6530 [====>.........................] - ETA: 1s - loss: 0.1981
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2065
4800/6530 [=====================>........] - ETA: 0s - loss: 0.2305
1648/6530 [======>.......................] - ETA: 1s - loss: 0.1925
6464/6530 [============================>.] - ETA: 0s - loss: 0.2028
5152/6530 [======================>.......] - ETA: 0s - loss: 0.2202
2064/6530 [========>.....................] - ETA: 1s - loss: 0.1917
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2110
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1886
6530/6530 [==============================] - 2s 370us/step - loss: 0.2017 - val_loss: 0.0438

5872/6530 [=========================>....] - ETA: 0s - loss: 0.2022
2896/6530 [============>.................] - ETA: 0s - loss: 0.1875
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1949
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1850
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1847
6530/6530 [==============================] - 1s 211us/step - loss: 0.1896 - val_loss: 0.0576

4080/6530 [=================>............] - ETA: 0s - loss: 0.1843
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1845
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1839
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1833
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1827
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1823
6530/6530 [==============================] - 1s 169us/step - loss: 0.1815 - val_loss: 0.1814

# training | RMSE: 0.2329, MAE: 0.1858
worker 2  xfile  [3, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1830453648773626}, 'layer_1_size': 89, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36110923197292344}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 9, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1782147798722572}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.23290918479771722, 'rmse': 0.23290918479771722, 'mae': 0.1857776743985883, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  27 | activation: relu    | extras: dropout - rate: 37.1% 
layer 2 | size:  77 | activation: relu    | extras: dropout - rate: 49.6% 
layer 3 | size:  48 | activation: sigmoid | extras: dropout - rate: 20.5% 
layer 4 | size:  13 | activation: sigmoid | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 28s - loss: 0.8034
1216/6530 [====>.........................] - ETA: 1s - loss: 0.6608 
# training | RMSE: 0.2056, MAE: 0.1635
worker 1  xfile  [1, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4688037995543516}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20561758402975297, 'rmse': 0.20561758402975297, 'mae': 0.16350082769938348, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  31 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  57 | activation: relu    | extras: dropout - rate: 22.8% 
layer 3 | size:   4 | activation: sigmoid | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 9s - loss: 0.1653
2240/6530 [=========>....................] - ETA: 0s - loss: 0.5306
4096/6530 [=================>............] - ETA: 0s - loss: 0.1050
3456/6530 [==============>...............] - ETA: 0s - loss: 0.4258
4736/6530 [====================>.........] - ETA: 0s - loss: 0.3681
6530/6530 [==============================] - 1s 81us/step - loss: 0.0890 - val_loss: 0.0429

6016/6530 [==========================>...] - ETA: 0s - loss: 0.3344
6530/6530 [==============================] - 1s 92us/step - loss: 0.3247 - val_loss: 0.2091

# training | RMSE: 0.2197, MAE: 0.1807
worker 0  xfile  [4, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2090886416592586}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 62, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.21969950026056517, 'rmse': 0.21969950026056517, 'mae': 0.18072384169099978, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  90 | activation: sigmoid | extras: None 
layer 2 | size:  93 | activation: sigmoid | extras: None 
layer 3 | size:  74 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  65 | activation: relu    | extras: dropout - rate: 35.5% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 16s - loss: 0.6286
3072/6530 [=============>................] - ETA: 0s - loss: 0.2988 
6144/6530 [===========================>..] - ETA: 0s - loss: 0.2342
6530/6530 [==============================] - 0s 75us/step - loss: 0.2300 - val_loss: 0.8486

# training | RMSE: 0.2578, MAE: 0.2112
worker 2  xfile  [6, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3706207963655883}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4962179305960813}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20464899395687103}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 59, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.2578200870767448, 'rmse': 0.2578200870767448, 'mae': 0.2112159819868957, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  83 | activation: relu    | extras: batchnorm 
layer 2 | size:  29 | activation: tanh    | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 15s - loss: 0.6802
3072/6530 [=============>................] - ETA: 0s - loss: 0.3618 
6016/6530 [==========================>...] - ETA: 0s - loss: 0.2586
6530/6530 [==============================] - 0s 70us/step - loss: 0.2489 - val_loss: 0.1605

# training | RMSE: 0.2059, MAE: 0.1654
worker 1  xfile  [5, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22759357904567784}, 'layer_2_size': 57, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 4, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40909023674512535}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.20585709302544397, 'rmse': 0.20585709302544397, 'mae': 0.16542987848086604, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  49 | activation: tanh    | extras: dropout - rate: 38.7% 
layer 2 | size:  90 | activation: tanh    | extras: batchnorm 
layer 3 | size:  58 | activation: relu    | extras: dropout - rate: 30.9% 
layer 4 | size:  23 | activation: sigmoid | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 51s - loss: 0.2243
# training | RMSE: 0.9491, MAE: 0.8570
worker 0  xfile  [7, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35500839960594033}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4901778376958633}, 'layer_5_size': 79, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.949085503471072, 'rmse': 0.949085503471072, 'mae': 0.8570311591795735, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  87 | activation: tanh    | extras: dropout - rate: 45.0% 
layer 2 | size:   8 | activation: relu    | extras: None 
layer 3 | size:  33 | activation: tanh    | extras: None 
layer 4 | size:  91 | activation: sigmoid | extras: None 
layer 5 | size:  22 | activation: relu    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:09 - loss: 1.2537
 896/6530 [===>..........................] - ETA: 3s - loss: 0.1619 
 576/6530 [=>............................] - ETA: 4s - loss: 0.3516  
1856/6530 [=======>......................] - ETA: 1s - loss: 0.1075
1216/6530 [====>.........................] - ETA: 1s - loss: 0.2180
2944/6530 [============>.................] - ETA: 0s - loss: 0.0851
1888/6530 [=======>......................] - ETA: 1s - loss: 0.1635
4032/6530 [=================>............] - ETA: 0s - loss: 0.0735
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1357
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0671
3264/6530 [=============>................] - ETA: 0s - loss: 0.1158
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0622
4000/6530 [=================>............] - ETA: 0s - loss: 0.1029
# training | RMSE: 0.1944, MAE: 0.1551
worker 2  xfile  [9, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.19444634507967123, 'rmse': 0.19444634507967123, 'mae': 0.15512155739259711, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  24 | activation: sigmoid | extras: dropout - rate: 38.4% 
layer 2 | size:  27 | activation: sigmoid | extras: dropout - rate: 23.2% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 5s - loss: 0.3745
6530/6530 [==============================] - 1s 137us/step - loss: 0.0612 - val_loss: 0.0380

4672/6530 [====================>.........] - ETA: 0s - loss: 0.0940
6530/6530 [==============================] - 0s 45us/step - loss: 0.0935 - val_loss: 0.0673

5344/6530 [=======================>......] - ETA: 0s - loss: 0.0876
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0827
6530/6530 [==============================] - 1s 136us/step - loss: 0.0793 - val_loss: 0.0390

# training | RMSE: 0.2608, MAE: 0.2136
worker 2  xfile  [11, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3835566305063546}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23192250650061058}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 43, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 18, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.26077592574063185, 'rmse': 0.26077592574063185, 'mae': 0.2136343914831388, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  13 | activation: sigmoid | extras: None 
layer 2 | size:  94 | activation: relu    | extras: dropout - rate: 22.0% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 1:25 - loss: 0.8177
 592/6530 [=>............................] - ETA: 2s - loss: 0.2227  
1168/6530 [====>.........................] - ETA: 1s - loss: 0.1980
1744/6530 [=======>......................] - ETA: 0s - loss: 0.1858
# training | RMSE: 0.1910, MAE: 0.1503
worker 1  xfile  [8, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38664454623440137}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3089910751745466}, 'layer_3_size': 58, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.1909963451123023, 'rmse': 0.1909963451123023, 'mae': 0.1502959830543466, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:   6 | activation: sigmoid | extras: dropout - rate: 20.2% 
layer 2 | size:  16 | activation: relu    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 13s - loss: 2.7146
2384/6530 [=========>....................] - ETA: 0s - loss: 0.1789
3712/6530 [================>.............] - ETA: 0s - loss: 2.2505 
2992/6530 [============>.................] - ETA: 0s - loss: 0.1744
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1719
6530/6530 [==============================] - 0s 61us/step - loss: 1.8858 - val_loss: 0.9419

4224/6530 [==================>...........] - ETA: 0s - loss: 0.1700
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1694
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1673
# training | RMSE: 0.1920, MAE: 0.1556
worker 0  xfile  [10, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4499329686752288}, 'layer_1_size': 87, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 33, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 91, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 22, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.1919746888041542, 'rmse': 0.1919746888041542, 'mae': 0.15558377889969358, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  63 | activation: tanh    | extras: None 
layer 2 | size:   6 | activation: tanh    | extras: dropout - rate: 38.2% 
layer 3 | size:  99 | activation: sigmoid | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:03 - loss: 0.0792
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1666
 832/6530 [==>...........................] - ETA: 2s - loss: 0.0708  
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0656
6530/6530 [==============================] - 1s 121us/step - loss: 0.1657 - val_loss: 0.1842

2464/6530 [==========>...................] - ETA: 0s - loss: 0.0601
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0561
4128/6530 [=================>............] - ETA: 0s - loss: 0.0533
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0516
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0503
6530/6530 [==============================] - 1s 118us/step - loss: 0.0498 - val_loss: 0.0461

# training | RMSE: 1.0218, MAE: 0.9143
worker 1  xfile  [13, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20176026945598938}, 'layer_1_size': 6, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 16, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 87, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11281343145256294}, 'layer_4_size': 31, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.14994890442267358}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 1.0217624080335945, 'rmse': 1.0217624080335945, 'mae': 0.9142615280815155, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:   3 | activation: tanh    | extras: batchnorm 
layer 2 | size:  15 | activation: tanh    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:12 - loss: 0.6216
 320/6530 [>.............................] - ETA: 10s - loss: 0.6558 
 608/6530 [=>............................] - ETA: 5s - loss: 0.5718 
 944/6530 [===>..........................] - ETA: 3s - loss: 0.5106
# training | RMSE: 0.2222, MAE: 0.1840
worker 2  xfile  [12, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22029350655847801}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 44, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20479056109862254}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2222211860747114, 'rmse': 0.2222211860747114, 'mae': 0.18397093119156083, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  27 | activation: sigmoid | extras: dropout - rate: 16.9% 
layer 2 | size:  99 | activation: relu    | extras: dropout - rate: 46.4% 
layer 3 | size:  57 | activation: relu    | extras: dropout - rate: 31.6% 
layer 4 | size:  87 | activation: relu    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:20 - loss: 0.5029
1296/6530 [====>.........................] - ETA: 2s - loss: 0.4475
# training | RMSE: 0.2135, MAE: 0.1756
worker 0  xfile  [14, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 63, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38217105892249426}, 'layer_2_size': 6, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30502556147053894}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.21351072176423466, 'rmse': 0.21351072176423466, 'mae': 0.17557865500180073, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  58 | activation: tanh    | extras: None 
layer 2 | size:   8 | activation: tanh    | extras: None 
layer 3 | size:  80 | activation: tanh    | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 1:36 - loss: 1.0736
 416/6530 [>.............................] - ETA: 5s - loss: 0.2021  
1664/6530 [======>.......................] - ETA: 2s - loss: 0.4019
 560/6530 [=>............................] - ETA: 3s - loss: 0.3216  
 816/6530 [==>...........................] - ETA: 3s - loss: 0.1423
2016/6530 [========>.....................] - ETA: 1s - loss: 0.3689
1088/6530 [===>..........................] - ETA: 1s - loss: 0.2556
1200/6530 [====>.........................] - ETA: 2s - loss: 0.1220
2352/6530 [=========>....................] - ETA: 1s - loss: 0.3355
1648/6530 [======>.......................] - ETA: 1s - loss: 0.2277
1552/6530 [======>.......................] - ETA: 1s - loss: 0.1150
2736/6530 [===========>..................] - ETA: 1s - loss: 0.3019
2176/6530 [========>.....................] - ETA: 0s - loss: 0.2111
1952/6530 [=======>......................] - ETA: 1s - loss: 0.1061
3120/6530 [=============>................] - ETA: 1s - loss: 0.2738
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1994
2336/6530 [=========>....................] - ETA: 1s - loss: 0.1029
3504/6530 [===============>..............] - ETA: 0s - loss: 0.2536
3248/6530 [=============>................] - ETA: 0s - loss: 0.1911
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0979
3840/6530 [================>.............] - ETA: 0s - loss: 0.2375
3776/6530 [================>.............] - ETA: 0s - loss: 0.1836
3136/6530 [=============>................] - ETA: 0s - loss: 0.0952
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2220
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1783
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0920
4624/6530 [====================>.........] - ETA: 0s - loss: 0.2093
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1731
3952/6530 [=================>............] - ETA: 0s - loss: 0.0905
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1980
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1686
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0894
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1655
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1888
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0871
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1807
6416/6530 [============================>.] - ETA: 0s - loss: 0.1618
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0862
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1729
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0853
6530/6530 [==============================] - 1s 140us/step - loss: 0.1612 - val_loss: 0.1406

5920/6530 [==========================>...] - ETA: 0s - loss: 0.0843
6320/6530 [============================>.] - ETA: 0s - loss: 0.0829
6530/6530 [==============================] - 1s 225us/step - loss: 0.1667 - val_loss: 0.0563

6530/6530 [==============================] - 1s 194us/step - loss: 0.0823 - val_loss: 0.0732

# training | RMSE: 0.1712, MAE: 0.1328
worker 0  xfile  [17, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 8, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 80, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1302302520515125}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 62, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.17119929375756285, 'rmse': 0.17119929375756285, 'mae': 0.1327715363584132, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  39 | activation: tanh    | extras: None 
layer 2 | size:  34 | activation: tanh    | extras: None 
layer 3 | size:  48 | activation: sigmoid | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 11s - loss: 1.7042
3840/6530 [================>.............] - ETA: 0s - loss: 0.1724 
6530/6530 [==============================] - 0s 56us/step - loss: 0.1169 - val_loss: 0.0567

# training | RMSE: 0.2411, MAE: 0.1920
worker 1  xfile  [15, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.147847915715405}, 'layer_4_size': 4, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.24112061864512194, 'rmse': 0.24112061864512194, 'mae': 0.19197719169117355, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  19 | activation: sigmoid | extras: dropout - rate: 13.8% 
layer 2 | size:  32 | activation: sigmoid | extras: None 
layer 3 | size:  79 | activation: relu    | extras: dropout - rate: 37.0% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 7s - loss: 0.7353
6530/6530 [==============================] - 0s 59us/step - loss: 0.2530 - val_loss: 0.2119

# training | RMSE: 0.2694, MAE: 0.2188
worker 2  xfile  [16, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1694124464538399}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46390761362282207}, 'layer_2_size': 99, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3161741413669089}, 'layer_3_size': 57, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 12, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.26940377375396285, 'rmse': 0.26940377375396285, 'mae': 0.2187831493624938, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  77 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  29 | activation: sigmoid | extras: dropout - rate: 13.8% 
layer 3 | size:  68 | activation: tanh    | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 22s - loss: 0.4316
1920/6530 [=======>......................] - ETA: 1s - loss: 0.2644 
3968/6530 [=================>............] - ETA: 0s - loss: 0.2436
6144/6530 [===========================>..] - ETA: 0s - loss: 0.2290
# training | RMSE: 0.2307, MAE: 0.1843
worker 0  xfile  [18, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.23074099910571452, 'rmse': 0.23074099910571452, 'mae': 0.1843176113189077, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  48 | activation: tanh    | extras: None 
layer 2 | size:  50 | activation: relu    | extras: None 
layer 3 | size:  39 | activation: sigmoid | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:10 - loss: 0.8323
 480/6530 [=>............................] - ETA: 4s - loss: 0.3413  
6530/6530 [==============================] - 1s 106us/step - loss: 0.2261 - val_loss: 0.1799

 912/6530 [===>..........................] - ETA: 2s - loss: 0.2165
1360/6530 [=====>........................] - ETA: 1s - loss: 0.1669
1776/6530 [=======>......................] - ETA: 1s - loss: 0.1447
2224/6530 [=========>....................] - ETA: 1s - loss: 0.1296
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1198
# training | RMSE: 0.2605, MAE: 0.2131
worker 1  xfile  [19, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13811544797959066}, 'layer_1_size': 19, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37009762923770795}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 90, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2604627506378063, 'rmse': 0.2604627506378063, 'mae': 0.2131277219790262, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  27 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  28 | activation: tanh    | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:30 - loss: 0.6958
3088/6530 [=============>................] - ETA: 0s - loss: 0.1108
 768/6530 [==>...........................] - ETA: 3s - loss: 0.6267  
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1047
1536/6530 [======>.......................] - ETA: 1s - loss: 0.4782
3968/6530 [=================>............] - ETA: 0s - loss: 0.0995
2304/6530 [=========>....................] - ETA: 1s - loss: 0.3784
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0947
2976/6530 [============>.................] - ETA: 0s - loss: 0.3321
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0902
3744/6530 [================>.............] - ETA: 0s - loss: 0.2991
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0860
# training | RMSE: 0.2256, MAE: 0.1755
worker 2  xfile  [20, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1379856625934365}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 68, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.22556560846911952, 'rmse': 0.22556560846911952, 'mae': 0.1755384187017281, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  77 | activation: sigmoid | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 28s - loss: 0.7305
4512/6530 [===================>..........] - ETA: 0s - loss: 0.2787
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0826
2624/6530 [===========>..................] - ETA: 0s - loss: 0.2697 
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2640
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0798
5184/6530 [======================>.......] - ETA: 0s - loss: 0.2191
6016/6530 [==========================>...] - ETA: 0s - loss: 0.2518
6530/6530 [==============================] - 0s 70us/step - loss: 0.2074 - val_loss: 0.1685

6530/6530 [==============================] - 1s 176us/step - loss: 0.0776 - val_loss: 0.0428

6530/6530 [==============================] - 1s 147us/step - loss: 0.2451 - val_loss: 0.2972

# training | RMSE: 0.2057, MAE: 0.1634
worker 2  xfile  [23, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 79, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3541750557794504}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 57, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.20572730164821268, 'rmse': 0.20572730164821268, 'mae': 0.16338492578280436, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  32 | activation: relu    | extras: dropout - rate: 11.2% 
layer 2 | size:  72 | activation: tanh    | extras: dropout - rate: 10.1% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:08 - loss: 0.4564
 368/6530 [>.............................] - ETA: 6s - loss: 0.2458  
 720/6530 [==>...........................] - ETA: 3s - loss: 0.1742
1088/6530 [===>..........................] - ETA: 2s - loss: 0.1409
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1213
1984/6530 [========>.....................] - ETA: 1s - loss: 0.1069
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0969
3024/6530 [============>.................] - ETA: 0s - loss: 0.0902
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0833
4128/6530 [=================>............] - ETA: 0s - loss: 0.0787
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0748
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0721
# training | RMSE: 0.2035, MAE: 0.1618
worker 0  xfile  [21, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 39, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 14, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2034654510549401, 'rmse': 0.2034654510549401, 'mae': 0.1618458420688361, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  22 | activation: sigmoid | extras: dropout - rate: 38.4% 
layer 2 | size:  63 | activation: tanh    | extras: batchnorm 
layer 3 | size:   7 | activation: relu    | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 57s - loss: 0.6768
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0699
1152/6530 [====>.........................] - ETA: 2s - loss: 0.6143 
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0674
# training | RMSE: 0.3649, MAE: 0.2987
worker 1  xfile  [22, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42750165664118256}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40234224544075115}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.36489891389570295, 'rmse': 0.36489891389570295, 'mae': 0.29869774767619084, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  81 | activation: tanh    | extras: None 
layer 2 | size:  60 | activation: relu    | extras: batchnorm 
layer 3 | size:  76 | activation: tanh    | extras: dropout - rate: 36.3% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:56 - loss: 0.8708
2240/6530 [=========>....................] - ETA: 1s - loss: 0.5138
 576/6530 [=>............................] - ETA: 6s - loss: 0.5623  
3520/6530 [===============>..............] - ETA: 0s - loss: 0.4440
6530/6530 [==============================] - 1s 170us/step - loss: 0.0656 - val_loss: 0.0517

1152/6530 [====>.........................] - ETA: 3s - loss: 0.4473
4672/6530 [====================>.........] - ETA: 0s - loss: 0.4037
1760/6530 [=======>......................] - ETA: 1s - loss: 0.3857
5824/6530 [=========================>....] - ETA: 0s - loss: 0.3777
2336/6530 [=========>....................] - ETA: 1s - loss: 0.3544
2912/6530 [============>.................] - ETA: 1s - loss: 0.3274
6530/6530 [==============================] - 1s 142us/step - loss: 0.3633 - val_loss: 0.2572

3456/6530 [==============>...............] - ETA: 0s - loss: 0.3092
4032/6530 [=================>............] - ETA: 0s - loss: 0.2941
4640/6530 [====================>.........] - ETA: 0s - loss: 0.2799
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2702
5920/6530 [==========================>...] - ETA: 0s - loss: 0.2607
6530/6530 [==============================] - 1s 184us/step - loss: 0.2517 - val_loss: 0.2114

# training | RMSE: 0.2246, MAE: 0.1721
worker 2  xfile  [24, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1121895844612825}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10065942982829018}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4642438706544345}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.461764616219107}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.22456140137291297, 'rmse': 0.22456140137291297, 'mae': 0.1721402673947596, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  56 | activation: relu    | extras: None 
layer 2 | size:  34 | activation: sigmoid | extras: dropout - rate: 47.2% 
layer 3 | size:  22 | activation: sigmoid | extras: None 
layer 4 | size:  82 | activation: sigmoid | extras: None 
layer 5 | size:  54 | activation: tanh    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:07 - loss: 1.4543
 384/6530 [>.............................] - ETA: 8s - loss: 0.3365  
 784/6530 [==>...........................] - ETA: 4s - loss: 0.2000
1232/6530 [====>.........................] - ETA: 2s - loss: 0.1547
1616/6530 [======>.......................] - ETA: 2s - loss: 0.1323
# training | RMSE: 0.3162, MAE: 0.2565
worker 0  xfile  [25, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38392317341419036}, 'layer_1_size': 22, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 63, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2124436695855239}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.3162097691032418, 'rmse': 0.3162097691032418, 'mae': 0.25650126072206186, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  71 | activation: sigmoid | extras: dropout - rate: 10.6% 
layer 2 | size:  34 | activation: sigmoid | extras: None 
layer 3 | size:  27 | activation: relu    | extras: None 
layer 4 | size:  98 | activation: relu    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 51s - loss: 0.7057
2000/6530 [========>.....................] - ETA: 1s - loss: 0.1196
1536/6530 [======>.......................] - ETA: 1s - loss: 0.6032 
2416/6530 [==========>...................] - ETA: 1s - loss: 0.1107
3072/6530 [=============>................] - ETA: 0s - loss: 0.4296
2848/6530 [============>.................] - ETA: 1s - loss: 0.1039
4736/6530 [====================>.........] - ETA: 0s - loss: 0.3546
3248/6530 [=============>................] - ETA: 0s - loss: 0.0986
6272/6530 [===========================>..] - ETA: 0s - loss: 0.3207
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0943
4048/6530 [=================>............] - ETA: 0s - loss: 0.0903
6530/6530 [==============================] - 1s 120us/step - loss: 0.3160 - val_loss: 0.2119

4432/6530 [===================>..........] - ETA: 0s - loss: 0.0874
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0844
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0818
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0802
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0779
# training | RMSE: 0.2486, MAE: 0.1988
worker 1  xfile  [26, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3634488551903946}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19465049393506934}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.24861559583080184, 'rmse': 0.24861559583080184, 'mae': 0.1988478443698782, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  76 | activation: relu    | extras: batchnorm 
layer 2 | size:  47 | activation: sigmoid | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 26s - loss: 0.6166
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0767
3072/6530 [=============>................] - ETA: 0s - loss: 0.2534 
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2004
6530/6530 [==============================] - 1s 216us/step - loss: 0.0752 - val_loss: 0.0404

6530/6530 [==============================] - 1s 110us/step - loss: 0.1941 - val_loss: 0.1412

# training | RMSE: 0.2616, MAE: 0.2145
worker 0  xfile  [28, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10568616291913142}, 'layer_1_size': 71, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 98, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4282996641394471}, 'layer_5_size': 69, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.26161854527564565, 'rmse': 0.26161854527564565, 'mae': 0.21448844567244854, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  88 | activation: sigmoid | extras: None 
layer 2 | size:  64 | activation: relu    | extras: dropout - rate: 30.0% 
layer 3 | size:  60 | activation: sigmoid | extras: dropout - rate: 40.0% 
layer 4 | size:  86 | activation: relu    | extras: None 
layer 5 | size:   7 | activation: sigmoid | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 51s - loss: 0.2945
1024/6530 [===>..........................] - ETA: 3s - loss: 0.2528 
2112/6530 [========>.....................] - ETA: 1s - loss: 0.2351
# training | RMSE: 0.1995, MAE: 0.1618
worker 2  xfile  [27, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 56, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4720637156453483}, 'layer_2_size': 34, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 22, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.19949572928557258, 'rmse': 0.19949572928557258, 'mae': 0.16180943177068846, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  78 | activation: tanh    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 17s - loss: 0.6026
3200/6530 [=============>................] - ETA: 0s - loss: 0.2300
3968/6530 [=================>............] - ETA: 0s - loss: 0.4756 
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2260
5312/6530 [=======================>......] - ETA: 0s - loss: 0.2205
6530/6530 [==============================] - 1s 79us/step - loss: 0.3922 - val_loss: 0.1690

6464/6530 [============================>.] - ETA: 0s - loss: 0.2159
6530/6530 [==============================] - 1s 137us/step - loss: 0.2157 - val_loss: 0.1674

# training | RMSE: 0.1796, MAE: 0.1430
worker 1  xfile  [29, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 76, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.17961634533654597, 'rmse': 0.17961634533654597, 'mae': 0.1430496255213611, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  50 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  67 | activation: sigmoid | extras: dropout - rate: 46.0% 
layer 3 | size:  38 | activation: tanh    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:59 - loss: 0.7574
 544/6530 [=>............................] - ETA: 7s - loss: 0.3242  
1120/6530 [====>.........................] - ETA: 3s - loss: 0.2612
1792/6530 [=======>......................] - ETA: 1s - loss: 0.2333
2432/6530 [==========>...................] - ETA: 1s - loss: 0.2170
3072/6530 [=============>................] - ETA: 0s - loss: 0.2083
3776/6530 [================>.............] - ETA: 0s - loss: 0.2020
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1970
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1936
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1911
6528/6530 [============================>.] - ETA: 0s - loss: 0.1893
6530/6530 [==============================] - 1s 184us/step - loss: 0.1893 - val_loss: 0.1811

# training | RMSE: 0.4091, MAE: 0.3662
worker 2  xfile  [32, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21905772316395972}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.40912709249982054, 'rmse': 0.40912709249982054, 'mae': 0.36616288798160707, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  34 | activation: sigmoid | extras: None 
layer 2 | size:  33 | activation: relu    | extras: None 
layer 3 | size: 100 | activation: relu    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 24s - loss: 1.1886
2688/6530 [===========>..................] - ETA: 0s - loss: 0.4649 
5504/6530 [========================>.....] - ETA: 0s - loss: 0.3317
# training | RMSE: 0.2061, MAE: 0.1686
worker 0  xfile  [30, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 88, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3002688216560363}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3995628521870124}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20607960070134793, 'rmse': 0.20607960070134793, 'mae': 0.16857823547177542, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  18 | activation: sigmoid | extras: dropout - rate: 16.1% 
layer 2 | size:  38 | activation: tanh    | extras: dropout - rate: 11.8% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:08 - loss: 2.2806
6530/6530 [==============================] - 1s 104us/step - loss: 0.3071 - val_loss: 0.1833

 368/6530 [>.............................] - ETA: 8s - loss: 0.7574  
 800/6530 [==>...........................] - ETA: 4s - loss: 0.4535
1184/6530 [====>.........................] - ETA: 2s - loss: 0.3679
1536/6530 [======>.......................] - ETA: 2s - loss: 0.3155
1920/6530 [=======>......................] - ETA: 1s - loss: 0.2799
2304/6530 [=========>....................] - ETA: 1s - loss: 0.2521
2656/6530 [===========>..................] - ETA: 1s - loss: 0.2326
3040/6530 [============>.................] - ETA: 1s - loss: 0.2147
3376/6530 [==============>...............] - ETA: 0s - loss: 0.2016
3776/6530 [================>.............] - ETA: 0s - loss: 0.1878
4128/6530 [=================>............] - ETA: 0s - loss: 0.1784
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1697
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1631
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1564
# training | RMSE: 0.2209, MAE: 0.1821
worker 2  xfile  [33, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30225186523979475}, 'layer_5_size': 5, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.2208821917541853, 'rmse': 0.2208821917541853, 'mae': 0.18211561591937103, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  46 | activation: relu    | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 37s - loss: 0.9422
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1501
2112/6530 [========>.....................] - ETA: 0s - loss: 0.4332 
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1445
4352/6530 [==================>...........] - ETA: 0s - loss: 0.3480
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1392
6528/6530 [============================>.] - ETA: 0s - loss: 0.3079
6530/6530 [==============================] - 1s 89us/step - loss: 0.3079 - val_loss: 0.2275

6530/6530 [==============================] - 1s 225us/step - loss: 0.1353 - val_loss: 0.0502

# training | RMSE: 0.2187, MAE: 0.1800
worker 1  xfile  [31, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4599107245569436}, 'layer_2_size': 67, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35545272342653067}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2186529963665769, 'rmse': 0.2186529963665769, 'mae': 0.18000033289616155, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  45 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  66 | activation: tanh    | extras: None 
layer 3 | size:  84 | activation: relu    | extras: dropout - rate: 27.1% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:04 - loss: 0.5906
 576/6530 [=>............................] - ETA: 6s - loss: 0.1402  
1184/6530 [====>.........................] - ETA: 3s - loss: 0.0949
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0781
2592/6530 [==========>...................] - ETA: 1s - loss: 0.0706
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0635
4128/6530 [=================>............] - ETA: 0s - loss: 0.0599
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0573
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0542
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0523
6530/6530 [==============================] - 1s 182us/step - loss: 0.0516 - val_loss: 0.0345

# training | RMSE: 0.2762, MAE: 0.2226
worker 2  xfile  [36, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1896035259027695}, 'layer_4_size': 18, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.27619233596833614, 'rmse': 0.27619233596833614, 'mae': 0.22264391000807218, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  65 | activation: tanh    | extras: None 
layer 2 | size:  68 | activation: tanh    | extras: dropout - rate: 39.0% 
layer 3 | size:  53 | activation: relu    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 12s - loss: 0.5764
6144/6530 [===========================>..] - ETA: 0s - loss: 0.4259 
6530/6530 [==============================] - 1s 102us/step - loss: 0.4108 - val_loss: 0.1267

# training | RMSE: 0.2240, MAE: 0.1838
worker 0  xfile  [34, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1611382916869748}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.11824700906257526}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 100, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2239501787605658, 'rmse': 0.2239501787605658, 'mae': 0.1838465328409698, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  46 | activation: sigmoid | extras: dropout - rate: 13.5% 
layer 2 | size:  44 | activation: tanh    | extras: None 
layer 3 | size:  49 | activation: sigmoid | extras: None 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 14s - loss: 2.2135
6530/6530 [==============================] - 1s 106us/step - loss: 0.1731 - val_loss: 0.0682

# training | RMSE: 0.3539, MAE: 0.3048
worker 2  xfile  [37, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 65, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3897060985256203}, 'layer_2_size': 68, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1594077157195362}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.35393217494499135, 'rmse': 0.35393217494499135, 'mae': 0.3048353891689616, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  32 | activation: sigmoid | extras: None 
layer 2 | size:  73 | activation: tanh    | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 24s - loss: 0.6065
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1951 
# training | RMSE: 0.2637, MAE: 0.2162
worker 0  xfile  [38, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13464396181250873}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 44, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2636935583241373, 'rmse': 0.2636935583241373, 'mae': 0.21619528611876082, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  50 | activation: relu    | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 21s - loss: 0.4657
3072/6530 [=============>................] - ETA: 0s - loss: 0.1733
# training | RMSE: 0.1805, MAE: 0.1420
worker 1  xfile  [35, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27147461663629957}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17359862039199497}, 'layer_4_size': 5, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1662718832327227}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.18051610373449958, 'rmse': 0.18051610373449958, 'mae': 0.14203059368956775, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  79 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  64 | activation: sigmoid | extras: dropout - rate: 38.5% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 33s - loss: 2.2133
3712/6530 [================>.............] - ETA: 0s - loss: 0.3746 
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1196
2560/6530 [==========>...................] - ETA: 1s - loss: 1.2662 
4608/6530 [====================>.........] - ETA: 0s - loss: 0.8382
6530/6530 [==============================] - 1s 116us/step - loss: 0.1163 - val_loss: 0.0653

6530/6530 [==============================] - 1s 92us/step - loss: 0.3347 - val_loss: 0.2615

6530/6530 [==============================] - 1s 138us/step - loss: 0.6101 - val_loss: 0.0462

# training | RMSE: 0.2569, MAE: 0.2101
worker 2  xfile  [40, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1400293556848392}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.2568829692744277, 'rmse': 0.2568829692744277, 'mae': 0.21009799179161445, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:   9 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  37 | activation: sigmoid | extras: dropout - rate: 49.8% 
layer 3 | size:  72 | activation: tanh    | extras: dropout - rate: 10.7% 
layer 4 | size:  94 | activation: sigmoid | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:23 - loss: 0.2032
# training | RMSE: 0.3301, MAE: 0.2635
worker 0  xfile  [41, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14432071788110462}, 'layer_2_size': 44, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1877390820355882}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.3300947462553907, 'rmse': 0.3300947462553907, 'mae': 0.2635282983290291, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  48 | activation: sigmoid | extras: None 
layer 2 | size:  89 | activation: sigmoid | extras: batchnorm 
layer 3 | size:   8 | activation: relu    | extras: dropout - rate: 19.3% 
layer 4 | size:  26 | activation: relu    | extras: None 
layer 5 | size:  56 | activation: sigmoid | extras: dropout - rate: 22.6% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:21 - loss: 1.9518
 192/6530 [..............................] - ETA: 27s - loss: 0.1315 
 896/6530 [===>..........................] - ETA: 5s - loss: 1.5119  
 400/6530 [>.............................] - ETA: 13s - loss: 0.1194
1728/6530 [======>.......................] - ETA: 2s - loss: 1.0914
 640/6530 [=>............................] - ETA: 8s - loss: 0.1100 
2624/6530 [===========>..................] - ETA: 1s - loss: 0.8409
 880/6530 [===>..........................] - ETA: 6s - loss: 0.1048
3648/6530 [===============>..............] - ETA: 0s - loss: 0.6810
1136/6530 [====>.........................] - ETA: 5s - loss: 0.0973
4736/6530 [====================>.........] - ETA: 0s - loss: 0.5774
1408/6530 [=====>........................] - ETA: 4s - loss: 0.0911
5760/6530 [=========================>....] - ETA: 0s - loss: 0.5146
1680/6530 [======>.......................] - ETA: 3s - loss: 0.0872
1952/6530 [=======>......................] - ETA: 2s - loss: 0.0847
2224/6530 [=========>....................] - ETA: 2s - loss: 0.0820
6530/6530 [==============================] - 1s 193us/step - loss: 0.4769 - val_loss: 0.1793

2496/6530 [==========>...................] - ETA: 2s - loss: 0.0806
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0795
# training | RMSE: 0.2090, MAE: 0.1619
worker 1  xfile  [39, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38547274004859367}, 'layer_2_size': 64, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23627536327943913}, 'layer_4_size': 39, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2089536230764239, 'rmse': 0.2089536230764239, 'mae': 0.16189222572593726, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  93 | activation: relu    | extras: batchnorm 
layer 2 | size:  20 | activation: relu    | extras: batchnorm 
layer 3 | size:  53 | activation: relu    | extras: batchnorm 
layer 4 | size:  97 | activation: tanh    | extras: dropout - rate: 12.5% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 51s - loss: 0.9734
3024/6530 [============>.................] - ETA: 1s - loss: 0.0773
1792/6530 [=======>......................] - ETA: 2s - loss: 0.3858 
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0761
3456/6530 [==============>...............] - ETA: 1s - loss: 0.2607
3568/6530 [===============>..............] - ETA: 1s - loss: 0.0752
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2024
3824/6530 [================>.............] - ETA: 1s - loss: 0.0739
4128/6530 [=================>............] - ETA: 0s - loss: 0.0727
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0714
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0703
6530/6530 [==============================] - 1s 217us/step - loss: 0.1738 - val_loss: 0.1732

4960/6530 [=====================>........] - ETA: 0s - loss: 0.0693
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0687
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0680
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0673
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0666
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0662
6530/6530 [==============================] - 2s 336us/step - loss: 0.0655 - val_loss: 0.0395

# training | RMSE: 0.2227, MAE: 0.1766
worker 0  xfile  [43, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 89, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.19275846177406197}, 'layer_3_size': 8, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.22571997875034966}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.22265978297811298, 'rmse': 0.22265978297811298, 'mae': 0.17660320548313363, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  82 | activation: relu    | extras: batchnorm 
layer 2 | size:  80 | activation: relu    | extras: dropout - rate: 22.5% 
layer 3 | size:  12 | activation: tanh    | extras: batchnorm 
layer 4 | size:  27 | activation: tanh    | extras: dropout - rate: 29.3% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:41 - loss: 0.8620
# training | RMSE: 0.3961, MAE: 0.3178
worker 1  xfile  [44, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 53, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12535511325541668}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24701655990175164}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.3961354766333239, 'rmse': 0.3961354766333239, 'mae': 0.31777668429719624, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  58 | activation: relu    | extras: batchnorm 
layer 2 | size:  85 | activation: sigmoid | extras: dropout - rate: 26.8% 
layer 3 | size:  89 | activation: tanh    | extras: dropout - rate: 12.9% 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:33 - loss: 0.4457
 704/6530 [==>...........................] - ETA: 8s - loss: 0.7221  
 480/6530 [=>............................] - ETA: 10s - loss: 0.3008 
1408/6530 [=====>........................] - ETA: 4s - loss: 0.6256
1024/6530 [===>..........................] - ETA: 4s - loss: 0.2364 
2176/6530 [========>.....................] - ETA: 2s - loss: 0.5429
1568/6530 [======>.......................] - ETA: 2s - loss: 0.2117
2944/6530 [============>.................] - ETA: 1s - loss: 0.4742
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1867
3776/6530 [================>.............] - ETA: 0s - loss: 0.4239
2720/6530 [===========>..................] - ETA: 1s - loss: 0.1702
4608/6530 [====================>.........] - ETA: 0s - loss: 0.3881
3296/6530 [==============>...............] - ETA: 1s - loss: 0.1558
5504/6530 [========================>.....] - ETA: 0s - loss: 0.3578
3872/6530 [================>.............] - ETA: 0s - loss: 0.1444
6400/6530 [============================>.] - ETA: 0s - loss: 0.3341
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1360
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1295
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1241
6496/6530 [============================>.] - ETA: 0s - loss: 0.1190
6530/6530 [==============================] - 2s 242us/step - loss: 0.3310 - val_loss: 0.1692

6530/6530 [==============================] - 1s 221us/step - loss: 0.1186 - val_loss: 0.0791

# training | RMSE: 0.1978, MAE: 0.1566
worker 2  xfile  [42, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.49815714481728435}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10719070900249368}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.19775734818493118, 'rmse': 0.19775734818493118, 'mae': 0.15663902402952182, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  18 | activation: sigmoid | extras: dropout - rate: 26.9% 
layer 2 | size:  91 | activation: relu    | extras: batchnorm 
layer 3 | size:  79 | activation: tanh    | extras: dropout - rate: 32.1% 
layer 4 | size:  70 | activation: sigmoid | extras: batchnorm 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 21s - loss: 0.5218
3840/6530 [================>.............] - ETA: 0s - loss: 0.3060 
6530/6530 [==============================] - 1s 165us/step - loss: 0.2678 - val_loss: 0.2118

# training | RMSE: 0.2091, MAE: 0.1627
worker 0  xfile  [45, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22493601036887703}, 'layer_2_size': 80, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 12, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.29306150019762184}, 'layer_4_size': 27, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3819503917456204}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20912731593057068, 'rmse': 0.20912731593057068, 'mae': 0.16269988557082302, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  20 | activation: tanh    | extras: None 
layer 2 | size:  47 | activation: tanh    | extras: None 
layer 3 | size:  19 | activation: tanh    | extras: dropout - rate: 30.3% 
layer 4 | size:  80 | activation: tanh    | extras: dropout - rate: 26.6% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:33 - loss: 1.2808
 224/6530 [>.............................] - ETA: 20s - loss: 0.8096 
 416/6530 [>.............................] - ETA: 11s - loss: 0.6880
 608/6530 [=>............................] - ETA: 8s - loss: 0.6007 
 848/6530 [==>...........................] - ETA: 5s - loss: 0.5355
1056/6530 [===>..........................] - ETA: 4s - loss: 0.4935
1280/6530 [====>.........................] - ETA: 4s - loss: 0.4552
1504/6530 [=====>........................] - ETA: 3s - loss: 0.4234
1696/6530 [======>.......................] - ETA: 3s - loss: 0.4051
1904/6530 [=======>......................] - ETA: 2s - loss: 0.3880
2128/6530 [========>.....................] - ETA: 2s - loss: 0.3726
# training | RMSE: 0.2773, MAE: 0.2344
worker 1  xfile  [46, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 58, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.26829488673006174}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12943210900540214}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 53, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.29719529485172025}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.27730545442607407, 'rmse': 0.27730545442607407, 'mae': 0.2344025305719126, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  60 | activation: tanh    | extras: batchnorm 
layer 2 | size:  23 | activation: sigmoid | extras: dropout - rate: 46.7% 
layer 3 | size:  98 | activation: tanh    | extras: None 
layer 4 | size:  68 | activation: tanh    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:31 - loss: 1.4086
2368/6530 [=========>....................] - ETA: 2s - loss: 0.3596
1216/6530 [====>.........................] - ETA: 4s - loss: 0.1710  
2640/6530 [===========>..................] - ETA: 1s - loss: 0.3458
2240/6530 [=========>....................] - ETA: 1s - loss: 0.1324
2928/6530 [============>.................] - ETA: 1s - loss: 0.3334
3264/6530 [=============>................] - ETA: 1s - loss: 0.1179
3216/6530 [=============>................] - ETA: 1s - loss: 0.3223
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1059
3536/6530 [===============>..............] - ETA: 1s - loss: 0.3110
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0968
3872/6530 [================>.............] - ETA: 1s - loss: 0.3029
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2955
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2883
6530/6530 [==============================] - 1s 204us/step - loss: 0.0923 - val_loss: 0.1341

4816/6530 [=====================>........] - ETA: 0s - loss: 0.2814
5184/6530 [======================>.......] - ETA: 0s - loss: 0.2750
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2691
5872/6530 [=========================>....] - ETA: 0s - loss: 0.2640
# training | RMSE: 0.2616, MAE: 0.2144
worker 2  xfile  [47, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2685498972279782}, 'layer_1_size': 18, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3211616980545791}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.26163937912677593, 'rmse': 0.26163937912677593, 'mae': 0.21443773372940597, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:   5 | activation: tanh    | extras: batchnorm 
layer 2 | size:  86 | activation: sigmoid | extras: None 
layer 3 | size:  68 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  80 | activation: sigmoid | extras: None 
layer 5 | size:  39 | activation: relu    | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 46s - loss: 0.8158
6192/6530 [===========================>..] - ETA: 0s - loss: 0.2604
1920/6530 [=======>......................] - ETA: 2s - loss: 0.1222 
6512/6530 [============================>.] - ETA: 0s - loss: 0.2561
3840/6530 [================>.............] - ETA: 0s - loss: 0.0927
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0816
6530/6530 [==============================] - 2s 312us/step - loss: 0.2558 - val_loss: 0.1971

6530/6530 [==============================] - 1s 188us/step - loss: 0.0787 - val_loss: 0.0725

# training | RMSE: 0.3618, MAE: 0.3113
worker 1  xfile  [49, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4672537943168995}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 98, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 68, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.36182716144373767, 'rmse': 0.36182716144373767, 'mae': 0.3113284885310492, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  16 | activation: relu    | extras: batchnorm 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:05 - loss: 0.6950
1248/6530 [====>.........................] - ETA: 2s - loss: 0.4980  
2496/6530 [==========>...................] - ETA: 1s - loss: 0.3743
3776/6530 [================>.............] - ETA: 0s - loss: 0.3057
5152/6530 [======================>.......] - ETA: 0s - loss: 0.2698
6400/6530 [============================>.] - ETA: 0s - loss: 0.2493
6530/6530 [==============================] - 1s 150us/step - loss: 0.2476 - val_loss: 0.1680

# training | RMSE: 0.2631, MAE: 0.2122
worker 2  xfile  [50, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 5, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 80, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.26314833637935375, 'rmse': 0.26314833637935375, 'mae': 0.21216221158082782, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  28 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  15 | activation: relu    | extras: None 
layer 3 | size:  64 | activation: relu    | extras: None 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:21 - loss: 1.4918
1152/6530 [====>.........................] - ETA: 4s - loss: 0.6717  
2304/6530 [=========>....................] - ETA: 1s - loss: 0.4633
3456/6530 [==============>...............] - ETA: 0s - loss: 0.3736
4672/6530 [====================>.........] - ETA: 0s - loss: 0.3147
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2729
6530/6530 [==============================] - 1s 184us/step - loss: 0.2590 - val_loss: 0.1081

# training | RMSE: 0.2410, MAE: 0.1906
worker 0  xfile  [48, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 20, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3030003037194491}, 'layer_3_size': 19, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2656113327891144}, 'layer_4_size': 80, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.24096973998426338, 'rmse': 0.24096973998426338, 'mae': 0.19058701011776563, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  60 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  24 | activation: sigmoid | extras: None 
layer 3 | size:  79 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  31 | activation: sigmoid | extras: batchnorm 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 23s - loss: 0.8403
3584/6530 [===============>..............] - ETA: 0s - loss: 0.6521 
6530/6530 [==============================] - 1s 189us/step - loss: 0.4912 - val_loss: 0.2438

# training | RMSE: 0.2060, MAE: 0.1643
worker 1  xfile  [51, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 16, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46901823491831196}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4790271524690606}, 'layer_3_size': 74, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.20600833902811574, 'rmse': 0.20600833902811574, 'mae': 0.16429474930678592, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  24 | activation: relu    | extras: batchnorm 
layer 2 | size:  45 | activation: sigmoid | extras: None 
layer 3 | size:  86 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  60 | activation: sigmoid | extras: None 

vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 27s - loss: 0.8586
3072/6530 [=============>................] - ETA: 1s - loss: 0.5750 
5888/6530 [==========================>...] - ETA: 0s - loss: 0.4041
6530/6530 [==============================] - 1s 208us/step - loss: 0.3847 - val_loss: 0.1982

# training | RMSE: 0.2483, MAE: 0.1953
worker 1  xfile  [54, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 60, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2483081832157355, 'rmse': 0.2483081832157355, 'mae': 0.19526265831485062, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  17 | activation: tanh    | extras: dropout - rate: 37.9% 
layer 2 | size:  49 | activation: sigmoid | extras: dropout - rate: 36.2% 
layer 3 | size:  56 | activation: relu    | extras: batchnorm 
layer 4 | size:  78 | activation: sigmoid | extras: batchnorm 
layer 5 | size:  74 | activation: tanh    | extras: batchnorm 

vggnet init done 1

# training | RMSE: 0.3242, MAE: 0.2584
worker 2  xfile  [52, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 28, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4757643375911187}, 'layer_4_size': 64, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.3241659066406219, 'rmse': 0.3241659066406219, 'mae': 0.258401360555338, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  71 | activation: tanh    | extras: batchnorm 
layer 2 | size:  67 | activation: relu    | extras: batchnorm 
layer 3 | size:  23 | activation: tanh    | extras: batchnorm 
layer 4 | size:  64 | activation: tanh    | extras: dropout - rate: 47.5% 

vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

# training | RMSE: 0.2952, MAE: 0.2395
worker 0  xfile  [53, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.295241484219604, 'rmse': 0.295241484219604, 'mae': 0.23951361857756065, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  81 | activation: sigmoid | extras: dropout - rate: 19.0% 
layer 2 | size:  13 | activation: relu    | extras: None 
layer 3 | size:  37 | activation: relu    | extras: batchnorm 
layer 4 | size:   8 | activation: sigmoid | extras: batchnorm 
layer 5 | size:  52 | activation: tanh    | extras: dropout - rate: 46.5% 

vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1
