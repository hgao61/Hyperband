loading data...
None
[0, 1, 2]
s=4
T is of size 81
T=[{'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2150110655683083}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.30853774892657976}, 'layer_2_size': 45, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10136199605571763}, 'layer_3_size': 94, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22287393078460238}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 62, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 22, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19752241638812038}, 'layer_2_size': 9, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2780697801742765}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23723016883152925}, 'layer_1_size': 6, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3165904406088582}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22745862465174038}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3792834483496076}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 38, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.49077967255642363}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3733832559265301}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45475052824382667}, 'layer_4_size': 73, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25147212839194794}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13479473667840766}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28352394709618267}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2234126750360737}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1084275837765758}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2170455638724607}, 'layer_4_size': 75, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38932116854679577}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2108098130182266}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41772377223411683}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4991790277388184}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16600289991869716}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11166500994018819}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.341966490634019}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2913727541463967}, 'layer_3_size': 90, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23475121083658856}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25520177460215787}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4002281187630936}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.36317162439805983}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 54, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1642540019582842}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 84, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 23, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4138471263039468}, 'layer_2_size': 53, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18207146516559847}, 'layer_5_size': 71, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 35, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22798208251503316}, 'layer_3_size': 61, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18933212958222226}, 'layer_2_size': 69, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19717756093010141}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 40, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4807453823900655}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 52, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4464609648303235}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2622069029337047}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 7, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.48436536545622033}, 'layer_3_size': 13, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15949515245989582}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41082052473880537}, 'layer_2_size': 3, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23078166883833942}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.45495583915048865}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3012706228163049}, 'layer_1_size': 65, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 87, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1622125798402077}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20745875785035764}, 'layer_1_size': 25, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 53, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2902882899441013}, 'layer_3_size': 61, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 38, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47984720572905093}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16912925021930786}, 'layer_4_size': 48, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18160482028331157}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2972163783791718}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3222195153732551}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2611965458343125}, 'layer_1_size': 76, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46751205660245876}, 'layer_2_size': 94, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 20, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2853979696618783}, 'layer_5_size': 87, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18270289824801844}, 'layer_2_size': 96, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 100, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2588783119778918}, 'layer_5_size': 58, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 87, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20814825567027848}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1894976648203621}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16212772803130662}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42662975185148233}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.274374808804679}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1905469287551103}, 'layer_4_size': 84, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.466603952813784}, 'layer_1_size': 83, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 18, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1453841631243427}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1042701844285122}, 'layer_3_size': 10, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 56, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15972309563133835}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.49199211984550784}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 74, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 26, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18158560923298125}, 'layer_2_size': 48, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 38, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.40621233779217614}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 34, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 6, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3391769490316632}, 'layer_2_size': 17, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2680918038208383}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 44, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 12, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 10, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3310992508704168}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 99, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15469433863810475}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10403732447552155}, 'layer_1_size': 71, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21971599150711124}, 'layer_3_size': 19, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 21, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3239797430120135}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35610897362546245}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36405670543434376}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.18514730240839805}, 'layer_4_size': 61, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 37, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14326840322746015}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45719432982493546}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.43560069792488576}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21297786188304155}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23042388916549342}, 'layer_2_size': 2, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19208008183545605}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37777862236608384}, 'layer_3_size': 67, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13567262716003206}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2326240252899456}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4360415196650578}, 'layer_2_size': 79, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 51, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.40017991511747686}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18773446907879549}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 25, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 39, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1443971582596648}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27465858512765084}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3621760645395955}, 'layer_4_size': 13, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21611938768998995}, 'layer_2_size': 95, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 37, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 5, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1571438211657431}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35780144767704614}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14158320923807055}, 'layer_2_size': 17, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3600223727260521}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 86, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 87, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17428262977474496}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 21, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3458900173434585}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2903589045207877}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3798607936703877}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17299076057675788}, 'layer_1_size': 81, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 3, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 67, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2206849446027349}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 62, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 10, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 12, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 25, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 16, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16281607759160277}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26586403133864034}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 61, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1345181000055521}, 'layer_4_size': 48, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3607717761113999}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 79, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11798350093744388}, 'layer_4_size': 82, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4187733871005491}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 61, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2924372832826604}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 56, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4759499917698068}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 75, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 25, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3186953775811115}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4133704717959732}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 32, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3661626782085272}, 'layer_4_size': 76, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23222679665205323}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39180291294730896}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3475827664775537}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3634795999481858}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 6, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]
newly formed T structure is:[[0, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2150110655683083}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.30853774892657976}, 'layer_2_size': 45, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10136199605571763}, 'layer_3_size': 94, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [1, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [2, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22287393078460238}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 62, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [3, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 22, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19752241638812038}, 'layer_2_size': 9, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2780697801742765}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [4, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23723016883152925}, 'layer_1_size': 6, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3165904406088582}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22745862465174038}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3792834483496076}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [5, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 38, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.49077967255642363}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [6, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3733832559265301}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45475052824382667}, 'layer_4_size': 73, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25147212839194794}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [7, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [8, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13479473667840766}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28352394709618267}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [9, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2234126750360737}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1084275837765758}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2170455638724607}, 'layer_4_size': 75, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38932116854679577}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [10, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2108098130182266}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [11, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41772377223411683}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4991790277388184}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16600289991869716}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11166500994018819}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [12, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.341966490634019}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2913727541463967}, 'layer_3_size': 90, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23475121083658856}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [13, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25520177460215787}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4002281187630936}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.36317162439805983}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [14, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 54, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1642540019582842}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 84, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [15, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 23, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4138471263039468}, 'layer_2_size': 53, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18207146516559847}, 'layer_5_size': 71, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [16, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 35, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22798208251503316}, 'layer_3_size': 61, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [17, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [18, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18933212958222226}, 'layer_2_size': 69, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19717756093010141}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [19, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 40, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4807453823900655}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [20, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 52, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4464609648303235}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2622069029337047}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [21, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 7, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.48436536545622033}, 'layer_3_size': 13, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15949515245989582}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [22, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41082052473880537}, 'layer_2_size': 3, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23078166883833942}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.45495583915048865}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [23, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3012706228163049}, 'layer_1_size': 65, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 87, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1622125798402077}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [24, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20745875785035764}, 'layer_1_size': 25, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 53, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [25, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2902882899441013}, 'layer_3_size': 61, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 38, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [26, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47984720572905093}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16912925021930786}, 'layer_4_size': 48, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [27, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18160482028331157}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2972163783791718}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3222195153732551}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [28, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2611965458343125}, 'layer_1_size': 76, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46751205660245876}, 'layer_2_size': 94, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [29, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 20, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2853979696618783}, 'layer_5_size': 87, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [30, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18270289824801844}, 'layer_2_size': 96, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 100, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2588783119778918}, 'layer_5_size': 58, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [31, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 87, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20814825567027848}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [32, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1894976648203621}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16212772803130662}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42662975185148233}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [33, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.274374808804679}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [34, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1905469287551103}, 'layer_4_size': 84, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [35, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.466603952813784}, 'layer_1_size': 83, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 18, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1453841631243427}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [36, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1042701844285122}, 'layer_3_size': 10, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [37, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 56, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15972309563133835}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [38, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.49199211984550784}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 74, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [39, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 26, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [40, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18158560923298125}, 'layer_2_size': 48, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [41, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 38, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.40621233779217614}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 34, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [42, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 6, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3391769490316632}, 'layer_2_size': 17, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2680918038208383}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [43, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [44, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 44, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 12, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 10, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [45, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3310992508704168}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 99, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15469433863810475}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [46, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10403732447552155}, 'layer_1_size': 71, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21971599150711124}, 'layer_3_size': 19, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [47, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [48, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 21, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [49, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3239797430120135}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [50, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35610897362546245}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [51, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36405670543434376}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.18514730240839805}, 'layer_4_size': 61, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [52, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 37, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14326840322746015}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45719432982493546}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.43560069792488576}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [53, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21297786188304155}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [54, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23042388916549342}, 'layer_2_size': 2, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19208008183545605}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [55, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37777862236608384}, 'layer_3_size': 67, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [56, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13567262716003206}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2326240252899456}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [57, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4360415196650578}, 'layer_2_size': 79, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [58, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 51, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.40017991511747686}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18773446907879549}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [59, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 25, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 39, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}], [60, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1443971582596648}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27465858512765084}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3621760645395955}, 'layer_4_size': 13, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [61, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21611938768998995}, 'layer_2_size': 95, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 37, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 5, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [62, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1571438211657431}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [63, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35780144767704614}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}], [64, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14158320923807055}, 'layer_2_size': 17, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [65, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3600223727260521}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 86, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 87, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [66, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17428262977474496}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 21, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [67, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3458900173434585}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2903589045207877}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3798607936703877}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}], [68, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17299076057675788}, 'layer_1_size': 81, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 3, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 67, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [69, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2206849446027349}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 62, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 10, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [70, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 12, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 25, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}], [71, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 16, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [72, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16281607759160277}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26586403133864034}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [73, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 61, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1345181000055521}, 'layer_4_size': 48, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3607717761113999}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [74, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 79, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11798350093744388}, 'layer_4_size': 82, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4187733871005491}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [75, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 61, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2924372832826604}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}], [76, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 56, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4759499917698068}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [77, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 75, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 25, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3186953775811115}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [78, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4133704717959732}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 32, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3661626782085272}, 'layer_4_size': 76, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23222679665205323}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [79, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39180291294730896}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3475827664775537}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [80, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3634795999481858}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 6, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]] 

*** 81 configurations x 1.0 iterations each

1 | Thu Sep 27 22:12:33 2018 | lowest loss so far: inf (run -1)

{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  42 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca41fd828>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:28 - loss: 0.6171{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  86 | activation: sigmoid | extras: dropout - rate: 22.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 45s - loss: 1.7334
2112/6530 [========>.....................] - ETA: 1s - loss: 0.4634  
4608/6530 [====================>.........] - ETA: 0s - loss: 0.4222 
4352/6530 [==================>...........] - ETA: 0s - loss: 0.3154{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  53 | activation: relu    | extras: dropout - rate: 21.5% 
layer 2 | size:  45 | activation: sigmoid | extras: dropout - rate: 30.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:46 - loss: 0.1654
6530/6530 [==============================] - 1s 152us/step - loss: 0.3219 - val_loss: 0.0637

 496/6530 [=>............................] - ETA: 10s - loss: 0.2345 
6530/6530 [==============================] - 1s 159us/step - loss: 0.2253 - val_loss: 0.0413

1008/6530 [===>..........................] - ETA: 5s - loss: 0.2248 
1520/6530 [=====>........................] - ETA: 3s - loss: 0.2145
2016/6530 [========>.....................] - ETA: 2s - loss: 0.2068
2544/6530 [==========>...................] - ETA: 1s - loss: 0.2013
3072/6530 [=============>................] - ETA: 1s - loss: 0.1962
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1929
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1894
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1865
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1833
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1815
6384/6530 [============================>.] - ETA: 0s - loss: 0.1793
6530/6530 [==============================] - 2s 232us/step - loss: 0.1787 - val_loss: 0.1479

# training | RMSE: 0.2553, MAE: 0.2066
worker 2  xfile  [2, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22287393078460238}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 62, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.25525409078301103, 'rmse': 0.25525409078301103, 'mae': 0.2066340653570517, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  22 | activation: relu    | extras: batchnorm 
layer 2 | size:   9 | activation: tanh    | extras: dropout - rate: 19.8% 
layer 3 | size:  70 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca41fdb38>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:32 - loss: 0.7024
 256/6530 [>.............................] - ETA: 10s - loss: 0.3587 
 512/6530 [=>............................] - ETA: 5s - loss: 0.2570 
# training | RMSE: 0.2024, MAE: 0.1621
worker 1  xfile  [1, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20237436758353153, 'rmse': 0.20237436758353153, 'mae': 0.16210405021137456, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:   6 | activation: relu    | extras: dropout - rate: 23.7% 
layer 2 | size:  78 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  56 | activation: tanh    | extras: dropout - rate: 31.7% 
layer 4 | size:  38 | activation: sigmoid | extras: dropout - rate: 22.7% 
layer 5 | size:  69 | activation: sigmoid | extras: dropout - rate: 37.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:02 - loss: 0.8926
 800/6530 [==>...........................] - ETA: 3s - loss: 0.2119
 320/6530 [>.............................] - ETA: 9s - loss: 0.1271  
1088/6530 [===>..........................] - ETA: 2s - loss: 0.1855
 624/6530 [=>............................] - ETA: 5s - loss: 0.1037
1376/6530 [=====>........................] - ETA: 2s - loss: 0.1620
 944/6530 [===>..........................] - ETA: 3s - loss: 0.0947
1664/6530 [======>.......................] - ETA: 1s - loss: 0.1483
1264/6530 [====>.........................] - ETA: 2s - loss: 0.0906
1968/6530 [========>.....................] - ETA: 1s - loss: 0.1380
1584/6530 [======>.......................] - ETA: 2s - loss: 0.0854
2240/6530 [=========>....................] - ETA: 1s - loss: 0.1295
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0826
2544/6530 [==========>...................] - ETA: 1s - loss: 0.1216
# training | RMSE: 0.1827, MAE: 0.1419
worker 0  xfile  [0, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2150110655683083}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.30853774892657976}, 'layer_2_size': 45, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10136199605571763}, 'layer_3_size': 94, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.18265415634906074, 'rmse': 0.18265415634906074, 'mae': 0.14186631529906155, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:   4 | activation: sigmoid | extras: None 
layer 2 | size:  38 | activation: sigmoid | extras: None 
layer 3 | size:  90 | activation: relu    | extras: None 
layer 4 | size:  76 | activation: tanh    | extras: dropout - rate: 49.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca41fda58>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 20s - loss: 1.3500
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0810
2832/6530 [============>.................] - ETA: 1s - loss: 0.1160
1856/6530 [=======>......................] - ETA: 0s - loss: 0.3613 
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0801
3120/6530 [=============>................] - ETA: 1s - loss: 0.1106
3712/6530 [================>.............] - ETA: 0s - loss: 0.2802
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0779
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1059
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2516
3088/6530 [=============>................] - ETA: 1s - loss: 0.0766
3712/6530 [================>.............] - ETA: 0s - loss: 0.1024
6530/6530 [==============================] - 0s 63us/step - loss: 0.2405 - val_loss: 0.1850

3408/6530 [==============>...............] - ETA: 0s - loss: 0.0760
4016/6530 [=================>............] - ETA: 0s - loss: 0.0994
3712/6530 [================>.............] - ETA: 0s - loss: 0.0757
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0964
4016/6530 [=================>............] - ETA: 0s - loss: 0.0751
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0942
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0748
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0921
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0741
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0902
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0736
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0883
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0733
# training | RMSE: 0.2302, MAE: 0.1822
worker 0  xfile  [5, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 38, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.49077967255642363}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.23015157651989426, 'rmse': 0.23015157651989426, 'mae': 0.1821995716994748, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  42 | activation: sigmoid | extras: dropout - rate: 37.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca34ffe10>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 23s - loss: 0.7138
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0869
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0728
1504/6530 [=====>........................] - ETA: 0s - loss: 0.2329 
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0854
2944/6530 [============>.................] - ETA: 0s - loss: 0.2052
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0727
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0839
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1955
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0722
6528/6530 [============================>.] - ETA: 0s - loss: 0.0825
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1878
6496/6530 [============================>.] - ETA: 0s - loss: 0.0714
6530/6530 [==============================] - 2s 247us/step - loss: 0.0825 - val_loss: 0.0465

6530/6530 [==============================] - 0s 57us/step - loss: 0.1845 - val_loss: 0.1629

6530/6530 [==============================] - 2s 247us/step - loss: 0.0714 - val_loss: 0.0516

# training | RMSE: 0.2317, MAE: 0.1878
worker 1  xfile  [4, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23723016883152925}, 'layer_1_size': 6, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3165904406088582}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22745862465174038}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3792834483496076}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.23166259510331597, 'rmse': 0.23166259510331597, 'mae': 0.18784298885288736, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  77 | activation: tanh    | extras: dropout - rate: 22.3% 
layer 2 | size:   4 | activation: relu    | extras: dropout - rate: 10.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc73543438>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 3s - loss: 0.5987
6530/6530 [==============================] - 0s 35us/step - loss: 0.5339 - val_loss: 0.4866

# training | RMSE: 0.2125, MAE: 0.1723
worker 2  xfile  [3, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 22, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19752241638812038}, 'layer_2_size': 9, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2780697801742765}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.21252898123144556, 'rmse': 0.21252898123144556, 'mae': 0.1723217818561635, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  17 | activation: tanh    | extras: dropout - rate: 13.5% 
layer 2 | size:  58 | activation: tanh    | extras: dropout - rate: 28.4% 
layer 3 | size:  37 | activation: relu    | extras: None 
layer 4 | size:  81 | activation: relu    | extras: batchnorm 
layer 5 | size:  43 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca0509908>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 44s - loss: 1.4747
1152/6530 [====>.........................] - ETA: 2s - loss: 0.7994 
# training | RMSE: 0.2041, MAE: 0.1614
worker 0  xfile  [6, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3733832559265301}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45475052824382667}, 'layer_4_size': 73, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25147212839194794}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.20413120260236672, 'rmse': 0.20413120260236672, 'mae': 0.16144281152953788, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  78 | activation: sigmoid | extras: None 
layer 2 | size:  85 | activation: tanh    | extras: None 
layer 3 | size:  58 | activation: tanh    | extras: batchnorm 
layer 4 | size:  83 | activation: tanh    | extras: batchnorm 
layer 5 | size:  59 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc76f319e8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:44 - loss: 2.6529
# training | RMSE: 0.6911, MAE: 0.6436
worker 1  xfile  [9, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2234126750360737}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1084275837765758}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2170455638724607}, 'layer_4_size': 75, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38932116854679577}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.6911086360660346, 'rmse': 0.6911086360660346, 'mae': 0.6436048907185381, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  46 | activation: sigmoid | extras: None 
layer 2 | size:  92 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc735430f0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 1:21 - loss: 0.8459
2112/6530 [========>.....................] - ETA: 1s - loss: 0.6015
 448/6530 [=>............................] - ETA: 7s - loss: 0.5360  
 496/6530 [=>............................] - ETA: 3s - loss: 0.2965  
3136/6530 [=============>................] - ETA: 0s - loss: 0.4883
 928/6530 [===>..........................] - ETA: 3s - loss: 0.3159
 992/6530 [===>..........................] - ETA: 1s - loss: 0.2459
4224/6530 [==================>...........] - ETA: 0s - loss: 0.4196
1504/6530 [=====>........................] - ETA: 1s - loss: 0.2240
1408/6530 [=====>........................] - ETA: 2s - loss: 0.2441
5312/6530 [=======================>......] - ETA: 0s - loss: 0.3707
2016/6530 [========>.....................] - ETA: 0s - loss: 0.2074
1888/6530 [=======>......................] - ETA: 1s - loss: 0.2001
6464/6530 [============================>.] - ETA: 0s - loss: 0.3344
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1977
2368/6530 [=========>....................] - ETA: 1s - loss: 0.1719
3072/6530 [=============>................] - ETA: 0s - loss: 0.1925
6530/6530 [==============================] - 1s 124us/step - loss: 0.3326 - val_loss: 0.1069

2880/6530 [============>.................] - ETA: 1s - loss: 0.1499
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1911
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1354
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1900
3872/6530 [================>.............] - ETA: 0s - loss: 0.1244
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1890
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1157
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1877
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1087
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1863
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1030
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1836
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0982
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0942
6530/6530 [==============================] - 1s 135us/step - loss: 0.1827 - val_loss: 0.1707

6530/6530 [==============================] - 1s 201us/step - loss: 0.0911 - val_loss: 0.0604

# training | RMSE: 0.2089, MAE: 0.1691
worker 1  xfile  [10, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2108098130182266}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20891904517593532, 'rmse': 0.20891904517593532, 'mae': 0.16914115016341547, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  82 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc5c02a8d0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 3s - loss: 2.1109
6530/6530 [==============================] - 0s 31us/step - loss: 0.2133 - val_loss: 0.0417

# training | RMSE: 0.3296, MAE: 0.2631
worker 2  xfile  [8, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13479473667840766}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28352394709618267}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.3295668259425225, 'rmse': 0.3295668259425225, 'mae': 0.26305086872849703, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  50 | activation: tanh    | extras: batchnorm 
layer 2 | size:  21 | activation: tanh    | extras: dropout - rate: 41.8% 
layer 3 | size:  67 | activation: relu    | extras: dropout - rate: 49.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc8412e208>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:28 - loss: 1.6628
 336/6530 [>.............................] - ETA: 7s - loss: 0.7621  
 640/6530 [=>............................] - ETA: 4s - loss: 0.6574
1024/6530 [===>..........................] - ETA: 2s - loss: 0.5838
1360/6530 [=====>........................] - ETA: 2s - loss: 0.5429
1712/6530 [======>.......................] - ETA: 1s - loss: 0.5107
2096/6530 [========>.....................] - ETA: 1s - loss: 0.4869
2464/6530 [==========>...................] - ETA: 1s - loss: 0.4666
2848/6530 [============>.................] - ETA: 0s - loss: 0.4484
3248/6530 [=============>................] - ETA: 0s - loss: 0.4334
# training | RMSE: 0.2407, MAE: 0.1920
worker 0  xfile  [7, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.24068571295454153, 'rmse': 0.24068571295454153, 'mae': 0.19197868792554124, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  46 | activation: sigmoid | extras: None 
layer 2 | size:   4 | activation: relu    | extras: None 
layer 3 | size:  45 | activation: sigmoid | extras: dropout - rate: 25.5% 
layer 4 | size:  85 | activation: tanh    | extras: dropout - rate: 40.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc741b4a58>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 16s - loss: 3.9151
3632/6530 [===============>..............] - ETA: 0s - loss: 0.4210
3072/6530 [=============>................] - ETA: 0s - loss: 0.7552 
4000/6530 [=================>............] - ETA: 0s - loss: 0.4094
5888/6530 [==========================>...] - ETA: 0s - loss: 0.5192
4384/6530 [===================>..........] - ETA: 0s - loss: 0.3994
6530/6530 [==============================] - 0s 76us/step - loss: 0.4912 - val_loss: 0.0784

4768/6530 [====================>.........] - ETA: 0s - loss: 0.3903
5168/6530 [======================>.......] - ETA: 0s - loss: 0.3819
5568/6530 [========================>.....] - ETA: 0s - loss: 0.3746
5952/6530 [==========================>...] - ETA: 0s - loss: 0.3673
6336/6530 [============================>.] - ETA: 0s - loss: 0.3622
6530/6530 [==============================] - 1s 204us/step - loss: 0.3594 - val_loss: 0.1855

# training | RMSE: 0.2035, MAE: 0.1648
worker 1  xfile  [12, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.341966490634019}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2913727541463967}, 'layer_3_size': 90, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23475121083658856}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.20350925564023373, 'rmse': 0.20350925564023373, 'mae': 0.16483837110885421, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  54 | activation: relu    | extras: batchnorm 
layer 2 | size:  77 | activation: relu    | extras: None 
layer 3 | size:  49 | activation: relu    | extras: dropout - rate: 16.4% 
layer 4 | size:  84 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc44401dd8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:22 - loss: 0.6537
# training | RMSE: 0.2787, MAE: 0.2261
worker 0  xfile  [13, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25520177460215787}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4002281187630936}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.36317162439805983}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.27868994318097323, 'rmse': 0.27868994318097323, 'mae': 0.22606997544525542, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  23 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc96152ec18>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 4s - loss: 0.4327
 272/6530 [>.............................] - ETA: 12s - loss: 0.5299 
6530/6530 [==============================] - 0s 39us/step - loss: 0.3231 - val_loss: 0.2744

 528/6530 [=>............................] - ETA: 6s - loss: 0.4032 
 800/6530 [==>...........................] - ETA: 4s - loss: 0.3370
1072/6530 [===>..........................] - ETA: 3s - loss: 0.2955
1392/6530 [=====>........................] - ETA: 2s - loss: 0.2610
1680/6530 [======>.......................] - ETA: 2s - loss: 0.2402
1984/6530 [========>.....................] - ETA: 1s - loss: 0.2249
2272/6530 [=========>....................] - ETA: 1s - loss: 0.2141
2560/6530 [==========>...................] - ETA: 1s - loss: 0.2080
2880/6530 [============>.................] - ETA: 1s - loss: 0.1994
# training | RMSE: 0.3463, MAE: 0.2786
worker 0  xfile  [15, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 23, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4138471263039468}, 'layer_2_size': 53, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18207146516559847}, 'layer_5_size': 71, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.3463418664853805, 'rmse': 0.3463418664853805, 'mae': 0.27855898198707874, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  35 | activation: sigmoid | extras: None 
layer 2 | size:  23 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc96152ea20>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 21s - loss: 0.2863
3152/6530 [=============>................] - ETA: 1s - loss: 0.1936
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0725 
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0563
3440/6530 [==============>...............] - ETA: 1s - loss: 0.1886
3712/6530 [================>.............] - ETA: 0s - loss: 0.1836
6530/6530 [==============================] - 0s 57us/step - loss: 0.0523 - val_loss: 0.0567

4016/6530 [=================>............] - ETA: 0s - loss: 0.1794
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1760
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1732
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1706
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1676
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1658
# training | RMSE: 0.2250, MAE: 0.1801
worker 2  xfile  [11, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41772377223411683}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4991790277388184}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16600289991869716}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11166500994018819}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2250422008500032, 'rmse': 0.2250422008500032, 'mae': 0.18013781271391205, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  85 | activation: relu    | extras: None 
layer 2 | size:  98 | activation: relu    | extras: batchnorm 
layer 3 | size:  71 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc84178160>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 19s - loss: 0.7871
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1637
2816/6530 [===========>..................] - ETA: 0s - loss: 0.2759 
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1619
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2105
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1601
6530/6530 [==============================] - 1s 86us/step - loss: 0.1977 - val_loss: 0.1560

6368/6530 [============================>.] - ETA: 0s - loss: 0.1581
6530/6530 [==============================] - 2s 277us/step - loss: 0.1573 - val_loss: 0.1241

# training | RMSE: 0.2328, MAE: 0.1922
worker 0  xfile  [16, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 35, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22798208251503316}, 'layer_3_size': 61, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.23281436824178556, 'rmse': 0.23281436824178556, 'mae': 0.1921631978383386, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  14 | activation: tanh    | extras: batchnorm 
layer 2 | size:  69 | activation: relu    | extras: dropout - rate: 18.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc9617dfcc0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:55 - loss: 1.4086
 256/6530 [>.............................] - ETA: 11s - loss: 0.9232 
 544/6530 [=>............................] - ETA: 5s - loss: 0.7012 
 816/6530 [==>...........................] - ETA: 4s - loss: 0.5668
1120/6530 [====>.........................] - ETA: 3s - loss: 0.4756
1456/6530 [=====>........................] - ETA: 2s - loss: 0.4073
1808/6530 [=======>......................] - ETA: 1s - loss: 0.3574
2160/6530 [========>.....................] - ETA: 1s - loss: 0.3224
2496/6530 [==========>...................] - ETA: 1s - loss: 0.2971
2816/6530 [===========>..................] - ETA: 1s - loss: 0.2770
3136/6530 [=============>................] - ETA: 1s - loss: 0.2599
3440/6530 [==============>...............] - ETA: 0s - loss: 0.2468
3792/6530 [================>.............] - ETA: 0s - loss: 0.2325
4144/6530 [==================>...........] - ETA: 0s - loss: 0.2210
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2122
4816/6530 [=====================>........] - ETA: 0s - loss: 0.2033
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1958
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1881
# training | RMSE: 0.1455, MAE: 0.1160
worker 1  xfile  [14, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 54, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1642540019582842}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 84, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.14554345422591847, 'rmse': 0.14554345422591847, 'mae': 0.11602695269456904, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  34 | activation: relu    | extras: None 
layer 2 | size:  52 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  38 | activation: sigmoid | extras: dropout - rate: 44.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc44401c50>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 26s - loss: 1.4237
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1822
2560/6530 [==========>...................] - ETA: 0s - loss: 1.1713 
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1767
# training | RMSE: 0.1913, MAE: 0.1544
worker 2  xfile  [17, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.19132747446863585, 'rmse': 0.19132747446863585, 'mae': 0.15440494575161703, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  17 | activation: relu    | extras: None 
layer 2 | size:  17 | activation: tanh    | extras: batchnorm 
layer 3 | size:  40 | activation: relu    | extras: batchnorm 
layer 4 | size:  14 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc964015320>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:35 - loss: 0.5191
4992/6530 [=====================>........] - ETA: 0s - loss: 0.9309
6448/6530 [============================>.] - ETA: 0s - loss: 0.1720
 224/6530 [>.............................] - ETA: 20s - loss: 0.5472 
 448/6530 [=>............................] - ETA: 10s - loss: 0.4594
6530/6530 [==============================] - 1s 112us/step - loss: 0.7927 - val_loss: 0.2467

6530/6530 [==============================] - 2s 238us/step - loss: 0.1710 - val_loss: 0.0646

 640/6530 [=>............................] - ETA: 7s - loss: 0.3547 
 832/6530 [==>...........................] - ETA: 6s - loss: 0.2838
1040/6530 [===>..........................] - ETA: 4s - loss: 0.2348
1248/6530 [====>.........................] - ETA: 4s - loss: 0.2009
1456/6530 [=====>........................] - ETA: 3s - loss: 0.1781
1680/6530 [======>.......................] - ETA: 3s - loss: 0.1581
1904/6530 [=======>......................] - ETA: 2s - loss: 0.1429
2112/6530 [========>.....................] - ETA: 2s - loss: 0.1316
2336/6530 [=========>....................] - ETA: 2s - loss: 0.1222
2576/6530 [==========>...................] - ETA: 2s - loss: 0.1138
2800/6530 [===========>..................] - ETA: 1s - loss: 0.1066
3024/6530 [============>.................] - ETA: 1s - loss: 0.1002
3264/6530 [=============>................] - ETA: 1s - loss: 0.0943
3488/6530 [===============>..............] - ETA: 1s - loss: 0.0901
3696/6530 [===============>..............] - ETA: 1s - loss: 0.0864
3888/6530 [================>.............] - ETA: 1s - loss: 0.0836
4080/6530 [=================>............] - ETA: 1s - loss: 0.0807
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0782
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0759
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0736
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0716
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0696
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0676
# training | RMSE: 0.2495, MAE: 0.1984
worker 0  xfile  [18, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18933212958222226}, 'layer_2_size': 69, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19717756093010141}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2494638855270626, 'rmse': 0.2494638855270626, 'mae': 0.19840044229925388, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  75 | activation: tanh    | extras: batchnorm 
layer 2 | size:   3 | activation: sigmoid | extras: dropout - rate: 41.1% 
layer 3 | size:  64 | activation: sigmoid | extras: dropout - rate: 23.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc960980320>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:40 - loss: 0.6919
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0658
 640/6530 [=>............................] - ETA: 5s - loss: 0.2797  
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0644
1280/6530 [====>.........................] - ETA: 2s - loss: 0.1739
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0625
1952/6530 [=======>......................] - ETA: 1s - loss: 0.1370
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0609
2592/6530 [==========>...................] - ETA: 1s - loss: 0.1188
6352/6530 [============================>.] - ETA: 0s - loss: 0.0596
3264/6530 [=============>................] - ETA: 0s - loss: 0.1085
# training | RMSE: 0.3090, MAE: 0.2412
worker 1  xfile  [20, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 52, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4464609648303235}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2622069029337047}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.308956536853825, 'rmse': 0.308956536853825, 'mae': 0.2411589102238553, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:   7 | activation: relu    | extras: None 
layer 2 | size:  11 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  13 | activation: relu    | extras: dropout - rate: 48.4% 
layer 4 | size:  12 | activation: relu    | extras: batchnorm 
layer 5 | size:  32 | activation: tanh    | extras: dropout - rate: 15.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc96161a080>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 36s - loss: 0.7445
3904/6530 [================>.............] - ETA: 0s - loss: 0.1010
1920/6530 [=======>......................] - ETA: 1s - loss: 0.6073 
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0961
6530/6530 [==============================] - 2s 366us/step - loss: 0.0584 - val_loss: 0.0191

3712/6530 [================>.............] - ETA: 0s - loss: 0.5298
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0913
5504/6530 [========================>.....] - ETA: 0s - loss: 0.4639
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0873
6528/6530 [============================>.] - ETA: 0s - loss: 0.0849
6530/6530 [==============================] - 1s 151us/step - loss: 0.4334 - val_loss: 0.2358

6530/6530 [==============================] - 1s 167us/step - loss: 0.0848 - val_loss: 0.0556

# training | RMSE: 0.2377, MAE: 0.1935
worker 0  xfile  [22, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41082052473880537}, 'layer_2_size': 3, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23078166883833942}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.45495583915048865}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.23765483956164085, 'rmse': 0.23765483956164085, 'mae': 0.19352895695148672, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  99 | activation: tanh    | extras: None 
layer 2 | size:  19 | activation: sigmoid | extras: None 
layer 3 | size:  61 | activation: relu    | extras: dropout - rate: 29.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc96063a080>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 8s - loss: 0.6077
6400/6530 [============================>.] - ETA: 0s - loss: 0.4292
6530/6530 [==============================] - 0s 70us/step - loss: 0.4247 - val_loss: 0.2097

# training | RMSE: 0.2943, MAE: 0.2331
worker 1  xfile  [21, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 7, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.48436536545622033}, 'layer_3_size': 13, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15949515245989582}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.29430989277202224, 'rmse': 0.29430989277202224, 'mae': 0.2330686018677908, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  65 | activation: tanh    | extras: dropout - rate: 30.1% 
layer 2 | size:  87 | activation: tanh    | extras: batchnorm 
layer 3 | size:  86 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc960890eb8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 13s - loss: 3.6044
3840/6530 [================>.............] - ETA: 0s - loss: 0.9837 
# training | RMSE: 0.1343, MAE: 0.1050
worker 2  xfile  [19, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 40, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4807453823900655}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.13429060784786478, 'rmse': 0.13429060784786478, 'mae': 0.10502809123000229, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  25 | activation: tanh    | extras: dropout - rate: 20.7% 
layer 2 | size:  95 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  81 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc95d472198>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:02 - loss: 1.0845
 896/6530 [===>..........................] - ETA: 4s - loss: 0.6401  
6530/6530 [==============================] - 1s 113us/step - loss: 0.6970 - val_loss: 0.2742

1792/6530 [=======>......................] - ETA: 1s - loss: 0.4716
2688/6530 [===========>..................] - ETA: 1s - loss: 0.4010
3584/6530 [===============>..............] - ETA: 0s - loss: 0.3735
4480/6530 [===================>..........] - ETA: 0s - loss: 0.3502
5440/6530 [=======================>......] - ETA: 0s - loss: 0.3292
6336/6530 [============================>.] - ETA: 0s - loss: 0.3147
6530/6530 [==============================] - 1s 164us/step - loss: 0.3125 - val_loss: 0.2334

# training | RMSE: 0.4531, MAE: 0.3821
worker 0  xfile  [25, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2902882899441013}, 'layer_3_size': 61, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 38, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.4531362325492726, 'rmse': 0.4531362325492726, 'mae': 0.3820690153317292, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  11 | activation: sigmoid | extras: None 
layer 2 | size:  52 | activation: tanh    | extras: dropout - rate: 48.0% 
layer 3 | size:  83 | activation: relu    | extras: None 
layer 4 | size:  48 | activation: sigmoid | extras: dropout - rate: 16.9% 
layer 5 | size:  55 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc94bdb5198>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 27s - loss: 0.5842
2432/6530 [==========>...................] - ETA: 1s - loss: 0.2958 
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2708
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2594
# training | RMSE: 0.5234, MAE: 0.4445
worker 1  xfile  [23, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3012706228163049}, 'layer_1_size': 65, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 87, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1622125798402077}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.5233968034935774, 'rmse': 0.5233968034935774, 'mae': 0.4444681019275501, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  64 | activation: relu    | extras: None 
layer 2 | size:  41 | activation: relu    | extras: dropout - rate: 18.2% 
layer 3 | size:  24 | activation: sigmoid | extras: dropout - rate: 29.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc960890da0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:46 - loss: 0.5026
6530/6530 [==============================] - 1s 120us/step - loss: 0.2578 - val_loss: 0.2038

 704/6530 [==>...........................] - ETA: 4s - loss: 0.2900  
1440/6530 [=====>........................] - ETA: 2s - loss: 0.2566
2240/6530 [=========>....................] - ETA: 1s - loss: 0.2345
3104/6530 [=============>................] - ETA: 0s - loss: 0.2183
4064/6530 [=================>............] - ETA: 0s - loss: 0.2096
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2014
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1955
6530/6530 [==============================] - 1s 151us/step - loss: 0.1914 - val_loss: 0.1360

# training | RMSE: 0.2876, MAE: 0.2319
worker 2  xfile  [24, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20745875785035764}, 'layer_1_size': 25, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 53, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2876131687146577, 'rmse': 0.2876131687146577, 'mae': 0.2318522185937443, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  76 | activation: relu    | extras: dropout - rate: 26.1% 
layer 2 | size:  94 | activation: tanh    | extras: dropout - rate: 46.8% 
layer 3 | size:  29 | activation: tanh    | extras: batchnorm 
layer 4 | size:  20 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc95ccb4eb8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:14 - loss: 0.6035
# training | RMSE: 0.2542, MAE: 0.2078
worker 0  xfile  [26, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47984720572905093}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16912925021930786}, 'layer_4_size': 48, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2541637219928215, 'rmse': 0.2541637219928215, 'mae': 0.20779089862401368, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  79 | activation: tanh    | extras: batchnorm 
layer 2 | size:  55 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc94bdcfd30>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 24s - loss: 0.8411
 480/6530 [=>............................] - ETA: 9s - loss: 0.5414  
2688/6530 [===========>..................] - ETA: 0s - loss: 0.3974 
 992/6530 [===>..........................] - ETA: 4s - loss: 0.4709
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2772
1504/6530 [=====>........................] - ETA: 2s - loss: 0.3891
2112/6530 [========>.....................] - ETA: 1s - loss: 0.3057
6530/6530 [==============================] - 1s 103us/step - loss: 0.2589 - val_loss: 0.1729

2688/6530 [===========>..................] - ETA: 1s - loss: 0.2501
3360/6530 [==============>...............] - ETA: 0s - loss: 0.2082
3904/6530 [================>.............] - ETA: 0s - loss: 0.1845
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1666
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1513
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1395
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1302
6530/6530 [==============================] - 1s 206us/step - loss: 0.1265 - val_loss: 0.9330

# training | RMSE: 0.2216, MAE: 0.1702
worker 0  xfile  [29, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 20, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2853979696618783}, 'layer_5_size': 87, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.22156884621205752, 'rmse': 0.22156884621205752, 'mae': 0.17015768174125306, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  81 | activation: relu    | extras: batchnorm 
layer 2 | size:  91 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc94baab940>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 15s - loss: 2.1620
4608/6530 [====================>.........] - ETA: 0s - loss: 1.0404 
6530/6530 [==============================] - 1s 118us/step - loss: 0.8910 - val_loss: 0.4162

# training | RMSE: 0.1727, MAE: 0.1352
worker 1  xfile  [27, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18160482028331157}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2972163783791718}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3222195153732551}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1726693652664509, 'rmse': 0.1726693652664509, 'mae': 0.13516991707657652, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  86 | activation: relu    | extras: batchnorm 
layer 2 | size:  96 | activation: relu    | extras: dropout - rate: 18.3% 
layer 3 | size:  55 | activation: sigmoid | extras: None 
layer 4 | size: 100 | activation: sigmoid | extras: batchnorm 
layer 5 | size:  58 | activation: sigmoid | extras: dropout - rate: 25.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc9615d7d68>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:57 - loss: 1.5989
 352/6530 [>.............................] - ETA: 16s - loss: 1.4643 
 704/6530 [==>...........................] - ETA: 8s - loss: 1.2347 
1088/6530 [===>..........................] - ETA: 5s - loss: 1.0088
1536/6530 [======>.......................] - ETA: 3s - loss: 0.7887
# training | RMSE: 0.9631, MAE: 0.9467
worker 2  xfile  [28, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2611965458343125}, 'layer_1_size': 76, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46751205660245876}, 'layer_2_size': 94, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.9630671046564773, 'rmse': 0.9630671046564773, 'mae': 0.9467191976708993, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  89 | activation: relu    | extras: batchnorm 
layer 2 | size:  88 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc95ccb4c88>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:38 - loss: 0.5060
1984/6530 [========>.....................] - ETA: 2s - loss: 0.6573
 400/6530 [>.............................] - ETA: 9s - loss: 0.2471  
2400/6530 [==========>...................] - ETA: 2s - loss: 0.5749
 800/6530 [==>...........................] - ETA: 4s - loss: 0.2145
2816/6530 [===========>..................] - ETA: 1s - loss: 0.5158
1152/6530 [====>.........................] - ETA: 3s - loss: 0.1987
3200/6530 [=============>................] - ETA: 1s - loss: 0.4721
1520/6530 [=====>........................] - ETA: 2s - loss: 0.1902
3584/6530 [===============>..............] - ETA: 1s - loss: 0.4393
1872/6530 [=======>......................] - ETA: 1s - loss: 0.1843
3936/6530 [=================>............] - ETA: 0s - loss: 0.4126
2240/6530 [=========>....................] - ETA: 1s - loss: 0.1776
4320/6530 [==================>...........] - ETA: 0s - loss: 0.3889
2624/6530 [===========>..................] - ETA: 1s - loss: 0.1750
4768/6530 [====================>.........] - ETA: 0s - loss: 0.3659
3008/6530 [============>.................] - ETA: 1s - loss: 0.1706
5216/6530 [======================>.......] - ETA: 0s - loss: 0.3467
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1671
5664/6530 [=========================>....] - ETA: 0s - loss: 0.3299
3728/6530 [================>.............] - ETA: 0s - loss: 0.1623
6048/6530 [==========================>...] - ETA: 0s - loss: 0.3177
4096/6530 [=================>............] - ETA: 0s - loss: 0.1592
6464/6530 [============================>.] - ETA: 0s - loss: 0.3059
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1556
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1534
6530/6530 [==============================] - 2s 283us/step - loss: 0.3038 - val_loss: 0.1335

5248/6530 [=======================>......] - ETA: 0s - loss: 0.1519
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1500
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1482
6384/6530 [============================>.] - ETA: 0s - loss: 0.1467
6530/6530 [==============================] - 2s 234us/step - loss: 0.1461 - val_loss: 0.1099

# training | RMSE: 0.5074, MAE: 0.4010
worker 0  xfile  [31, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 87, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20814825567027848}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.5073865153688992, 'rmse': 0.5073865153688992, 'mae': 0.4010024806164959, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  33 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  11 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc94b71b9b0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 31s - loss: 0.7626
2688/6530 [===========>..................] - ETA: 0s - loss: 0.6743 
5248/6530 [=======================>......] - ETA: 0s - loss: 0.5424
6530/6530 [==============================] - 1s 129us/step - loss: 0.4929 - val_loss: 0.3644

# training | RMSE: 0.1313, MAE: 0.1038
worker 2  xfile  [32, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1894976648203621}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16212772803130662}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42662975185148233}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.1313159652416736, 'rmse': 0.1313159652416736, 'mae': 0.10376423394910837, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  83 | activation: relu    | extras: dropout - rate: 46.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc95c30a8d0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 19s - loss: 0.6639
4736/6530 [====================>.........] - ETA: 0s - loss: 0.3926 
# training | RMSE: 0.1580, MAE: 0.1266
worker 1  xfile  [30, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18270289824801844}, 'layer_2_size': 96, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 100, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2588783119778918}, 'layer_5_size': 58, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.15800770883144385, 'rmse': 0.15800770883144385, 'mae': 0.12662641973845362, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  39 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  49 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc9602c4780>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:51 - loss: 0.2333
 384/6530 [>.............................] - ETA: 9s - loss: 0.1933  
6530/6530 [==============================] - 1s 79us/step - loss: 0.3528 - val_loss: 0.2471

 768/6530 [==>...........................] - ETA: 5s - loss: 0.1904
1168/6530 [====>.........................] - ETA: 3s - loss: 0.1845
1536/6530 [======>.......................] - ETA: 2s - loss: 0.1785
1872/6530 [=======>......................] - ETA: 2s - loss: 0.1775
2224/6530 [=========>....................] - ETA: 1s - loss: 0.1757
2592/6530 [==========>...................] - ETA: 1s - loss: 0.1751
# training | RMSE: 0.4398, MAE: 0.3528
worker 0  xfile  [33, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.274374808804679}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.43980243887831244, 'rmse': 0.43980243887831244, 'mae': 0.35275858992183995, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  62 | activation: tanh    | extras: batchnorm 
layer 2 | size:  93 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc94b3ae470>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 15s - loss: 0.7147
2960/6530 [============>.................] - ETA: 1s - loss: 0.1730
5888/6530 [==========================>...] - ETA: 0s - loss: 0.4217 
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1718
6530/6530 [==============================] - 1s 116us/step - loss: 0.4007 - val_loss: 0.2190

3744/6530 [================>.............] - ETA: 0s - loss: 0.1716
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1715
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1713
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1705
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1697
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1688
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1683
6448/6530 [============================>.] - ETA: 0s - loss: 0.1676
6530/6530 [==============================] - 2s 239us/step - loss: 0.1675 - val_loss: 0.1577

# training | RMSE: 0.3030, MAE: 0.2467
worker 2  xfile  [35, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.466603952813784}, 'layer_1_size': 83, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 18, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1453841631243427}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.30301166290521797, 'rmse': 0.30301166290521797, 'mae': 0.2466623465917509, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  93 | activation: tanh    | extras: None 
layer 2 | size:  56 | activation: sigmoid | extras: None 
layer 3 | size:  73 | activation: relu    | extras: None 
layer 4 | size:  54 | activation: tanh    | extras: dropout - rate: 16.0% 
layer 5 | size:  53 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92ff4d470>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:07 - loss: 0.7088
 672/6530 [==>...........................] - ETA: 5s - loss: 0.3172  
1344/6530 [=====>........................] - ETA: 2s - loss: 0.2663
2080/6530 [========>.....................] - ETA: 1s - loss: 0.2414
2816/6530 [===========>..................] - ETA: 1s - loss: 0.2257
3552/6530 [===============>..............] - ETA: 0s - loss: 0.2148
4320/6530 [==================>...........] - ETA: 0s - loss: 0.2076
5184/6530 [======================>.......] - ETA: 0s - loss: 0.2027
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1989
# training | RMSE: 0.2762, MAE: 0.2152
worker 0  xfile  [36, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1042701844285122}, 'layer_3_size': 10, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2761555182787955, 'rmse': 0.2761555182787955, 'mae': 0.2152482482891386, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  27 | activation: relu    | extras: batchnorm 
layer 2 | size:  12 | activation: tanh    | extras: None 
layer 3 | size:  28 | activation: relu    | extras: dropout - rate: 49.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc94baab0b8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 31s - loss: 0.7274
6530/6530 [==============================] - 1s 175us/step - loss: 0.1960 - val_loss: 0.1682

2432/6530 [==========>...................] - ETA: 1s - loss: 0.4114 
4992/6530 [=====================>........] - ETA: 0s - loss: 0.3042
6530/6530 [==============================] - 1s 129us/step - loss: 0.2762 - val_loss: 0.2047

# training | RMSE: 0.1937, MAE: 0.1560
worker 1  xfile  [34, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1905469287551103}, 'layer_4_size': 84, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.1937377282154803, 'rmse': 0.1937377282154803, 'mae': 0.1559545333779845, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  67 | activation: tanh    | extras: batchnorm 
layer 2 | size:  26 | activation: relu    | extras: None 
layer 3 | size: 100 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92ba02470>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 37s - loss: 1.0887
2048/6530 [========>.....................] - ETA: 1s - loss: 0.6807 
3968/6530 [=================>............] - ETA: 0s - loss: 0.4693
6144/6530 [===========================>..] - ETA: 0s - loss: 0.3670
# training | RMSE: 0.2474, MAE: 0.1941
worker 0  xfile  [38, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.49199211984550784}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 74, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2474400226054578, 'rmse': 0.2474400226054578, 'mae': 0.1941052148082133, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  38 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc94ac15390>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 12s - loss: 0.5986
6530/6530 [==============================] - 1s 154us/step - loss: 0.3549 - val_loss: 0.1740

# training | RMSE: 0.2065, MAE: 0.1675
worker 2  xfile  [37, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 56, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15972309563133835}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.20648472448080538, 'rmse': 0.20648472448080538, 'mae': 0.16753619517393376, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  89 | activation: sigmoid | extras: None 
layer 2 | size:  48 | activation: sigmoid | extras: dropout - rate: 18.2% 
layer 3 | size:  62 | activation: tanh    | extras: None 
layer 4 | size:  10 | activation: tanh    | extras: None 
layer 5 | size:  11 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92fe5f2b0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:55 - loss: 0.6771
 352/6530 [>.............................] - ETA: 11s - loss: 0.2871 
 672/6530 [==>...........................] - ETA: 5s - loss: 0.2533 
6530/6530 [==============================] - 1s 97us/step - loss: 0.2864 - val_loss: 0.0853

 992/6530 [===>..........................] - ETA: 4s - loss: 0.2423
1312/6530 [=====>........................] - ETA: 3s - loss: 0.2343
1600/6530 [======>.......................] - ETA: 2s - loss: 0.2327
1920/6530 [=======>......................] - ETA: 2s - loss: 0.2292
2240/6530 [=========>....................] - ETA: 1s - loss: 0.2269
2560/6530 [==========>...................] - ETA: 1s - loss: 0.2266
2896/6530 [============>.................] - ETA: 1s - loss: 0.2245
3264/6530 [=============>................] - ETA: 1s - loss: 0.2248
3648/6530 [===============>..............] - ETA: 0s - loss: 0.2235
4032/6530 [=================>............] - ETA: 0s - loss: 0.2217
4416/6530 [===================>..........] - ETA: 0s - loss: 0.2216
4784/6530 [====================>.........] - ETA: 0s - loss: 0.2225
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2210
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2212
5872/6530 [=========================>....] - ETA: 0s - loss: 0.2202
6256/6530 [===========================>..] - ETA: 0s - loss: 0.2203
6530/6530 [==============================] - 2s 255us/step - loss: 0.2204 - val_loss: 0.2118

# training | RMSE: 0.2097, MAE: 0.1689
worker 1  xfile  [39, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 26, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2097414770287425, 'rmse': 0.2097414770287425, 'mae': 0.16888311904883072, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:   6 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  17 | activation: tanh    | extras: dropout - rate: 33.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92b645d30>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 18s - loss: 0.7233
4352/6530 [==================>...........] - ETA: 0s - loss: 0.6837 
6530/6530 [==============================] - 1s 141us/step - loss: 0.6297 - val_loss: 0.2794

# training | RMSE: 0.2910, MAE: 0.2492
worker 0  xfile  [41, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 38, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.40621233779217614}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 34, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.2909710196421898, 'rmse': 0.2909710196421898, 'mae': 0.2492496113930572, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  64 | activation: tanh    | extras: batchnorm 
layer 2 | size:  29 | activation: relu    | extras: batchnorm 
layer 3 | size:  27 | activation: relu    | extras: batchnorm 
layer 4 | size:  71 | activation: relu    | extras: None 
layer 5 | size:  92 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc94a82ce80>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:46 - loss: 0.8128
 640/6530 [=>............................] - ETA: 10s - loss: 0.4414 
1344/6530 [=====>........................] - ETA: 4s - loss: 0.3009 
# training | RMSE: 0.2608, MAE: 0.2125
worker 2  xfile  [40, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18158560923298125}, 'layer_2_size': 48, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.260846066501631, 'rmse': 0.260846066501631, 'mae': 0.21249403865450392, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  84 | activation: relu    | extras: None 
layer 2 | size:  44 | activation: tanh    | extras: batchnorm 
layer 3 | size:  12 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92fac1da0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:04 - loss: 2.1060
1984/6530 [========>.....................] - ETA: 2s - loss: 0.2480
1280/6530 [====>.........................] - ETA: 2s - loss: 0.4277  
2688/6530 [===========>..................] - ETA: 1s - loss: 0.2109
2432/6530 [==========>...................] - ETA: 1s - loss: 0.2952
3392/6530 [==============>...............] - ETA: 1s - loss: 0.1857
3840/6530 [================>.............] - ETA: 0s - loss: 0.2273
4096/6530 [=================>............] - ETA: 0s - loss: 0.1687
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1909
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1564
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1461
6530/6530 [==============================] - 1s 150us/step - loss: 0.1697 - val_loss: 0.0750

6272/6530 [===========================>..] - ETA: 0s - loss: 0.1383
6530/6530 [==============================] - 2s 258us/step - loss: 0.1359 - val_loss: 0.0663

# training | RMSE: 0.3355, MAE: 0.2744
worker 1  xfile  [42, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 6, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3391769490316632}, 'layer_2_size': 17, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2680918038208383}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.3355202621617262, 'rmse': 0.3355202621617262, 'mae': 0.27439076153572023, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  17 | activation: relu    | extras: batchnorm 
layer 2 | size:  45 | activation: sigmoid | extras: dropout - rate: 33.1% 
layer 3 | size:  57 | activation: tanh    | extras: batchnorm 
layer 4 | size:  99 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92b09de48>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:03 - loss: 0.6956
 448/6530 [=>............................] - ETA: 12s - loss: 0.2435 
 832/6530 [==>...........................] - ETA: 6s - loss: 0.2258 
1184/6530 [====>.........................] - ETA: 4s - loss: 0.2092
1600/6530 [======>.......................] - ETA: 3s - loss: 0.2004
1920/6530 [=======>......................] - ETA: 2s - loss: 0.1957
2304/6530 [=========>....................] - ETA: 2s - loss: 0.1912
2720/6530 [===========>..................] - ETA: 1s - loss: 0.1867
3200/6530 [=============>................] - ETA: 1s - loss: 0.1834
3744/6530 [================>.............] - ETA: 1s - loss: 0.1795
# training | RMSE: 0.2575, MAE: 0.2045
worker 2  xfile  [44, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 44, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 12, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 10, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2575467586570486, 'rmse': 0.2575467586570486, 'mae': 0.20447594659830684, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  71 | activation: tanh    | extras: dropout - rate: 10.4% 
layer 2 | size:  58 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92f6fb9e8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 28s - loss: 0.8036
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1770
3712/6530 [================>.............] - ETA: 0s - loss: 0.2881 
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1737
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1719
6530/6530 [==============================] - 1s 113us/step - loss: 0.2007 - val_loss: 0.0952

5600/6530 [========================>.....] - ETA: 0s - loss: 0.1692
# training | RMSE: 0.2482, MAE: 0.1973
worker 0  xfile  [43, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.24820613423298793, 'rmse': 0.24820613423298793, 'mae': 0.19729903484363015, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:   4 | activation: tanh    | extras: None 
layer 2 | size:  73 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc94a68a320>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 30s - loss: 0.5033
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1675
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0776 
6530/6530 [==============================] - 1s 118us/step - loss: 0.0734 - val_loss: 0.0702

6530/6530 [==============================] - 2s 278us/step - loss: 0.1660 - val_loss: 0.1574

# training | RMSE: 0.3117, MAE: 0.2511
worker 2  xfile  [46, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10403732447552155}, 'layer_1_size': 71, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21971599150711124}, 'layer_3_size': 19, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.31165223741440595, 'rmse': 0.31165223741440595, 'mae': 0.25112303368033445, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  74 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92f09fa90>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:42 - loss: 0.9419
 496/6530 [=>............................] - ETA: 7s - loss: 0.4562  
1008/6530 [===>..........................] - ETA: 3s - loss: 0.3021
1552/6530 [======>.......................] - ETA: 2s - loss: 0.2246
2160/6530 [========>.....................] - ETA: 1s - loss: 0.1811
2800/6530 [===========>..................] - ETA: 1s - loss: 0.1567
# training | RMSE: 0.2703, MAE: 0.2193
worker 0  xfile  [47, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2702519415014441, 'rmse': 0.2702519415014441, 'mae': 0.2193346591611518, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  90 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc949e1d0b8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:08 - loss: 0.4830
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1385
 592/6530 [=>............................] - ETA: 6s - loss: 0.3375  
4048/6530 [=================>............] - ETA: 0s - loss: 0.1273
1152/6530 [====>.........................] - ETA: 3s - loss: 0.2801
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1182
1728/6530 [======>.......................] - ETA: 2s - loss: 0.2500
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1110
2288/6530 [=========>....................] - ETA: 1s - loss: 0.2350
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1048
2864/6530 [============>.................] - ETA: 1s - loss: 0.2230
6496/6530 [============================>.] - ETA: 0s - loss: 0.0996
3456/6530 [==============>...............] - ETA: 0s - loss: 0.2133
4080/6530 [=================>............] - ETA: 0s - loss: 0.2056
6530/6530 [==============================] - 1s 186us/step - loss: 0.0994 - val_loss: 0.0558

4624/6530 [====================>.........] - ETA: 0s - loss: 0.2005
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1967
# training | RMSE: 0.1928, MAE: 0.1548
worker 1  xfile  [45, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3310992508704168}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 99, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15469433863810475}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.19283302748724637, 'rmse': 0.19283302748724637, 'mae': 0.15480607706201402, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  21 | activation: relu    | extras: None 
layer 2 | size:  85 | activation: relu    | extras: None 
layer 3 | size:  81 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92b595710>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:26 - loss: 0.4524
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1923
 832/6530 [==>...........................] - ETA: 5s - loss: 0.2604  
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1892
1600/6530 [======>.......................] - ETA: 2s - loss: 0.2382
2368/6530 [=========>....................] - ETA: 1s - loss: 0.2268
3136/6530 [=============>................] - ETA: 1s - loss: 0.2220
6530/6530 [==============================] - 1s 201us/step - loss: 0.1871 - val_loss: 0.1464

3936/6530 [=================>............] - ETA: 0s - loss: 0.2151
4736/6530 [====================>.........] - ETA: 0s - loss: 0.2081
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2026
6400/6530 [============================>.] - ETA: 0s - loss: 0.1972
6530/6530 [==============================] - 1s 191us/step - loss: 0.1962 - val_loss: 0.1619

# training | RMSE: 0.2358, MAE: 0.1818
worker 2  xfile  [48, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 21, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.23583336818385997, 'rmse': 0.23583336818385997, 'mae': 0.18180263356174303, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  14 | activation: relu    | extras: batchnorm 
layer 2 | size:  20 | activation: relu    | extras: None 
layer 3 | size:  46 | activation: tanh    | extras: dropout - rate: 36.4% 
layer 4 | size:  61 | activation: tanh    | extras: dropout - rate: 18.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92f09f898>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 21s - loss: 1.0270
4096/6530 [=================>............] - ETA: 0s - loss: 0.5822 
# training | RMSE: 0.1846, MAE: 0.1419
worker 0  xfile  [49, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3239797430120135}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.1846137048174472, 'rmse': 0.1846137048174472, 'mae': 0.14193256257221162, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  37 | activation: relu    | extras: batchnorm 
layer 2 | size:  32 | activation: sigmoid | extras: dropout - rate: 14.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc949cb5ba8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:09 - loss: 0.8703
6530/6530 [==============================] - 1s 160us/step - loss: 0.5175 - val_loss: 0.2398

 336/6530 [>.............................] - ETA: 14s - loss: 0.2802 
 656/6530 [==>...........................] - ETA: 7s - loss: 0.2184 
1024/6530 [===>..........................] - ETA: 4s - loss: 0.1976
1360/6530 [=====>........................] - ETA: 3s - loss: 0.1863
1664/6530 [======>.......................] - ETA: 2s - loss: 0.1811
2048/6530 [========>.....................] - ETA: 2s - loss: 0.1755
2432/6530 [==========>...................] - ETA: 1s - loss: 0.1718
2800/6530 [===========>..................] - ETA: 1s - loss: 0.1689
3168/6530 [=============>................] - ETA: 1s - loss: 0.1658
3520/6530 [===============>..............] - ETA: 1s - loss: 0.1629
3872/6530 [================>.............] - ETA: 0s - loss: 0.1600
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1589
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1581
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1570
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1561
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1550
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1540
6432/6530 [============================>.] - ETA: 0s - loss: 0.1526
# training | RMSE: 0.3018, MAE: 0.2412
worker 2  xfile  [51, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36405670543434376}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.18514730240839805}, 'layer_4_size': 61, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.3018215571205848, 'rmse': 0.3018215571205848, 'mae': 0.2412181854190126, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  77 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92f7689b0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 14s - loss: 0.7036
6530/6530 [==============================] - 2s 283us/step - loss: 0.1524 - val_loss: 0.1313

6530/6530 [==============================] - 1s 109us/step - loss: 0.5950 - val_loss: 0.4561

# training | RMSE: 0.2074, MAE: 0.1595
worker 1  xfile  [50, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35610897362546245}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2073798404924326, 'rmse': 0.2073798404924326, 'mae': 0.15951083474959415, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:   9 | activation: relu    | extras: dropout - rate: 21.3% 
layer 2 | size:  60 | activation: tanh    | extras: batchnorm 
layer 3 | size:  44 | activation: tanh    | extras: None 
layer 4 | size:  79 | activation: sigmoid | extras: batchnorm 
layer 5 | size: 100 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92a8fdeb8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 2:01 - loss: 0.4995
 704/6530 [==>...........................] - ETA: 10s - loss: 0.4882 
1280/6530 [====>.........................] - ETA: 5s - loss: 0.4033 
1792/6530 [=======>......................] - ETA: 3s - loss: 0.3263
2432/6530 [==========>...................] - ETA: 2s - loss: 0.2579
3136/6530 [=============>................] - ETA: 1s - loss: 0.2113
3840/6530 [================>.............] - ETA: 1s - loss: 0.1815
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1596
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1444
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1323
6530/6530 [==============================] - 2s 284us/step - loss: 0.1253 - val_loss: 0.0439

# training | RMSE: 0.5108, MAE: 0.4526
worker 2  xfile  [54, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23042388916549342}, 'layer_2_size': 2, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19208008183545605}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.5108101369309934, 'rmse': 0.5108101369309934, 'mae': 0.4526478303299612, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  42 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92fe5cb38>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:46 - loss: 0.6603
 528/6530 [=>............................] - ETA: 6s - loss: 0.6353  
1120/6530 [====>.........................] - ETA: 3s - loss: 0.5173
1648/6530 [======>.......................] - ETA: 2s - loss: 0.4204
2224/6530 [=========>....................] - ETA: 1s - loss: 0.3544
2864/6530 [============>.................] - ETA: 1s - loss: 0.3111
3456/6530 [==============>...............] - ETA: 0s - loss: 0.2855
4064/6530 [=================>............] - ETA: 0s - loss: 0.2672
4688/6530 [====================>.........] - ETA: 0s - loss: 0.2537
5312/6530 [=======================>......] - ETA: 0s - loss: 0.2434
5920/6530 [==========================>...] - ETA: 0s - loss: 0.2348
6530/6530 [==============================] - 1s 187us/step - loss: 0.2279 - val_loss: 0.1671

# training | RMSE: 0.1648, MAE: 0.1272
worker 0  xfile  [52, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 37, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14326840322746015}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45719432982493546}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.43560069792488576}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.16479095120122836, 'rmse': 0.16479095120122836, 'mae': 0.1271585036799888, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  72 | activation: sigmoid | extras: dropout - rate: 13.6% 
layer 2 | size:  82 | activation: sigmoid | extras: dropout - rate: 23.3% 
layer 3 | size:  33 | activation: tanh    | extras: batchnorm 
layer 4 | size:   9 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc949ae4dd8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 7:44 - loss: 1.2871
 240/6530 [>.............................] - ETA: 31s - loss: 1.1626 
 480/6530 [=>............................] - ETA: 15s - loss: 1.0260
 736/6530 [==>...........................] - ETA: 10s - loss: 0.9277
1008/6530 [===>..........................] - ETA: 7s - loss: 0.8447 
# training | RMSE: 0.2086, MAE: 0.1675
worker 1  xfile  [53, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21297786188304155}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20863524584019735, 'rmse': 0.20863524584019735, 'mae': 0.16747014432569035, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  60 | activation: sigmoid | extras: None 
layer 2 | size:  79 | activation: sigmoid | extras: dropout - rate: 43.6% 
layer 3 | size:  95 | activation: relu    | extras: None 
layer 4 | size:  64 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92a8fdcf8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:16 - loss: 1.1727
1280/6530 [====>.........................] - ETA: 5s - loss: 0.7768
 368/6530 [>.............................] - ETA: 16s - loss: 0.5282 
1536/6530 [======>.......................] - ETA: 4s - loss: 0.7250
 720/6530 [==>...........................] - ETA: 8s - loss: 0.4438 
1792/6530 [=======>......................] - ETA: 3s - loss: 0.6802
1072/6530 [===>..........................] - ETA: 5s - loss: 0.3845
2032/6530 [========>.....................] - ETA: 3s - loss: 0.6485
1424/6530 [=====>........................] - ETA: 4s - loss: 0.3578
2272/6530 [=========>....................] - ETA: 3s - loss: 0.6206
1760/6530 [=======>......................] - ETA: 3s - loss: 0.3352
2512/6530 [==========>...................] - ETA: 2s - loss: 0.5986
2096/6530 [========>.....................] - ETA: 2s - loss: 0.3205
2736/6530 [===========>..................] - ETA: 2s - loss: 0.5822
2368/6530 [=========>....................] - ETA: 2s - loss: 0.3129
2944/6530 [============>.................] - ETA: 2s - loss: 0.5667
2688/6530 [===========>..................] - ETA: 1s - loss: 0.3020
3184/6530 [=============>................] - ETA: 1s - loss: 0.5520
3024/6530 [============>.................] - ETA: 1s - loss: 0.2938
3424/6530 [==============>...............] - ETA: 1s - loss: 0.5385
3360/6530 [==============>...............] - ETA: 1s - loss: 0.2860
3648/6530 [===============>..............] - ETA: 1s - loss: 0.5277
3680/6530 [===============>..............] - ETA: 1s - loss: 0.2806
3872/6530 [================>.............] - ETA: 1s - loss: 0.5152
4000/6530 [=================>............] - ETA: 0s - loss: 0.2756
4112/6530 [=================>............] - ETA: 1s - loss: 0.5053
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2730
4368/6530 [===================>..........] - ETA: 1s - loss: 0.4937
4720/6530 [====================>.........] - ETA: 0s - loss: 0.2680
4608/6530 [====================>.........] - ETA: 0s - loss: 0.4852
5056/6530 [======================>.......] - ETA: 0s - loss: 0.2637
4864/6530 [=====================>........] - ETA: 0s - loss: 0.4771
5408/6530 [=======================>......] - ETA: 0s - loss: 0.2597
# training | RMSE: 0.2057, MAE: 0.1662
worker 2  xfile  [55, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37777862236608384}, 'layer_3_size': 67, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20567013094161413, 'rmse': 0.20567013094161413, 'mae': 0.1661717880593185, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  49 | activation: relu    | extras: batchnorm 
layer 2 | size:  38 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92fe5cda0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 37s - loss: 0.3348
5104/6530 [======================>.......] - ETA: 0s - loss: 0.4689
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2562
3072/6530 [=============>................] - ETA: 0s - loss: 0.1660 
5360/6530 [=======================>......] - ETA: 0s - loss: 0.4607
6096/6530 [===========================>..] - ETA: 0s - loss: 0.2528
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1529
5600/6530 [========================>.....] - ETA: 0s - loss: 0.4535
6416/6530 [============================>.] - ETA: 0s - loss: 0.2499
5824/6530 [=========================>....] - ETA: 0s - loss: 0.4472
6530/6530 [==============================] - 1s 149us/step - loss: 0.1521 - val_loss: 0.1691

6048/6530 [==========================>...] - ETA: 0s - loss: 0.4415
6272/6530 [===========================>..] - ETA: 0s - loss: 0.4361
6530/6530 [==============================] - 2s 318us/step - loss: 0.2492 - val_loss: 0.2585

6528/6530 [============================>.] - ETA: 0s - loss: 0.4305
6530/6530 [==============================] - 3s 414us/step - loss: 0.4304 - val_loss: 0.2101

# training | RMSE: 0.3042, MAE: 0.2561
worker 1  xfile  [57, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4360415196650578}, 'layer_2_size': 79, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.30423868779676344, 'rmse': 0.30423868779676344, 'mae': 0.25614066086246284, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  13 | activation: tanh    | extras: dropout - rate: 14.4% 
layer 2 | size:  29 | activation: tanh    | extras: batchnorm 
layer 3 | size:  20 | activation: sigmoid | extras: dropout - rate: 27.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc929df8208>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:39 - loss: 0.5235
1088/6530 [===>..........................] - ETA: 5s - loss: 0.3409  
# training | RMSE: 0.2556, MAE: 0.2089
worker 0  xfile  [56, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13567262716003206}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2326240252899456}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2556056073794619, 'rmse': 0.2556056073794619, 'mae': 0.20892976060273938, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  66 | activation: tanh    | extras: None 
layer 2 | size:  95 | activation: sigmoid | extras: dropout - rate: 21.6% 
layer 3 | size:  37 | activation: sigmoid | extras: None 
layer 4 | size:   5 | activation: tanh    | extras: None 
layer 5 | size:  31 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc949775630>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 21s - loss: 0.4240
2176/6530 [========>.....................] - ETA: 2s - loss: 0.2185
4608/6530 [====================>.........] - ETA: 0s - loss: 0.2294 
3392/6530 [==============>...............] - ETA: 1s - loss: 0.1593
# training | RMSE: 0.2029, MAE: 0.1592
worker 2  xfile  [58, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 51, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.40017991511747686}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18773446907879549}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.202936994188723, 'rmse': 0.202936994188723, 'mae': 0.1591844750324925, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  52 | activation: tanh    | extras: batchnorm 
layer 2 | size:  25 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  39 | activation: relu    | extras: batchnorm 
layer 4 | size:  77 | activation: tanh    | extras: None 
layer 5 | size:  91 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92e97acc0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 8:03 - loss: 0.6672
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1327
 192/6530 [..............................] - ETA: 40s - loss: 0.5844 
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1162
6530/6530 [==============================] - 1s 161us/step - loss: 0.2203 - val_loss: 0.1803

 384/6530 [>.............................] - ETA: 20s - loss: 0.4967
 576/6530 [=>............................] - ETA: 13s - loss: 0.4585
 752/6530 [==>...........................] - ETA: 10s - loss: 0.4401
 944/6530 [===>..........................] - ETA: 8s - loss: 0.4204 
6530/6530 [==============================] - 1s 216us/step - loss: 0.1080 - val_loss: 0.0439

1104/6530 [====>.........................] - ETA: 7s - loss: 0.4054
1280/6530 [====>.........................] - ETA: 6s - loss: 0.3938
1456/6530 [=====>........................] - ETA: 5s - loss: 0.3791
1648/6530 [======>.......................] - ETA: 4s - loss: 0.3676
1808/6530 [=======>......................] - ETA: 4s - loss: 0.3603
1984/6530 [========>.....................] - ETA: 4s - loss: 0.3552
2160/6530 [========>.....................] - ETA: 3s - loss: 0.3450
2336/6530 [=========>....................] - ETA: 3s - loss: 0.3397
2528/6530 [==========>...................] - ETA: 3s - loss: 0.3325
2736/6530 [===========>..................] - ETA: 2s - loss: 0.3283
2928/6530 [============>.................] - ETA: 2s - loss: 0.3217
3120/6530 [=============>................] - ETA: 2s - loss: 0.3157
3312/6530 [==============>...............] - ETA: 2s - loss: 0.3124
3504/6530 [===============>..............] - ETA: 1s - loss: 0.3094
3712/6530 [================>.............] - ETA: 1s - loss: 0.3053
3904/6530 [================>.............] - ETA: 1s - loss: 0.3012
4048/6530 [=================>............] - ETA: 1s - loss: 0.2990
4192/6530 [==================>...........] - ETA: 1s - loss: 0.2974
4336/6530 [==================>...........] - ETA: 1s - loss: 0.2952
4496/6530 [===================>..........] - ETA: 1s - loss: 0.2927
4656/6530 [====================>.........] - ETA: 1s - loss: 0.2899
4816/6530 [=====================>........] - ETA: 0s - loss: 0.2870
# training | RMSE: 0.2246, MAE: 0.1829
worker 0  xfile  [61, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21611938768998995}, 'layer_2_size': 95, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 37, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 5, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2246003420658188, 'rmse': 0.2246003420658188, 'mae': 0.18288164983674396, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  37 | activation: relu    | extras: dropout - rate: 15.7% 
layer 2 | size:  17 | activation: tanh    | extras: None 
layer 3 | size:  80 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc949e5b6d8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 19s - loss: 0.7558
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2858
5184/6530 [======================>.......] - ETA: 0s - loss: 0.2831
5360/6530 [=======================>......] - ETA: 0s - loss: 0.2807
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2791
6530/6530 [==============================] - 1s 149us/step - loss: 0.1893 - val_loss: 0.0613

5712/6530 [=========================>....] - ETA: 0s - loss: 0.2776
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2759
6064/6530 [==========================>...] - ETA: 0s - loss: 0.2739
6240/6530 [===========================>..] - ETA: 0s - loss: 0.2719
6400/6530 [============================>.] - ETA: 0s - loss: 0.2697
# training | RMSE: 0.2098, MAE: 0.1699
worker 1  xfile  [60, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1443971582596648}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27465858512765084}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3621760645395955}, 'layer_4_size': 13, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.20980304073235106, 'rmse': 0.20980304073235106, 'mae': 0.16990084220414836, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:   4 | activation: relu    | extras: batchnorm 
layer 2 | size:  54 | activation: sigmoid | extras: None 
layer 3 | size:  54 | activation: tanh    | extras: dropout - rate: 35.8% 
layer 4 | size:  81 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc929926198>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:24 - loss: 0.5262
6530/6530 [==============================] - 3s 508us/step - loss: 0.2682 - val_loss: 0.1825

 608/6530 [=>............................] - ETA: 10s - loss: 0.4693 
1120/6530 [====>.........................] - ETA: 5s - loss: 0.3176 
1632/6530 [======>.......................] - ETA: 3s - loss: 0.2492
2144/6530 [========>.....................] - ETA: 2s - loss: 0.2137
2624/6530 [===========>..................] - ETA: 1s - loss: 0.1934
3104/6530 [=============>................] - ETA: 1s - loss: 0.1766
3552/6530 [===============>..............] - ETA: 1s - loss: 0.1658
3968/6530 [=================>............] - ETA: 0s - loss: 0.1565
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1470
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1395
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1323
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1276
6530/6530 [==============================] - 2s 276us/step - loss: 0.1255 - val_loss: 0.0795

# training | RMSE: 0.2402, MAE: 0.1930
worker 0  xfile  [62, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1571438211657431}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.24020893381351385, 'rmse': 0.24020893381351385, 'mae': 0.19298023966716887, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  72 | activation: relu    | extras: None 
layer 2 | size:  17 | activation: tanh    | extras: dropout - rate: 14.2% 
layer 3 | size:  48 | activation: sigmoid | extras: None 
layer 4 | size:  33 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc948e26f98>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 43s - loss: 0.8639
2944/6530 [============>.................] - ETA: 1s - loss: 0.2448 
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2174
6530/6530 [==============================] - 1s 170us/step - loss: 0.2129 - val_loss: 0.2223

# training | RMSE: 0.2853, MAE: 0.2366
worker 1  xfile  [63, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35780144767704614}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2852781242839184, 'rmse': 0.2852781242839184, 'mae': 0.23659314224796563, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  87 | activation: sigmoid | extras: None 
layer 2 | size:  28 | activation: sigmoid | extras: None 
layer 3 | size:  91 | activation: sigmoid | extras: dropout - rate: 17.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc928ecd438>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 42s - loss: 1.2859
3072/6530 [=============>................] - ETA: 1s - loss: 0.3216 
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2604
6530/6530 [==============================] - 1s 165us/step - loss: 0.2581 - val_loss: 0.3147

# training | RMSE: 0.2261, MAE: 0.1785
worker 2  xfile  [59, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 25, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 39, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.22614737878768285, 'rmse': 0.22614737878768285, 'mae': 0.17852631581568756, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  73 | activation: tanh    | extras: dropout - rate: 36.0% 
layer 2 | size:  86 | activation: tanh    | extras: batchnorm 
layer 3 | size:  23 | activation: sigmoid | extras: None 
layer 4 | size:  87 | activation: sigmoid | extras: batchnorm 
layer 5 | size: 100 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92e5f1d30>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 8:01 - loss: 0.6058
 208/6530 [..............................] - ETA: 37s - loss: 0.2603 
 384/6530 [>.............................] - ETA: 20s - loss: 0.1729
 560/6530 [=>............................] - ETA: 14s - loss: 0.1426
 752/6530 [==>...........................] - ETA: 10s - loss: 0.1202
# training | RMSE: 0.2643, MAE: 0.2203
worker 0  xfile  [64, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14158320923807055}, 'layer_2_size': 17, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2643319719774942, 'rmse': 0.2643319719774942, 'mae': 0.2203130197352707, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  54 | activation: sigmoid | extras: dropout - rate: 34.6% 
layer 2 | size:   4 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  28 | activation: sigmoid | extras: dropout - rate: 29.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc94904e4a8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:18 - loss: 0.1480
 976/6530 [===>..........................] - ETA: 8s - loss: 0.1039 
 672/6530 [==>...........................] - ETA: 8s - loss: 0.0838  
1200/6530 [====>.........................] - ETA: 6s - loss: 0.0933
1312/6530 [=====>........................] - ETA: 4s - loss: 0.0792
1424/6530 [=====>........................] - ETA: 5s - loss: 0.0860
1984/6530 [========>.....................] - ETA: 2s - loss: 0.0774
1648/6530 [======>.......................] - ETA: 4s - loss: 0.0796
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0753
1872/6530 [=======>......................] - ETA: 4s - loss: 0.0753
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0739
2096/6530 [========>.....................] - ETA: 3s - loss: 0.0719
3968/6530 [=================>............] - ETA: 0s - loss: 0.0730
2320/6530 [=========>....................] - ETA: 3s - loss: 0.0701
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0718
2528/6530 [==========>...................] - ETA: 2s - loss: 0.0682
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0711
2720/6530 [===========>..................] - ETA: 2s - loss: 0.0667
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0710
2928/6530 [============>.................] - ETA: 2s - loss: 0.0648
6464/6530 [============================>.] - ETA: 0s - loss: 0.0707
3120/6530 [=============>................] - ETA: 2s - loss: 0.0631
3312/6530 [==============>...............] - ETA: 1s - loss: 0.0624
3520/6530 [===============>..............] - ETA: 1s - loss: 0.0616
6530/6530 [==============================] - 2s 253us/step - loss: 0.0706 - val_loss: 0.0663

3728/6530 [================>.............] - ETA: 1s - loss: 0.0608
3952/6530 [=================>............] - ETA: 1s - loss: 0.0603
4160/6530 [==================>...........] - ETA: 1s - loss: 0.0596
4352/6530 [==================>...........] - ETA: 1s - loss: 0.0588
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0583
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0577
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0571
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0569
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0563
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0558
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0557
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0552
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0549
6432/6530 [============================>.] - ETA: 0s - loss: 0.0547
6530/6530 [==============================] - 3s 462us/step - loss: 0.0544 - val_loss: 0.0436

# training | RMSE: 0.3789, MAE: 0.3202
worker 1  xfile  [66, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17428262977474496}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 21, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.37887572313901, 'rmse': 0.37887572313901, 'mae': 0.3201636343475339, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  81 | activation: sigmoid | extras: dropout - rate: 17.3% 
layer 2 | size:   3 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  46 | activation: relu    | extras: batchnorm 
layer 4 | size:  28 | activation: relu    | extras: batchnorm 
layer 5 | size:  67 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc928ecd278>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 34s - loss: 0.6150
2560/6530 [==========>...................] - ETA: 2s - loss: 0.2271 
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1698
# training | RMSE: 0.2588, MAE: 0.2116
worker 0  xfile  [67, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3458900173434585}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2903589045207877}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3798607936703877}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.25879347846388795, 'rmse': 0.25879347846388795, 'mae': 0.21158981122571813, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  23 | activation: sigmoid | extras: dropout - rate: 22.1% 
layer 2 | size:  34 | activation: tanh    | extras: None 
layer 3 | size:  86 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc948896710>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 49s - loss: 0.7012
2944/6530 [============>.................] - ETA: 1s - loss: 0.5091 
5632/6530 [========================>.....] - ETA: 0s - loss: 0.3841
6530/6530 [==============================] - 2s 261us/step - loss: 0.1546 - val_loss: 0.0826

6530/6530 [==============================] - 1s 193us/step - loss: 0.3614 - val_loss: 0.2107

# training | RMSE: 0.2087, MAE: 0.1703
worker 2  xfile  [65, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3600223727260521}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 86, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 87, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2087397443287876, 'rmse': 0.2087397443287876, 'mae': 0.17031296817374092, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  66 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  14 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92d67af60>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:16 - loss: 0.1229
 304/6530 [>.............................] - ETA: 20s - loss: 0.0615 
 624/6530 [=>............................] - ETA: 9s - loss: 0.0539 
 912/6530 [===>..........................] - ETA: 6s - loss: 0.0527
1200/6530 [====>.........................] - ETA: 5s - loss: 0.0520
1472/6530 [=====>........................] - ETA: 4s - loss: 0.0518
1776/6530 [=======>......................] - ETA: 3s - loss: 0.0507
2112/6530 [========>.....................] - ETA: 2s - loss: 0.0499
2464/6530 [==========>...................] - ETA: 2s - loss: 0.0487
2832/6530 [============>.................] - ETA: 1s - loss: 0.0484
# training | RMSE: 0.2882, MAE: 0.2328
worker 1  xfile  [68, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17299076057675788}, 'layer_1_size': 81, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 3, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 67, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.28817775396127776, 'rmse': 0.28817775396127776, 'mae': 0.23281951706997503, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  16 | activation: tanh    | extras: None 
layer 2 | size:   9 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc928c4d048>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:24 - loss: 1.1187
3200/6530 [=============>................] - ETA: 1s - loss: 0.0476
 464/6530 [=>............................] - ETA: 13s - loss: 0.5352 
# training | RMSE: 0.2613, MAE: 0.2147
worker 0  xfile  [69, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2206849446027349}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 62, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 10, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.26125172720322937, 'rmse': 0.26125172720322937, 'mae': 0.21472352543911194, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  13 | activation: sigmoid | extras: dropout - rate: 16.3% 
layer 2 | size:  42 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc94844cb70>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 45s - loss: 0.6074
3536/6530 [===============>..............] - ETA: 1s - loss: 0.0469
 944/6530 [===>..........................] - ETA: 6s - loss: 0.3636 
3712/6530 [================>.............] - ETA: 0s - loss: 0.1056 
3888/6530 [================>.............] - ETA: 1s - loss: 0.0463
1424/6530 [=====>........................] - ETA: 3s - loss: 0.2718
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0459
1936/6530 [=======>......................] - ETA: 2s - loss: 0.2201
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0453
2416/6530 [==========>...................] - ETA: 2s - loss: 0.1911
6530/6530 [==============================] - 1s 173us/step - loss: 0.0892 - val_loss: 0.0505

4976/6530 [=====================>........] - ETA: 0s - loss: 0.0452
2912/6530 [============>.................] - ETA: 1s - loss: 0.1703
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0451
3424/6530 [==============>...............] - ETA: 1s - loss: 0.1544
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0451
3904/6530 [================>.............] - ETA: 0s - loss: 0.1433
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0449
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1345
6496/6530 [============================>.] - ETA: 0s - loss: 0.0448
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1272
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1208
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1155
6530/6530 [==============================] - 2s 317us/step - loss: 0.0448 - val_loss: 0.0342

6208/6530 [===========================>..] - ETA: 0s - loss: 0.1108
6530/6530 [==============================] - 2s 277us/step - loss: 0.1076 - val_loss: 0.0502

# training | RMSE: 0.2268, MAE: 0.1842
worker 0  xfile  [72, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16281607759160277}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26586403133864034}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.22678289840828497, 'rmse': 0.22678289840828497, 'mae': 0.1841735199273602, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  26 | activation: sigmoid | extras: None 
layer 2 | size:  46 | activation: relu    | extras: None 
layer 3 | size:  61 | activation: tanh    | extras: None 
layer 4 | size:  48 | activation: relu    | extras: dropout - rate: 13.5% 
layer 5 | size:  77 | activation: relu    | extras: dropout - rate: 36.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92fe3e160>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:52 - loss: 0.6434
 960/6530 [===>..........................] - ETA: 6s - loss: 0.5391  
1856/6530 [=======>......................] - ETA: 3s - loss: 0.4111
2752/6530 [===========>..................] - ETA: 1s - loss: 0.3084
3840/6530 [================>.............] - ETA: 0s - loss: 0.2418
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2063
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1807
# training | RMSE: 0.2264, MAE: 0.1851
worker 1  xfile  [71, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 16, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.22635639272841815, 'rmse': 0.22635639272841815, 'mae': 0.18508553161475486, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  70 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc928717588>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:39 - loss: 0.5483
# training | RMSE: 0.1850, MAE: 0.1488
worker 2  xfile  [70, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 12, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 25, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.18497009061848166, 'rmse': 0.18497009061848166, 'mae': 0.14877099002782737, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  28 | activation: tanh    | extras: None 
layer 2 | size:  88 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  79 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92d67ae10>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:54 - loss: 0.9538
1792/6530 [=======>......................] - ETA: 2s - loss: 0.1296  
 832/6530 [==>...........................] - ETA: 8s - loss: 0.4434  
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1057
1664/6530 [======>.......................] - ETA: 3s - loss: 0.3024
6530/6530 [==============================] - 2s 255us/step - loss: 0.1707 - val_loss: 0.0615

5312/6530 [=======================>......] - ETA: 0s - loss: 0.0934
2560/6530 [==========>...................] - ETA: 1s - loss: 0.2455
3520/6530 [===============>..............] - ETA: 1s - loss: 0.2082
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1789
6530/6530 [==============================] - 1s 199us/step - loss: 0.0870 - val_loss: 0.0631

5312/6530 [=======================>......] - ETA: 0s - loss: 0.1597
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1452
6530/6530 [==============================] - 2s 257us/step - loss: 0.1405 - val_loss: 0.5504

# training | RMSE: 0.2502, MAE: 0.2031
worker 1  xfile  [75, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 61, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2924372832826604}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2501728113103709, 'rmse': 0.2501728113103709, 'mae': 0.20313430891824696, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  75 | activation: sigmoid | extras: None 
layer 2 | size:  25 | activation: sigmoid | extras: None 
layer 3 | size:  21 | activation: sigmoid | extras: dropout - rate: 31.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc9285387f0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:46 - loss: 0.8286
 288/6530 [>.............................] - ETA: 22s - loss: 0.6350 
 576/6530 [=>............................] - ETA: 11s - loss: 0.5151
 896/6530 [===>..........................] - ETA: 7s - loss: 0.4102 
1232/6530 [====>.........................] - ETA: 5s - loss: 0.3600
1504/6530 [=====>........................] - ETA: 4s - loss: 0.3321
1760/6530 [=======>......................] - ETA: 3s - loss: 0.3157
2016/6530 [========>.....................] - ETA: 3s - loss: 0.3023
2304/6530 [=========>....................] - ETA: 2s - loss: 0.2929
2640/6530 [===========>..................] - ETA: 2s - loss: 0.2852
2992/6530 [============>.................] - ETA: 1s - loss: 0.2766
3312/6530 [==============>...............] - ETA: 1s - loss: 0.2706
3632/6530 [===============>..............] - ETA: 1s - loss: 0.2668
3920/6530 [=================>............] - ETA: 1s - loss: 0.2644
4192/6530 [==================>...........] - ETA: 0s - loss: 0.2601
# training | RMSE: 0.2490, MAE: 0.2038
worker 0  xfile  [73, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 61, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1345181000055521}, 'layer_4_size': 48, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3607717761113999}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.24901649615822305, 'rmse': 0.24901649615822305, 'mae': 0.20381030907145625, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  75 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  56 | activation: relu    | extras: batchnorm 
layer 3 | size:  45 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  62 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92fe3cf28>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 9:32 - loss: 0.6563
4544/6530 [===================>..........] - ETA: 0s - loss: 0.2557
 208/6530 [..............................] - ETA: 44s - loss: 0.6236 
4896/6530 [=====================>........] - ETA: 0s - loss: 0.2527
 384/6530 [>.............................] - ETA: 24s - loss: 0.5492
# training | RMSE: 0.7471, MAE: 0.6237
worker 2  xfile  [74, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 79, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11798350093744388}, 'layer_4_size': 82, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4187733871005491}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.7470965440891973, 'rmse': 0.7470965440891973, 'mae': 0.6236653514766912, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  52 | activation: relu    | extras: dropout - rate: 41.3% 
layer 2 | size:  32 | activation: relu    | extras: batchnorm 
layer 3 | size:  22 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  76 | activation: relu    | extras: dropout - rate: 36.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92d2f8cc0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 2:03 - loss: 0.9808
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2496
 576/6530 [=>............................] - ETA: 16s - loss: 0.4829
 960/6530 [===>..........................] - ETA: 7s - loss: 0.3445  
5616/6530 [========================>.....] - ETA: 0s - loss: 0.2465
 768/6530 [==>...........................] - ETA: 12s - loss: 0.4332
1856/6530 [=======>......................] - ETA: 3s - loss: 0.2464
6000/6530 [==========================>...] - ETA: 0s - loss: 0.2444
 960/6530 [===>..........................] - ETA: 9s - loss: 0.3944 
2816/6530 [===========>..................] - ETA: 1s - loss: 0.1989
6384/6530 [============================>.] - ETA: 0s - loss: 0.2424
1152/6530 [====>.........................] - ETA: 8s - loss: 0.3643
3712/6530 [================>.............] - ETA: 1s - loss: 0.1719
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1562
1344/6530 [=====>........................] - ETA: 6s - loss: 0.3426
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1442
1520/6530 [=====>........................] - ETA: 5s - loss: 0.3259
6336/6530 [============================>.] - ETA: 0s - loss: 0.1354
1696/6530 [======>.......................] - ETA: 5s - loss: 0.3124
6530/6530 [==============================] - 2s 342us/step - loss: 0.2417 - val_loss: 0.2086

1888/6530 [=======>......................] - ETA: 4s - loss: 0.2985
2064/6530 [========>.....................] - ETA: 4s - loss: 0.2885
2240/6530 [=========>....................] - ETA: 3s - loss: 0.2804
6530/6530 [==============================] - 2s 270us/step - loss: 0.1334 - val_loss: 0.0680

2416/6530 [==========>...................] - ETA: 3s - loss: 0.2744
2576/6530 [==========>...................] - ETA: 3s - loss: 0.2690
2736/6530 [===========>..................] - ETA: 3s - loss: 0.2624
2912/6530 [============>.................] - ETA: 2s - loss: 0.2558
3072/6530 [=============>................] - ETA: 2s - loss: 0.2499
3248/6530 [=============>................] - ETA: 2s - loss: 0.2448
3440/6530 [==============>...............] - ETA: 2s - loss: 0.2414
3648/6530 [===============>..............] - ETA: 1s - loss: 0.2367
3840/6530 [================>.............] - ETA: 1s - loss: 0.2330
4032/6530 [=================>............] - ETA: 1s - loss: 0.2296
4224/6530 [==================>...........] - ETA: 1s - loss: 0.2258
4432/6530 [===================>..........] - ETA: 1s - loss: 0.2228
4608/6530 [====================>.........] - ETA: 1s - loss: 0.2206
4768/6530 [====================>.........] - ETA: 1s - loss: 0.2183
4928/6530 [=====================>........] - ETA: 0s - loss: 0.2161
5088/6530 [======================>.......] - ETA: 0s - loss: 0.2141
5264/6530 [=======================>......] - ETA: 0s - loss: 0.2112
5424/6530 [=======================>......] - ETA: 0s - loss: 0.2097
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2077
5728/6530 [=========================>....] - ETA: 0s - loss: 0.2061
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2051
6048/6530 [==========================>...] - ETA: 0s - loss: 0.2034
6192/6530 [===========================>..] - ETA: 0s - loss: 0.2021
6352/6530 [============================>.] - ETA: 0s - loss: 0.2005
6496/6530 [============================>.] - ETA: 0s - loss: 0.1992
6530/6530 [==============================] - 4s 550us/step - loss: 0.1991 - val_loss: 0.2018

# training | RMSE: 0.2591, MAE: 0.2097
worker 2  xfile  [78, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4133704717959732}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 32, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3661626782085272}, 'layer_4_size': 76, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23222679665205323}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2591299870442675, 'rmse': 0.2591299870442675, 'mae': 0.209661623253728, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  70 | activation: sigmoid | extras: None 
layer 2 | size:   7 | activation: relu    | extras: dropout - rate: 39.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92d6a35c0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:04 - loss: 0.4418
 400/6530 [>.............................] - ETA: 14s - loss: 0.1407 
 800/6530 [==>...........................] - ETA: 7s - loss: 0.1005 
1280/6530 [====>.........................] - ETA: 4s - loss: 0.0795
1696/6530 [======>.......................] - ETA: 3s - loss: 0.0697
2080/6530 [========>.....................] - ETA: 2s - loss: 0.0649
2432/6530 [==========>...................] - ETA: 2s - loss: 0.0607
2704/6530 [===========>..................] - ETA: 1s - loss: 0.0585
2976/6530 [============>.................] - ETA: 1s - loss: 0.0566
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0548
3696/6530 [===============>..............] - ETA: 1s - loss: 0.0536
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0517
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0498
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0484
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0470
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0460
# training | RMSE: 0.2532, MAE: 0.2066
worker 0  xfile  [76, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 56, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4759499917698068}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.25316214520018193, 'rmse': 0.25316214520018193, 'mae': 0.20658920915086362, 'early_stop': False}
vggnet done  0

6530/6530 [==============================] - 2s 284us/step - loss: 0.0454 - val_loss: 0.0410

# training | RMSE: 0.2568, MAE: 0.2095
worker 1  xfile  [77, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 75, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 25, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3186953775811115}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.2567913747448272, 'rmse': 0.2567913747448272, 'mae': 0.20953951618737873, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  77 | activation: relu    | extras: batchnorm 
layer 2 | size:  98 | activation: tanh    | extras: dropout - rate: 36.3% 
layer 3 | size:  95 | activation: sigmoid | extras: batchnorm 
layer 4 | size:   6 | activation: sigmoid | extras: None 
layer 5 | size:  38 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc9283a90b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 1:09 - loss: 0.7795
1792/6530 [=======>......................] - ETA: 3s - loss: 0.3852  
3328/6530 [==============>...............] - ETA: 1s - loss: 0.2840
4736/6530 [====================>.........] - ETA: 0s - loss: 0.2442
6016/6530 [==========================>...] - ETA: 0s - loss: 0.2222
6530/6530 [==============================] - 2s 277us/step - loss: 0.2156 - val_loss: 0.1481

# training | RMSE: 0.2002, MAE: 0.1549
worker 2  xfile  [79, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39180291294730896}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3475827664775537}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.20019149433129757, 'rmse': 0.20019149433129757, 'mae': 0.15489580402274966, 'early_stop': False}
vggnet done  2

# training | RMSE: 0.1828, MAE: 0.1446
worker 1  xfile  [80, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3634795999481858}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 6, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.18278709077081648, 'rmse': 0.18278709077081648, 'mae': 0.14457362421463024, 'early_stop': False}
vggnet done  1
all of workers have been done
calculation finished
#2 epoch=1.0 loss={'loss': 0.25525409078301103, 'rmse': 0.25525409078301103, 'mae': 0.2066340653570517, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22287393078460238}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 62, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#1 epoch=1.0 loss={'loss': 0.20237436758353153, 'rmse': 0.20237436758353153, 'mae': 0.16210405021137456, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#0 epoch=1.0 loss={'loss': 0.18265415634906074, 'rmse': 0.18265415634906074, 'mae': 0.14186631529906155, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2150110655683083}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.30853774892657976}, 'layer_2_size': 45, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10136199605571763}, 'layer_3_size': 94, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#5 epoch=1.0 loss={'loss': 0.23015157651989426, 'rmse': 0.23015157651989426, 'mae': 0.1821995716994748, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 38, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.49077967255642363}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#6 epoch=1.0 loss={'loss': 0.20413120260236672, 'rmse': 0.20413120260236672, 'mae': 0.16144281152953788, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3733832559265301}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45475052824382667}, 'layer_4_size': 73, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25147212839194794}, 'layer_5_size': 13, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#3 epoch=1.0 loss={'loss': 0.21252898123144556, 'rmse': 0.21252898123144556, 'mae': 0.1723217818561635, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 22, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.19752241638812038}, 'layer_2_size': 9, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 70, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 23, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2780697801742765}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#4 epoch=1.0 loss={'loss': 0.23166259510331597, 'rmse': 0.23166259510331597, 'mae': 0.18784298885288736, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23723016883152925}, 'layer_1_size': 6, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3165904406088582}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22745862465174038}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3792834483496076}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#9 epoch=1.0 loss={'loss': 0.6911086360660346, 'rmse': 0.6911086360660346, 'mae': 0.6436048907185381, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2234126750360737}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1084275837765758}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2170455638724607}, 'layer_4_size': 75, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38932116854679577}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#8 epoch=1.0 loss={'loss': 0.3295668259425225, 'rmse': 0.3295668259425225, 'mae': 0.26305086872849703, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13479473667840766}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28352394709618267}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 37, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 81, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#10 epoch=1.0 loss={'loss': 0.20891904517593532, 'rmse': 0.20891904517593532, 'mae': 0.16914115016341547, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2108098130182266}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#7 epoch=1.0 loss={'loss': 0.24068571295454153, 'rmse': 0.24068571295454153, 'mae': 0.19197868792554124, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 59, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#12 epoch=1.0 loss={'loss': 0.20350925564023373, 'rmse': 0.20350925564023373, 'mae': 0.16483837110885421, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.341966490634019}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2913727541463967}, 'layer_3_size': 90, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23475121083658856}, 'layer_5_size': 15, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#13 epoch=1.0 loss={'loss': 0.27868994318097323, 'rmse': 0.27868994318097323, 'mae': 0.22606997544525542, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25520177460215787}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4002281187630936}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.36317162439805983}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#15 epoch=1.0 loss={'loss': 0.3463418664853805, 'rmse': 0.3463418664853805, 'mae': 0.27855898198707874, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 23, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4138471263039468}, 'layer_2_size': 53, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 58, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 37, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18207146516559847}, 'layer_5_size': 71, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#11 epoch=1.0 loss={'loss': 0.2250422008500032, 'rmse': 0.2250422008500032, 'mae': 0.18013781271391205, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41772377223411683}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4991790277388184}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16600289991869716}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11166500994018819}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#16 epoch=1.0 loss={'loss': 0.23281436824178556, 'rmse': 0.23281436824178556, 'mae': 0.1921631978383386, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 35, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22798208251503316}, 'layer_3_size': 61, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#17 epoch=1.0 loss={'loss': 0.19132747446863585, 'rmse': 0.19132747446863585, 'mae': 0.15440494575161703, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#14 epoch=1.0 loss={'loss': 0.14554345422591847, 'rmse': 0.14554345422591847, 'mae': 0.11602695269456904, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 54, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1642540019582842}, 'layer_3_size': 49, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 84, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#20 epoch=1.0 loss={'loss': 0.308956536853825, 'rmse': 0.308956536853825, 'mae': 0.2411589102238553, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 52, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4464609648303235}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2622069029337047}, 'layer_4_size': 42, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 58, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#18 epoch=1.0 loss={'loss': 0.2494638855270626, 'rmse': 0.2494638855270626, 'mae': 0.19840044229925388, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18933212958222226}, 'layer_2_size': 69, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 60, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19717756093010141}, 'layer_5_size': 33, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#21 epoch=1.0 loss={'loss': 0.29430989277202224, 'rmse': 0.29430989277202224, 'mae': 0.2330686018677908, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 7, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.48436536545622033}, 'layer_3_size': 13, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15949515245989582}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#19 epoch=1.0 loss={'loss': 0.13429060784786478, 'rmse': 0.13429060784786478, 'mae': 0.10502809123000229, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 40, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 14, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4807453823900655}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#22 epoch=1.0 loss={'loss': 0.23765483956164085, 'rmse': 0.23765483956164085, 'mae': 0.19352895695148672, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41082052473880537}, 'layer_2_size': 3, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23078166883833942}, 'layer_3_size': 64, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.45495583915048865}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#25 epoch=1.0 loss={'loss': 0.4531362325492726, 'rmse': 0.4531362325492726, 'mae': 0.3820690153317292, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2902882899441013}, 'layer_3_size': 61, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 38, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#23 epoch=1.0 loss={'loss': 0.5233968034935774, 'rmse': 0.5233968034935774, 'mae': 0.4444681019275501, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3012706228163049}, 'layer_1_size': 65, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 87, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1622125798402077}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#24 epoch=1.0 loss={'loss': 0.2876131687146577, 'rmse': 0.2876131687146577, 'mae': 0.2318522185937443, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.20745875785035764}, 'layer_1_size': 25, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 53, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#26 epoch=1.0 loss={'loss': 0.2541637219928215, 'rmse': 0.2541637219928215, 'mae': 0.20779089862401368, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 11, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47984720572905093}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16912925021930786}, 'layer_4_size': 48, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#27 epoch=1.0 loss={'loss': 0.1726693652664509, 'rmse': 0.1726693652664509, 'mae': 0.13516991707657652, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18160482028331157}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2972163783791718}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3222195153732551}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#29 epoch=1.0 loss={'loss': 0.22156884621205752, 'rmse': 0.22156884621205752, 'mae': 0.17015768174125306, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 79, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 20, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 16, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2853979696618783}, 'layer_5_size': 87, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#28 epoch=1.0 loss={'loss': 0.9630671046564773, 'rmse': 0.9630671046564773, 'mae': 0.9467191976708993, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2611965458343125}, 'layer_1_size': 76, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.46751205660245876}, 'layer_2_size': 94, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#31 epoch=1.0 loss={'loss': 0.5073865153688992, 'rmse': 0.5073865153688992, 'mae': 0.4010024806164959, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 87, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20814825567027848}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#30 epoch=1.0 loss={'loss': 0.15800770883144385, 'rmse': 0.15800770883144385, 'mae': 0.12662641973845362, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18270289824801844}, 'layer_2_size': 96, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 100, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2588783119778918}, 'layer_5_size': 58, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#32 epoch=1.0 loss={'loss': 0.1313159652416736, 'rmse': 0.1313159652416736, 'mae': 0.10376423394910837, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1894976648203621}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16212772803130662}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42662975185148233}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#33 epoch=1.0 loss={'loss': 0.43980243887831244, 'rmse': 0.43980243887831244, 'mae': 0.35275858992183995, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.274374808804679}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
#35 epoch=1.0 loss={'loss': 0.30301166290521797, 'rmse': 0.30301166290521797, 'mae': 0.2466623465917509, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.466603952813784}, 'layer_1_size': 83, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 18, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 100, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1453841631243427}, 'layer_4_size': 60, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#36 epoch=1.0 loss={'loss': 0.2761555182787955, 'rmse': 0.2761555182787955, 'mae': 0.2152482482891386, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1042701844285122}, 'layer_3_size': 10, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#34 epoch=1.0 loss={'loss': 0.1937377282154803, 'rmse': 0.1937377282154803, 'mae': 0.1559545333779845, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1905469287551103}, 'layer_4_size': 84, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#37 epoch=1.0 loss={'loss': 0.20648472448080538, 'rmse': 0.20648472448080538, 'mae': 0.16753619517393376, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 56, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15972309563133835}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#38 epoch=1.0 loss={'loss': 0.2474400226054578, 'rmse': 0.2474400226054578, 'mae': 0.1941052148082133, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.49199211984550784}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 55, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 74, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#39 epoch=1.0 loss={'loss': 0.2097414770287425, 'rmse': 0.2097414770287425, 'mae': 0.16888311904883072, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 26, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#41 epoch=1.0 loss={'loss': 0.2909710196421898, 'rmse': 0.2909710196421898, 'mae': 0.2492496113930572, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 38, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.40621233779217614}, 'layer_2_size': 30, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 34, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#40 epoch=1.0 loss={'loss': 0.260846066501631, 'rmse': 0.260846066501631, 'mae': 0.21249403865450392, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18158560923298125}, 'layer_2_size': 48, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 62, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#42 epoch=1.0 loss={'loss': 0.3355202621617262, 'rmse': 0.3355202621617262, 'mae': 0.27439076153572023, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 6, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3391769490316632}, 'layer_2_size': 17, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2680918038208383}, 'layer_4_size': 69, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
#44 epoch=1.0 loss={'loss': 0.2575467586570486, 'rmse': 0.2575467586570486, 'mae': 0.20447594659830684, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 44, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 12, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 10, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#43 epoch=1.0 loss={'loss': 0.24820613423298793, 'rmse': 0.24820613423298793, 'mae': 0.19729903484363015, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 27, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#46 epoch=1.0 loss={'loss': 0.31165223741440595, 'rmse': 0.31165223741440595, 'mae': 0.25112303368033445, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10403732447552155}, 'layer_1_size': 71, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21971599150711124}, 'layer_3_size': 19, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 5, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#47 epoch=1.0 loss={'loss': 0.2702519415014441, 'rmse': 0.2702519415014441, 'mae': 0.2193346591611518, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#45 epoch=1.0 loss={'loss': 0.19283302748724637, 'rmse': 0.19283302748724637, 'mae': 0.15480607706201402, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3310992508704168}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 99, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15469433863810475}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#48 epoch=1.0 loss={'loss': 0.23583336818385997, 'rmse': 0.23583336818385997, 'mae': 0.18180263356174303, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 21, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#49 epoch=1.0 loss={'loss': 0.1846137048174472, 'rmse': 0.1846137048174472, 'mae': 0.14193256257221162, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3239797430120135}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#50 epoch=1.0 loss={'loss': 0.2073798404924326, 'rmse': 0.2073798404924326, 'mae': 0.15951083474959415, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35610897362546245}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#51 epoch=1.0 loss={'loss': 0.3018215571205848, 'rmse': 0.3018215571205848, 'mae': 0.2412181854190126, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.36405670543434376}, 'layer_3_size': 46, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.18514730240839805}, 'layer_4_size': 61, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#54 epoch=1.0 loss={'loss': 0.5108101369309934, 'rmse': 0.5108101369309934, 'mae': 0.4526478303299612, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23042388916549342}, 'layer_2_size': 2, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19208008183545605}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#52 epoch=1.0 loss={'loss': 0.16479095120122836, 'rmse': 0.16479095120122836, 'mae': 0.1271585036799888, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 37, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14326840322746015}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45719432982493546}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.43560069792488576}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#53 epoch=1.0 loss={'loss': 0.20863524584019735, 'rmse': 0.20863524584019735, 'mae': 0.16747014432569035, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21297786188304155}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#55 epoch=1.0 loss={'loss': 0.20567013094161413, 'rmse': 0.20567013094161413, 'mae': 0.1661717880593185, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37777862236608384}, 'layer_3_size': 67, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#58 epoch=1.0 loss={'loss': 0.202936994188723, 'rmse': 0.202936994188723, 'mae': 0.1591844750324925, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 51, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.40017991511747686}, 'layer_4_size': 24, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18773446907879549}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#57 epoch=1.0 loss={'loss': 0.30423868779676344, 'rmse': 0.30423868779676344, 'mae': 0.25614066086246284, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4360415196650578}, 'layer_2_size': 79, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 95, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#56 epoch=1.0 loss={'loss': 0.2556056073794619, 'rmse': 0.2556056073794619, 'mae': 0.20892976060273938, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13567262716003206}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2326240252899456}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#61 epoch=1.0 loss={'loss': 0.2246003420658188, 'rmse': 0.2246003420658188, 'mae': 0.18288164983674396, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21611938768998995}, 'layer_2_size': 95, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 37, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 5, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 31, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#60 epoch=1.0 loss={'loss': 0.20980304073235106, 'rmse': 0.20980304073235106, 'mae': 0.16990084220414836, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1443971582596648}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27465858512765084}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3621760645395955}, 'layer_4_size': 13, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#62 epoch=1.0 loss={'loss': 0.24020893381351385, 'rmse': 0.24020893381351385, 'mae': 0.19298023966716887, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1571438211657431}, 'layer_1_size': 37, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 17, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 80, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 47, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#59 epoch=1.0 loss={'loss': 0.22614737878768285, 'rmse': 0.22614737878768285, 'mae': 0.17852631581568756, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 25, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 39, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
#63 epoch=1.0 loss={'loss': 0.2852781242839184, 'rmse': 0.2852781242839184, 'mae': 0.23659314224796563, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.35780144767704614}, 'layer_3_size': 54, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 81, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 84, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}
#64 epoch=1.0 loss={'loss': 0.2643319719774942, 'rmse': 0.2643319719774942, 'mae': 0.2203130197352707, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.14158320923807055}, 'layer_2_size': 17, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#66 epoch=1.0 loss={'loss': 0.37887572313901, 'rmse': 0.37887572313901, 'mae': 0.3201636343475339, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 28, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17428262977474496}, 'layer_3_size': 91, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 21, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#67 epoch=1.0 loss={'loss': 0.25879347846388795, 'rmse': 0.25879347846388795, 'mae': 0.21158981122571813, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3458900173434585}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2903589045207877}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3798607936703877}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}
#65 epoch=1.0 loss={'loss': 0.2087397443287876, 'rmse': 0.2087397443287876, 'mae': 0.17031296817374092, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3600223727260521}, 'layer_1_size': 73, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 86, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 23, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 87, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#68 epoch=1.0 loss={'loss': 0.28817775396127776, 'rmse': 0.28817775396127776, 'mae': 0.23281951706997503, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17299076057675788}, 'layer_1_size': 81, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 3, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 67, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#69 epoch=1.0 loss={'loss': 0.26125172720322937, 'rmse': 0.26125172720322937, 'mae': 0.21472352543911194, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2206849446027349}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 62, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 10, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
#72 epoch=1.0 loss={'loss': 0.22678289840828497, 'rmse': 0.22678289840828497, 'mae': 0.1841735199273602, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16281607759160277}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.26586403133864034}, 'layer_3_size': 39, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 57, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#70 epoch=1.0 loss={'loss': 0.18497009061848166, 'rmse': 0.18497009061848166, 'mae': 0.14877099002782737, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 12, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 25, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': True}
#71 epoch=1.0 loss={'loss': 0.22635639272841815, 'rmse': 0.22635639272841815, 'mae': 0.18508553161475486, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 16, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#73 epoch=1.0 loss={'loss': 0.24901649615822305, 'rmse': 0.24901649615822305, 'mae': 0.20381030907145625, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 26, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 46, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 61, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1345181000055521}, 'layer_4_size': 48, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3607717761113999}, 'layer_5_size': 77, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#75 epoch=1.0 loss={'loss': 0.2501728113103709, 'rmse': 0.2501728113103709, 'mae': 0.20313430891824696, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 61, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 51, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2924372832826604}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}
#74 epoch=1.0 loss={'loss': 0.7470965440891973, 'rmse': 0.7470965440891973, 'mae': 0.6236653514766912, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 28, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 79, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.11798350093744388}, 'layer_4_size': 82, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4187733871005491}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#78 epoch=1.0 loss={'loss': 0.2591299870442675, 'rmse': 0.2591299870442675, 'mae': 0.209661623253728, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4133704717959732}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 32, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3661626782085272}, 'layer_4_size': 76, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23222679665205323}, 'layer_5_size': 50, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#77 epoch=1.0 loss={'loss': 0.2567913747448272, 'rmse': 0.2567913747448272, 'mae': 0.20953951618737873, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 75, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 25, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3186953775811115}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
#76 epoch=1.0 loss={'loss': 0.25316214520018193, 'rmse': 0.25316214520018193, 'mae': 0.20658920915086362, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 56, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 45, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 62, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4759499917698068}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#79 epoch=1.0 loss={'loss': 0.20019149433129757, 'rmse': 0.20019149433129757, 'mae': 0.15489580402274966, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39180291294730896}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3475827664775537}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#80 epoch=1.0 loss={'loss': 0.18278709077081648, 'rmse': 0.18278709077081648, 'mae': 0.14457362421463024, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3634795999481858}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 6, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
get a list [results] of length 81
get a list [loss] of length 81
get a list [val_loss] of length 81
length of indices is [32 21 17 31 53 27  2 80 49 71 16 47 36 79  1 56 11  4 55 37 50 54 67  9
 39 60  5 28 59 14 62 72 70  3  6 15 48 22 61 10 38 44 73 19 74 78 26  0
 58 77 43 66 76 41 69 64 46 35 12 63 25 68 40 20 51 34 57 18 45  8 42 13
 65 33 23 30 52 24  7 75 29]
length of indices is 81
length of T is 81
newly formed T structure is:[[0, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1894976648203621}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16212772803130662}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42662975185148233}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [1, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 7, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.48436536545622033}, 'layer_3_size': 13, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15949515245989582}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [2, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [3, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 87, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20814825567027848}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [4, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21297786188304155}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [5, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18160482028331157}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2972163783791718}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3222195153732551}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [6, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22287393078460238}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 62, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [7, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3634795999481858}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 6, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [8, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3239797430120135}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [9, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 16, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [10, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 35, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22798208251503316}, 'layer_3_size': 61, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [11, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [12, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1042701844285122}, 'layer_3_size': 10, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [13, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39180291294730896}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3475827664775537}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [14, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [15, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13567262716003206}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2326240252899456}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [16, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41772377223411683}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4991790277388184}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16600289991869716}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11166500994018819}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [17, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23723016883152925}, 'layer_1_size': 6, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3165904406088582}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22745862465174038}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3792834483496076}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [18, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37777862236608384}, 'layer_3_size': 67, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [19, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 56, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15972309563133835}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [20, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35610897362546245}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [21, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23042388916549342}, 'layer_2_size': 2, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19208008183545605}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [22, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3458900173434585}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2903589045207877}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3798607936703877}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}], [23, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2234126750360737}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1084275837765758}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2170455638724607}, 'layer_4_size': 75, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38932116854679577}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [24, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 26, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [25, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1443971582596648}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27465858512765084}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3621760645395955}, 'layer_4_size': 13, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [26, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 38, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.49077967255642363}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]] 

*** 27.0 configurations x 3.0 iterations each

81 | Thu Sep 27 22:13:29 2018 | lowest loss so far: 0.1313 (run 32)

{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  89 | activation: relu    | extras: batchnorm 
layer 2 | size:  88 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 5:26 - loss: 0.6624
 336/6530 [>.............................] - ETA: 15s - loss: 0.2686 
 608/6530 [=>............................] - ETA: 8s - loss: 0.2329 
 976/6530 [===>..........................] - ETA: 5s - loss: 0.2097
1344/6530 [=====>........................] - ETA: 3s - loss: 0.1956{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  85 | activation: relu    | extras: None 
layer 2 | size:  98 | activation: relu    | extras: batchnorm 
layer 3 | size:  71 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 41s - loss: 0.6199
1744/6530 [=======>......................] - ETA: 2s - loss: 0.1850
2816/6530 [===========>..................] - ETA: 1s - loss: 0.2326 
2128/6530 [========>.....................] - ETA: 2s - loss: 0.1803
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1872
2528/6530 [==========>...................] - ETA: 1s - loss: 0.1743
6530/6530 [==============================] - 1s 149us/step - loss: 0.1807 - val_loss: 0.1542
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1584
2928/6530 [============>.................] - ETA: 1s - loss: 0.1687
3200/6530 [=============>................] - ETA: 0s - loss: 0.1253
3312/6530 [==============>...............] - ETA: 1s - loss: 0.1639
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1212
6530/6530 [==============================] - 0s 19us/step - loss: 0.1196 - val_loss: 0.1392
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1158
3680/6530 [===============>..............] - ETA: 1s - loss: 0.1605
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1063
4080/6530 [=================>............] - ETA: 0s - loss: 0.1582
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1066
6530/6530 [==============================] - 0s 19us/step - loss: 0.1059 - val_loss: 0.1402

4496/6530 [===================>..........] - ETA: 0s - loss: 0.1556
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1522
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1495{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:   7 | activation: relu    | extras: None 
layer 2 | size:  11 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  13 | activation: relu    | extras: dropout - rate: 48.4% 
layer 4 | size:  12 | activation: relu    | extras: batchnorm 
layer 5 | size:  32 | activation: tanh    | extras: dropout - rate: 15.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 52s - loss: 0.7095
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1474
1792/6530 [=======>......................] - ETA: 2s - loss: 0.6268 
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1455
3712/6530 [================>.............] - ETA: 0s - loss: 0.5425
6528/6530 [============================>.] - ETA: 0s - loss: 0.1441
5504/6530 [========================>.....] - ETA: 0s - loss: 0.4750
6530/6530 [==============================] - 2s 264us/step - loss: 0.1441 - val_loss: 0.1220
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1084
 448/6530 [=>............................] - ETA: 0s - loss: 0.1244
6530/6530 [==============================] - 1s 198us/step - loss: 0.4450 - val_loss: 0.2538
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2881
 848/6530 [==>...........................] - ETA: 0s - loss: 0.1208
1792/6530 [=======>......................] - ETA: 0s - loss: 0.2608
1264/6530 [====>.........................] - ETA: 0s - loss: 0.1195
3712/6530 [================>.............] - ETA: 0s - loss: 0.2503
1648/6530 [======>.......................] - ETA: 0s - loss: 0.1189
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2448
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1164
6530/6530 [==============================] - 0s 30us/step - loss: 0.2435 - val_loss: 0.2246
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2319
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1145
1920/6530 [=======>......................] - ETA: 0s - loss: 0.2265
2864/6530 [============>.................] - ETA: 0s - loss: 0.1142
3840/6530 [================>.............] - ETA: 0s - loss: 0.2254
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1129
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2272
6530/6530 [==============================] - 0s 29us/step - loss: 0.2280 - val_loss: 0.2177

3664/6530 [===============>..............] - ETA: 0s - loss: 0.1130
4064/6530 [=================>............] - ETA: 0s - loss: 0.1120
# training | RMSE: 0.1650, MAE: 0.1314
worker 2  xfile  [2, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.16504812945088806, 'rmse': 0.16504812945088806, 'mae': 0.1313625544093016, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  81 | activation: relu    | extras: batchnorm 
layer 2 | size:  91 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca41f14e0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 7s - loss: 1.0908
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1121
6144/6530 [===========================>..] - ETA: 0s - loss: 0.5298
6530/6530 [==============================] - 0s 61us/step - loss: 0.5174 - val_loss: 0.3128

4848/6530 [=====================>........] - ETA: 0s - loss: 0.1123Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2976
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1117
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2757
6530/6530 [==============================] - 0s 10us/step - loss: 0.2717 - val_loss: 0.2310
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2223
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1117
6400/6530 [============================>.] - ETA: 0s - loss: 0.2162
6530/6530 [==============================] - 0s 9us/step - loss: 0.2160 - val_loss: 0.1945

6080/6530 [==========================>...] - ETA: 0s - loss: 0.1114
6496/6530 [============================>.] - ETA: 0s - loss: 0.1111
6530/6530 [==============================] - 1s 130us/step - loss: 0.1112 - val_loss: 0.1198
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0995
 416/6530 [>.............................] - ETA: 0s - loss: 0.1008
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1020
1232/6530 [====>.........................] - ETA: 0s - loss: 0.1031
1616/6530 [======>.......................] - ETA: 0s - loss: 0.1008
1968/6530 [========>.....................] - ETA: 0s - loss: 0.1022
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1022
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1024
# training | RMSE: 0.2456, MAE: 0.1904
worker 2  xfile  [3, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 87, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20814825567027848}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.24564403047752328, 'rmse': 0.24564403047752328, 'mae': 0.19042188750146072, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  64 | activation: relu    | extras: None 
layer 2 | size:  41 | activation: relu    | extras: dropout - rate: 18.2% 
layer 3 | size:  24 | activation: sigmoid | extras: dropout - rate: 29.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca02b2f98>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 42s - loss: 0.5728
3136/6530 [=============>................] - ETA: 0s - loss: 0.1028
1024/6530 [===>..........................] - ETA: 1s - loss: 0.3120 
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1023
2016/6530 [========>.....................] - ETA: 0s - loss: 0.2612
3936/6530 [=================>............] - ETA: 0s - loss: 0.1024
2944/6530 [============>.................] - ETA: 0s - loss: 0.2422
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1024
3872/6530 [================>.............] - ETA: 0s - loss: 0.2277
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1025
4832/6530 [=====================>........] - ETA: 0s - loss: 0.2167
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1023
5792/6530 [=========================>....] - ETA: 0s - loss: 0.2086
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1022
6530/6530 [==============================] - 1s 91us/step - loss: 0.2029 - val_loss: 0.1443
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1761
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1026
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1609
6336/6530 [============================>.] - ETA: 0s - loss: 0.1024
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1578
6530/6530 [==============================] - 1s 134us/step - loss: 0.1027 - val_loss: 0.1073

2912/6530 [============>.................] - ETA: 0s - loss: 0.1555
3936/6530 [=================>............] - ETA: 0s - loss: 0.1528
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1515
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1503
6530/6530 [==============================] - 0s 54us/step - loss: 0.1488 - val_loss: 0.1267
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1422
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1386
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1366
# training | RMSE: 0.2687, MAE: 0.2198
worker 1  xfile  [1, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 7, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.48436536545622033}, 'layer_3_size': 13, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15949515245989582}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2686896000615916, 'rmse': 0.2686896000615916, 'mae': 0.2198467637312069, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:   9 | activation: relu    | extras: dropout - rate: 21.3% 
layer 2 | size:  60 | activation: tanh    | extras: batchnorm 
layer 3 | size:  44 | activation: tanh    | extras: None 
layer 4 | size:  79 | activation: sigmoid | extras: batchnorm 
layer 5 | size: 100 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca41f1518>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 1:13 - loss: 0.6383
2880/6530 [============>.................] - ETA: 0s - loss: 0.1373
 896/6530 [===>..........................] - ETA: 4s - loss: 0.4764  
3840/6530 [================>.............] - ETA: 0s - loss: 0.1358
1728/6530 [======>.......................] - ETA: 2s - loss: 0.3230
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1352
2560/6530 [==========>...................] - ETA: 1s - loss: 0.2390
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1348
# training | RMSE: 0.1263, MAE: 0.0993
worker 0  xfile  [0, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1894976648203621}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16212772803130662}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42662975185148233}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.12631875809934154, 'rmse': 0.12631875809934154, 'mae': 0.0993112349286573, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  86 | activation: sigmoid | extras: dropout - rate: 22.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca41f1470>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 6s - loss: 0.9068
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1954
6530/6530 [==============================] - 0s 56us/step - loss: 0.1340 - val_loss: 0.1164

5504/6530 [========================>.....] - ETA: 0s - loss: 0.2261
4096/6530 [=================>............] - ETA: 0s - loss: 0.1678
6530/6530 [==============================] - 0s 31us/step - loss: 0.2040 - val_loss: 0.0783
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0807
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1470
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0636
6530/6530 [==============================] - 0s 10us/step - loss: 0.0616 - val_loss: 0.0491
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0508
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1331
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0473
6530/6530 [==============================] - 0s 10us/step - loss: 0.0468 - val_loss: 0.0436

6464/6530 [============================>.] - ETA: 0s - loss: 0.1230
6530/6530 [==============================] - 1s 188us/step - loss: 0.1221 - val_loss: 0.0400
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0478
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0407
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0407
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0404
3200/6530 [=============>................] - ETA: 0s - loss: 0.0414
# training | RMSE: 0.2105, MAE: 0.1701
worker 0  xfile  [6, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22287393078460238}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 62, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.21050511441568898, 'rmse': 0.21050511441568898, 'mae': 0.17013935961284415, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  90 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca02eb160>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 54s - loss: 1.0483
3904/6530 [================>.............] - ETA: 0s - loss: 0.0415
 672/6530 [==>...........................] - ETA: 1s - loss: 0.5375 
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0412
1360/6530 [=====>........................] - ETA: 0s - loss: 0.3728
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0407
2048/6530 [========>.....................] - ETA: 0s - loss: 0.3075
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0408
2752/6530 [===========>..................] - ETA: 0s - loss: 0.2739
6530/6530 [==============================] - 0s 70us/step - loss: 0.0408 - val_loss: 0.0347
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0382
3472/6530 [==============>...............] - ETA: 0s - loss: 0.2513
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0408
4192/6530 [==================>...........] - ETA: 0s - loss: 0.2358
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0385
4912/6530 [=====================>........] - ETA: 0s - loss: 0.2245
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0375
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2161
3072/6530 [=============>................] - ETA: 0s - loss: 0.0383
6192/6530 [===========================>..] - ETA: 0s - loss: 0.2105
3840/6530 [================>.............] - ETA: 0s - loss: 0.0382
6530/6530 [==============================] - 1s 99us/step - loss: 0.2071 - val_loss: 0.1492
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1489
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0380
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1450
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0376
1424/6530 [=====>........................] - ETA: 0s - loss: 0.1437
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0376
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1420
6530/6530 [==============================] - 0s 70us/step - loss: 0.0379 - val_loss: 0.0338

2848/6530 [============>.................] - ETA: 0s - loss: 0.1425
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1419
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1410
# training | RMSE: 0.1445, MAE: 0.1109
worker 2  xfile  [5, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18160482028331157}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2972163783791718}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3222195153732551}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.14453920086003996, 'rmse': 0.14453920086003996, 'mae': 0.11086149231331438, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  77 | activation: relu    | extras: batchnorm 
layer 2 | size:  98 | activation: tanh    | extras: dropout - rate: 36.3% 
layer 3 | size:  95 | activation: sigmoid | extras: batchnorm 
layer 4 | size:   6 | activation: sigmoid | extras: None 
layer 5 | size:  38 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc8b6a96d8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 26s - loss: 1.0287
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1409
2176/6530 [========>.....................] - ETA: 1s - loss: 0.4989 
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1401
4096/6530 [=================>............] - ETA: 0s - loss: 0.3443
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1404
6016/6530 [==========================>...] - ETA: 0s - loss: 0.2800
6530/6530 [==============================] - 1s 77us/step - loss: 0.1399 - val_loss: 0.1333
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1241
 752/6530 [==>...........................] - ETA: 0s - loss: 0.1292
6530/6530 [==============================] - 1s 117us/step - loss: 0.2687 - val_loss: 0.1425
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1320
1456/6530 [=====>........................] - ETA: 0s - loss: 0.1290
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1326
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1289
3840/6530 [================>.............] - ETA: 0s - loss: 0.1263
2880/6530 [============>.................] - ETA: 0s - loss: 0.1292
# training | RMSE: 0.1810, MAE: 0.1425
worker 1  xfile  [4, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21297786188304155}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.18101866090818358, 'rmse': 0.18101866090818358, 'mae': 0.14253699162101557, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  16 | activation: tanh    | extras: None 
layer 2 | size:   9 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc93645f60>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:26 - loss: 1.3504
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1235
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1294
6530/6530 [==============================] - 0s 30us/step - loss: 0.1222 - val_loss: 0.1069

 608/6530 [=>............................] - ETA: 2s - loss: 0.8965  Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1127
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1285
1200/6530 [====>.........................] - ETA: 1s - loss: 0.6042
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1125
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1284
1792/6530 [=======>......................] - ETA: 0s - loss: 0.4424
3968/6530 [=================>............] - ETA: 0s - loss: 0.1105
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1279
2368/6530 [=========>....................] - ETA: 0s - loss: 0.3547
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1098
6530/6530 [==============================] - 0s 29us/step - loss: 0.1102 - val_loss: 0.1441

6288/6530 [===========================>..] - ETA: 0s - loss: 0.1284
2960/6530 [============>.................] - ETA: 0s - loss: 0.2984
6530/6530 [==============================] - 0s 75us/step - loss: 0.1278 - val_loss: 0.1256

3568/6530 [===============>..............] - ETA: 0s - loss: 0.2603
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2327
4752/6530 [====================>.........] - ETA: 0s - loss: 0.2120
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1946
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1805
6530/6530 [==============================] - 1s 124us/step - loss: 0.1703 - val_loss: 0.0572
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0698
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0613
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0582
# training | RMSE: 0.1766, MAE: 0.1444
worker 2  xfile  [7, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3634795999481858}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 6, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.17656164724795378, 'rmse': 0.17656164724795378, 'mae': 0.1443546736173766, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  35 | activation: sigmoid | extras: None 
layer 2 | size:  23 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc88166748>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 20s - loss: 0.3046
# training | RMSE: 0.1651, MAE: 0.1212
worker 0  xfile  [8, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3239797430120135}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.16505105273537213, 'rmse': 0.16505105273537213, 'mae': 0.12116673396791514, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:   4 | activation: tanh    | extras: None 
layer 2 | size:  73 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca01924a8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 7s - loss: 0.9512
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0563
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0910 
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1001
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0547
6530/6530 [==============================] - 0s 36us/step - loss: 0.0932 - val_loss: 0.0672

5120/6530 [======================>.......] - ETA: 0s - loss: 0.0677Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0592
3088/6530 [=============>................] - ETA: 0s - loss: 0.0528
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0668
6530/6530 [==============================] - 0s 56us/step - loss: 0.0615 - val_loss: 0.0526
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0465
6530/6530 [==============================] - 0s 11us/step - loss: 0.0669 - val_loss: 0.0696

3696/6530 [===============>..............] - ETA: 0s - loss: 0.0523Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0747
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0409
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0649
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0512
6530/6530 [==============================] - 0s 11us/step - loss: 0.0642 - val_loss: 0.0685

4864/6530 [=====================>........] - ETA: 0s - loss: 0.0408
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0509
6530/6530 [==============================] - 0s 22us/step - loss: 0.0399 - val_loss: 0.0475
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0415
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0503
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0390
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0493
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0385
6528/6530 [============================>.] - ETA: 0s - loss: 0.0487
6530/6530 [==============================] - 0s 21us/step - loss: 0.0380 - val_loss: 0.0448

6530/6530 [==============================] - 1s 89us/step - loss: 0.0486 - val_loss: 0.0433
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0490
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0458
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0436
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0429
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0425
3120/6530 [=============>................] - ETA: 0s - loss: 0.0414
3712/6530 [================>.............] - ETA: 0s - loss: 0.0419
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0418
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0421
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0418
# training | RMSE: 0.2642, MAE: 0.2148
worker 0  xfile  [11, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2642457384774719, 'rmse': 0.2642457384774719, 'mae': 0.21476207389128554, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  62 | activation: tanh    | extras: batchnorm 
layer 2 | size:  93 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca00666d8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 7s - loss: 0.6898
# training | RMSE: 0.2101, MAE: 0.1721
worker 2  xfile  [10, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 35, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22798208251503316}, 'layer_3_size': 61, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.21010246869613358, 'rmse': 0.21010246869613358, 'mae': 0.1721366206859716, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  70 | activation: sigmoid | extras: None 
layer 2 | size:   7 | activation: relu    | extras: dropout - rate: 39.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc8005b5c0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:36 - loss: 0.2138
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0418
6400/6530 [============================>.] - ETA: 0s - loss: 0.4035
 592/6530 [=>............................] - ETA: 2s - loss: 0.1354  
6530/6530 [==============================] - 1s 88us/step - loss: 0.0415 - val_loss: 0.0415

6530/6530 [==============================] - 0s 61us/step - loss: 0.3995 - val_loss: 0.2176
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2143
1184/6530 [====>.........................] - ETA: 1s - loss: 0.1011
6530/6530 [==============================] - 0s 8us/step - loss: 0.1799 - val_loss: 0.2112
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1700
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0832
6530/6530 [==============================] - 0s 9us/step - loss: 0.1608 - val_loss: 0.1787

2352/6530 [=========>....................] - ETA: 0s - loss: 0.0743
2960/6530 [============>.................] - ETA: 0s - loss: 0.0679
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0629
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0598
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0578
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0557
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0544
# training | RMSE: 0.2353, MAE: 0.1839
worker 0  xfile  [12, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1042701844285122}, 'layer_3_size': 10, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.23533220860188045, 'rmse': 0.23533220860188045, 'mae': 0.18392197791669193, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  42 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca01aac18>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 17s - loss: 0.6314
6530/6530 [==============================] - 1s 128us/step - loss: 0.0534 - val_loss: 0.0491
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0665
2752/6530 [===========>..................] - ETA: 0s - loss: 0.4279 
 544/6530 [=>............................] - ETA: 0s - loss: 0.0370
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2652
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0374
6530/6530 [==============================] - 0s 48us/step - loss: 0.2323 - val_loss: 0.0414
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0343
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0383
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0404
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0390
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0412
6530/6530 [==============================] - 0s 19us/step - loss: 0.0409 - val_loss: 0.0415
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0531
2896/6530 [============>.................] - ETA: 0s - loss: 0.0386
2880/6530 [============>.................] - ETA: 0s - loss: 0.0407
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0376
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0407
4016/6530 [=================>............] - ETA: 0s - loss: 0.0380
6530/6530 [==============================] - 0s 19us/step - loss: 0.0408 - val_loss: 0.0410

4608/6530 [====================>.........] - ETA: 0s - loss: 0.0381
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0382
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0378
6336/6530 [============================>.] - ETA: 0s - loss: 0.0382
6530/6530 [==============================] - 1s 92us/step - loss: 0.0382 - val_loss: 0.0365
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0399
 624/6530 [=>............................] - ETA: 0s - loss: 0.0350
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0348
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0345
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0342
# training | RMSE: 0.2032, MAE: 0.1656
worker 1  xfile  [9, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 16, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.20321158293135516, 'rmse': 0.20321158293135516, 'mae': 0.1655600579163336, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  72 | activation: sigmoid | extras: dropout - rate: 13.6% 
layer 2 | size:  82 | activation: sigmoid | extras: dropout - rate: 23.3% 
layer 3 | size:  33 | activation: tanh    | extras: batchnorm 
layer 4 | size:   9 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca004e5c0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 3:50 - loss: 0.8844
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0342
 240/6530 [>.............................] - ETA: 16s - loss: 0.7350 
3232/6530 [=============>................] - ETA: 0s - loss: 0.0343
 480/6530 [=>............................] - ETA: 8s - loss: 0.7016 
3776/6530 [================>.............] - ETA: 0s - loss: 0.0345
 720/6530 [==>...........................] - ETA: 5s - loss: 0.6774
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0346
 912/6530 [===>..........................] - ETA: 4s - loss: 0.6551
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0340
1136/6530 [====>.........................] - ETA: 3s - loss: 0.6238
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0337
1392/6530 [=====>........................] - ETA: 3s - loss: 0.5902
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0336
1648/6530 [======>.......................] - ETA: 2s - loss: 0.5649
6530/6530 [==============================] - 1s 97us/step - loss: 0.0335 - val_loss: 0.0345

1904/6530 [=======>......................] - ETA: 2s - loss: 0.5380
# training | RMSE: 0.2013, MAE: 0.1611
worker 0  xfile  [14, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2013112425966048, 'rmse': 0.2013112425966048, 'mae': 0.16106085720449054, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  50 | activation: tanh    | extras: batchnorm 
layer 2 | size:  21 | activation: tanh    | extras: dropout - rate: 41.8% 
layer 3 | size:  67 | activation: relu    | extras: dropout - rate: 49.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc5c79e080>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:34 - loss: 1.2860
2160/6530 [========>.....................] - ETA: 2s - loss: 0.5141
 368/6530 [>.............................] - ETA: 7s - loss: 0.6577  
2432/6530 [==========>...................] - ETA: 1s - loss: 0.4929
 736/6530 [==>...........................] - ETA: 3s - loss: 0.5688
2704/6530 [===========>..................] - ETA: 1s - loss: 0.4736
1104/6530 [====>.........................] - ETA: 2s - loss: 0.5166
2976/6530 [============>.................] - ETA: 1s - loss: 0.4561
1488/6530 [=====>........................] - ETA: 1s - loss: 0.4759
3248/6530 [=============>................] - ETA: 1s - loss: 0.4406
1856/6530 [=======>......................] - ETA: 1s - loss: 0.4491
3520/6530 [===============>..............] - ETA: 1s - loss: 0.4292
2240/6530 [=========>....................] - ETA: 1s - loss: 0.4295
3792/6530 [================>.............] - ETA: 0s - loss: 0.4179
2608/6530 [==========>...................] - ETA: 1s - loss: 0.4164
4048/6530 [=================>............] - ETA: 0s - loss: 0.4087
2992/6530 [============>.................] - ETA: 0s - loss: 0.4040
4320/6530 [==================>...........] - ETA: 0s - loss: 0.3991
3360/6530 [==============>...............] - ETA: 0s - loss: 0.3925
4608/6530 [====================>.........] - ETA: 0s - loss: 0.3906
3712/6530 [================>.............] - ETA: 0s - loss: 0.3848
4896/6530 [=====================>........] - ETA: 0s - loss: 0.3835
4080/6530 [=================>............] - ETA: 0s - loss: 0.3767
5152/6530 [======================>.......] - ETA: 0s - loss: 0.3768
4448/6530 [===================>..........] - ETA: 0s - loss: 0.3696
5424/6530 [=======================>......] - ETA: 0s - loss: 0.3704
4816/6530 [=====================>........] - ETA: 0s - loss: 0.3625
5712/6530 [=========================>....] - ETA: 0s - loss: 0.3635
5184/6530 [======================>.......] - ETA: 0s - loss: 0.3579
6000/6530 [==========================>...] - ETA: 0s - loss: 0.3581
5568/6530 [========================>.....] - ETA: 0s - loss: 0.3527
6288/6530 [===========================>..] - ETA: 0s - loss: 0.3534
5920/6530 [==========================>...] - ETA: 0s - loss: 0.3474
6304/6530 [===========================>..] - ETA: 0s - loss: 0.3417
6530/6530 [==============================] - 2s 296us/step - loss: 0.3495 - val_loss: 0.1994
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.2181
 320/6530 [>.............................] - ETA: 1s - loss: 0.2521
6530/6530 [==============================] - 1s 212us/step - loss: 0.3392 - val_loss: 0.2339
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.2238
 608/6530 [=>............................] - ETA: 1s - loss: 0.2557
 896/6530 [===>..........................] - ETA: 0s - loss: 0.2517
 384/6530 [>.............................] - ETA: 0s - loss: 0.2594
 736/6530 [==>...........................] - ETA: 0s - loss: 0.2674
1184/6530 [====>.........................] - ETA: 0s - loss: 0.2475
1072/6530 [===>..........................] - ETA: 0s - loss: 0.2645
1456/6530 [=====>........................] - ETA: 0s - loss: 0.2448
1456/6530 [=====>........................] - ETA: 0s - loss: 0.2628
1744/6530 [=======>......................] - ETA: 0s - loss: 0.2427
1808/6530 [=======>......................] - ETA: 0s - loss: 0.2655
2016/6530 [========>.....................] - ETA: 0s - loss: 0.2454
2176/6530 [========>.....................] - ETA: 0s - loss: 0.2656
2288/6530 [=========>....................] - ETA: 0s - loss: 0.2413
# training | RMSE: 0.1848, MAE: 0.1517
worker 2  xfile  [13, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39180291294730896}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3475827664775537}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.18476870740969736, 'rmse': 0.18476870740969736, 'mae': 0.1516778025377694, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:   6 | activation: relu    | extras: dropout - rate: 23.7% 
layer 2 | size:  78 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  56 | activation: tanh    | extras: dropout - rate: 31.7% 
layer 4 | size:  38 | activation: sigmoid | extras: dropout - rate: 22.7% 
layer 5 | size:  69 | activation: sigmoid | extras: dropout - rate: 37.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc786e6f98>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 3:43 - loss: 0.0705
2544/6530 [==========>...................] - ETA: 0s - loss: 0.2638
2560/6530 [==========>...................] - ETA: 0s - loss: 0.2393
 304/6530 [>.............................] - ETA: 12s - loss: 0.1097 
2912/6530 [============>.................] - ETA: 0s - loss: 0.2627
2800/6530 [===========>..................] - ETA: 0s - loss: 0.2397
 608/6530 [=>............................] - ETA: 6s - loss: 0.0904 
3280/6530 [==============>...............] - ETA: 0s - loss: 0.2603
3072/6530 [=============>................] - ETA: 0s - loss: 0.2373
 912/6530 [===>..........................] - ETA: 4s - loss: 0.0855
3648/6530 [===============>..............] - ETA: 0s - loss: 0.2610
3344/6530 [==============>...............] - ETA: 0s - loss: 0.2364
1216/6530 [====>.........................] - ETA: 3s - loss: 0.0818
4000/6530 [=================>............] - ETA: 0s - loss: 0.2596
3632/6530 [===============>..............] - ETA: 0s - loss: 0.2347
1504/6530 [=====>........................] - ETA: 2s - loss: 0.0793
4368/6530 [===================>..........] - ETA: 0s - loss: 0.2586
3904/6530 [================>.............] - ETA: 0s - loss: 0.2340
1808/6530 [=======>......................] - ETA: 2s - loss: 0.0776
4720/6530 [====================>.........] - ETA: 0s - loss: 0.2573
4176/6530 [==================>...........] - ETA: 0s - loss: 0.2336
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0768
5104/6530 [======================>.......] - ETA: 0s - loss: 0.2566
4448/6530 [===================>..........] - ETA: 0s - loss: 0.2324
2416/6530 [==========>...................] - ETA: 1s - loss: 0.0767
5472/6530 [========================>.....] - ETA: 0s - loss: 0.2552
4720/6530 [====================>.........] - ETA: 0s - loss: 0.2321
2704/6530 [===========>..................] - ETA: 1s - loss: 0.0750
5840/6530 [=========================>....] - ETA: 0s - loss: 0.2543
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2310
3008/6530 [============>.................] - ETA: 1s - loss: 0.0743
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2532
5264/6530 [=======================>......] - ETA: 0s - loss: 0.2306
3312/6530 [==============>...............] - ETA: 1s - loss: 0.0730
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2300
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0724
6530/6530 [==============================] - 1s 145us/step - loss: 0.2515 - val_loss: 0.2376
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.2281
5824/6530 [=========================>....] - ETA: 0s - loss: 0.2288
3904/6530 [================>.............] - ETA: 0s - loss: 0.0714
 384/6530 [>.............................] - ETA: 0s - loss: 0.2265
6096/6530 [===========================>..] - ETA: 0s - loss: 0.2285
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0709
 736/6530 [==>...........................] - ETA: 0s - loss: 0.2212
6384/6530 [============================>.] - ETA: 0s - loss: 0.2275
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0700
1104/6530 [====>.........................] - ETA: 0s - loss: 0.2256
1472/6530 [=====>........................] - ETA: 0s - loss: 0.2250
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0698
6530/6530 [==============================] - 1s 192us/step - loss: 0.2276 - val_loss: 0.1779
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.2049
1840/6530 [=======>......................] - ETA: 0s - loss: 0.2275
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0694
 288/6530 [>.............................] - ETA: 1s - loss: 0.2026
2192/6530 [=========>....................] - ETA: 0s - loss: 0.2284
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0690
 560/6530 [=>............................] - ETA: 1s - loss: 0.2041
2576/6530 [==========>...................] - ETA: 0s - loss: 0.2301
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0689
 848/6530 [==>...........................] - ETA: 1s - loss: 0.1995
2944/6530 [============>.................] - ETA: 0s - loss: 0.2308
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0689
1120/6530 [====>.........................] - ETA: 0s - loss: 0.2046
3312/6530 [==============>...............] - ETA: 0s - loss: 0.2318
6336/6530 [============================>.] - ETA: 0s - loss: 0.0688
1360/6530 [=====>........................] - ETA: 0s - loss: 0.2030
3680/6530 [===============>..............] - ETA: 0s - loss: 0.2319
1632/6530 [======>.......................] - ETA: 0s - loss: 0.2020
4064/6530 [=================>............] - ETA: 0s - loss: 0.2314
1904/6530 [=======>......................] - ETA: 0s - loss: 0.2018
6530/6530 [==============================] - 2s 271us/step - loss: 0.0684 - val_loss: 0.0826
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1146
4432/6530 [===================>..........] - ETA: 0s - loss: 0.2314
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1995
 320/6530 [>.............................] - ETA: 1s - loss: 0.0631
4800/6530 [=====================>........] - ETA: 0s - loss: 0.2316
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1992
 624/6530 [=>............................] - ETA: 0s - loss: 0.0586
5152/6530 [======================>.......] - ETA: 0s - loss: 0.2314
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1993
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0564
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2317
3024/6530 [============>.................] - ETA: 0s - loss: 0.1991
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0565
5872/6530 [=========================>....] - ETA: 0s - loss: 0.2318
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1992
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0557
6240/6530 [===========================>..] - ETA: 0s - loss: 0.2321
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1992
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0571
3840/6530 [================>.............] - ETA: 0s - loss: 0.2000
6530/6530 [==============================] - 1s 145us/step - loss: 0.2313 - val_loss: 0.1900

2128/6530 [========>.....................] - ETA: 0s - loss: 0.0578
4128/6530 [=================>............] - ETA: 0s - loss: 0.1996
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0581
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1992
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0584
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1986
3024/6530 [============>.................] - ETA: 0s - loss: 0.0579
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1987
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0580
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1983
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0582
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1980
3936/6530 [=================>............] - ETA: 0s - loss: 0.0583
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1978
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0580
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1975
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0578
6336/6530 [============================>.] - ETA: 0s - loss: 0.1971
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0579
6530/6530 [==============================] - 1s 193us/step - loss: 0.1974 - val_loss: 0.1615

5152/6530 [======================>.......] - ETA: 0s - loss: 0.0583
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0579
# training | RMSE: 0.2311, MAE: 0.1890
worker 0  xfile  [16, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41772377223411683}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4991790277388184}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16600289991869716}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11166500994018819}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2311025115672419, 'rmse': 0.2311025115672419, 'mae': 0.18903194706918167, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  42 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc5c079d68>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:20 - loss: 0.6649
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0577
 688/6530 [==>...........................] - ETA: 2s - loss: 0.6099  
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0578
1360/6530 [=====>........................] - ETA: 1s - loss: 0.4757
6336/6530 [============================>.] - ETA: 0s - loss: 0.0576
2064/6530 [========>.....................] - ETA: 0s - loss: 0.3743
6530/6530 [==============================] - 1s 177us/step - loss: 0.0579 - val_loss: 0.0655
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0784
2736/6530 [===========>..................] - ETA: 0s - loss: 0.3225
 320/6530 [>.............................] - ETA: 1s - loss: 0.0540
3456/6530 [==============>...............] - ETA: 0s - loss: 0.2887
 592/6530 [=>............................] - ETA: 1s - loss: 0.0553
4176/6530 [==================>...........] - ETA: 0s - loss: 0.2669
 880/6530 [===>..........................] - ETA: 0s - loss: 0.0546
4880/6530 [=====================>........] - ETA: 0s - loss: 0.2527
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0537
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2410
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0548
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2326
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0550
6530/6530 [==============================] - 1s 111us/step - loss: 0.2296 - val_loss: 0.1676
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1790
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0555
 640/6530 [=>............................] - ETA: 0s - loss: 0.1689
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0555
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1628
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0553
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1625
3024/6530 [============>.................] - ETA: 0s - loss: 0.0559
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1626
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0553
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1619
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0560
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1620
3904/6530 [================>.............] - ETA: 0s - loss: 0.0560
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1628
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0560
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1624
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0552
# training | RMSE: 0.2020, MAE: 0.1620
worker 1  xfile  [15, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13567262716003206}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2326240252899456}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.20195572741424397, 'rmse': 0.20195572741424397, 'mae': 0.1620385975308255, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  93 | activation: tanh    | extras: None 
layer 2 | size:  56 | activation: sigmoid | extras: None 
layer 3 | size:  73 | activation: relu    | extras: None 
layer 4 | size:  54 | activation: tanh    | extras: dropout - rate: 16.0% 
layer 5 | size:  53 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc5c7f4550>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:13 - loss: 0.7062
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1628
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0554
 832/6530 [==>...........................] - ETA: 2s - loss: 0.3023  
6530/6530 [==============================] - 1s 77us/step - loss: 0.1625 - val_loss: 0.1658
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1808
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0553
1632/6530 [======>.......................] - ETA: 1s - loss: 0.2563
 688/6530 [==>...........................] - ETA: 0s - loss: 0.1676
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0547
2432/6530 [==========>...................] - ETA: 0s - loss: 0.2324
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1625
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0543
3264/6530 [=============>................] - ETA: 0s - loss: 0.2172
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1616
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0544
4064/6530 [=================>............] - ETA: 0s - loss: 0.2087
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1620
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0543
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2038
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1616
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1987
4112/6530 [=================>............] - ETA: 0s - loss: 0.1618
6530/6530 [==============================] - 1s 178us/step - loss: 0.0542 - val_loss: 0.0455

6496/6530 [============================>.] - ETA: 0s - loss: 0.1951
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1626
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1622
6530/6530 [==============================] - 1s 128us/step - loss: 0.1949 - val_loss: 0.2668
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2556
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1626
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1777
6530/6530 [==============================] - 1s 78us/step - loss: 0.1623 - val_loss: 0.1667

1632/6530 [======>.......................] - ETA: 0s - loss: 0.1733
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1720
3232/6530 [=============>................] - ETA: 0s - loss: 0.1701
4064/6530 [=================>............] - ETA: 0s - loss: 0.1706
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1714
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1720
6530/6530 [==============================] - 0s 63us/step - loss: 0.1707 - val_loss: 0.2774
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2667
 864/6530 [==>...........................] - ETA: 0s - loss: 0.1771
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1711
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1701
# training | RMSE: 0.2054, MAE: 0.1662
worker 0  xfile  [18, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37777862236608384}, 'layer_3_size': 67, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20536144854224692, 'rmse': 0.20536144854224692, 'mae': 0.16622564878257323, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  77 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc446d6f60>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 4s - loss: 0.7445
3136/6530 [=============>................] - ETA: 0s - loss: 0.1678
3808/6530 [================>.............] - ETA: 0s - loss: 0.1677
6530/6530 [==============================] - 0s 40us/step - loss: 0.5961 - val_loss: 0.4587
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.4430
6530/6530 [==============================] - 0s 5us/step - loss: 0.3249 - val_loss: 0.2035
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2021
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1678
6530/6530 [==============================] - 0s 5us/step - loss: 0.1708 - val_loss: 0.1612

5408/6530 [=======================>......] - ETA: 0s - loss: 0.1681
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1679
# training | RMSE: 0.2090, MAE: 0.1698
worker 2  xfile  [17, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23723016883152925}, 'layer_1_size': 6, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3165904406088582}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22745862465174038}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3792834483496076}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.20904656345684153, 'rmse': 0.20904656345684153, 'mae': 0.1697519445293774, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  21 | activation: relu    | extras: None 
layer 2 | size:  85 | activation: relu    | extras: None 
layer 3 | size:  81 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc80001f98>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:11 - loss: 0.6837
6530/6530 [==============================] - 0s 68us/step - loss: 0.1675 - val_loss: 0.1633

 928/6530 [===>..........................] - ETA: 2s - loss: 0.3378  
1856/6530 [=======>......................] - ETA: 1s - loss: 0.2836
2688/6530 [===========>..................] - ETA: 0s - loss: 0.2569
3616/6530 [===============>..............] - ETA: 0s - loss: 0.2369
4544/6530 [===================>..........] - ETA: 0s - loss: 0.2267
5472/6530 [========================>.....] - ETA: 0s - loss: 0.2166
6400/6530 [============================>.] - ETA: 0s - loss: 0.2087
6530/6530 [==============================] - 1s 118us/step - loss: 0.2078 - val_loss: 0.1578
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1667
 864/6530 [==>...........................] - ETA: 0s - loss: 0.1591
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1554
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1535
3264/6530 [=============>................] - ETA: 0s - loss: 0.1507
4096/6530 [=================>............] - ETA: 0s - loss: 0.1477
# training | RMSE: 0.2024, MAE: 0.1607
worker 1  xfile  [19, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 56, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15972309563133835}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.20244548579127364, 'rmse': 0.20244548579127364, 'mae': 0.16071673503684497, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  77 | activation: tanh    | extras: dropout - rate: 22.3% 
layer 2 | size:   4 | activation: relu    | extras: dropout - rate: 10.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc5c0ea978>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 7s - loss: 0.5995
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1448
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1453
6530/6530 [==============================] - 0s 61us/step - loss: 0.5426 - val_loss: 0.5091
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.5198
6496/6530 [============================>.] - ETA: 0s - loss: 0.1436
6530/6530 [==============================] - 0s 65us/step - loss: 0.1436 - val_loss: 0.1404
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1501
6530/6530 [==============================] - 0s 7us/step - loss: 0.4546 - val_loss: 0.4001
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.4112
# training | RMSE: 0.2052, MAE: 0.1604
worker 0  xfile  [21, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23042388916549342}, 'layer_2_size': 2, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19208008183545605}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.20519507224296163, 'rmse': 0.20519507224296163, 'mae': 0.1604392069263025, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  54 | activation: sigmoid | extras: dropout - rate: 34.6% 
layer 2 | size:   4 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  28 | activation: sigmoid | extras: dropout - rate: 29.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca0191320>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:28 - loss: 0.4104
6530/6530 [==============================] - 0s 7us/step - loss: 0.3309 - val_loss: 0.2549

 864/6530 [==>...........................] - ETA: 0s - loss: 0.1333
 704/6530 [==>...........................] - ETA: 4s - loss: 0.1161  
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1288
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0946
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1292
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0869
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1268
2848/6530 [============>.................] - ETA: 0s - loss: 0.0823
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1259
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0796
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1257
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0772
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1249
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0752
6530/6530 [==============================] - 0s 61us/step - loss: 0.1245 - val_loss: 0.1272

5824/6530 [=========================>....] - ETA: 0s - loss: 0.0742
6530/6530 [==============================] - 1s 147us/step - loss: 0.0730 - val_loss: 0.0642
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0819
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0693
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0685
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0685
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0682
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0672
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0660
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0654
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0653
6496/6530 [============================>.] - ETA: 0s - loss: 0.0650
6530/6530 [==============================] - 0s 75us/step - loss: 0.0649 - val_loss: 0.0571
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0615
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0633
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0625
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0632
# training | RMSE: 0.5031, MAE: 0.4570
worker 1  xfile  [23, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2234126750360737}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1084275837765758}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2170455638724607}, 'layer_4_size': 75, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38932116854679577}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.5031023052497814, 'rmse': 0.5031023052497814, 'mae': 0.456965494916397, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  67 | activation: tanh    | extras: batchnorm 
layer 2 | size:  26 | activation: relu    | extras: None 
layer 3 | size: 100 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc958e4e940>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 29s - loss: 0.7851
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0627
2304/6530 [=========>....................] - ETA: 1s - loss: 0.4134 
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0620
4736/6530 [====================>.........] - ETA: 0s - loss: 0.3068
4096/6530 [=================>............] - ETA: 0s - loss: 0.0618
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0617
6530/6530 [==============================] - 1s 121us/step - loss: 0.2692 - val_loss: 0.1743
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1793
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0615
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1663
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0614
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1589
6530/6530 [==============================] - 1s 77us/step - loss: 0.0612 - val_loss: 0.0525

6530/6530 [==============================] - 0s 23us/step - loss: 0.1559 - val_loss: 0.1699
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1445
# training | RMSE: 0.1539, MAE: 0.1202
worker 2  xfile  [20, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35610897362546245}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.15387105670453205, 'rmse': 0.15387105670453205, 'mae': 0.1201747747875056, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  13 | activation: tanh    | extras: dropout - rate: 14.4% 
layer 2 | size:  29 | activation: tanh    | extras: batchnorm 
layer 3 | size:  20 | activation: sigmoid | extras: dropout - rate: 27.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc557f5eb8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 56s - loss: 0.6559
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1414
1344/6530 [=====>........................] - ETA: 2s - loss: 0.3485 
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1413
6530/6530 [==============================] - 0s 21us/step - loss: 0.1410 - val_loss: 0.1836

2624/6530 [===========>..................] - ETA: 0s - loss: 0.2213
4032/6530 [=================>............] - ETA: 0s - loss: 0.1629
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1350
# training | RMSE: 0.2305, MAE: 0.1769
worker 1  xfile  [24, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 26, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2305238697461609, 'rmse': 0.2305238697461609, 'mae': 0.1768583650805556, 'early_stop': False}
vggnet done  1

6530/6530 [==============================] - 1s 137us/step - loss: 0.1199 - val_loss: 0.0436
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0375
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0499
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0478
4096/6530 [=================>............] - ETA: 0s - loss: 0.0481
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0481
6528/6530 [============================>.] - ETA: 0s - loss: 0.0476
6530/6530 [==============================] - 0s 41us/step - loss: 0.0476 - val_loss: 0.0419
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0453
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0466
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0466
3840/6530 [================>.............] - ETA: 0s - loss: 0.0458
# training | RMSE: 0.2297, MAE: 0.1875
worker 0  xfile  [22, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3458900173434585}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2903589045207877}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3798607936703877}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2297478202513274, 'rmse': 0.2297478202513274, 'mae': 0.18747669650361043, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:   4 | activation: sigmoid | extras: None 
layer 2 | size:  38 | activation: sigmoid | extras: None 
layer 3 | size:  90 | activation: relu    | extras: None 
layer 4 | size:  76 | activation: tanh    | extras: dropout - rate: 49.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc445b53c8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 35s - loss: 0.8160
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0461
1664/6530 [======>.......................] - ETA: 1s - loss: 0.2978 
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0460
6530/6530 [==============================] - 0s 44us/step - loss: 0.0462 - val_loss: 0.0408

3200/6530 [=============>................] - ETA: 0s - loss: 0.2552
4736/6530 [====================>.........] - ETA: 0s - loss: 0.2397
6400/6530 [============================>.] - ETA: 0s - loss: 0.2301
# training | RMSE: 0.2016, MAE: 0.1617
worker 2  xfile  [25, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1443971582596648}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27465858512765084}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3621760645395955}, 'layer_4_size': 13, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.20159064021355166, 'rmse': 0.20159064021355166, 'mae': 0.16173558372518723, 'early_stop': False}
vggnet done  2

6530/6530 [==============================] - 1s 98us/step - loss: 0.2296 - val_loss: 0.1978
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2077
1600/6530 [======>.......................] - ETA: 0s - loss: 0.2022
3136/6530 [=============>................] - ETA: 0s - loss: 0.2006
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1981
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1973
6530/6530 [==============================] - 0s 34us/step - loss: 0.1971 - val_loss: 0.1915
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2125
1792/6530 [=======>......................] - ETA: 0s - loss: 0.2037
3264/6530 [=============>................] - ETA: 0s - loss: 0.1998
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1961
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1957
6530/6530 [==============================] - 0s 40us/step - loss: 0.1954 - val_loss: 0.2124

# training | RMSE: 0.2573, MAE: 0.2088
worker 0  xfile  [26, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 38, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.49077967255642363}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.2572781736051239, 'rmse': 0.2572781736051239, 'mae': 0.20877129431794836, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#2 epoch=3.0 loss={'loss': 0.16504812945088806, 'rmse': 0.16504812945088806, 'mae': 0.1313625544093016, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 85, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 51, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#1 epoch=3.0 loss={'loss': 0.2686896000615916, 'rmse': 0.2686896000615916, 'mae': 0.2198467637312069, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 7, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.48436536545622033}, 'layer_3_size': 13, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15949515245989582}, 'layer_5_size': 32, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#3 epoch=3.0 loss={'loss': 0.24564403047752328, 'rmse': 0.24564403047752328, 'mae': 0.19042188750146072, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 87, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20814825567027848}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#0 epoch=3.0 loss={'loss': 0.12631875809934154, 'rmse': 0.12631875809934154, 'mae': 0.0993112349286573, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1894976648203621}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16212772803130662}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42662975185148233}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#5 epoch=3.0 loss={'loss': 0.14453920086003996, 'rmse': 0.14453920086003996, 'mae': 0.11086149231331438, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18160482028331157}, 'layer_2_size': 41, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2972163783791718}, 'layer_3_size': 24, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3222195153732551}, 'layer_4_size': 26, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#6 epoch=3.0 loss={'loss': 0.21050511441568898, 'rmse': 0.21050511441568898, 'mae': 0.17013935961284415, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22287393078460238}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 62, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#4 epoch=3.0 loss={'loss': 0.18101866090818358, 'rmse': 0.18101866090818358, 'mae': 0.14253699162101557, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21297786188304155}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#7 epoch=3.0 loss={'loss': 0.17656164724795378, 'rmse': 0.17656164724795378, 'mae': 0.1443546736173766, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3634795999481858}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 6, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#8 epoch=3.0 loss={'loss': 0.16505105273537213, 'rmse': 0.16505105273537213, 'mae': 0.12116673396791514, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3239797430120135}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#11 epoch=3.0 loss={'loss': 0.2642457384774719, 'rmse': 0.2642457384774719, 'mae': 0.21476207389128554, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 11, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#10 epoch=3.0 loss={'loss': 0.21010246869613358, 'rmse': 0.21010246869613358, 'mae': 0.1721366206859716, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 35, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.22798208251503316}, 'layer_3_size': 61, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#12 epoch=3.0 loss={'loss': 0.23533220860188045, 'rmse': 0.23533220860188045, 'mae': 0.18392197791669193, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 62, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1042701844285122}, 'layer_3_size': 10, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#9 epoch=3.0 loss={'loss': 0.20321158293135516, 'rmse': 0.20321158293135516, 'mae': 0.1655600579163336, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 16, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 57, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#14 epoch=3.0 loss={'loss': 0.2013112425966048, 'rmse': 0.2013112425966048, 'mae': 0.16106085720449054, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#13 epoch=3.0 loss={'loss': 0.18476870740969736, 'rmse': 0.18476870740969736, 'mae': 0.1516778025377694, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39180291294730896}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3475827664775537}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#16 epoch=3.0 loss={'loss': 0.2311025115672419, 'rmse': 0.2311025115672419, 'mae': 0.18903194706918167, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41772377223411683}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4991790277388184}, 'layer_3_size': 67, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16600289991869716}, 'layer_4_size': 33, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11166500994018819}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#15 epoch=3.0 loss={'loss': 0.20195572741424397, 'rmse': 0.20195572741424397, 'mae': 0.1620385975308255, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13567262716003206}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2326240252899456}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 9, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#17 epoch=3.0 loss={'loss': 0.20904656345684153, 'rmse': 0.20904656345684153, 'mae': 0.1697519445293774, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23723016883152925}, 'layer_1_size': 6, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 78, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3165904406088582}, 'layer_3_size': 56, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.22745862465174038}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3792834483496076}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#18 epoch=3.0 loss={'loss': 0.20536144854224692, 'rmse': 0.20536144854224692, 'mae': 0.16622564878257323, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 64, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.37777862236608384}, 'layer_3_size': 67, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 37, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#21 epoch=3.0 loss={'loss': 0.20519507224296163, 'rmse': 0.20519507224296163, 'mae': 0.1604392069263025, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.23042388916549342}, 'layer_2_size': 2, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 41, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19208008183545605}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#19 epoch=3.0 loss={'loss': 0.20244548579127364, 'rmse': 0.20244548579127364, 'mae': 0.16071673503684497, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 93, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 56, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 73, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.15972309563133835}, 'layer_4_size': 54, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 53, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#23 epoch=3.0 loss={'loss': 0.5031023052497814, 'rmse': 0.5031023052497814, 'mae': 0.456965494916397, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2234126750360737}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1084275837765758}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2170455638724607}, 'layer_4_size': 75, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.38932116854679577}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#20 epoch=3.0 loss={'loss': 0.15387105670453205, 'rmse': 0.15387105670453205, 'mae': 0.1201747747875056, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.35610897362546245}, 'layer_4_size': 12, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#22 epoch=3.0 loss={'loss': 0.2297478202513274, 'rmse': 0.2297478202513274, 'mae': 0.18747669650361043, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3458900173434585}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2903589045207877}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3798607936703877}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}
#24 epoch=3.0 loss={'loss': 0.2305238697461609, 'rmse': 0.2305238697461609, 'mae': 0.1768583650805556, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 67, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 26, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 100, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 92, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#25 epoch=3.0 loss={'loss': 0.20159064021355166, 'rmse': 0.20159064021355166, 'mae': 0.16173558372518723, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1443971582596648}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.27465858512765084}, 'layer_3_size': 20, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3621760645395955}, 'layer_4_size': 13, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 74, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#26 epoch=3.0 loss={'loss': 0.2572781736051239, 'rmse': 0.2572781736051239, 'mae': 0.20877129431794836, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 4, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 38, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.49077967255642363}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 15, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': True}
get a list [results] of length 108
get a list [loss] of length 27
get a list [val_loss] of length 27
length of indices is [ 3  4 22  0  8  7  6 14 13 25 16 20 12 19 18 17 10  5 23 24 15 11  2 26
  9  1 21]
length of indices is 27
length of T is 27
newly formed T structure is:[[0, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 87, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20814825567027848}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [1, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21297786188304155}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [2, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3458900173434585}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2903589045207877}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3798607936703877}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}], [3, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1894976648203621}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16212772803130662}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42662975185148233}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [4, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3239797430120135}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [5, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3634795999481858}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 6, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [6, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22287393078460238}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 62, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [7, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [8, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39180291294730896}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3475827664775537}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]] 

*** 9.0 configurations x 9.0 iterations each

27 | Thu Sep 27 22:13:48 2018 | lowest loss so far: 0.1263 (run 0)

{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  81 | activation: relu    | extras: batchnorm 
layer 2 | size:  91 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 21s - loss: 1.2682
4864/6530 [=====================>........] - ETA: 0s - loss: 0.6391 
6530/6530 [==============================] - 1s 151us/step - loss: 0.5678 - val_loss: 0.3469
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.3367
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2963
6530/6530 [==============================] - 0s 11us/step - loss: 0.2874 - val_loss: 0.2661
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2520{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  54 | activation: sigmoid | extras: dropout - rate: 34.6% 
layer 2 | size:   4 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  28 | activation: sigmoid | extras: dropout - rate: 29.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 3:23 - loss: 1.0982
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2296
 704/6530 [==>...........................] - ETA: 8s - loss: 0.3106  
6530/6530 [==============================] - 0s 10us/step - loss: 0.2277 - val_loss: 0.2218
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2069
1376/6530 [=====>........................] - ETA: 4s - loss: 0.1971
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2016
6530/6530 [==============================] - 0s 10us/step - loss: 0.2007 - val_loss: 0.1988
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1848
2080/6530 [========>.....................] - ETA: 2s - loss: 0.1552
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1806
6530/6530 [==============================] - 0s 9us/step - loss: 0.1801 - val_loss: 0.1823
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1839
2816/6530 [===========>..................] - ETA: 1s - loss: 0.1328
6400/6530 [============================>.] - ETA: 0s - loss: 0.1694
6530/6530 [==============================] - 0s 9us/step - loss: 0.1697 - val_loss: 0.1725
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1459
3584/6530 [===============>..............] - ETA: 1s - loss: 0.1192
6400/6530 [============================>.] - ETA: 0s - loss: 0.1599
6530/6530 [==============================] - 0s 9us/step - loss: 0.1602 - val_loss: 0.1637
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1539
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1101
6400/6530 [============================>.] - ETA: 0s - loss: 0.1508
6530/6530 [==============================] - 0s 9us/step - loss: 0.1507 - val_loss: 0.1582
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1452
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1036
6400/6530 [============================>.] - ETA: 0s - loss: 0.1434
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0992
6530/6530 [==============================] - 0s 9us/step - loss: 0.1432 - val_loss: 0.1587

6496/6530 [============================>.] - ETA: 0s - loss: 0.0956
6530/6530 [==============================] - 2s 232us/step - loss: 0.0953 - val_loss: 0.0645
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0829
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0705
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0710
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0689{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:   9 | activation: relu    | extras: dropout - rate: 21.3% 
layer 2 | size:  60 | activation: tanh    | extras: batchnorm 
layer 3 | size:  44 | activation: tanh    | extras: None 
layer 4 | size:  79 | activation: sigmoid | extras: batchnorm 
layer 5 | size: 100 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca41c1fd0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 1:52 - loss: 0.5368
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0680
 896/6530 [===>..........................] - ETA: 7s - loss: 0.4538  
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0670
1728/6530 [======>.......................] - ETA: 3s - loss: 0.3244
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0660
2560/6530 [==========>...................] - ETA: 1s - loss: 0.2382
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0656
3392/6530 [==============>...............] - ETA: 1s - loss: 0.1924
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0660
3968/6530 [=================>............] - ETA: 0s - loss: 0.1719
6496/6530 [============================>.] - ETA: 0s - loss: 0.0656
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1498
6530/6530 [==============================] - 0s 73us/step - loss: 0.0654 - val_loss: 0.0596
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0717
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1335
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0674
6464/6530 [============================>.] - ETA: 0s - loss: 0.1212
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0658
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0651
6530/6530 [==============================] - 2s 246us/step - loss: 0.1205 - val_loss: 0.0516
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0475
# training | RMSE: 0.1794, MAE: 0.1386
worker 0  xfile  [0, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 87, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20814825567027848}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.17942389077391996, 'rmse': 0.17942389077391996, 'mae': 0.13863609169769855, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  89 | activation: relu    | extras: batchnorm 
layer 2 | size:  88 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca41fa240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 1:53 - loss: 0.6798
3040/6530 [============>.................] - ETA: 0s - loss: 0.0637
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0387
 400/6530 [>.............................] - ETA: 5s - loss: 0.2850  
3776/6530 [================>.............] - ETA: 0s - loss: 0.0638
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0382
 784/6530 [==>...........................] - ETA: 2s - loss: 0.2402
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0635
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0378
1184/6530 [====>.........................] - ETA: 1s - loss: 0.2140
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0630
3072/6530 [=============>................] - ETA: 0s - loss: 0.0380
1584/6530 [======>.......................] - ETA: 1s - loss: 0.1965
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0627
3904/6530 [================>.............] - ETA: 0s - loss: 0.0385
1984/6530 [========>.....................] - ETA: 1s - loss: 0.1848
6530/6530 [==============================] - 0s 72us/step - loss: 0.0623 - val_loss: 0.0547
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0722
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0377
2368/6530 [=========>....................] - ETA: 1s - loss: 0.1796
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0634
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0375
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1749
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0637
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0378
3152/6530 [=============>................] - ETA: 0s - loss: 0.1699
6530/6530 [==============================] - 0s 69us/step - loss: 0.0377 - val_loss: 0.0302
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0355
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0634
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1660
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0342
3008/6530 [============>.................] - ETA: 0s - loss: 0.0620
3920/6530 [=================>............] - ETA: 0s - loss: 0.1627
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0358
3712/6530 [================>.............] - ETA: 0s - loss: 0.0620
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1602
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0343
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0616
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1568
3200/6530 [=============>................] - ETA: 0s - loss: 0.0351
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0612
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1552
3968/6530 [=================>............] - ETA: 0s - loss: 0.0350
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0608
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1523
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0346
6530/6530 [==============================] - 0s 72us/step - loss: 0.0604 - val_loss: 0.0513
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0767
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1505
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0347
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0613
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1485
6336/6530 [============================>.] - ETA: 0s - loss: 0.0350
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0591
6530/6530 [==============================] - 0s 68us/step - loss: 0.0349 - val_loss: 0.0355
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0395
6530/6530 [==============================] - 1s 182us/step - loss: 0.1473 - val_loss: 0.1463
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1307
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0585
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0329
 416/6530 [>.............................] - ETA: 0s - loss: 0.1145
3072/6530 [=============>................] - ETA: 0s - loss: 0.0575
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0337
 816/6530 [==>...........................] - ETA: 0s - loss: 0.1174
3840/6530 [================>.............] - ETA: 0s - loss: 0.0578
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0328
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0575
1232/6530 [====>.........................] - ETA: 0s - loss: 0.1182
3200/6530 [=============>................] - ETA: 0s - loss: 0.0321
1616/6530 [======>.......................] - ETA: 0s - loss: 0.1173
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0577
4032/6530 [=================>............] - ETA: 0s - loss: 0.0323
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1177
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0574
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0318
6530/6530 [==============================] - 0s 70us/step - loss: 0.0571 - val_loss: 0.0487
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0647
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1159
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0320
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0588
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1157
6464/6530 [============================>.] - ETA: 0s - loss: 0.0320
6530/6530 [==============================] - 0s 67us/step - loss: 0.0320 - val_loss: 0.0388

1440/6530 [=====>........................] - ETA: 0s - loss: 0.0578
3216/6530 [=============>................] - ETA: 0s - loss: 0.1149Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0352
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0318
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0567
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1143
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0299
2912/6530 [============>.................] - ETA: 0s - loss: 0.0562
4016/6530 [=================>............] - ETA: 0s - loss: 0.1133
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0565
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0299
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1127
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0560
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1125
3264/6530 [=============>................] - ETA: 0s - loss: 0.0304
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0558
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1117
4032/6530 [=================>............] - ETA: 0s - loss: 0.0295
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0560
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0294
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1110
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1109
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0296
6530/6530 [==============================] - 0s 72us/step - loss: 0.0556 - val_loss: 0.0471
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0669
6400/6530 [============================>.] - ETA: 0s - loss: 0.1111
6400/6530 [============================>.] - ETA: 0s - loss: 0.0294
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0579
6530/6530 [==============================] - 0s 67us/step - loss: 0.0295 - val_loss: 0.0842
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0818
6530/6530 [==============================] - 1s 133us/step - loss: 0.1110 - val_loss: 0.1150
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1208
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0560
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0388
 384/6530 [>.............................] - ETA: 0s - loss: 0.0945
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0556
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0341
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0966
2976/6530 [============>.................] - ETA: 0s - loss: 0.0555
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0317
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0992
3744/6530 [================>.............] - ETA: 0s - loss: 0.0550
3200/6530 [=============>................] - ETA: 0s - loss: 0.0308
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1009
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0550
3968/6530 [=================>............] - ETA: 0s - loss: 0.0301
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1010
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0548
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0294
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1016
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0549
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0293
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1022
6530/6530 [==============================] - 0s 72us/step - loss: 0.0544 - val_loss: 0.0460
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0659
6336/6530 [============================>.] - ETA: 0s - loss: 0.0291
3040/6530 [============>.................] - ETA: 0s - loss: 0.1027
6530/6530 [==============================] - 0s 69us/step - loss: 0.0291 - val_loss: 0.0197
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0364
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0571
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1025
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0253
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0549
3824/6530 [================>.............] - ETA: 0s - loss: 0.1026
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0240
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0541
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1023
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0245
2976/6530 [============>.................] - ETA: 0s - loss: 0.0538
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1015
3264/6530 [=============>................] - ETA: 0s - loss: 0.0251
3712/6530 [================>.............] - ETA: 0s - loss: 0.0535
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1020
4096/6530 [=================>............] - ETA: 0s - loss: 0.0253
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0536
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1018
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0535
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0262
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1012
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0534
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0265
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1013
6528/6530 [============================>.] - ETA: 0s - loss: 0.0266
6530/6530 [==============================] - 0s 72us/step - loss: 0.0532 - val_loss: 0.0453
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0673
6530/6530 [==============================] - 0s 67us/step - loss: 0.0265 - val_loss: 0.0192
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0263
6530/6530 [==============================] - 1s 136us/step - loss: 0.1012 - val_loss: 0.0960
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0851
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0558
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0248
 432/6530 [>.............................] - ETA: 0s - loss: 0.1071
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0525
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0251
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1023
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0524
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0256
1232/6530 [====>.........................] - ETA: 0s - loss: 0.1017
3008/6530 [============>.................] - ETA: 0s - loss: 0.0516
3200/6530 [=============>................] - ETA: 0s - loss: 0.0254
1632/6530 [======>.......................] - ETA: 0s - loss: 0.1013
3776/6530 [================>.............] - ETA: 0s - loss: 0.0522
4032/6530 [=================>............] - ETA: 0s - loss: 0.0262
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1026
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0524
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0261
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1014
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0521
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0259
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1016
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0520
6528/6530 [============================>.] - ETA: 0s - loss: 0.0262
3216/6530 [=============>................] - ETA: 0s - loss: 0.1010
6530/6530 [==============================] - 0s 67us/step - loss: 0.0262 - val_loss: 0.0296
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0406
6530/6530 [==============================] - 0s 72us/step - loss: 0.0518 - val_loss: 0.0446

3632/6530 [===============>..............] - ETA: 0s - loss: 0.0999
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0264
4032/6530 [=================>............] - ETA: 0s - loss: 0.0992
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0270
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0998
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0273
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0989
3264/6530 [=============>................] - ETA: 0s - loss: 0.0271
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0990
4032/6530 [=================>............] - ETA: 0s - loss: 0.0275
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0978
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0272
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0982
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0276
6384/6530 [============================>.] - ETA: 0s - loss: 0.0982
6336/6530 [============================>.] - ETA: 0s - loss: 0.0277
6530/6530 [==============================] - 0s 68us/step - loss: 0.0279 - val_loss: 0.0559

6530/6530 [==============================] - 1s 133us/step - loss: 0.0984 - val_loss: 0.0888
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0799
 416/6530 [>.............................] - ETA: 0s - loss: 0.0949
# training | RMSE: 0.2102, MAE: 0.1703
worker 2  xfile  [2, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3458900173434585}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2903589045207877}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3798607936703877}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.21017657114420105, 'rmse': 0.21017657114420105, 'mae': 0.17031409557907307, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  90 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca41fa320>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 53s - loss: 0.3693
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0937
 640/6530 [=>............................] - ETA: 1s - loss: 0.3232 
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0923
1312/6530 [=====>........................] - ETA: 0s - loss: 0.2541
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0943
1968/6530 [========>.....................] - ETA: 0s - loss: 0.2270
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0947
2608/6530 [==========>...................] - ETA: 0s - loss: 0.2144
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0943
3312/6530 [==============>...............] - ETA: 0s - loss: 0.2035
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0949
4016/6530 [=================>............] - ETA: 0s - loss: 0.1966
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1916
3104/6530 [=============>................] - ETA: 0s - loss: 0.0951
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1868
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0949
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1833
3920/6530 [=================>............] - ETA: 0s - loss: 0.0948
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0951
6530/6530 [==============================] - 1s 100us/step - loss: 0.1815 - val_loss: 0.1515
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1384
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0953
 720/6530 [==>...........................] - ETA: 0s - loss: 0.1484
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0947
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1415
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0949
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1403
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0949
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1422
6384/6530 [============================>.] - ETA: 0s - loss: 0.0944
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1414
6530/6530 [==============================] - 1s 135us/step - loss: 0.0945 - val_loss: 0.1313

4112/6530 [=================>............] - ETA: 0s - loss: 0.1405Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1584
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1404
 416/6530 [>.............................] - ETA: 0s - loss: 0.0929
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1389
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0958
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1390
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0936
6530/6530 [==============================] - 1s 77us/step - loss: 0.1386 - val_loss: 0.1336
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1137
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0944
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1269
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0937
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1232
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0952
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1237
2832/6530 [============>.................] - ETA: 0s - loss: 0.0951
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1263
3232/6530 [=============>................] - ETA: 0s - loss: 0.0952
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1242
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0951
3984/6530 [=================>............] - ETA: 0s - loss: 0.1241
4048/6530 [=================>............] - ETA: 0s - loss: 0.0947
# training | RMSE: 0.2341, MAE: 0.1930
worker 1  xfile  [1, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21297786188304155}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.23410999396705168, 'rmse': 0.23410999396705168, 'mae': 0.19299610536108694, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  77 | activation: relu    | extras: batchnorm 
layer 2 | size:  98 | activation: tanh    | extras: dropout - rate: 36.3% 
layer 3 | size:  95 | activation: sigmoid | extras: batchnorm 
layer 4 | size:   6 | activation: sigmoid | extras: None 
layer 5 | size:  38 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca41fa2e8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 30s - loss: 1.3772
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1235
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0939
1920/6530 [=======>......................] - ETA: 1s - loss: 0.8706 
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1226
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0931
3840/6530 [================>.............] - ETA: 0s - loss: 0.5836
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1222
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0930
5760/6530 [=========================>....] - ETA: 0s - loss: 0.4442
6530/6530 [==============================] - 1s 80us/step - loss: 0.1220 - val_loss: 0.1198

5648/6530 [========================>.....] - ETA: 0s - loss: 0.0925Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0858
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0921
 720/6530 [==>...........................] - ETA: 0s - loss: 0.1132
6530/6530 [==============================] - 1s 131us/step - loss: 0.4098 - val_loss: 0.1875
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1846
6464/6530 [============================>.] - ETA: 0s - loss: 0.0921
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1110
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1459
6530/6530 [==============================] - 1s 131us/step - loss: 0.0922 - val_loss: 0.0894
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0936
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1111
3968/6530 [=================>............] - ETA: 0s - loss: 0.1435
 416/6530 [>.............................] - ETA: 0s - loss: 0.0900
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1124
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1389
6530/6530 [==============================] - 0s 28us/step - loss: 0.1377 - val_loss: 0.1539
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1716
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0901
3408/6530 [==============>...............] - ETA: 0s - loss: 0.1109
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1197
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0894
4032/6530 [=================>............] - ETA: 0s - loss: 0.1106
4096/6530 [=================>............] - ETA: 0s - loss: 0.1175
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0899
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1097
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1161
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0900
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1089
6530/6530 [==============================] - 0s 28us/step - loss: 0.1158 - val_loss: 0.1344
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1360
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0900
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1085
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1107
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0900
6530/6530 [==============================] - 1s 79us/step - loss: 0.1084 - val_loss: 0.1077
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0756
3968/6530 [=================>............] - ETA: 0s - loss: 0.1108
3216/6530 [=============>................] - ETA: 0s - loss: 0.0893
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1031
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1099
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0890
6530/6530 [==============================] - 0s 28us/step - loss: 0.1097 - val_loss: 0.1228
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1394
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1027
3968/6530 [=================>............] - ETA: 0s - loss: 0.0889
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1047
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1021
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0890
3968/6530 [=================>............] - ETA: 0s - loss: 0.1055
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1027
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0880
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1051
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1005
6530/6530 [==============================] - 0s 28us/step - loss: 0.1047 - val_loss: 0.1235
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1219
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0881
4016/6530 [=================>............] - ETA: 0s - loss: 0.1000
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1003
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0879
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0990
3968/6530 [=================>............] - ETA: 0s - loss: 0.1004
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0879
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0984
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1002
6352/6530 [============================>.] - ETA: 0s - loss: 0.0885
6530/6530 [==============================] - 0s 28us/step - loss: 0.0999 - val_loss: 0.1038

6048/6530 [==========================>...] - ETA: 0s - loss: 0.0983Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1057
6530/6530 [==============================] - 1s 134us/step - loss: 0.0886 - val_loss: 0.0885
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.2238
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0962
6530/6530 [==============================] - 1s 79us/step - loss: 0.0982 - val_loss: 0.1003
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0734
 416/6530 [>.............................] - ETA: 0s - loss: 0.0922
3968/6530 [=================>............] - ETA: 0s - loss: 0.0956
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0935
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0860
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0964
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0941
6530/6530 [==============================] - 0s 29us/step - loss: 0.0963 - val_loss: 0.1104
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1371
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0880
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0939
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0987
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0891
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0944
3968/6530 [=================>............] - ETA: 0s - loss: 0.0988
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0880
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0929
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0954
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0887
4032/6530 [=================>............] - ETA: 0s - loss: 0.0923
6530/6530 [==============================] - 0s 28us/step - loss: 0.0956 - val_loss: 0.1146
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1131
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0897
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0915
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0923
3200/6530 [=============>................] - ETA: 0s - loss: 0.0894
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0910
3840/6530 [================>.............] - ETA: 0s - loss: 0.0936
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0892
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0908
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0937
3984/6530 [=================>............] - ETA: 0s - loss: 0.0894
6530/6530 [==============================] - 0s 29us/step - loss: 0.0941 - val_loss: 0.1425

6530/6530 [==============================] - 1s 79us/step - loss: 0.0910 - val_loss: 0.0974
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0742
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0898
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0895
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0894
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0889
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0895
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0889
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0891
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0891
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0895
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0878
6464/6530 [============================>.] - ETA: 0s - loss: 0.0892
4048/6530 [=================>............] - ETA: 0s - loss: 0.0874
6530/6530 [==============================] - 1s 131us/step - loss: 0.0893 - val_loss: 0.0809
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0655
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0869
 368/6530 [>.............................] - ETA: 0s - loss: 0.0842
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0865
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0814
# training | RMSE: 0.1610, MAE: 0.1390
worker 1  xfile  [5, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3634795999481858}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 6, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.1609857102942788, 'rmse': 0.1609857102942788, 'mae': 0.13896329870670726, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  86 | activation: sigmoid | extras: dropout - rate: 22.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc8379a5c0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 9s - loss: 1.9925
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0862
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0835
5376/6530 [=======================>......] - ETA: 0s - loss: 0.4529
6530/6530 [==============================] - 0s 42us/step - loss: 0.3871 - val_loss: 0.0746
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0774
6530/6530 [==============================] - 1s 80us/step - loss: 0.0865 - val_loss: 0.0939
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0646
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0831
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0573
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0864
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0844
6530/6530 [==============================] - 0s 11us/step - loss: 0.0555 - val_loss: 0.0506
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0513
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0862
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0851
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0467
6530/6530 [==============================] - 0s 11us/step - loss: 0.0461 - val_loss: 0.0464
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0470
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0859
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0849
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0448
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0858
6530/6530 [==============================] - 0s 11us/step - loss: 0.0444 - val_loss: 0.0453
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0459
3120/6530 [=============>................] - ETA: 0s - loss: 0.0853
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0848
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0442
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0856
6530/6530 [==============================] - 0s 10us/step - loss: 0.0438 - val_loss: 0.0448
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0454
3968/6530 [=================>............] - ETA: 0s - loss: 0.0843
3920/6530 [=================>............] - ETA: 0s - loss: 0.0856
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0439
6530/6530 [==============================] - 0s 10us/step - loss: 0.0434 - val_loss: 0.0444
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0450
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0837
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0862
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0435
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0834
6530/6530 [==============================] - 0s 11us/step - loss: 0.0431 - val_loss: 0.0440
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0446
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0862
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0833
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0432
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0865
6530/6530 [==============================] - 0s 11us/step - loss: 0.0428 - val_loss: 0.0436
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0442
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0869
6530/6530 [==============================] - 1s 79us/step - loss: 0.0836 - val_loss: 0.0924
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0654
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0428
6530/6530 [==============================] - 0s 11us/step - loss: 0.0424 - val_loss: 0.0433

5904/6530 [==========================>...] - ETA: 0s - loss: 0.0866
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0833
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0865
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0838
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0827
6530/6530 [==============================] - 1s 134us/step - loss: 0.0865 - val_loss: 0.0792

2768/6530 [===========>..................] - ETA: 0s - loss: 0.0825
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0821
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0811
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0809
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0808
# training | RMSE: 0.2050, MAE: 0.1636
worker 1  xfile  [6, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22287393078460238}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 62, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.20497029719258075, 'rmse': 0.20497029719258075, 'mae': 0.16357594590902466, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  42 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca0100470>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 20s - loss: 0.6652
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0809
2752/6530 [===========>..................] - ETA: 0s - loss: 0.4232 
6530/6530 [==============================] - 1s 77us/step - loss: 0.0809 - val_loss: 0.0909

5568/6530 [========================>.....] - ETA: 0s - loss: 0.2555
6530/6530 [==============================] - 0s 54us/step - loss: 0.2239 - val_loss: 0.0412
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0310
# training | RMSE: 0.1095, MAE: 0.0804
worker 2  xfile  [4, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3239797430120135}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.10947985878393582, 'rmse': 0.10947985878393582, 'mae': 0.08039481437055036, 'early_stop': False}
vggnet done  2

2688/6530 [===========>..................] - ETA: 0s - loss: 0.0409
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0413
6530/6530 [==============================] - 0s 19us/step - loss: 0.0407 - val_loss: 0.0411

# training | RMSE: 0.0992, MAE: 0.0746
worker 0  xfile  [3, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1894976648203621}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16212772803130662}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42662975185148233}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.09915172773573919, 'rmse': 0.09915172773573919, 'mae': 0.07458344890922838, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  70 | activation: sigmoid | extras: None 
layer 2 | size:   7 | activation: relu    | extras: dropout - rate: 39.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca02ab7f0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 1:06 - loss: 0.9262Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0377
 672/6530 [==>...........................] - ETA: 1s - loss: 0.1354  
2944/6530 [============>.................] - ETA: 0s - loss: 0.0400
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1000
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0404
6530/6530 [==============================] - 0s 19us/step - loss: 0.0407 - val_loss: 0.0414
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0320
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0850
2880/6530 [============>.................] - ETA: 0s - loss: 0.0394
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0764
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0407
6530/6530 [==============================] - 0s 19us/step - loss: 0.0408 - val_loss: 0.0414
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0286
3248/6530 [=============>................] - ETA: 0s - loss: 0.0711
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0403
3872/6530 [================>.............] - ETA: 0s - loss: 0.0674
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0404
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0643
6530/6530 [==============================] - 0s 20us/step - loss: 0.0406 - val_loss: 0.0405
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0480
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0623
2880/6530 [============>.................] - ETA: 0s - loss: 0.0411
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0603
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0400
6530/6530 [==============================] - 0s 19us/step - loss: 0.0405 - val_loss: 0.0408
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0412
6384/6530 [============================>.] - ETA: 0s - loss: 0.0587
3008/6530 [============>.................] - ETA: 0s - loss: 0.0398
6530/6530 [==============================] - 1s 110us/step - loss: 0.0584 - val_loss: 0.0433
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0457
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0402
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0437
6530/6530 [==============================] - 0s 19us/step - loss: 0.0404 - val_loss: 0.0406
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0330
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0436
2944/6530 [============>.................] - ETA: 0s - loss: 0.0416
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0434
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0404
6530/6530 [==============================] - 0s 19us/step - loss: 0.0404 - val_loss: 0.0405
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0315
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0428
2880/6530 [============>.................] - ETA: 0s - loss: 0.0390
3248/6530 [=============>................] - ETA: 0s - loss: 0.0425
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0396
3888/6530 [================>.............] - ETA: 0s - loss: 0.0420
6530/6530 [==============================] - 0s 19us/step - loss: 0.0403 - val_loss: 0.0406

4544/6530 [===================>..........] - ETA: 0s - loss: 0.0418
# training | RMSE: 0.1999, MAE: 0.1602
worker 1  xfile  [7, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.19990630026577738, 'rmse': 0.19990630026577738, 'mae': 0.16017898600582928, 'early_stop': False}
vggnet done  1

5184/6530 [======================>.......] - ETA: 0s - loss: 0.0417
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0417
6530/6530 [==============================] - 1s 80us/step - loss: 0.0412 - val_loss: 0.0447
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0583
 624/6530 [=>............................] - ETA: 0s - loss: 0.0388
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0384
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0367
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0366
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0363
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0357
3824/6530 [================>.............] - ETA: 0s - loss: 0.0347
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0343
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0340
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0339
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0339
6448/6530 [============================>.] - ETA: 0s - loss: 0.0338
6530/6530 [==============================] - 1s 99us/step - loss: 0.0336 - val_loss: 0.0298
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0207
 512/6530 [=>............................] - ETA: 0s - loss: 0.0272
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0285
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0277
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0285
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0283
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0283
3232/6530 [=============>................] - ETA: 0s - loss: 0.0280
3744/6530 [================>.............] - ETA: 0s - loss: 0.0278
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0274
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0275
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0272
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0270
6530/6530 [==============================] - 1s 103us/step - loss: 0.0271 - val_loss: 0.0286
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0660
 416/6530 [>.............................] - ETA: 0s - loss: 0.0254
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0244
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0235
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0233
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0239
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0240
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0241
3152/6530 [=============>................] - ETA: 0s - loss: 0.0242
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0241
3904/6530 [================>.............] - ETA: 0s - loss: 0.0241
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0237
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0236
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0234
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0236
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0234
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0233
6416/6530 [============================>.] - ETA: 0s - loss: 0.0231
6530/6530 [==============================] - 1s 143us/step - loss: 0.0232 - val_loss: 0.0206
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0162
 368/6530 [>.............................] - ETA: 0s - loss: 0.0211
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0225
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0225
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0214
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0216
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0217
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0218
3168/6530 [=============>................] - ETA: 0s - loss: 0.0216
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0217
4064/6530 [=================>............] - ETA: 0s - loss: 0.0215
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0217
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0216
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0215
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0216
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0214
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0214
6320/6530 [============================>.] - ETA: 0s - loss: 0.0213
6530/6530 [==============================] - 1s 145us/step - loss: 0.0214 - val_loss: 0.0274
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0204
 336/6530 [>.............................] - ETA: 0s - loss: 0.0176
 624/6530 [=>............................] - ETA: 0s - loss: 0.0188
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0196
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0189
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0196
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0196
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0194
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0198
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0198
3168/6530 [=============>................] - ETA: 0s - loss: 0.0199
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0198
3808/6530 [================>.............] - ETA: 0s - loss: 0.0197
4112/6530 [=================>............] - ETA: 0s - loss: 0.0195
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0197
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0197
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0197
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0198
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0198
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0196
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0196
6496/6530 [============================>.] - ETA: 0s - loss: 0.0195
6530/6530 [==============================] - 1s 172us/step - loss: 0.0196 - val_loss: 0.0226
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0491
 336/6530 [>.............................] - ETA: 0s - loss: 0.0173
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0191
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0184
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0192
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0190
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0194
2880/6530 [============>.................] - ETA: 0s - loss: 0.0192
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0195
3712/6530 [================>.............] - ETA: 0s - loss: 0.0192
4064/6530 [=================>............] - ETA: 0s - loss: 0.0192
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0189
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0187
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0188
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0186
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0185
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0187
6512/6530 [============================>.] - ETA: 0s - loss: 0.0188
6530/6530 [==============================] - 1s 141us/step - loss: 0.0188 - val_loss: 0.0508
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0455
 352/6530 [>.............................] - ETA: 0s - loss: 0.0172
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0166
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0170
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0173
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0175
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0175
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0178
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0178
2864/6530 [============>.................] - ETA: 0s - loss: 0.0177
3152/6530 [=============>................] - ETA: 0s - loss: 0.0177
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0179
3744/6530 [================>.............] - ETA: 0s - loss: 0.0180
4032/6530 [=================>............] - ETA: 0s - loss: 0.0178
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0179
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0179
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0179
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0180
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0180
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0180
6432/6530 [============================>.] - ETA: 0s - loss: 0.0180
6530/6530 [==============================] - 1s 167us/step - loss: 0.0179 - val_loss: 0.0169

# training | RMSE: 0.1221, MAE: 0.0947
worker 0  xfile  [8, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39180291294730896}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3475827664775537}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.12214426654988964, 'rmse': 0.12214426654988964, 'mae': 0.09465713345848711, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=9.0 loss={'loss': 0.17942389077391996, 'rmse': 0.17942389077391996, 'mae': 0.13863609169769855, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 91, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 87, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 89, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20814825567027848}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#2 epoch=9.0 loss={'loss': 0.21017657114420105, 'rmse': 0.21017657114420105, 'mae': 0.17031409557907307, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3458900173434585}, 'layer_1_size': 54, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2903589045207877}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3798607936703877}, 'layer_4_size': 58, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}
#1 epoch=9.0 loss={'loss': 0.23410999396705168, 'rmse': 0.23410999396705168, 'mae': 0.19299610536108694, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21297786188304155}, 'layer_1_size': 9, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 60, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#5 epoch=9.0 loss={'loss': 0.1609857102942788, 'rmse': 0.1609857102942788, 'mae': 0.13896329870670726, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3634795999481858}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 6, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#6 epoch=9.0 loss={'loss': 0.20497029719258075, 'rmse': 0.20497029719258075, 'mae': 0.16357594590902466, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22287393078460238}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 62, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#3 epoch=9.0 loss={'loss': 0.09915172773573919, 'rmse': 0.09915172773573919, 'mae': 0.07458344890922838, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 89, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1894976648203621}, 'layer_3_size': 44, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.16212772803130662}, 'layer_4_size': 49, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42662975185148233}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#4 epoch=9.0 loss={'loss': 0.10947985878393582, 'rmse': 0.10947985878393582, 'mae': 0.08039481437055036, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 90, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 69, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3239797430120135}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#7 epoch=9.0 loss={'loss': 0.19990630026577738, 'rmse': 0.19990630026577738, 'mae': 0.16017898600582928, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 58, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 67, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 7, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#8 epoch=9.0 loss={'loss': 0.12214426654988964, 'rmse': 0.12214426654988964, 'mae': 0.09465713345848711, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39180291294730896}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3475827664775537}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
get a list [results] of length 117
get a list [loss] of length 9
get a list [val_loss] of length 9
length of indices is [5 6 8 3 0 7 4 1 2]
length of indices is 9
length of T is 9
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3634795999481858}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 6, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [1, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22287393078460238}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 62, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [2, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39180291294730896}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3475827664775537}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]] 

*** 3.0 configurations x 27.0 iterations each

9 | Thu Sep 27 22:14:07 2018 | lowest loss so far: 0.0992 (run 3)

{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  86 | activation: sigmoid | extras: dropout - rate: 22.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 128/6530 [..............................] - ETA: 37s - loss: 1.7334{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  70 | activation: sigmoid | extras: None 
layer 2 | size:   7 | activation: relu    | extras: dropout - rate: 39.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 4:43 - loss: 0.5499
4096/6530 [=================>............] - ETA: 0s - loss: 0.4639 
 416/6530 [>.............................] - ETA: 11s - loss: 0.3581 
6530/6530 [==============================] - 1s 130us/step - loss: 0.3219 - val_loss: 0.0637
Epoch 2/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0561
 912/6530 [===>..........................] - ETA: 4s - loss: 0.2185 
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0541
1504/6530 [=====>........................] - ETA: 2s - loss: 0.1555
6530/6530 [==============================] - 0s 11us/step - loss: 0.0527 - val_loss: 0.0477
Epoch 3/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0441
2064/6530 [========>.....................] - ETA: 1s - loss: 0.1262
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0450
6530/6530 [==============================] - 0s 11us/step - loss: 0.0448 - val_loss: 0.0454
Epoch 4/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0417
2640/6530 [===========>..................] - ETA: 1s - loss: 0.1086
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0432
6530/6530 [==============================] - 0s 11us/step - loss: 0.0432 - val_loss: 0.0450

3184/6530 [=============>................] - ETA: 1s - loss: 0.0972Epoch 5/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0411
3776/6530 [================>.............] - ETA: 0s - loss: 0.0879
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0427
6530/6530 [==============================] - 0s 10us/step - loss: 0.0427 - val_loss: 0.0448
Epoch 6/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0408
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0815
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0424
6530/6530 [==============================] - 0s 11us/step - loss: 0.0424 - val_loss: 0.0446
Epoch 7/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0406
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0756
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0422
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0714
6530/6530 [==============================] - 0s 10us/step - loss: 0.0422 - val_loss: 0.0444
Epoch 8/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0404
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0676
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0420
6530/6530 [==============================] - 0s 10us/step - loss: 0.0419 - val_loss: 0.0441
Epoch 9/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0402
6530/6530 [==============================] - 1s 203us/step - loss: 0.0655 - val_loss: 0.0327
Epoch 2/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0198
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0417
6530/6530 [==============================] - 0s 11us/step - loss: 0.0417 - val_loss: 0.0438
Epoch 10/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0400
 640/6530 [=>............................] - ETA: 0s - loss: 0.0358
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0414
6530/6530 [==============================] - 0s 10us/step - loss: 0.0414 - val_loss: 0.0435
Epoch 11/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0398
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0355
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0409
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0346
6530/6530 [==============================] - 0s 10us/step - loss: 0.0411 - val_loss: 0.0432
Epoch 12/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0396
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0337
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0408
6530/6530 [==============================] - 0s 11us/step - loss: 0.0408 - val_loss: 0.0428
Epoch 13/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0393
3072/6530 [=============>................] - ETA: 0s - loss: 0.0330{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  77 | activation: relu    | extras: batchnorm 
layer 2 | size:  98 | activation: tanh    | extras: dropout - rate: 36.3% 
layer 3 | size:  95 | activation: sigmoid | extras: batchnorm 
layer 4 | size:   6 | activation: sigmoid | extras: None 
layer 5 | size:  38 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 128/6530 [..............................] - ETA: 51s - loss: 0.4721
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0406
6530/6530 [==============================] - 0s 11us/step - loss: 0.0405 - val_loss: 0.0425
Epoch 14/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0391
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0326
2048/6530 [========>.....................] - ETA: 2s - loss: 0.2142 
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0400
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0322
6530/6530 [==============================] - 0s 10us/step - loss: 0.0402 - val_loss: 0.0421
Epoch 15/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0389
3840/6530 [================>.............] - ETA: 0s - loss: 0.1837
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0322
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0398
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1673
6530/6530 [==============================] - 0s 11us/step - loss: 0.0398 - val_loss: 0.0418
Epoch 16/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0386
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0323
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0394
6530/6530 [==============================] - 0s 10us/step - loss: 0.0395 - val_loss: 0.0414
Epoch 17/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0384
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0319
6530/6530 [==============================] - 1s 192us/step - loss: 0.1630 - val_loss: 0.1541
Epoch 2/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1659
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0391
6530/6530 [==============================] - 0s 10us/step - loss: 0.0391 - val_loss: 0.0410
Epoch 18/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0381
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1233
6530/6530 [==============================] - 1s 88us/step - loss: 0.0313 - val_loss: 0.0385
Epoch 3/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0348
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0388
3968/6530 [=================>............] - ETA: 0s - loss: 0.1231
 608/6530 [=>............................] - ETA: 0s - loss: 0.0295
6530/6530 [==============================] - 0s 10us/step - loss: 0.0387 - val_loss: 0.0406
Epoch 19/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0378
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1207
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0314
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0384
6530/6530 [==============================] - 0s 28us/step - loss: 0.1206 - val_loss: 0.1069

6530/6530 [==============================] - 0s 10us/step - loss: 0.0383 - val_loss: 0.0401
Epoch 20/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0375Epoch 3/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1128
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0311
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0380
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1153
6530/6530 [==============================] - 0s 11us/step - loss: 0.0379 - val_loss: 0.0397
Epoch 21/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0372
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0313
3968/6530 [=================>............] - ETA: 0s - loss: 0.1116
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0375
2992/6530 [============>.................] - ETA: 0s - loss: 0.0304
6530/6530 [==============================] - 0s 10us/step - loss: 0.0375 - val_loss: 0.0392
Epoch 22/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0369
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1102
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0300
6530/6530 [==============================] - 0s 29us/step - loss: 0.1104 - val_loss: 0.0989
Epoch 4/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1251
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0371
6530/6530 [==============================] - 0s 10us/step - loss: 0.0370 - val_loss: 0.0387
Epoch 23/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0365
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0294
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1097
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0366
6530/6530 [==============================] - 0s 10us/step - loss: 0.0365 - val_loss: 0.0382
Epoch 24/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0361
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0288
3840/6530 [================>.............] - ETA: 0s - loss: 0.1070
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0361
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0288
6530/6530 [==============================] - 0s 10us/step - loss: 0.0360 - val_loss: 0.0376
Epoch 25/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0357
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1054
6530/6530 [==============================] - 0s 29us/step - loss: 0.1060 - val_loss: 0.1092
Epoch 5/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1047
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0284
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0354
6530/6530 [==============================] - 0s 10us/step - loss: 0.0355 - val_loss: 0.0371
Epoch 26/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0353
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1028
6530/6530 [==============================] - 1s 88us/step - loss: 0.0284 - val_loss: 0.0284
Epoch 4/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0245
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0350
6530/6530 [==============================] - 0s 11us/step - loss: 0.0350 - val_loss: 0.0365
Epoch 27/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0349
3968/6530 [=================>............] - ETA: 0s - loss: 0.1017
 624/6530 [=>............................] - ETA: 0s - loss: 0.0280
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0345
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1008
6530/6530 [==============================] - 0s 10us/step - loss: 0.0345 - val_loss: 0.0359

1216/6530 [====>.........................] - ETA: 0s - loss: 0.0283
6530/6530 [==============================] - 0s 28us/step - loss: 0.1001 - val_loss: 0.1114
Epoch 6/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1023
# training | RMSE: 0.1847, MAE: 0.1493
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22287393078460238}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 62, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.18468599478390252, 'rmse': 0.18468599478390252, 'mae': 0.14932273404147878, 'early_stop': False}
vggnet done  1

1840/6530 [=======>......................] - ETA: 0s - loss: 0.0276
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1023
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0267
3968/6530 [=================>............] - ETA: 0s - loss: 0.0979
3136/6530 [=============>................] - ETA: 0s - loss: 0.0268
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0975
6530/6530 [==============================] - 0s 27us/step - loss: 0.0984 - val_loss: 0.1032
Epoch 7/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0993
3792/6530 [================>.............] - ETA: 0s - loss: 0.0265
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0918
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0264
4096/6530 [=================>............] - ETA: 0s - loss: 0.0933
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0263
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0942
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0260
6530/6530 [==============================] - 0s 27us/step - loss: 0.0951 - val_loss: 0.0878
Epoch 8/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0956
6320/6530 [============================>.] - ETA: 0s - loss: 0.0258
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0914
6530/6530 [==============================] - 1s 84us/step - loss: 0.0259 - val_loss: 0.0249
Epoch 5/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0209
4096/6530 [=================>............] - ETA: 0s - loss: 0.0944
 640/6530 [=>............................] - ETA: 0s - loss: 0.0241
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0944
6530/6530 [==============================] - 0s 27us/step - loss: 0.0945 - val_loss: 0.0894
Epoch 9/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0929
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0257
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0901
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0246
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0907
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0243
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0913
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0239
6530/6530 [==============================] - 0s 27us/step - loss: 0.0916 - val_loss: 0.1459
Epoch 10/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1522
3984/6530 [=================>............] - ETA: 0s - loss: 0.0239
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0955
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0236
4096/6530 [=================>............] - ETA: 0s - loss: 0.0914
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0237
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0914
6530/6530 [==============================] - 0s 27us/step - loss: 0.0919 - val_loss: 0.1086
Epoch 11/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1232
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0236
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0924
6528/6530 [============================>.] - ETA: 0s - loss: 0.0237
6530/6530 [==============================] - 1s 81us/step - loss: 0.0237 - val_loss: 0.0233
Epoch 6/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0117
4096/6530 [=================>............] - ETA: 0s - loss: 0.0904
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0215
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0910
6530/6530 [==============================] - 0s 27us/step - loss: 0.0903 - val_loss: 0.1122
Epoch 12/27

 128/6530 [..............................] - ETA: 0s - loss: 0.0983
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0222
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0863
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0218
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0875
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0219
6400/6530 [============================>.] - ETA: 0s - loss: 0.0884
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0213
6530/6530 [==============================] - 0s 26us/step - loss: 0.0884 - val_loss: 0.1270

3952/6530 [=================>............] - ETA: 0s - loss: 0.0212
# training | RMSE: 0.1443, MAE: 0.1223
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3634795999481858}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 6, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.144307630664082, 'rmse': 0.144307630664082, 'mae': 0.12227309817194515, 'early_stop': True}
vggnet done  0

4528/6530 [===================>..........] - ETA: 0s - loss: 0.0214
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0213
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0214
6530/6530 [==============================] - 1s 80us/step - loss: 0.0217 - val_loss: 0.0254
Epoch 7/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0198
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0210
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0205
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0200
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0197
2960/6530 [============>.................] - ETA: 0s - loss: 0.0199
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0197
4096/6530 [=================>............] - ETA: 0s - loss: 0.0197
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0198
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0199
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0200
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0202
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0201
6432/6530 [============================>.] - ETA: 0s - loss: 0.0201
6530/6530 [==============================] - 1s 108us/step - loss: 0.0201 - val_loss: 0.0252
Epoch 8/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0180
 448/6530 [=>............................] - ETA: 0s - loss: 0.0198
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0192
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0188
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0187
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0186
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0183
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0185
2832/6530 [============>.................] - ETA: 0s - loss: 0.0184
3232/6530 [=============>................] - ETA: 0s - loss: 0.0183
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0182
3936/6530 [=================>............] - ETA: 0s - loss: 0.0186
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0187
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0188
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0187
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0188
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0189
6530/6530 [==============================] - 1s 139us/step - loss: 0.0189 - val_loss: 0.0187
Epoch 9/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0180
 432/6530 [>.............................] - ETA: 0s - loss: 0.0175
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0166
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0177
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0181
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0186
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0184
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0182
3152/6530 [=============>................] - ETA: 0s - loss: 0.0182
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0184
3856/6530 [================>.............] - ETA: 0s - loss: 0.0183
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0181
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0180
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0179
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0179
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0178
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0179
6530/6530 [==============================] - 1s 134us/step - loss: 0.0178 - val_loss: 0.0182
Epoch 10/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0236
 416/6530 [>.............................] - ETA: 0s - loss: 0.0186
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0175
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0179
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0173
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0172
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0173
2848/6530 [============>.................] - ETA: 0s - loss: 0.0172
3216/6530 [=============>................] - ETA: 0s - loss: 0.0171
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0171
3856/6530 [================>.............] - ETA: 0s - loss: 0.0171
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0171
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0170
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0170
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0169
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0170
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0170
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0171
6464/6530 [============================>.] - ETA: 0s - loss: 0.0172
6530/6530 [==============================] - 1s 148us/step - loss: 0.0171 - val_loss: 0.0198
Epoch 11/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0253
 400/6530 [>.............................] - ETA: 0s - loss: 0.0167
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0172
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0168
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0172
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0169
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0171
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0169
2944/6530 [============>.................] - ETA: 0s - loss: 0.0167
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0166
3712/6530 [================>.............] - ETA: 0s - loss: 0.0165
4016/6530 [=================>............] - ETA: 0s - loss: 0.0165
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0165
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0164
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0164
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0165
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0165
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0165
6530/6530 [==============================] - 1s 146us/step - loss: 0.0164 - val_loss: 0.0304
Epoch 12/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0264
 400/6530 [>.............................] - ETA: 0s - loss: 0.0164
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0167
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0166
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0160
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0160
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0158
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0160
3232/6530 [=============>................] - ETA: 0s - loss: 0.0162
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0160
3968/6530 [=================>............] - ETA: 0s - loss: 0.0158
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0157
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0157
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0158
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0156
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0156
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0156
6480/6530 [============================>.] - ETA: 0s - loss: 0.0156
6530/6530 [==============================] - 1s 139us/step - loss: 0.0156 - val_loss: 0.0217
Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0168
 432/6530 [>.............................] - ETA: 0s - loss: 0.0148
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0149
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0150
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0149
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0150
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0150
3104/6530 [=============>................] - ETA: 0s - loss: 0.0150
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0149
3840/6530 [================>.............] - ETA: 0s - loss: 0.0148
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0148
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0147
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0148
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0149
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0149
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0150
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0149
6416/6530 [============================>.] - ETA: 0s - loss: 0.0150
6530/6530 [==============================] - 1s 143us/step - loss: 0.0150 - val_loss: 0.0177
Epoch 14/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0174
 304/6530 [>.............................] - ETA: 1s - loss: 0.0176
 624/6530 [=>............................] - ETA: 1s - loss: 0.0162
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0155
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0156
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0153
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0150
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0150
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0147
3024/6530 [============>.................] - ETA: 0s - loss: 0.0147
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0145
3728/6530 [================>.............] - ETA: 0s - loss: 0.0146
4096/6530 [=================>............] - ETA: 0s - loss: 0.0144
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0144
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0143
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0143
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0143
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0143
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0143
6512/6530 [============================>.] - ETA: 0s - loss: 0.0143
6530/6530 [==============================] - 1s 156us/step - loss: 0.0143 - val_loss: 0.0157
Epoch 15/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0117
 368/6530 [>.............................] - ETA: 0s - loss: 0.0122
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0130
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0135
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0132
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0135
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0135
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0135
3008/6530 [============>.................] - ETA: 0s - loss: 0.0138
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0138
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0139
3984/6530 [=================>............] - ETA: 0s - loss: 0.0137
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0138
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0137
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0137
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0137
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0136
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0137
6336/6530 [============================>.] - ETA: 0s - loss: 0.0136
6530/6530 [==============================] - 1s 152us/step - loss: 0.0137 - val_loss: 0.0227
Epoch 16/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0157
 320/6530 [>.............................] - ETA: 1s - loss: 0.0123
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0131
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0132
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0131
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0134
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0135
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0137
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0136
3088/6530 [=============>................] - ETA: 0s - loss: 0.0134
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0135
3760/6530 [================>.............] - ETA: 0s - loss: 0.0134
4096/6530 [=================>............] - ETA: 0s - loss: 0.0134
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0132
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0132
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0132
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0131
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0131
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0130
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0131
6496/6530 [============================>.] - ETA: 0s - loss: 0.0132
6530/6530 [==============================] - 1s 166us/step - loss: 0.0132 - val_loss: 0.0156
Epoch 17/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0220
 320/6530 [>.............................] - ETA: 1s - loss: 0.0126
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0125
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0125
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0121
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0118
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0122
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0124
2976/6530 [============>.................] - ETA: 0s - loss: 0.0124
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0124
3856/6530 [================>.............] - ETA: 0s - loss: 0.0125
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0126
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0127
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0127
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0127
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0128
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0128
6480/6530 [============================>.] - ETA: 0s - loss: 0.0127
6530/6530 [==============================] - 1s 142us/step - loss: 0.0127 - val_loss: 0.0159
Epoch 18/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0200
 320/6530 [>.............................] - ETA: 1s - loss: 0.0134
 624/6530 [=>............................] - ETA: 0s - loss: 0.0132
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0128
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0126
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0126
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0124
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0122
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0122
2992/6530 [============>.................] - ETA: 0s - loss: 0.0122
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0123
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0124
3984/6530 [=================>............] - ETA: 0s - loss: 0.0124
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0125
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0125
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0126
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0126
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0125
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0124
6464/6530 [============================>.] - ETA: 0s - loss: 0.0124
6530/6530 [==============================] - 1s 155us/step - loss: 0.0124 - val_loss: 0.0144
Epoch 19/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0101
 400/6530 [>.............................] - ETA: 0s - loss: 0.0130
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0126
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0118
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0120
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0120
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0119
2864/6530 [============>.................] - ETA: 0s - loss: 0.0120
3216/6530 [=============>................] - ETA: 0s - loss: 0.0119
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0120
3904/6530 [================>.............] - ETA: 0s - loss: 0.0120
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0121
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0122
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0123
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0122
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0122
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0121
6336/6530 [============================>.] - ETA: 0s - loss: 0.0121
6530/6530 [==============================] - 1s 142us/step - loss: 0.0120 - val_loss: 0.0181
Epoch 20/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0131
 416/6530 [>.............................] - ETA: 0s - loss: 0.0131
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0121
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0121
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0119
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0120
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0121
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0118
2864/6530 [============>.................] - ETA: 0s - loss: 0.0119
3184/6530 [=============>................] - ETA: 0s - loss: 0.0119
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0118
3904/6530 [================>.............] - ETA: 0s - loss: 0.0119
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0118
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0117
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0118
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0116
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0117
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0116
6400/6530 [============================>.] - ETA: 0s - loss: 0.0115
6530/6530 [==============================] - 1s 150us/step - loss: 0.0115 - val_loss: 0.0133
Epoch 21/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0154
 432/6530 [>.............................] - ETA: 0s - loss: 0.0108
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0106
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0115
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0112
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0109
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0113
2848/6530 [============>.................] - ETA: 0s - loss: 0.0112
3232/6530 [=============>................] - ETA: 0s - loss: 0.0112
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0112
4032/6530 [=================>............] - ETA: 0s - loss: 0.0112
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0112
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0112
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0112
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0112
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0113
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0113
6530/6530 [==============================] - 1s 139us/step - loss: 0.0112 - val_loss: 0.0128
Epoch 22/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0062
 336/6530 [>.............................] - ETA: 0s - loss: 0.0116
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0107
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0109
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0110
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0109
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0105
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0106
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0106
2960/6530 [============>.................] - ETA: 0s - loss: 0.0106
3264/6530 [=============>................] - ETA: 0s - loss: 0.0106
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0106
3952/6530 [=================>............] - ETA: 0s - loss: 0.0106
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0107
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0107
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0107
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0108
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0109
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0109
6320/6530 [============================>.] - ETA: 0s - loss: 0.0110
6530/6530 [==============================] - 1s 160us/step - loss: 0.0110 - val_loss: 0.0154
Epoch 23/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0138
 320/6530 [>.............................] - ETA: 1s - loss: 0.0114
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0104
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0100
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0106
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0106
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0107
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0108
2928/6530 [============>.................] - ETA: 0s - loss: 0.0109
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0108
3712/6530 [================>.............] - ETA: 0s - loss: 0.0108
4096/6530 [=================>............] - ETA: 0s - loss: 0.0107
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0107
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0107
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0107
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0106
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0106
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0106
6530/6530 [==============================] - 1s 145us/step - loss: 0.0106 - val_loss: 0.0130
Epoch 24/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0238
 384/6530 [>.............................] - ETA: 0s - loss: 0.0125
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0112
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0105
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0108
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0105
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0106
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0106
2864/6530 [============>.................] - ETA: 0s - loss: 0.0104
3248/6530 [=============>................] - ETA: 0s - loss: 0.0102
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0102
3984/6530 [=================>............] - ETA: 0s - loss: 0.0101
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0103
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0103
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0104
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0104
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0104
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0104
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0104
6530/6530 [==============================] - 1s 155us/step - loss: 0.0104 - val_loss: 0.0169
Epoch 25/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0121
 320/6530 [>.............................] - ETA: 1s - loss: 0.0115
 624/6530 [=>............................] - ETA: 1s - loss: 0.0112
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0109
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0104
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0105
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0106
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0104
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0103
3168/6530 [=============>................] - ETA: 0s - loss: 0.0103
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0102
3840/6530 [================>.............] - ETA: 0s - loss: 0.0102
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0104
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0105
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0103
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0102
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0102
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0102
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0103
6530/6530 [==============================] - 1s 155us/step - loss: 0.0103 - val_loss: 0.0372
Epoch 26/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0576
 304/6530 [>.............................] - ETA: 1s - loss: 0.0130
 624/6530 [=>............................] - ETA: 0s - loss: 0.0109
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0104
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0104
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0103
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0102
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0101
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0103
2848/6530 [============>.................] - ETA: 0s - loss: 0.0103
3152/6530 [=============>................] - ETA: 0s - loss: 0.0103
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0103
3776/6530 [================>.............] - ETA: 0s - loss: 0.0102
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0102
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0103
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0102
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0102
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0102
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0102
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0102
6432/6530 [============================>.] - ETA: 0s - loss: 0.0102
6530/6530 [==============================] - 1s 167us/step - loss: 0.0102 - val_loss: 0.0112
Epoch 27/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0090
 320/6530 [>.............................] - ETA: 1s - loss: 0.0098
 624/6530 [=>............................] - ETA: 1s - loss: 0.0098
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0104
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0102
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0100
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0100
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0098
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0100
3120/6530 [=============>................] - ETA: 0s - loss: 0.0100
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0101
3824/6530 [================>.............] - ETA: 0s - loss: 0.0100
4112/6530 [=================>............] - ETA: 0s - loss: 0.0100
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0099
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0100
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0100
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0099
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0100
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0100
6528/6530 [============================>.] - ETA: 0s - loss: 0.0099
6530/6530 [==============================] - 1s 156us/step - loss: 0.0099 - val_loss: 0.0112

# training | RMSE: 0.0970, MAE: 0.0754
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39180291294730896}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3475827664775537}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.09697652087829062, 'rmse': 0.09697652087829062, 'mae': 0.07543966009038518, 'early_stop': False}
vggnet done  2
all of workers have been done
calculation finished
#1 epoch=27.0 loss={'loss': 0.18468599478390252, 'rmse': 0.18468599478390252, 'mae': 0.14932273404147878, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22287393078460238}, 'layer_1_size': 86, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 62, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#0 epoch=27.0 loss={'loss': 0.144307630664082, 'rmse': 0.144307630664082, 'mae': 0.12227309817194515, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3634795999481858}, 'layer_2_size': 98, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 6, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 38, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#2 epoch=27.0 loss={'loss': 0.09697652087829062, 'rmse': 0.09697652087829062, 'mae': 0.07543966009038518, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39180291294730896}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3475827664775537}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
get a list [results] of length 120
get a list [loss] of length 3
get a list [val_loss] of length 3
length of indices is [2 1 0]
length of indices is 3
length of T is 3
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39180291294730896}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3475827664775537}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]] 

*** 1.0 configurations x 81.0 iterations each

3 | Thu Sep 27 22:14:32 2018 | lowest loss so far: 0.0970 (run 2)

vggnet done  1
vggnet done  2
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  70 | activation: sigmoid | extras: None 
layer 2 | size:   7 | activation: relu    | extras: dropout - rate: 39.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 4:33 - loss: 0.5499
 496/6530 [=>............................] - ETA: 8s - loss: 0.3238  
1040/6530 [===>..........................] - ETA: 4s - loss: 0.1996
1536/6530 [======>.......................] - ETA: 2s - loss: 0.1538
2000/6530 [========>.....................] - ETA: 1s - loss: 0.1289
2400/6530 [==========>...................] - ETA: 1s - loss: 0.1150
2864/6530 [============>.................] - ETA: 1s - loss: 0.1034
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0949
3728/6530 [================>.............] - ETA: 0s - loss: 0.0885
4112/6530 [=================>............] - ETA: 0s - loss: 0.0839
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0795
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0757
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0725
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0699
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0678
6400/6530 [============================>.] - ETA: 0s - loss: 0.0662
6530/6530 [==============================] - 2s 236us/step - loss: 0.0655 - val_loss: 0.0327
Epoch 2/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0198
 288/6530 [>.............................] - ETA: 1s - loss: 0.0392
 576/6530 [=>............................] - ETA: 1s - loss: 0.0370
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0362
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0356
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0344
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0343
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0340
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0337
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0332
3136/6530 [=============>................] - ETA: 0s - loss: 0.0327
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0327
3824/6530 [================>.............] - ETA: 0s - loss: 0.0324
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0322
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0319
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0321
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0322
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0322
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0317
6384/6530 [============================>.] - ETA: 0s - loss: 0.0315
6530/6530 [==============================] - 1s 159us/step - loss: 0.0313 - val_loss: 0.0385
Epoch 3/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0348
 400/6530 [>.............................] - ETA: 0s - loss: 0.0317
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0308
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0310
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0306
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0313
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0306
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0311
2848/6530 [============>.................] - ETA: 0s - loss: 0.0304
3184/6530 [=============>................] - ETA: 0s - loss: 0.0302
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0300
3808/6530 [================>.............] - ETA: 0s - loss: 0.0297
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0294
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0291
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0289
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0289
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0288
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0285
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0285
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0284
6530/6530 [==============================] - 1s 162us/step - loss: 0.0284 - val_loss: 0.0284
Epoch 4/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0245
 384/6530 [>.............................] - ETA: 0s - loss: 0.0310
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0280
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0283
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0278
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0277
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0272
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0269
2928/6530 [============>.................] - ETA: 0s - loss: 0.0268
3248/6530 [=============>................] - ETA: 0s - loss: 0.0270
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0267
3936/6530 [=================>............] - ETA: 0s - loss: 0.0263
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0263
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0265
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0265
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0260
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0261
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0260
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0258
6530/6530 [==============================] - 1s 154us/step - loss: 0.0259 - val_loss: 0.0249
Epoch 5/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0209
 432/6530 [>.............................] - ETA: 0s - loss: 0.0244
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0254
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0257
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0253
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0247
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0243
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0244
3184/6530 [=============>................] - ETA: 0s - loss: 0.0241
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0240
3968/6530 [=================>............] - ETA: 0s - loss: 0.0238
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0237
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0235
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0236
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0237
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0237
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0237
6432/6530 [============================>.] - ETA: 0s - loss: 0.0236
6530/6530 [==============================] - 1s 142us/step - loss: 0.0237 - val_loss: 0.0233
Epoch 6/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0117
 320/6530 [>.............................] - ETA: 1s - loss: 0.0213
 640/6530 [=>............................] - ETA: 0s - loss: 0.0218
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0209
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0223
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0220
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0217
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0222
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0220
2912/6530 [============>.................] - ETA: 0s - loss: 0.0216
3216/6530 [=============>................] - ETA: 0s - loss: 0.0214
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0212
3872/6530 [================>.............] - ETA: 0s - loss: 0.0211
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0211
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0214
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0213
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0213
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0215
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0214
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0215
6530/6530 [==============================] - 1s 162us/step - loss: 0.0217 - val_loss: 0.0254
Epoch 7/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0198
 384/6530 [>.............................] - ETA: 0s - loss: 0.0205
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0210
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0204
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0203
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0201
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0197
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0194
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0199
2864/6530 [============>.................] - ETA: 0s - loss: 0.0199
3184/6530 [=============>................] - ETA: 0s - loss: 0.0199
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0197
3824/6530 [================>.............] - ETA: 0s - loss: 0.0198
4128/6530 [=================>............] - ETA: 0s - loss: 0.0196
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0199
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0199
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0199
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0201
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0202
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0201
6400/6530 [============================>.] - ETA: 0s - loss: 0.0201
6530/6530 [==============================] - 1s 166us/step - loss: 0.0201 - val_loss: 0.0252
Epoch 8/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0180
 336/6530 [>.............................] - ETA: 1s - loss: 0.0200
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0188
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0187
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0188
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0187
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0185
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0187
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0183
3072/6530 [=============>................] - ETA: 0s - loss: 0.0183
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0183
3712/6530 [================>.............] - ETA: 0s - loss: 0.0183
4016/6530 [=================>............] - ETA: 0s - loss: 0.0186
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0187
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0188
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0187
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0188
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0188
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0188
6368/6530 [============================>.] - ETA: 0s - loss: 0.0189
6530/6530 [==============================] - 1s 158us/step - loss: 0.0189 - val_loss: 0.0187
Epoch 9/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0180
 400/6530 [>.............................] - ETA: 0s - loss: 0.0181
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0167
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0174
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0179
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0184
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0183
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0184
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0182
3136/6530 [=============>................] - ETA: 0s - loss: 0.0182
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0184
3840/6530 [================>.............] - ETA: 0s - loss: 0.0183
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0181
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0180
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0179
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0178
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0179
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0179
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0179
6512/6530 [============================>.] - ETA: 0s - loss: 0.0178
6530/6530 [==============================] - 1s 157us/step - loss: 0.0178 - val_loss: 0.0182
Epoch 10/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0236
 320/6530 [>.............................] - ETA: 1s - loss: 0.0189
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0183
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0179
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0173
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0172
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0172
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0172
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0172
3184/6530 [=============>................] - ETA: 0s - loss: 0.0171
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0171
3808/6530 [================>.............] - ETA: 0s - loss: 0.0171
4096/6530 [=================>............] - ETA: 0s - loss: 0.0171
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0170
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0170
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0169
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0169
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0170
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0171
6530/6530 [==============================] - 1s 155us/step - loss: 0.0171 - val_loss: 0.0198
Epoch 11/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0253
 336/6530 [>.............................] - ETA: 1s - loss: 0.0169
 640/6530 [=>............................] - ETA: 0s - loss: 0.0172
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0169
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0169
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0171
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0169
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0170
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0168
2928/6530 [============>.................] - ETA: 0s - loss: 0.0168
3264/6530 [=============>................] - ETA: 0s - loss: 0.0166
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0165
3888/6530 [================>.............] - ETA: 0s - loss: 0.0164
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0165
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0164
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0163
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0165
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0164
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0165
6416/6530 [============================>.] - ETA: 0s - loss: 0.0165
6530/6530 [==============================] - 1s 159us/step - loss: 0.0164 - val_loss: 0.0304
Epoch 12/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0264
 352/6530 [>.............................] - ETA: 0s - loss: 0.0164
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0167
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0165
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0161
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0160
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0159
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0159
2864/6530 [============>.................] - ETA: 0s - loss: 0.0160
3248/6530 [=============>................] - ETA: 0s - loss: 0.0161
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0160
3968/6530 [=================>............] - ETA: 0s - loss: 0.0158
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0157
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0157
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0157
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0156
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0156
6496/6530 [============================>.] - ETA: 0s - loss: 0.0156
6530/6530 [==============================] - 1s 139us/step - loss: 0.0156 - val_loss: 0.0217
Epoch 13/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0168
 432/6530 [>.............................] - ETA: 0s - loss: 0.0148
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0149
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0149
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0148
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0148
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0148
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0149
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0150
2976/6530 [============>.................] - ETA: 0s - loss: 0.0151
3248/6530 [=============>................] - ETA: 0s - loss: 0.0149
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0149
3856/6530 [================>.............] - ETA: 0s - loss: 0.0149
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0147
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0148
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0148
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0148
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0149
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0149
6400/6530 [============================>.] - ETA: 0s - loss: 0.0150
6530/6530 [==============================] - 1s 157us/step - loss: 0.0150 - val_loss: 0.0177
Epoch 14/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0174
 352/6530 [>.............................] - ETA: 0s - loss: 0.0173
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0163
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0156
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0155
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0153
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0151
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0148
2864/6530 [============>.................] - ETA: 0s - loss: 0.0149
3216/6530 [=============>................] - ETA: 0s - loss: 0.0145
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0146
3920/6530 [=================>............] - ETA: 0s - loss: 0.0145
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0144
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0144
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0143
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0143
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0142
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0143
6432/6530 [============================>.] - ETA: 0s - loss: 0.0144
6530/6530 [==============================] - 1s 148us/step - loss: 0.0143 - val_loss: 0.0157
Epoch 15/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0117
 336/6530 [>.............................] - ETA: 0s - loss: 0.0123
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0128
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0132
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0133
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0137
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0134
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0135
2912/6530 [============>.................] - ETA: 0s - loss: 0.0138
3248/6530 [=============>................] - ETA: 0s - loss: 0.0139
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0139
3952/6530 [=================>............] - ETA: 0s - loss: 0.0138
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0138
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0138
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0137
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0136
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0137
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0137
6416/6530 [============================>.] - ETA: 0s - loss: 0.0137
6530/6530 [==============================] - 1s 148us/step - loss: 0.0137 - val_loss: 0.0227
Epoch 16/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0157
 416/6530 [>.............................] - ETA: 0s - loss: 0.0126
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0133
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0132
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0133
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0135
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0136
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0137
2944/6530 [============>.................] - ETA: 0s - loss: 0.0135
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0135
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0134
4000/6530 [=================>............] - ETA: 0s - loss: 0.0134
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0133
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0132
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0131
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0132
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0131
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0131
6416/6530 [============================>.] - ETA: 0s - loss: 0.0131
6530/6530 [==============================] - 1s 149us/step - loss: 0.0132 - val_loss: 0.0156
Epoch 17/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0220
 368/6530 [>.............................] - ETA: 0s - loss: 0.0124
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0131
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0126
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0121
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0118
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0120
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0122
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0122
3024/6530 [============>.................] - ETA: 0s - loss: 0.0124
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0124
3744/6530 [================>.............] - ETA: 0s - loss: 0.0124
4064/6530 [=================>............] - ETA: 0s - loss: 0.0126
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0126
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0127
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0127
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0127
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0128
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0128
6530/6530 [==============================] - 1s 154us/step - loss: 0.0127 - val_loss: 0.0159
Epoch 18/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0200
 432/6530 [>.............................] - ETA: 0s - loss: 0.0128
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0128
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0125
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0125
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0123
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0122
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0122
2928/6530 [============>.................] - ETA: 0s - loss: 0.0123
3264/6530 [=============>................] - ETA: 0s - loss: 0.0123
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0124
4000/6530 [=================>............] - ETA: 0s - loss: 0.0124
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0124
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0125
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0126
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0126
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0125
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0125
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0125
6528/6530 [============================>.] - ETA: 0s - loss: 0.0124
6530/6530 [==============================] - 1s 157us/step - loss: 0.0124 - val_loss: 0.0144
Epoch 19/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0101
 320/6530 [>.............................] - ETA: 1s - loss: 0.0121
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0127
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0123
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0117
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0120
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0120
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0120
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0118
2960/6530 [============>.................] - ETA: 0s - loss: 0.0120
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0119
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0121
4048/6530 [=================>............] - ETA: 0s - loss: 0.0120
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0121
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0122
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0122
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0121
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0122
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0121
6368/6530 [============================>.] - ETA: 0s - loss: 0.0121
6530/6530 [==============================] - 1s 159us/step - loss: 0.0120 - val_loss: 0.0181
Epoch 20/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0131
 336/6530 [>.............................] - ETA: 0s - loss: 0.0128
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0117
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0122
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0119
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0121
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0121
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0121
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0120
3072/6530 [=============>................] - ETA: 0s - loss: 0.0120
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0118
3792/6530 [================>.............] - ETA: 0s - loss: 0.0119
4128/6530 [=================>............] - ETA: 0s - loss: 0.0118
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0117
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0118
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0117
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0116
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0116
6352/6530 [============================>.] - ETA: 0s - loss: 0.0115
6530/6530 [==============================] - 1s 149us/step - loss: 0.0115 - val_loss: 0.0133
Epoch 21/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0154
 512/6530 [=>............................] - ETA: 0s - loss: 0.0110
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0114
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0112
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0110
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0113
3088/6530 [=============>................] - ETA: 0s - loss: 0.0112
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0113
4064/6530 [=================>............] - ETA: 0s - loss: 0.0112
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0112
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0112
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0112
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0113
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0113
6530/6530 [==============================] - 1s 112us/step - loss: 0.0112 - val_loss: 0.0128
Epoch 22/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0062
 400/6530 [>.............................] - ETA: 0s - loss: 0.0114
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0113
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0110
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0110
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0107
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0105
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0106
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0106
3088/6530 [=============>................] - ETA: 0s - loss: 0.0106
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0106
3760/6530 [================>.............] - ETA: 0s - loss: 0.0106
4064/6530 [=================>............] - ETA: 0s - loss: 0.0105
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0106
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0107
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0107
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0108
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0108
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0109
6320/6530 [============================>.] - ETA: 0s - loss: 0.0110
6530/6530 [==============================] - 1s 161us/step - loss: 0.0110 - val_loss: 0.0154
Epoch 23/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0138
 352/6530 [>.............................] - ETA: 0s - loss: 0.0114
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0103
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0102
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0106
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0106
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0107
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0108
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0108
3088/6530 [=============>................] - ETA: 0s - loss: 0.0109
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0108
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0108
4032/6530 [=================>............] - ETA: 0s - loss: 0.0107
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0106
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0107
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0107
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0107
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0106
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0106
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0106
6528/6530 [============================>.] - ETA: 0s - loss: 0.0106
6530/6530 [==============================] - 1s 165us/step - loss: 0.0106 - val_loss: 0.0130
Epoch 24/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0238
 320/6530 [>.............................] - ETA: 1s - loss: 0.0133
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0113
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0104
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0104
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0106
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0105
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0106
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0106
2928/6530 [============>.................] - ETA: 0s - loss: 0.0103
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0102
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0102
3984/6530 [=================>............] - ETA: 0s - loss: 0.0101
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0103
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0103
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0104
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0104
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0104
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0103
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0104
6400/6530 [============================>.] - ETA: 0s - loss: 0.0104
6530/6530 [==============================] - 1s 166us/step - loss: 0.0104 - val_loss: 0.0169
Epoch 25/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0121
 416/6530 [>.............................] - ETA: 0s - loss: 0.0115
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0110
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0105
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0106
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0106
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0104
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0104
2992/6530 [============>.................] - ETA: 0s - loss: 0.0103
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0103
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0103
3952/6530 [=================>............] - ETA: 0s - loss: 0.0104
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0105
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0104
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0103
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0102
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0102
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0103
6336/6530 [============================>.] - ETA: 0s - loss: 0.0102
6530/6530 [==============================] - 1s 152us/step - loss: 0.0103 - val_loss: 0.0372
Epoch 26/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0576
 368/6530 [>.............................] - ETA: 0s - loss: 0.0118
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0108
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0105
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0104
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0104
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0102
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0103
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0102
2992/6530 [============>.................] - ETA: 0s - loss: 0.0103
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0103
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0103
4016/6530 [=================>............] - ETA: 0s - loss: 0.0102
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0102
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0103
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0102
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0102
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0102
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0102
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0102
6530/6530 [==============================] - 1s 162us/step - loss: 0.0102 - val_loss: 0.0112
Epoch 27/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0090
 400/6530 [>.............................] - ETA: 0s - loss: 0.0091
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0104
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0103
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0100
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0099
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0098
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0100
2912/6530 [============>.................] - ETA: 0s - loss: 0.0100
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0099
3792/6530 [================>.............] - ETA: 0s - loss: 0.0100
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0100
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0100
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0100
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0099
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0100
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0100
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0100
6448/6530 [============================>.] - ETA: 0s - loss: 0.0099
6530/6530 [==============================] - 1s 149us/step - loss: 0.0099 - val_loss: 0.0112
Epoch 28/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0134
 416/6530 [>.............................] - ETA: 0s - loss: 0.0097
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0096
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0097
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0100
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0099
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0097
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0097
2864/6530 [============>.................] - ETA: 0s - loss: 0.0097
3184/6530 [=============>................] - ETA: 0s - loss: 0.0097
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0097
3840/6530 [================>.............] - ETA: 0s - loss: 0.0097
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0097
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0099
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0100
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0099
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0099
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0099
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0098
6530/6530 [==============================] - 1s 155us/step - loss: 0.0097 - val_loss: 0.0110
Epoch 29/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0083
 320/6530 [>.............................] - ETA: 1s - loss: 0.0101
 624/6530 [=>............................] - ETA: 1s - loss: 0.0098
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0099
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0098
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0098
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0096
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0097
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0099
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0097
3088/6530 [=============>................] - ETA: 0s - loss: 0.0099
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0098
3744/6530 [================>.............] - ETA: 0s - loss: 0.0099
4048/6530 [=================>............] - ETA: 0s - loss: 0.0100
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0100
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0099
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0097
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0097
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0096
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0097
6480/6530 [============================>.] - ETA: 0s - loss: 0.0096
6530/6530 [==============================] - 1s 164us/step - loss: 0.0096 - val_loss: 0.0110
Epoch 30/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0153
 368/6530 [>.............................] - ETA: 0s - loss: 0.0090
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0095
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0091
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0093
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0094
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0092
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0091
2912/6530 [============>.................] - ETA: 0s - loss: 0.0092
3248/6530 [=============>................] - ETA: 0s - loss: 0.0093
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0090
3920/6530 [=================>............] - ETA: 0s - loss: 0.0090
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0092
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0093
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0092
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0092
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0092
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0093
6320/6530 [============================>.] - ETA: 0s - loss: 0.0093
6530/6530 [==============================] - 1s 151us/step - loss: 0.0094 - val_loss: 0.0148
Epoch 31/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0129
 336/6530 [>.............................] - ETA: 0s - loss: 0.0083
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0095
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0092
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0091
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0093
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0093
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0093
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0093
2832/6530 [============>.................] - ETA: 0s - loss: 0.0093
3104/6530 [=============>................] - ETA: 0s - loss: 0.0093
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0093
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0093
3952/6530 [=================>............] - ETA: 0s - loss: 0.0094
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0093
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0093
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0093
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0093
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0093
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0093
6320/6530 [============================>.] - ETA: 0s - loss: 0.0093
6530/6530 [==============================] - 1s 168us/step - loss: 0.0093 - val_loss: 0.0104
Epoch 32/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0186
 368/6530 [>.............................] - ETA: 0s - loss: 0.0098
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0089
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0088
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0087
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0091
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0091
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0089
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0090
2960/6530 [============>.................] - ETA: 0s - loss: 0.0090
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0091
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0092
3936/6530 [=================>............] - ETA: 0s - loss: 0.0092
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0091
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0092
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0090
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0091
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0092
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0092
6530/6530 [==============================] - 1s 153us/step - loss: 0.0092 - val_loss: 0.0206
Epoch 33/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0173
 352/6530 [>.............................] - ETA: 0s - loss: 0.0091
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0099
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0101
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0099
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0097
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0096
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0096
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0095
3024/6530 [============>.................] - ETA: 0s - loss: 0.0095
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0094
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0094
3984/6530 [=================>............] - ETA: 0s - loss: 0.0093
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0093
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0093
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0093
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0091
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0091
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0091
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0090
6530/6530 [==============================] - 1s 163us/step - loss: 0.0090 - val_loss: 0.0100
Epoch 34/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0044
 368/6530 [>.............................] - ETA: 0s - loss: 0.0081
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0073
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0073
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0078
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0080
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0081
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0083
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0084
3088/6530 [=============>................] - ETA: 0s - loss: 0.0085
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0084
3760/6530 [================>.............] - ETA: 0s - loss: 0.0086
4096/6530 [=================>............] - ETA: 0s - loss: 0.0086
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0086
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0088
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0088
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0088
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0089
6384/6530 [============================>.] - ETA: 0s - loss: 0.0088
6530/6530 [==============================] - 1s 149us/step - loss: 0.0089 - val_loss: 0.0101
Epoch 35/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0106
 368/6530 [>.............................] - ETA: 0s - loss: 0.0090
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0091
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0088
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0090
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0087
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0088
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0088
3040/6530 [============>.................] - ETA: 0s - loss: 0.0088
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0089
3760/6530 [================>.............] - ETA: 0s - loss: 0.0088
4112/6530 [=================>............] - ETA: 0s - loss: 0.0088
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0088
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0089
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0089
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0088
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0089
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0088
6336/6530 [============================>.] - ETA: 0s - loss: 0.0088
6530/6530 [==============================] - 1s 153us/step - loss: 0.0088 - val_loss: 0.0191
Epoch 36/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0148
 336/6530 [>.............................] - ETA: 1s - loss: 0.0075
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0080
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0086
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0087
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0084
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0085
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0087
2848/6530 [============>.................] - ETA: 0s - loss: 0.0085
3232/6530 [=============>................] - ETA: 0s - loss: 0.0085
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0084
3984/6530 [=================>............] - ETA: 0s - loss: 0.0086
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0085
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0085
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0085
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0085
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0086
6368/6530 [============================>.] - ETA: 0s - loss: 0.0086
6530/6530 [==============================] - 1s 144us/step - loss: 0.0087 - val_loss: 0.0119
Epoch 37/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0117
 368/6530 [>.............................] - ETA: 0s - loss: 0.0071
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0082
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0082
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0082
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0088
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0088
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0085
2880/6530 [============>.................] - ETA: 0s - loss: 0.0085
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0085
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0085
4048/6530 [=================>............] - ETA: 0s - loss: 0.0084
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0085
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0085
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0084
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0085
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0086
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0086
6384/6530 [============================>.] - ETA: 0s - loss: 0.0086
6530/6530 [==============================] - 1s 151us/step - loss: 0.0085 - val_loss: 0.0105
Epoch 38/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0058
 320/6530 [>.............................] - ETA: 1s - loss: 0.0090
 640/6530 [=>............................] - ETA: 0s - loss: 0.0089
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0089
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0090
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0087
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0088
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0088
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0087
3040/6530 [============>.................] - ETA: 0s - loss: 0.0089
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0087
3808/6530 [================>.............] - ETA: 0s - loss: 0.0086
4096/6530 [=================>............] - ETA: 0s - loss: 0.0086
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0085
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0085
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0085
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0085
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0085
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0085
6400/6530 [============================>.] - ETA: 0s - loss: 0.0085
6530/6530 [==============================] - 1s 158us/step - loss: 0.0085 - val_loss: 0.0094
Epoch 39/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0070
 368/6530 [>.............................] - ETA: 0s - loss: 0.0079
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0074
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0079
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0080
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0080
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0081
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0083
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0082
2944/6530 [============>.................] - ETA: 0s - loss: 0.0082
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0083
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0082
4016/6530 [=================>............] - ETA: 0s - loss: 0.0082
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0082
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0083
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0083
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0083
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0083
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0083
6400/6530 [============================>.] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 1s 159us/step - loss: 0.0083 - val_loss: 0.0092
Epoch 40/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0080
 384/6530 [>.............................] - ETA: 0s - loss: 0.0078
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0082
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0083
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0084
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0082
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0081
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0082
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0082
3072/6530 [=============>................] - ETA: 0s - loss: 0.0083
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0083
3744/6530 [================>.............] - ETA: 0s - loss: 0.0083
4064/6530 [=================>............] - ETA: 0s - loss: 0.0083
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0083
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0083
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0084
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0083
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0084
6368/6530 [============================>.] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 1s 149us/step - loss: 0.0083 - val_loss: 0.0106
Epoch 41/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0102
 432/6530 [>.............................] - ETA: 0s - loss: 0.0097
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0087
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0084
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0083
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0085
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0086
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0085
2976/6530 [============>.................] - ETA: 0s - loss: 0.0085
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0084
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0084
3920/6530 [=================>............] - ETA: 0s - loss: 0.0085
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0084
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0085
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0084
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0084
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0084
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0083
6384/6530 [============================>.] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 1s 151us/step - loss: 0.0083 - val_loss: 0.0130
Epoch 42/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0110
 352/6530 [>.............................] - ETA: 0s - loss: 0.0082
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0088
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0086
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0085
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0086
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0085
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0086
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0084
3072/6530 [=============>................] - ETA: 0s - loss: 0.0084
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0083
3808/6530 [================>.............] - ETA: 0s - loss: 0.0082
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0083
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0082
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0081
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0082
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0082
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0082
6530/6530 [==============================] - 1s 143us/step - loss: 0.0082 - val_loss: 0.0100
Epoch 43/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0113
 464/6530 [=>............................] - ETA: 0s - loss: 0.0090
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0085
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0088
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0089
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0088
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0085
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0083
3088/6530 [=============>................] - ETA: 0s - loss: 0.0082
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0082
3808/6530 [================>.............] - ETA: 0s - loss: 0.0083
4128/6530 [=================>............] - ETA: 0s - loss: 0.0084
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0085
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0083
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0083
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0082
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0082
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0082
6496/6530 [============================>.] - ETA: 0s - loss: 0.0081
6530/6530 [==============================] - 1s 147us/step - loss: 0.0081 - val_loss: 0.0112
Epoch 44/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0103
 400/6530 [>.............................] - ETA: 0s - loss: 0.0076
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0076
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0074
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0075
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0076
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0077
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0077
3248/6530 [=============>................] - ETA: 0s - loss: 0.0079
3744/6530 [================>.............] - ETA: 0s - loss: 0.0080
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0081
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0081
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0081
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0082
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0081
6530/6530 [==============================] - 1s 118us/step - loss: 0.0081 - val_loss: 0.0123

# training | RMSE: 0.1061, MAE: 0.0845
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39180291294730896}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3475827664775537}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.1060867194239247, 'rmse': 0.1060867194239247, 'mae': 0.08445336948852805, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=81.0 loss={'loss': 0.1060867194239247, 'rmse': 0.1060867194239247, 'mae': 0.08445336948852805, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 70, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39180291294730896}, 'layer_2_size': 7, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3475827664775537}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
get a list [results] of length 121
get a list [loss] of length 1
get a list [val_loss] of length 1
length of indices is [0]
length of indices is 1
length of T is 1
s=3
T is of size 34
T=[{'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18453780169533673}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 50, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4573419275160545}, 'layer_4_size': 19, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3875942870789325}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 93, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 98, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12822731338452031}, 'layer_3_size': 50, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15240141483301356}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 96, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25326331996434737}, 'layer_3_size': 9, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36533817719676887}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4372703468506898}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23163308861005694}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.414099247876896}, 'layer_4_size': 56, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.33297348780591707}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12036691254238692}, 'layer_1_size': 51, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31745590374793353}, 'layer_4_size': 37, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2874584730155907}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 63, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4799995916392109}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24846895451893564}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3480397037957643}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3528943621643851}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.16757928744849615}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 94, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 35, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11413478903314869}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3182160390631539}, 'layer_1_size': 56, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1120907421906558}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30368399365598964}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.24035461137473535}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4924475657499635}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4182553678929207}, 'layer_3_size': 47, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.331021309252039}, 'layer_4_size': 42, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25060012535856574}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2881345356795595}, 'layer_1_size': 8, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4223958557105504}, 'layer_3_size': 60, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.26967177492736316}, 'layer_5_size': 70, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 51, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 100, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39690890797011347}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.493460588847083}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.19809702626686612}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.48176897269261254}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.470436768334743}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3326184702177891}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 88, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 5, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41064930270372024}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32093918765125695}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 75, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4899487845427368}, 'layer_3_size': 58, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2537594906806875}, 'layer_5_size': 5, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18937320188285503}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3608381047162518}, 'layer_4_size': 53, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12456805446143138}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4799932907512584}, 'layer_2_size': 43, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 8, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 16, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.46506922168810017}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 96, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24611707829169843}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27030586391745315}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 88, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 26, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.47898783123588373}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1886379071183935}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.296248766422614}, 'layer_2_size': 96, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 77, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12319328277512098}, 'layer_2_size': 40, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24037986348181428}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 5, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 24, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3786541306750485}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4724664607557182}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30528210404435674}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]
newly formed T structure is:[[0, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18453780169533673}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 50, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4573419275160545}, 'layer_4_size': 19, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [1, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3875942870789325}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 93, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 98, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [2, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12822731338452031}, 'layer_3_size': 50, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15240141483301356}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [3, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 96, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25326331996434737}, 'layer_3_size': 9, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36533817719676887}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [4, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4372703468506898}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [5, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23163308861005694}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [6, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.414099247876896}, 'layer_4_size': 56, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.33297348780591707}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [7, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [8, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12036691254238692}, 'layer_1_size': 51, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31745590374793353}, 'layer_4_size': 37, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2874584730155907}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [9, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 63, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4799995916392109}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [10, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24846895451893564}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3480397037957643}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3528943621643851}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.16757928744849615}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [11, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 94, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 35, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11413478903314869}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [12, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3182160390631539}, 'layer_1_size': 56, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1120907421906558}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [13, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30368399365598964}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.24035461137473535}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4924475657499635}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [14, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4182553678929207}, 'layer_3_size': 47, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.331021309252039}, 'layer_4_size': 42, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25060012535856574}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [15, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2881345356795595}, 'layer_1_size': 8, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4223958557105504}, 'layer_3_size': 60, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.26967177492736316}, 'layer_5_size': 70, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [16, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 51, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [17, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 100, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [18, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39690890797011347}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [19, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.493460588847083}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.19809702626686612}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.48176897269261254}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [20, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.470436768334743}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3326184702177891}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [21, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 88, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 5, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [22, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41064930270372024}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32093918765125695}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [23, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 75, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4899487845427368}, 'layer_3_size': 58, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2537594906806875}, 'layer_5_size': 5, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [24, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18937320188285503}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3608381047162518}, 'layer_4_size': 53, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12456805446143138}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [25, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4799932907512584}, 'layer_2_size': 43, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 8, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [26, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [27, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 16, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.46506922168810017}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [28, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 96, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24611707829169843}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [29, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27030586391745315}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 88, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 26, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.47898783123588373}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [30, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1886379071183935}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.296248766422614}, 'layer_2_size': 96, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 77, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [31, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12319328277512098}, 'layer_2_size': 40, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24037986348181428}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [32, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 5, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 24, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3786541306750485}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [33, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4724664607557182}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30528210404435674}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]] 

*** 34 configurations x 3.0 iterations each

1 | Thu Sep 27 22:15:18 2018 | lowest loss so far: 0.0970 (run 2)

{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  34 | activation: tanh    | extras: None 
layer 2 | size:  20 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca41fae80>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 18s - loss: 0.6988
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2951 
6530/6530 [==============================] - 1s 128us/step - loss: 0.2781 - val_loss: 0.1677
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1735{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  23 | activation: sigmoid | extras: dropout - rate: 38.8% 
layer 2 | size:  20 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 1:17 - loss: 0.6806
6530/6530 [==============================] - 0s 8us/step - loss: 0.1615 - val_loss: 0.1628
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1589
1728/6530 [======>.......................] - ETA: 2s - loss: 0.4975  
6530/6530 [==============================] - 0s 7us/step - loss: 0.1596 - val_loss: 0.1618

3328/6530 [==============>...............] - ETA: 0s - loss: 0.4404
4992/6530 [=====================>........] - ETA: 0s - loss: 0.3974
6530/6530 [==============================] - 1s 153us/step - loss: 0.3681 - val_loss: 0.1953
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2487
1920/6530 [=======>......................] - ETA: 0s - loss: 0.2465
3968/6530 [=================>............] - ETA: 0s - loss: 0.2358
# training | RMSE: 0.2044, MAE: 0.1592
worker 2  xfile  [2, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12822731338452031}, 'layer_3_size': 50, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15240141483301356}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2043990688484676, 'rmse': 0.2043990688484676, 'mae': 0.15924162883418197, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  72 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 10s - loss: 0.7526
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2299
6530/6530 [==============================] - 0s 28us/step - loss: 0.2278 - val_loss: 0.2339
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2350
2944/6530 [============>.................] - ETA: 0s - loss: 0.3902 {'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  69 | activation: relu    | extras: None 
layer 2 | size:  11 | activation: sigmoid | extras: dropout - rate: 18.5% 
layer 3 | size:  50 | activation: tanh    | extras: batchnorm 
layer 4 | size:  19 | activation: tanh    | extras: dropout - rate: 45.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 6:27 - loss: 1.0798
1856/6530 [=======>......................] - ETA: 0s - loss: 0.2202
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2510
 336/6530 [>.............................] - ETA: 18s - loss: 0.6629 
6530/6530 [==============================] - 0s 36us/step - loss: 0.2287 - val_loss: 0.0619
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0630
3776/6530 [================>.............] - ETA: 0s - loss: 0.2164
 672/6530 [==>...........................] - ETA: 9s - loss: 0.5367 
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0561
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2152
 992/6530 [===>..........................] - ETA: 6s - loss: 0.4621
6530/6530 [==============================] - 0s 29us/step - loss: 0.2133 - val_loss: 0.1902

5440/6530 [=======================>......] - ETA: 0s - loss: 0.0542
6530/6530 [==============================] - 0s 20us/step - loss: 0.0531 - val_loss: 0.0496
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0498
1312/6530 [=====>........................] - ETA: 4s - loss: 0.4217
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0448
1632/6530 [======>.......................] - ETA: 3s - loss: 0.3942
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0433
1952/6530 [=======>......................] - ETA: 2s - loss: 0.3760
6530/6530 [==============================] - 0s 20us/step - loss: 0.0425 - val_loss: 0.0411

2272/6530 [=========>....................] - ETA: 2s - loss: 0.3607
2592/6530 [==========>...................] - ETA: 2s - loss: 0.3511
2912/6530 [============>.................] - ETA: 1s - loss: 0.3436
3232/6530 [=============>................] - ETA: 1s - loss: 0.3340
3536/6530 [===============>..............] - ETA: 1s - loss: 0.3260
3840/6530 [================>.............] - ETA: 1s - loss: 0.3218
4144/6530 [==================>...........] - ETA: 0s - loss: 0.3148
4448/6530 [===================>..........] - ETA: 0s - loss: 0.3081
4752/6530 [====================>.........] - ETA: 0s - loss: 0.3040
5072/6530 [======================>.......] - ETA: 0s - loss: 0.3002
5392/6530 [=======================>......] - ETA: 0s - loss: 0.2953
# training | RMSE: 0.2342, MAE: 0.1912
worker 1  xfile  [1, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3875942870789325}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 93, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 98, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.23422192936496805, 'rmse': 0.23422192936496805, 'mae': 0.19118437174023709, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  81 | activation: tanh    | extras: batchnorm 
layer 2 | size:  31 | activation: relu    | extras: None 
layer 3 | size:  76 | activation: relu    | extras: dropout - rate: 43.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca42130f0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:50 - loss: 0.8015
5712/6530 [=========================>....] - ETA: 0s - loss: 0.2916
 368/6530 [>.............................] - ETA: 5s - loss: 0.1864  
6016/6530 [==========================>...] - ETA: 0s - loss: 0.2882
 736/6530 [==>...........................] - ETA: 2s - loss: 0.1365
6336/6530 [============================>.] - ETA: 0s - loss: 0.2849
1088/6530 [===>..........................] - ETA: 2s - loss: 0.1132
6530/6530 [==============================] - 2s 318us/step - loss: 0.2833 - val_loss: 0.2904

1424/6530 [=====>........................] - ETA: 1s - loss: 0.1017Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.2057
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0938
 320/6530 [>.............................] - ETA: 1s - loss: 0.2078
# training | RMSE: 0.1992, MAE: 0.1569
worker 2  xfile  [3, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 96, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25326331996434737}, 'layer_3_size': 9, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36533817719676887}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.19923282981752033, 'rmse': 0.19923282981752033, 'mae': 0.15688454807468413, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  52 | activation: tanh    | extras: batchnorm 
layer 2 | size:  93 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca0769048>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:10 - loss: 1.7963
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0873
 624/6530 [=>............................] - ETA: 0s - loss: 0.2088
 336/6530 [>.............................] - ETA: 6s - loss: 0.9713  
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0828
 944/6530 [===>..........................] - ETA: 0s - loss: 0.2150
 688/6530 [==>...........................] - ETA: 3s - loss: 0.6543
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0794
1280/6530 [====>.........................] - ETA: 0s - loss: 0.2200
1040/6530 [===>..........................] - ETA: 2s - loss: 0.5238
3120/6530 [=============>................] - ETA: 0s - loss: 0.0767
1616/6530 [======>.......................] - ETA: 0s - loss: 0.2172
1360/6530 [=====>........................] - ETA: 2s - loss: 0.4559
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0732
1936/6530 [=======>......................] - ETA: 0s - loss: 0.2128
1696/6530 [======>.......................] - ETA: 1s - loss: 0.4073
3856/6530 [================>.............] - ETA: 0s - loss: 0.0704
2256/6530 [=========>....................] - ETA: 0s - loss: 0.2109
2032/6530 [========>.....................] - ETA: 1s - loss: 0.3729
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0674
2528/6530 [==========>...................] - ETA: 0s - loss: 0.2106
2384/6530 [=========>....................] - ETA: 1s - loss: 0.3485
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0652
2864/6530 [============>.................] - ETA: 0s - loss: 0.2094
2736/6530 [===========>..................] - ETA: 1s - loss: 0.3307
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0632
3200/6530 [=============>................] - ETA: 0s - loss: 0.2091
3088/6530 [=============>................] - ETA: 0s - loss: 0.3151
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0613
3520/6530 [===============>..............] - ETA: 0s - loss: 0.2093
3424/6530 [==============>...............] - ETA: 0s - loss: 0.3055
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0596
3856/6530 [================>.............] - ETA: 0s - loss: 0.2094
3776/6530 [================>.............] - ETA: 0s - loss: 0.2969
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0583
4192/6530 [==================>...........] - ETA: 0s - loss: 0.2083
4128/6530 [=================>............] - ETA: 0s - loss: 0.2886
6368/6530 [============================>.] - ETA: 0s - loss: 0.0572
4512/6530 [===================>..........] - ETA: 0s - loss: 0.2073
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2813
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2065
4832/6530 [=====================>........] - ETA: 0s - loss: 0.2747
6530/6530 [==============================] - 1s 195us/step - loss: 0.0565 - val_loss: 0.1608
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0358
5184/6530 [======================>.......] - ETA: 0s - loss: 0.2050
5184/6530 [======================>.......] - ETA: 0s - loss: 0.2687
 384/6530 [>.............................] - ETA: 0s - loss: 0.0346
5520/6530 [========================>.....] - ETA: 0s - loss: 0.2038
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2627
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0346
5840/6530 [=========================>....] - ETA: 0s - loss: 0.2023
5840/6530 [=========================>....] - ETA: 0s - loss: 0.2584
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0333
6128/6530 [===========================>..] - ETA: 0s - loss: 0.2015
6192/6530 [===========================>..] - ETA: 0s - loss: 0.2539
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0319
6448/6530 [============================>.] - ETA: 0s - loss: 0.2008
6528/6530 [============================>.] - ETA: 0s - loss: 0.2503
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0311
6530/6530 [==============================] - 1s 165us/step - loss: 0.2008 - val_loss: 0.2348
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1758
6530/6530 [==============================] - 1s 207us/step - loss: 0.2502 - val_loss: 0.1570
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1466
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0310
 352/6530 [>.............................] - ETA: 0s - loss: 0.1735
 368/6530 [>.............................] - ETA: 0s - loss: 0.1742
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0307
 688/6530 [==>...........................] - ETA: 0s - loss: 0.1803
 720/6530 [==>...........................] - ETA: 0s - loss: 0.1792
2832/6530 [============>.................] - ETA: 0s - loss: 0.0311
1008/6530 [===>..........................] - ETA: 0s - loss: 0.1769
1072/6530 [===>..........................] - ETA: 0s - loss: 0.1773
3200/6530 [=============>................] - ETA: 0s - loss: 0.0305
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1808
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1744
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0301
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1820
1808/6530 [=======>......................] - ETA: 0s - loss: 0.1739
3936/6530 [=================>............] - ETA: 0s - loss: 0.0298
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1826
2160/6530 [========>.....................] - ETA: 0s - loss: 0.1708
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0293
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1809
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1708
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0290
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1814
2864/6530 [============>.................] - ETA: 0s - loss: 0.1695
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0289
3008/6530 [============>.................] - ETA: 0s - loss: 0.1800
3232/6530 [=============>................] - ETA: 0s - loss: 0.1688
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0288
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1800
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1682
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0289
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1795
3936/6530 [=================>............] - ETA: 0s - loss: 0.1684
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0288
4000/6530 [=================>............] - ETA: 0s - loss: 0.1798
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1666
6512/6530 [============================>.] - ETA: 0s - loss: 0.0286
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1797
6530/6530 [==============================] - 1s 148us/step - loss: 0.0286 - val_loss: 0.0219
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0437
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1663
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1786
 384/6530 [>.............................] - ETA: 0s - loss: 0.0316
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1661
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1777
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0307
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1655
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1772
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0291
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1639
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1774
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0278
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1635
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1775
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0276
6432/6530 [============================>.] - ETA: 0s - loss: 0.1626
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1769
6530/6530 [==============================] - 1s 148us/step - loss: 0.1626 - val_loss: 0.1436
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1295
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0272
 368/6530 [>.............................] - ETA: 0s - loss: 0.1569
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0270
6530/6530 [==============================] - 1s 162us/step - loss: 0.1768 - val_loss: 0.1402

 720/6530 [==>...........................] - ETA: 0s - loss: 0.1586
2976/6530 [============>.................] - ETA: 0s - loss: 0.0266
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1584
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0267
3712/6530 [================>.............] - ETA: 0s - loss: 0.0264
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1568
4080/6530 [=================>............] - ETA: 0s - loss: 0.0264
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1531
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1508
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0260
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1512
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0259
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1501
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0257
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0257
3184/6530 [=============>................] - ETA: 0s - loss: 0.1484
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0254
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1488
6320/6530 [============================>.] - ETA: 0s - loss: 0.0253
3936/6530 [=================>............] - ETA: 0s - loss: 0.1488
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1478
6530/6530 [==============================] - 1s 142us/step - loss: 0.0253 - val_loss: 0.0168

4688/6530 [====================>.........] - ETA: 0s - loss: 0.1487
# training | RMSE: 0.1796, MAE: 0.1386
worker 0  xfile  [0, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18453780169533673}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 50, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4573419275160545}, 'layer_4_size': 19, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.17963382899359456, 'rmse': 0.17963382899359456, 'mae': 0.138573589184316, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  81 | activation: sigmoid | extras: None 
layer 2 | size:  24 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca42130f0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 16s - loss: 0.6605
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1484
1984/6530 [========>.....................] - ETA: 0s - loss: 0.3567 
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1478
3968/6530 [=================>............] - ETA: 0s - loss: 0.2832
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1463
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2558
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1458
6530/6530 [==============================] - 0s 54us/step - loss: 0.2505 - val_loss: 0.1948
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2004
6480/6530 [============================>.] - ETA: 0s - loss: 0.1452
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1936
6530/6530 [==============================] - 1s 147us/step - loss: 0.1456 - val_loss: 0.1304

3712/6530 [================>.............] - ETA: 0s - loss: 0.1903
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1871
6530/6530 [==============================] - 0s 29us/step - loss: 0.1855 - val_loss: 0.1715
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1840
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1760
3840/6530 [================>.............] - ETA: 0s - loss: 0.1711
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1705
6530/6530 [==============================] - 0s 28us/step - loss: 0.1700 - val_loss: 0.1884

# training | RMSE: 0.1544, MAE: 0.1216
worker 2  xfile  [5, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23163308861005694}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.15437207264276148, 'rmse': 0.15437207264276148, 'mae': 0.12158973772407727, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  51 | activation: relu    | extras: dropout - rate: 12.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc64f3fd30>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 50s - loss: 1.3304
 688/6530 [==>...........................] - ETA: 1s - loss: 0.2741 
1424/6530 [=====>........................] - ETA: 0s - loss: 0.1674
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1292
# training | RMSE: 0.2275, MAE: 0.1880
worker 0  xfile  [6, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.414099247876896}, 'layer_4_size': 56, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.33297348780591707}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.22745328813566315, 'rmse': 0.22745328813566315, 'mae': 0.18802184709330885, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  50 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca32f3978>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 28s - loss: 0.4476
# training | RMSE: 0.1288, MAE: 0.1009
worker 1  xfile  [4, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4372703468506898}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.1288035850358587, 'rmse': 0.1288035850358587, 'mae': 0.10092940980903108, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  77 | activation: tanh    | extras: batchnorm 
layer 2 | size:  23 | activation: relu    | extras: None 
layer 3 | size:   7 | activation: relu    | extras: None 
layer 4 | size:  47 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca01b52e8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 16s - loss: 0.7302
2896/6530 [============>.................] - ETA: 0s - loss: 0.1104
1408/6530 [=====>........................] - ETA: 0s - loss: 0.2503 
2816/6530 [===========>..................] - ETA: 0s - loss: 0.5168 
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0994
2784/6530 [===========>..................] - ETA: 0s - loss: 0.2196
5376/6530 [=======================>......] - ETA: 0s - loss: 0.3627
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0908
4128/6530 [=================>............] - ETA: 0s - loss: 0.2027
6530/6530 [==============================] - 0s 76us/step - loss: 0.3282 - val_loss: 0.1785
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2083
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0843
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1931
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1662
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0793
6530/6530 [==============================] - 0s 64us/step - loss: 0.1873 - val_loss: 0.1618
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1638
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1623
6496/6530 [============================>.] - ETA: 0s - loss: 0.0750
6530/6530 [==============================] - 0s 20us/step - loss: 0.1596 - val_loss: 0.2062
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1492
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1634
6530/6530 [==============================] - 1s 95us/step - loss: 0.0749 - val_loss: 0.0435
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0255
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1452
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1621
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0393
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1406
4032/6530 [=================>............] - ETA: 0s - loss: 0.1613
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0386
6530/6530 [==============================] - 0s 21us/step - loss: 0.1380 - val_loss: 0.1414

5376/6530 [=======================>......] - ETA: 0s - loss: 0.1617
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0380
6530/6530 [==============================] - 0s 40us/step - loss: 0.1616 - val_loss: 0.1623
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1910
2864/6530 [============>.................] - ETA: 0s - loss: 0.0376
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1635
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0379
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1622
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0380
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1616
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0377
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1612
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0377
6530/6530 [==============================] - 0s 38us/step - loss: 0.1614 - val_loss: 0.1632

6432/6530 [============================>.] - ETA: 0s - loss: 0.0381
6530/6530 [==============================] - 0s 74us/step - loss: 0.0381 - val_loss: 0.0389
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0167
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0360
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0356
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0343
3136/6530 [=============>................] - ETA: 0s - loss: 0.0349
3888/6530 [================>.............] - ETA: 0s - loss: 0.0349
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0347
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0348
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0346
6530/6530 [==============================] - 0s 72us/step - loss: 0.0346 - val_loss: 0.0367

# training | RMSE: 0.1746, MAE: 0.1321
worker 1  xfile  [7, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.17459430582675636, 'rmse': 0.17459430582675636, 'mae': 0.13210598159860065, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  24 | activation: relu    | extras: dropout - rate: 24.8% 
layer 2 | size:  98 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  42 | activation: tanh    | extras: dropout - rate: 34.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc72e4c9b0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 20s - loss: 0.7269
2048/6530 [========>.....................] - ETA: 1s - loss: 0.6468 
3840/6530 [================>.............] - ETA: 0s - loss: 0.4350
6016/6530 [==========================>...] - ETA: 0s - loss: 0.3356
6530/6530 [==============================] - 1s 98us/step - loss: 0.3213 - val_loss: 0.1794
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1865
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1587
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1560
6530/6530 [==============================] - 0s 23us/step - loss: 0.1519 - val_loss: 0.1571
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1635
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1480
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1448
6530/6530 [==============================] - 0s 23us/step - loss: 0.1434 - val_loss: 0.1464

# training | RMSE: 0.1827, MAE: 0.1429
worker 2  xfile  [8, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12036691254238692}, 'layer_1_size': 51, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31745590374793353}, 'layer_4_size': 37, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2874584730155907}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.18273615162016582, 'rmse': 0.18273615162016582, 'mae': 0.14288055427559002, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  56 | activation: sigmoid | extras: dropout - rate: 31.8% 
layer 2 | size:  47 | activation: tanh    | extras: None 
layer 3 | size:  92 | activation: relu    | extras: batchnorm 
layer 4 | size:  15 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc4c06ca90>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:14 - loss: 0.5826
 640/6530 [=>............................] - ETA: 3s - loss: 0.2663  
# training | RMSE: 0.2085, MAE: 0.1615
worker 0  xfile  [9, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 63, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4799995916392109}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2085394172071364, 'rmse': 0.2085394172071364, 'mae': 0.16148978195950536, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  94 | activation: relu    | extras: batchnorm 
layer 2 | size:  50 | activation: relu    | extras: None 
layer 3 | size:  69 | activation: tanh    | extras: None 
layer 4 | size:  35 | activation: tanh    | extras: batchnorm 
layer 5 | size:  88 | activation: relu    | extras: dropout - rate: 11.4% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca018ca90>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 2:09 - loss: 1.7093
1184/6530 [====>.........................] - ETA: 2s - loss: 0.1770
 416/6530 [>.............................] - ETA: 10s - loss: 0.9378 
1824/6530 [=======>......................] - ETA: 1s - loss: 0.1403
 768/6530 [==>...........................] - ETA: 5s - loss: 0.7583 
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1214
1152/6530 [====>.........................] - ETA: 3s - loss: 0.6531
3136/6530 [=============>................] - ETA: 0s - loss: 0.1100
1568/6530 [======>.......................] - ETA: 2s - loss: 0.5797
3840/6530 [================>.............] - ETA: 0s - loss: 0.1028
1920/6530 [=======>......................] - ETA: 2s - loss: 0.5415
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0967
2336/6530 [=========>....................] - ETA: 1s - loss: 0.5006
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0920
# training | RMSE: 0.1889, MAE: 0.1438
worker 1  xfile  [10, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24846895451893564}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3480397037957643}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3528943621643851}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.16757928744849615}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.18892803879384745, 'rmse': 0.18892803879384745, 'mae': 0.14381269477678604, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  25 | activation: sigmoid | extras: dropout - rate: 30.4% 
layer 2 | size:  24 | activation: tanh    | extras: None 
layer 3 | size:  49 | activation: tanh    | extras: dropout - rate: 24.0% 
layer 4 | size:  14 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca019c2e8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:00 - loss: 1.3749
2752/6530 [===========>..................] - ETA: 1s - loss: 0.4695
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0888
 896/6530 [===>..........................] - ETA: 2s - loss: 0.3183  
3168/6530 [=============>................] - ETA: 1s - loss: 0.4489
1792/6530 [=======>......................] - ETA: 1s - loss: 0.2714
3584/6530 [===============>..............] - ETA: 0s - loss: 0.4296
6530/6530 [==============================] - 1s 142us/step - loss: 0.0869 - val_loss: 0.0755
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0529
2720/6530 [===========>..................] - ETA: 0s - loss: 0.2535
4000/6530 [=================>............] - ETA: 0s - loss: 0.4137
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0613
3648/6530 [===============>..............] - ETA: 0s - loss: 0.2446
4416/6530 [===================>..........] - ETA: 0s - loss: 0.3996
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0614
4576/6530 [====================>.........] - ETA: 0s - loss: 0.2382
4832/6530 [=====================>........] - ETA: 0s - loss: 0.3885
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0602
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2330
5248/6530 [=======================>......] - ETA: 0s - loss: 0.3774
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0605
6368/6530 [============================>.] - ETA: 0s - loss: 0.2286
5664/6530 [=========================>....] - ETA: 0s - loss: 0.3665
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0596
6530/6530 [==============================] - 1s 110us/step - loss: 0.2279 - val_loss: 0.1758
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1708
6080/6530 [==========================>...] - ETA: 0s - loss: 0.3575
4128/6530 [=================>............] - ETA: 0s - loss: 0.0601
 896/6530 [===>..........................] - ETA: 0s - loss: 0.2004
6464/6530 [============================>.] - ETA: 0s - loss: 0.3496
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0603
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1997
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0598
2688/6530 [===========>..................] - ETA: 0s - loss: 0.2012
6530/6530 [==============================] - 2s 239us/step - loss: 0.3482 - val_loss: 0.2879
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2586
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0598
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1988
 448/6530 [=>............................] - ETA: 0s - loss: 0.2309
6530/6530 [==============================] - 1s 78us/step - loss: 0.0593 - val_loss: 0.0531
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0511
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1987
 832/6530 [==>...........................] - ETA: 0s - loss: 0.2311
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0576
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1968
1248/6530 [====>.........................] - ETA: 0s - loss: 0.2209
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0554
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1951
1664/6530 [======>.......................] - ETA: 0s - loss: 0.2203
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0550
6530/6530 [==============================] - 0s 60us/step - loss: 0.1950 - val_loss: 0.1704
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2182
2080/6530 [========>.....................] - ETA: 0s - loss: 0.2197
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0548
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1808
2496/6530 [==========>...................] - ETA: 0s - loss: 0.2165
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0551
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1848
2912/6530 [============>.................] - ETA: 0s - loss: 0.2161
4128/6530 [=================>............] - ETA: 0s - loss: 0.0550
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1857
3328/6530 [==============>...............] - ETA: 0s - loss: 0.2137
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1864
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0549
3744/6530 [================>.............] - ETA: 0s - loss: 0.2135
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1862
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0548
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2122
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0549
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1858
4576/6530 [====================>.........] - ETA: 0s - loss: 0.2108
6530/6530 [==============================] - 1s 77us/step - loss: 0.0549 - val_loss: 0.0479

6336/6530 [============================>.] - ETA: 0s - loss: 0.1859
6530/6530 [==============================] - 0s 59us/step - loss: 0.1853 - val_loss: 0.1625

4960/6530 [=====================>........] - ETA: 0s - loss: 0.2103
5344/6530 [=======================>......] - ETA: 0s - loss: 0.2097
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2083
6176/6530 [===========================>..] - ETA: 0s - loss: 0.2069
6530/6530 [==============================] - 1s 130us/step - loss: 0.2054 - val_loss: 0.2223
Epoch 3/3

  32/6530 [..............................] - ETA: 1s - loss: 0.2643
 448/6530 [=>............................] - ETA: 0s - loss: 0.1939
 864/6530 [==>...........................] - ETA: 0s - loss: 0.1953
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1878
1632/6530 [======>.......................] - ETA: 0s - loss: 0.1862
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1859
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1845
# training | RMSE: 0.2175, MAE: 0.1769
worker 2  xfile  [12, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3182160390631539}, 'layer_1_size': 56, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1120907421906558}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2175374703500337, 'rmse': 0.2175374703500337, 'mae': 0.17694699880583895, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  34 | activation: tanh    | extras: None 
layer 2 | size:  32 | activation: tanh    | extras: None 
layer 3 | size:  47 | activation: relu    | extras: dropout - rate: 41.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc4c06cc18>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 4s - loss: 2.2435
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1825
6530/6530 [==============================] - 0s 39us/step - loss: 0.2470 - val_loss: 0.0851
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0822
3072/6530 [=============>................] - ETA: 0s - loss: 0.1817
6530/6530 [==============================] - 0s 6us/step - loss: 0.0690 - val_loss: 0.0642
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0678
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1829
# training | RMSE: 0.2071, MAE: 0.1646
worker 1  xfile  [13, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30368399365598964}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.24035461137473535}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4924475657499635}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20709193384095323, 'rmse': 0.20709193384095323, 'mae': 0.16463934739852065, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:   8 | activation: relu    | extras: dropout - rate: 28.8% 
layer 2 | size:  70 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc441114a8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 15s - loss: 0.5603
6530/6530 [==============================] - 0s 6us/step - loss: 0.0553 - val_loss: 0.0551

3872/6530 [================>.............] - ETA: 0s - loss: 0.1819
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1879 
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1812
6530/6530 [==============================] - 0s 65us/step - loss: 0.1509 - val_loss: 0.0712
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0726
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1797
3840/6530 [================>.............] - ETA: 0s - loss: 0.0720
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1795
6530/6530 [==============================] - 0s 15us/step - loss: 0.0708 - val_loss: 0.0598
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0695
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1798
3840/6530 [================>.............] - ETA: 0s - loss: 0.0654
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1791
6530/6530 [==============================] - 0s 14us/step - loss: 0.0639 - val_loss: 0.0532

6272/6530 [===========================>..] - ETA: 0s - loss: 0.1786
6530/6530 [==============================] - 1s 137us/step - loss: 0.1785 - val_loss: 0.2192

# training | RMSE: 0.2358, MAE: 0.1922
worker 1  xfile  [15, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2881345356795595}, 'layer_1_size': 8, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4223958557105504}, 'layer_3_size': 60, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.26967177492736316}, 'layer_5_size': 70, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.23580164824108096, 'rmse': 0.23580164824108096, 'mae': 0.19215434661389558, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  50 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc961194c18>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 5s - loss: 1.0164
6530/6530 [==============================] - 0s 47us/step - loss: 0.3926 - val_loss: 0.0682
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0725
6530/6530 [==============================] - 0s 7us/step - loss: 0.0579 - val_loss: 0.0540
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0516
6530/6530 [==============================] - 0s 6us/step - loss: 0.0489 - val_loss: 0.0462

# training | RMSE: 0.2241, MAE: 0.1770
worker 2  xfile  [14, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4182553678929207}, 'layer_3_size': 47, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.331021309252039}, 'layer_4_size': 42, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25060012535856574}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.22409352712594083, 'rmse': 0.22409352712594083, 'mae': 0.17695407295972776, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  51 | activation: relu    | extras: batchnorm 
layer 2 | size:  44 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  10 | activation: relu    | extras: None 
layer 4 | size:  90 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc959a924e0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 49s - loss: 0.6658
 896/6530 [===>..........................] - ETA: 3s - loss: 0.3219 
1920/6530 [=======>......................] - ETA: 1s - loss: 0.2636
3008/6530 [============>.................] - ETA: 0s - loss: 0.2299
4096/6530 [=================>............] - ETA: 0s - loss: 0.2112
# training | RMSE: 0.2631, MAE: 0.2147
worker 0  xfile  [11, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 94, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 35, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11413478903314869}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.26313622577996204, 'rmse': 0.26313622577996204, 'mae': 0.21468259219383565, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  42 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  30 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc74049588>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 18s - loss: 0.7545
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1993
3328/6530 [==============>...............] - ETA: 0s - loss: 0.2492 
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1919
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1557
6530/6530 [==============================] - 1s 79us/step - loss: 0.1510 - val_loss: 0.0417
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0432
6530/6530 [==============================] - 1s 138us/step - loss: 0.1873 - val_loss: 0.1533
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1268
2944/6530 [============>.................] - ETA: 0s - loss: 0.0405
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1321
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0406
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1325
6530/6530 [==============================] - 0s 20us/step - loss: 0.0403 - val_loss: 0.0379
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0406
2880/6530 [============>.................] - ETA: 0s - loss: 0.1297
3200/6530 [=============>................] - ETA: 0s - loss: 0.0380
3904/6530 [================>.............] - ETA: 0s - loss: 0.1297
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0380
6530/6530 [==============================] - 0s 18us/step - loss: 0.0378 - val_loss: 0.0349

4992/6530 [=====================>........] - ETA: 0s - loss: 0.1268
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1252
6530/6530 [==============================] - 0s 53us/step - loss: 0.1251 - val_loss: 0.1237
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1354
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1160
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1165
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1131
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1125
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1121
6528/6530 [============================>.] - ETA: 0s - loss: 0.1108
6530/6530 [==============================] - 0s 50us/step - loss: 0.1108 - val_loss: 0.1153

# training | RMSE: 0.1882, MAE: 0.1498
worker 0  xfile  [18, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39690890797011347}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.18820849443585733, 'rmse': 0.18820849443585733, 'mae': 0.1498464306051124, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  69 | activation: sigmoid | extras: dropout - rate: 47.0% 
layer 2 | size:  92 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc584c0438>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 29s - loss: 0.4321
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1060 
# training | RMSE: 0.2137, MAE: 0.1693
worker 1  xfile  [17, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 100, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2136616321269215, 'rmse': 0.2136616321269215, 'mae': 0.16934420296381308, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  82 | activation: relu    | extras: batchnorm 
layer 2 | size:  27 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  17 | activation: sigmoid | extras: dropout - rate: 49.3% 
layer 4 | size:  66 | activation: sigmoid | extras: dropout - rate: 19.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc961194a20>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 33s - loss: 0.7866
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0917
1664/6530 [======>.......................] - ETA: 2s - loss: 0.4011 
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0856
3456/6530 [==============>...............] - ETA: 0s - loss: 0.2973
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2609
6530/6530 [==============================] - 1s 79us/step - loss: 0.0838 - val_loss: 0.0993
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1158
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0701
6530/6530 [==============================] - 1s 145us/step - loss: 0.2459 - val_loss: 0.1822
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2044
3776/6530 [================>.............] - ETA: 0s - loss: 0.0691
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1691
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0690
6530/6530 [==============================] - 0s 28us/step - loss: 0.0691 - val_loss: 0.0670
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1158
3968/6530 [=================>............] - ETA: 0s - loss: 0.1621
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0635
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1591
6530/6530 [==============================] - 0s 27us/step - loss: 0.1578 - val_loss: 0.1508
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1417
3776/6530 [================>.............] - ETA: 0s - loss: 0.0631
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1433
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0638
6530/6530 [==============================] - 0s 28us/step - loss: 0.0637 - val_loss: 0.0580

4096/6530 [=================>............] - ETA: 0s - loss: 0.1433
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1422
6530/6530 [==============================] - 0s 28us/step - loss: 0.1415 - val_loss: 0.1503

# training | RMSE: 0.1435, MAE: 0.1091
worker 2  xfile  [16, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 51, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.14351898387225226, 'rmse': 0.14351898387225226, 'mae': 0.10914194169182288, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  84 | activation: sigmoid | extras: None 
layer 2 | size:  88 | activation: tanh    | extras: None 
layer 3 | size:  17 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  13 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc9599b9080>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 26s - loss: 0.8772
1792/6530 [=======>......................] - ETA: 1s - loss: 0.6856 
3456/6530 [==============>...............] - ETA: 0s - loss: 0.5689
4864/6530 [=====================>........] - ETA: 0s - loss: 0.4941
6272/6530 [===========================>..] - ETA: 0s - loss: 0.4276
6530/6530 [==============================] - 1s 127us/step - loss: 0.4170 - val_loss: 0.3390
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1388
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1179
3200/6530 [=============>................] - ETA: 0s - loss: 0.0965
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0817
6528/6530 [============================>.] - ETA: 0s - loss: 0.0728
6530/6530 [==============================] - 0s 34us/step - loss: 0.0728 - val_loss: 0.0643
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0520
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0435
4096/6530 [=================>............] - ETA: 0s - loss: 0.0426
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0420
6530/6530 [==============================] - 0s 27us/step - loss: 0.0420 - val_loss: 0.0647

# training | RMSE: 0.1861, MAE: 0.1495
worker 1  xfile  [19, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.493460588847083}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.19809702626686612}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.48176897269261254}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.18609794729110354, 'rmse': 0.18609794729110354, 'mae': 0.149500604917733, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  75 | activation: tanh    | extras: None 
layer 2 | size:  66 | activation: tanh    | extras: batchnorm 
layer 3 | size:  58 | activation: relu    | extras: dropout - rate: 49.0% 
layer 4 | size:  83 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc4421e470>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 16s - loss: 0.3225
3072/6530 [=============>................] - ETA: 0s - loss: 0.1591 
# training | RMSE: 0.2438, MAE: 0.1984
worker 0  xfile  [20, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.470436768334743}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3326184702177891}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.24383227868807644, 'rmse': 0.24383227868807644, 'mae': 0.19840031093049676, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  13 | activation: tanh    | extras: batchnorm 
layer 2 | size:  13 | activation: sigmoid | extras: dropout - rate: 41.1% 
layer 3 | size:  17 | activation: relu    | extras: batchnorm 
layer 4 | size:  89 | activation: tanh    | extras: dropout - rate: 32.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc584c02e8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 2:23 - loss: 1.0684
6400/6530 [============================>.] - ETA: 0s - loss: 0.1171
 416/6530 [>.............................] - ETA: 11s - loss: 0.8539 
 800/6530 [==>...........................] - ETA: 5s - loss: 0.6538 
6530/6530 [==============================] - 1s 137us/step - loss: 0.1160 - val_loss: 0.0400
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0575
1152/6530 [====>.........................] - ETA: 4s - loss: 0.5571
3840/6530 [================>.............] - ETA: 0s - loss: 0.0563
1536/6530 [======>.......................] - ETA: 2s - loss: 0.4808
6530/6530 [==============================] - 0s 15us/step - loss: 0.0552 - val_loss: 0.0350
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0547
1952/6530 [=======>......................] - ETA: 2s - loss: 0.4362
4096/6530 [=================>............] - ETA: 0s - loss: 0.0469
2368/6530 [=========>....................] - ETA: 1s - loss: 0.3932
6530/6530 [==============================] - 0s 15us/step - loss: 0.0460 - val_loss: 0.0344

2816/6530 [===========>..................] - ETA: 1s - loss: 0.3572
3264/6530 [=============>................] - ETA: 1s - loss: 0.3305
3712/6530 [================>.............] - ETA: 0s - loss: 0.3075
# training | RMSE: 0.2529, MAE: 0.2135
worker 2  xfile  [21, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 88, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 5, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.25288164451897255, 'rmse': 0.25288164451897255, 'mae': 0.21349282028200672, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  62 | activation: sigmoid | extras: None 
layer 2 | size:  19 | activation: sigmoid | extras: dropout - rate: 18.9% 
layer 3 | size:  32 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fc958dd4c88>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 11s - loss: 0.5863
4128/6530 [=================>............] - ETA: 0s - loss: 0.2905
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2784 
4448/6530 [===================>..........] - ETA: 0s - loss: 0.2795
6530/6530 [==============================] - 1s 87us/step - loss: 0.2582 - val_loss: 0.0694
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0817
4800/6530 [=====================>........] - ETA: 0s - loss: 0.2684
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0755
5184/6530 [======================>.......] - ETA: 0s - loss: 0.2577
6530/6530 [==============================] - 0s 10us/step - loss: 0.0754 - val_loss: 0.0699
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0764
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2487
6400/6530 [============================>.] - ETA: 0s - loss: 0.0757
6530/6530 [==============================] - 0s 9us/step - loss: 0.0756 - val_loss: 0.0694

5920/6530 [==========================>...] - ETA: 0s - loss: 0.2412
6304/6530 [===========================>..] - ETA: 0s - loss: 0.2345
6530/6530 [==============================] - 2s 265us/step - loss: 0.2303 - val_loss: 0.0772
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1229
 416/6530 [>.............................] - ETA: 0s - loss: 0.1151
 800/6530 [==>...........................] - ETA: 0s - loss: 0.1196
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1181
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1165
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1150
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1132
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1120
2944/6530 [============>.................] - ETA: 0s - loss: 0.1107
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1099
3712/6530 [================>.............] - ETA: 0s - loss: 0.1078
4064/6530 [=================>............] - ETA: 0s - loss: 0.1059
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1053
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1041
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1027
# training | RMSE: 0.1787, MAE: 0.1378
worker 1  xfile  [23, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 75, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4899487845427368}, 'layer_3_size': 58, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2537594906806875}, 'layer_5_size': 5, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.17873273813414958, 'rmse': 0.17873273813414958, 'mae': 0.13781108789892543, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  48 | activation: tanh    | extras: None 
layer 2 | size:  43 | activation: tanh    | extras: dropout - rate: 48.0% 
layer 3 | size:   8 | activation: relu    | extras: batchnorm 
layer 4 | size:  44 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc960a3fe48>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 34s - loss: 0.7954
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1021
2304/6530 [=========>....................] - ETA: 1s - loss: 0.6922 
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1009
4352/6530 [==================>...........] - ETA: 0s - loss: 0.6087
6336/6530 [============================>.] - ETA: 0s - loss: 0.1002
6528/6530 [============================>.] - ETA: 0s - loss: 0.5284
6530/6530 [==============================] - 1s 144us/step - loss: 0.0995 - val_loss: 0.0621
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1043
 480/6530 [=>............................] - ETA: 0s - loss: 0.0987
6530/6530 [==============================] - 1s 142us/step - loss: 0.5283 - val_loss: 0.2142
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2994
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0936
2304/6530 [=========>....................] - ETA: 0s - loss: 0.2653
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0905
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2368
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0877
6530/6530 [==============================] - 0s 24us/step - loss: 0.2229 - val_loss: 0.1797

# training | RMSE: 0.2648, MAE: 0.2166
worker 2  xfile  [24, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18937320188285503}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3608381047162518}, 'layer_4_size': 53, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12456805446143138}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2648099085294089, 'rmse': 0.2648099085294089, 'mae': 0.21661916220981453, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  69 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  34 | activation: tanh    | extras: batchnorm 
layer 3 | size:  42 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc958aadbe0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 2:21 - loss: 1.4331Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1920
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0879
 544/6530 [=>............................] - ETA: 8s - loss: 0.8111  
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1903
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0866
1056/6530 [===>..........................] - ETA: 4s - loss: 0.6059
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1857
2848/6530 [============>.................] - ETA: 0s - loss: 0.0864
1568/6530 [======>.......................] - ETA: 2s - loss: 0.4874
6530/6530 [==============================] - 0s 25us/step - loss: 0.1817 - val_loss: 0.1642

3264/6530 [=============>................] - ETA: 0s - loss: 0.0857
2112/6530 [========>.....................] - ETA: 1s - loss: 0.4117
3712/6530 [================>.............] - ETA: 0s - loss: 0.0853
2624/6530 [===========>..................] - ETA: 1s - loss: 0.3598
4128/6530 [=================>............] - ETA: 0s - loss: 0.0842
3136/6530 [=============>................] - ETA: 1s - loss: 0.3208
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0833
3648/6530 [===============>..............] - ETA: 0s - loss: 0.2953
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0821
4192/6530 [==================>...........] - ETA: 0s - loss: 0.2728
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0818
4736/6530 [====================>.........] - ETA: 0s - loss: 0.2549
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0817
5312/6530 [=======================>......] - ETA: 0s - loss: 0.2375
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0809
5856/6530 [=========================>....] - ETA: 0s - loss: 0.2244
6530/6530 [==============================] - 1s 130us/step - loss: 0.0804 - val_loss: 0.0647

6400/6530 [============================>.] - ETA: 0s - loss: 0.2125
6530/6530 [==============================] - 1s 219us/step - loss: 0.2103 - val_loss: 0.0749
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1274
 576/6530 [=>............................] - ETA: 0s - loss: 0.0818
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0815
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0798
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0784
# training | RMSE: 0.2054, MAE: 0.1648
worker 1  xfile  [25, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4799932907512584}, 'layer_2_size': 43, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 8, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.20535298092546403, 'rmse': 0.20535298092546403, 'mae': 0.1647658548701616, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  25 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc960601128>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 10s - loss: 0.0865
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0763
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0740
6530/6530 [==============================] - 1s 80us/step - loss: 0.0694 - val_loss: 0.0627
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0667
6530/6530 [==============================] - 0s 7us/step - loss: 0.0622 - val_loss: 0.0565
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0608
3904/6530 [================>.............] - ETA: 0s - loss: 0.0742
6530/6530 [==============================] - 0s 6us/step - loss: 0.0567 - val_loss: 0.0518

4448/6530 [===================>..........] - ETA: 0s - loss: 0.0735
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0729
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0723
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0717
6496/6530 [============================>.] - ETA: 0s - loss: 0.0711
6530/6530 [==============================] - 1s 100us/step - loss: 0.0710 - val_loss: 0.0550
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1067
 576/6530 [=>............................] - ETA: 0s - loss: 0.0597
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0600
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0589
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0584
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0576
3200/6530 [=============>................] - ETA: 0s - loss: 0.0560
3776/6530 [================>.............] - ETA: 0s - loss: 0.0560
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0564
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0564
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0559
# training | RMSE: 0.2326, MAE: 0.1903
worker 1  xfile  [27, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 16, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.46506922168810017}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.23260418010136963, 'rmse': 0.23260418010136963, 'mae': 0.1902978651327383, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  55 | activation: sigmoid | extras: dropout - rate: 27.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92be7bda0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:22 - loss: 1.8844
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0554
1216/6530 [====>.........................] - ETA: 2s - loss: 0.6130  
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0556
2464/6530 [==========>...................] - ETA: 0s - loss: 0.3458
6530/6530 [==============================] - 1s 102us/step - loss: 0.0554 - val_loss: 0.0466

3776/6530 [================>.............] - ETA: 0s - loss: 0.2481
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1968
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1685
6530/6530 [==============================] - 1s 114us/step - loss: 0.1641 - val_loss: 0.0465

# training | RMSE: 0.2520, MAE: 0.2017
worker 0  xfile  [22, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41064930270372024}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32093918765125695}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.25197201551508475, 'rmse': 0.25197201551508475, 'mae': 0.20169120046797048, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  49 | activation: tanh    | extras: batchnorm 
layer 2 | size:  96 | activation: sigmoid | extras: None 
layer 3 | size:  38 | activation: tanh    | extras: batchnorm 
layer 4 | size:  86 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc58262c88>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 2:20 - loss: 3.5179Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0520
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0447
 544/6530 [=>............................] - ETA: 8s - loss: 1.8426  
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0448
1088/6530 [===>..........................] - ETA: 3s - loss: 1.0123
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0432
1600/6530 [======>.......................] - ETA: 2s - loss: 0.7027
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0424
2112/6530 [========>.....................] - ETA: 1s - loss: 0.5441
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0417
2624/6530 [===========>..................] - ETA: 1s - loss: 0.4458
6530/6530 [==============================] - 0s 43us/step - loss: 0.0418 - val_loss: 0.0420
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0267
3136/6530 [=============>................] - ETA: 1s - loss: 0.3792
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0396
3648/6530 [===============>..............] - ETA: 0s - loss: 0.3310
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0391
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2950
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0398
4672/6530 [====================>.........] - ETA: 0s - loss: 0.2667
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0401
5184/6530 [======================>.......] - ETA: 0s - loss: 0.2435
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0409
5696/6530 [=========================>....] - ETA: 0s - loss: 0.2249
6530/6530 [==============================] - 0s 44us/step - loss: 0.0410 - val_loss: 0.0413

6176/6530 [===========================>..] - ETA: 0s - loss: 0.2101
6530/6530 [==============================] - 1s 220us/step - loss: 0.2004 - val_loss: 0.0324
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0234
 544/6530 [=>............................] - ETA: 0s - loss: 0.0296
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0283
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0300
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0287
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0284
3104/6530 [=============>................] - ETA: 0s - loss: 0.0279
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0277
4000/6530 [=================>............] - ETA: 0s - loss: 0.0274
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0270
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0266
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0265
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0265
6368/6530 [============================>.] - ETA: 0s - loss: 0.0265
# training | RMSE: 0.2016, MAE: 0.1617
worker 1  xfile  [29, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27030586391745315}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 88, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 26, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.47898783123588373}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20161830288568228, 'rmse': 0.20161830288568228, 'mae': 0.1617192199240646, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  21 | activation: sigmoid | extras: None 
layer 2 | size:  40 | activation: relu    | extras: dropout - rate: 12.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc92bc7b240>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 23s - loss: 0.5291
6530/6530 [==============================] - 1s 110us/step - loss: 0.0263 - val_loss: 0.0236
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0282
4096/6530 [=================>............] - ETA: 0s - loss: 0.2973 
 512/6530 [=>............................] - ETA: 0s - loss: 0.0237
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0234
6530/6530 [==============================] - 1s 95us/step - loss: 0.2173 - val_loss: 0.0625
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0585
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0236
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0621
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0228
6530/6530 [==============================] - 0s 12us/step - loss: 0.0611 - val_loss: 0.0620
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0653
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0224
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0570
6530/6530 [==============================] - 0s 12us/step - loss: 0.0569 - val_loss: 0.0587

2976/6530 [============>.................] - ETA: 0s - loss: 0.0223
# training | RMSE: 0.2013, MAE: 0.1613
worker 2  xfile  [26, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20128604638161898, 'rmse': 0.20128604638161898, 'mae': 0.16131123833485106, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  29 | activation: tanh    | extras: dropout - rate: 18.9% 
layer 2 | size:  96 | activation: tanh    | extras: dropout - rate: 29.6% 
layer 3 | size:  77 | activation: tanh    | extras: batchnorm 
layer 4 | size:  96 | activation: tanh    | extras: batchnorm 
layer 5 | size:   8 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc958aada58>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 23s - loss: 0.6988
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0220
3328/6530 [==============>...............] - ETA: 0s - loss: 0.7085 
3968/6530 [=================>............] - ETA: 0s - loss: 0.0219
6144/6530 [===========================>..] - ETA: 0s - loss: 0.6900
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0220
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0217
6530/6530 [==============================] - 1s 179us/step - loss: 0.6877 - val_loss: 0.4069
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.6269
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0216
3584/6530 [===============>..............] - ETA: 0s - loss: 0.5926
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0217
6530/6530 [==============================] - 0s 17us/step - loss: 0.5380 - val_loss: 0.2115
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.3580
6496/6530 [============================>.] - ETA: 0s - loss: 0.0217
6530/6530 [==============================] - 1s 107us/step - loss: 0.0217 - val_loss: 0.0184

3584/6530 [===============>..............] - ETA: 0s - loss: 0.2810
6400/6530 [============================>.] - ETA: 0s - loss: 0.2322
6530/6530 [==============================] - 0s 18us/step - loss: 0.2309 - val_loss: 0.1683

# training | RMSE: 0.1267, MAE: 0.0987
worker 0  xfile  [28, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 96, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24611707829169843}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.12666983019137465, 'rmse': 0.12666983019137465, 'mae': 0.0987053518258301, 'early_stop': False}
vggnet done  0

# training | RMSE: 0.2414, MAE: 0.1966
worker 1  xfile  [31, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12319328277512098}, 'layer_2_size': 40, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24037986348181428}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.24140735192374485, 'rmse': 0.24140735192374485, 'mae': 0.1966059188041613, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  98 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fc960542e48>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 21s - loss: 0.6974
4224/6530 [==================>...........] - ETA: 0s - loss: 0.5374 
6530/6530 [==============================] - 1s 91us/step - loss: 0.4204 - val_loss: 0.1694
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1808
3840/6530 [================>.............] - ETA: 0s - loss: 0.1623
6530/6530 [==============================] - 0s 16us/step - loss: 0.1628 - val_loss: 0.1647
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1866
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1624
6530/6530 [==============================] - 0s 16us/step - loss: 0.1610 - val_loss: 0.1629

# training | RMSE: 0.2103, MAE: 0.1647
worker 2  xfile  [30, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1886379071183935}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.296248766422614}, 'layer_2_size': 96, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 77, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.2103044929750992, 'rmse': 0.2103044929750992, 'mae': 0.16470378139623693, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  10 | activation: sigmoid | extras: dropout - rate: 47.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7fc958e49668>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:16 - loss: 0.1898
1184/6530 [====>.........................] - ETA: 1s - loss: 0.1393  
# training | RMSE: 0.2063, MAE: 0.1603
worker 1  xfile  [32, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 5, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 24, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3786541306750485}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2062568010375274, 'rmse': 0.2062568010375274, 'mae': 0.16033113204660185, 'early_stop': False}
vggnet done  1

2176/6530 [========>.....................] - ETA: 0s - loss: 0.1246
3168/6530 [=============>................] - ETA: 0s - loss: 0.1185
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1098
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1035
6530/6530 [==============================] - 1s 113us/step - loss: 0.1004 - val_loss: 0.0717
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0668
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0706
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0700
3072/6530 [=============>................] - ETA: 0s - loss: 0.0684
3968/6530 [=================>............] - ETA: 0s - loss: 0.0659
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0643
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0625
6530/6530 [==============================] - 0s 53us/step - loss: 0.0620 - val_loss: 0.0539
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0458
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0526
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0513
2976/6530 [============>.................] - ETA: 0s - loss: 0.0503
4064/6530 [=================>............] - ETA: 0s - loss: 0.0507
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0506
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0502
6530/6530 [==============================] - 0s 53us/step - loss: 0.0501 - val_loss: 0.0465

# training | RMSE: 0.2160, MAE: 0.1763
worker 2  xfile  [33, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4724664607557182}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30528210404435674}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.21601176357296403, 'rmse': 0.21601176357296403, 'mae': 0.17632709993510726, 'early_stop': False}
vggnet done  2
all of workers have been done
calculation finished
#2 epoch=3.0 loss={'loss': 0.2043990688484676, 'rmse': 0.2043990688484676, 'mae': 0.15924162883418197, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.12822731338452031}, 'layer_3_size': 50, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.15240141483301356}, 'layer_5_size': 44, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#1 epoch=3.0 loss={'loss': 0.23422192936496805, 'rmse': 0.23422192936496805, 'mae': 0.19118437174023709, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3875942870789325}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 93, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 98, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#3 epoch=3.0 loss={'loss': 0.19923282981752033, 'rmse': 0.19923282981752033, 'mae': 0.15688454807468413, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 96, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25326331996434737}, 'layer_3_size': 9, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36533817719676887}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#0 epoch=3.0 loss={'loss': 0.17963382899359456, 'rmse': 0.17963382899359456, 'mae': 0.138573589184316, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 69, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18453780169533673}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 50, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4573419275160545}, 'layer_4_size': 19, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 21, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#4 epoch=3.0 loss={'loss': 0.1288035850358587, 'rmse': 0.1288035850358587, 'mae': 0.10092940980903108, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4372703468506898}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#5 epoch=3.0 loss={'loss': 0.15437207264276148, 'rmse': 0.15437207264276148, 'mae': 0.12158973772407727, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23163308861005694}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#6 epoch=3.0 loss={'loss': 0.22745328813566315, 'rmse': 0.22745328813566315, 'mae': 0.18802184709330885, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 48, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.414099247876896}, 'layer_4_size': 56, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.33297348780591707}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#7 epoch=3.0 loss={'loss': 0.17459430582675636, 'rmse': 0.17459430582675636, 'mae': 0.13210598159860065, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#9 epoch=3.0 loss={'loss': 0.2085394172071364, 'rmse': 0.2085394172071364, 'mae': 0.16148978195950536, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 63, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4799995916392109}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#8 epoch=3.0 loss={'loss': 0.18273615162016582, 'rmse': 0.18273615162016582, 'mae': 0.14288055427559002, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.12036691254238692}, 'layer_1_size': 51, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31745590374793353}, 'layer_4_size': 37, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2874584730155907}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#10 epoch=3.0 loss={'loss': 0.18892803879384745, 'rmse': 0.18892803879384745, 'mae': 0.14381269477678604, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24846895451893564}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3480397037957643}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3528943621643851}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.16757928744849615}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#12 epoch=3.0 loss={'loss': 0.2175374703500337, 'rmse': 0.2175374703500337, 'mae': 0.17694699880583895, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3182160390631539}, 'layer_1_size': 56, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 47, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 92, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 15, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1120907421906558}, 'layer_5_size': 55, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#13 epoch=3.0 loss={'loss': 0.20709193384095323, 'rmse': 0.20709193384095323, 'mae': 0.16463934739852065, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.30368399365598964}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 24, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.24035461137473535}, 'layer_3_size': 49, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 14, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4924475657499635}, 'layer_5_size': 55, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#14 epoch=3.0 loss={'loss': 0.22409352712594083, 'rmse': 0.22409352712594083, 'mae': 0.17695407295972776, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4182553678929207}, 'layer_3_size': 47, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.331021309252039}, 'layer_4_size': 42, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.25060012535856574}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#15 epoch=3.0 loss={'loss': 0.23580164824108096, 'rmse': 0.23580164824108096, 'mae': 0.19215434661389558, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2881345356795595}, 'layer_1_size': 8, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 70, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4223958557105504}, 'layer_3_size': 60, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.26967177492736316}, 'layer_5_size': 70, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#11 epoch=3.0 loss={'loss': 0.26313622577996204, 'rmse': 0.26313622577996204, 'mae': 0.21468259219383565, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 94, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 50, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 35, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11413478903314869}, 'layer_5_size': 88, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#17 epoch=3.0 loss={'loss': 0.2136616321269215, 'rmse': 0.2136616321269215, 'mae': 0.16934420296381308, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 100, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#18 epoch=3.0 loss={'loss': 0.18820849443585733, 'rmse': 0.18820849443585733, 'mae': 0.1498464306051124, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39690890797011347}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#16 epoch=3.0 loss={'loss': 0.14351898387225226, 'rmse': 0.14351898387225226, 'mae': 0.10914194169182288, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 51, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 44, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 90, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#20 epoch=3.0 loss={'loss': 0.24383227868807644, 'rmse': 0.24383227868807644, 'mae': 0.19840031093049676, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.470436768334743}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3326184702177891}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#19 epoch=3.0 loss={'loss': 0.18609794729110354, 'rmse': 0.18609794729110354, 'mae': 0.149500604917733, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 82, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.493460588847083}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.19809702626686612}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.48176897269261254}, 'layer_5_size': 60, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#21 epoch=3.0 loss={'loss': 0.25288164451897255, 'rmse': 0.25288164451897255, 'mae': 0.21349282028200672, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 88, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 13, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 5, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#23 epoch=3.0 loss={'loss': 0.17873273813414958, 'rmse': 0.17873273813414958, 'mae': 0.13781108789892543, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 75, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4899487845427368}, 'layer_3_size': 58, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 83, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2537594906806875}, 'layer_5_size': 5, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#24 epoch=3.0 loss={'loss': 0.2648099085294089, 'rmse': 0.2648099085294089, 'mae': 0.21661916220981453, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.18937320188285503}, 'layer_2_size': 19, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 32, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3608381047162518}, 'layer_4_size': 53, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12456805446143138}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#25 epoch=3.0 loss={'loss': 0.20535298092546403, 'rmse': 0.20535298092546403, 'mae': 0.1647658548701616, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4799932907512584}, 'layer_2_size': 43, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 8, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#22 epoch=3.0 loss={'loss': 0.25197201551508475, 'rmse': 0.25197201551508475, 'mae': 0.20169120046797048, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41064930270372024}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32093918765125695}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#27 epoch=3.0 loss={'loss': 0.23260418010136963, 'rmse': 0.23260418010136963, 'mae': 0.1902978651327383, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 25, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 16, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 47, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 67, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.46506922168810017}, 'layer_5_size': 60, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#26 epoch=3.0 loss={'loss': 0.20128604638161898, 'rmse': 0.20128604638161898, 'mae': 0.16131123833485106, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 69, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#29 epoch=3.0 loss={'loss': 0.20161830288568228, 'rmse': 0.20161830288568228, 'mae': 0.1617192199240646, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.27030586391745315}, 'layer_1_size': 55, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 88, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 26, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.47898783123588373}, 'layer_5_size': 58, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#31 epoch=3.0 loss={'loss': 0.24140735192374485, 'rmse': 0.24140735192374485, 'mae': 0.1966059188041613, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12319328277512098}, 'layer_2_size': 40, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24037986348181428}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#30 epoch=3.0 loss={'loss': 0.2103044929750992, 'rmse': 0.2103044929750992, 'mae': 0.16470378139623693, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1886379071183935}, 'layer_1_size': 29, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.296248766422614}, 'layer_2_size': 96, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 77, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 8, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
#28 epoch=3.0 loss={'loss': 0.12666983019137465, 'rmse': 0.12666983019137465, 'mae': 0.0987053518258301, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 49, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 96, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 86, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24611707829169843}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#32 epoch=3.0 loss={'loss': 0.2062568010375274, 'rmse': 0.2062568010375274, 'mae': 0.16033113204660185, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 98, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 5, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 24, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3786541306750485}, 'layer_5_size': 3, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#33 epoch=3.0 loss={'loss': 0.21601176357296403, 'rmse': 0.21601176357296403, 'mae': 0.17632709993510726, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4724664607557182}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 38, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.30528210404435674}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': True}
get a list [results] of length 155
get a list [loss] of length 34
get a list [val_loss] of length 34
length of indices is [31  4 18  5  7 22  3  9 20 17 10  2 27 28  0 24 32 12  8 30 16 33 11 13
  6 26  1 14 29 19 25 21 15 23]
length of indices is 34
length of T is 34
newly formed T structure is:[[0, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12319328277512098}, 'layer_2_size': 40, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24037986348181428}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [1, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4372703468506898}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [2, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39690890797011347}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [3, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23163308861005694}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [4, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [5, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41064930270372024}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32093918765125695}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [6, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 96, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25326331996434737}, 'layer_3_size': 9, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36533817719676887}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [7, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 63, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4799995916392109}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [8, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.470436768334743}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3326184702177891}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [9, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 100, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [10, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24846895451893564}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3480397037957643}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3528943621643851}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.16757928744849615}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]] 

*** 11.333333333333332 configurations x 9.0 iterations each

34 | Thu Sep 27 22:15:42 2018 | lowest loss so far: 0.0970 (run 2)

{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  21 | activation: sigmoid | extras: None 
layer 2 | size:  40 | activation: relu    | extras: dropout - rate: 12.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 33s - loss: 0.5583
2560/6530 [==========>...................] - ETA: 1s - loss: 0.3496 
6400/6530 [============================>.] - ETA: 0s - loss: 0.1926
6530/6530 [==============================] - 1s 121us/step - loss: 0.1903 - val_loss: 0.0673
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0853
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0617
6530/6530 [==============================] - 0s 13us/step - loss: 0.0616 - val_loss: 0.0584
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0574
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0578
6530/6530 [==============================] - 0s 12us/step - loss: 0.0569 - val_loss: 0.0590
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0546
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0537
6530/6530 [==============================] - 0s 12us/step - loss: 0.0532 - val_loss: 0.0572
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0653
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0508{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  42 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  30 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 43s - loss: 0.6406
6530/6530 [==============================] - 0s 12us/step - loss: 0.0501 - val_loss: 0.0541
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0425
3200/6530 [=============>................] - ETA: 0s - loss: 0.1997 
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0464
6530/6530 [==============================] - 0s 12us/step - loss: 0.0474 - val_loss: 0.0504
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0532
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1264{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  81 | activation: tanh    | extras: batchnorm 
layer 2 | size:  31 | activation: relu    | extras: None 
layer 3 | size:  76 | activation: relu    | extras: dropout - rate: 43.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 6:37 - loss: 0.3507
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0453
6530/6530 [==============================] - 1s 156us/step - loss: 0.1212 - val_loss: 0.0412
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0440
6530/6530 [==============================] - 0s 12us/step - loss: 0.0452 - val_loss: 0.0474
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0443
 384/6530 [>.............................] - ETA: 16s - loss: 0.1443 
3072/6530 [=============>................] - ETA: 0s - loss: 0.0397
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0442
 752/6530 [==>...........................] - ETA: 8s - loss: 0.1183 
6530/6530 [==============================] - 0s 12us/step - loss: 0.0438 - val_loss: 0.0438
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0458
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0399
6530/6530 [==============================] - 0s 18us/step - loss: 0.0397 - val_loss: 0.0372
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0406
1120/6530 [====>.........................] - ETA: 5s - loss: 0.1020
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0431
6530/6530 [==============================] - 0s 12us/step - loss: 0.0426 - val_loss: 0.0485

3200/6530 [=============>................] - ETA: 0s - loss: 0.0359
1488/6530 [=====>........................] - ETA: 4s - loss: 0.0935
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0357
1872/6530 [=======>......................] - ETA: 3s - loss: 0.0868
6530/6530 [==============================] - 0s 18us/step - loss: 0.0356 - val_loss: 0.0325
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0369
2240/6530 [=========>....................] - ETA: 2s - loss: 0.0835
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0320
2592/6530 [==========>...................] - ETA: 2s - loss: 0.0792
6530/6530 [==============================] - 0s 16us/step - loss: 0.0312 - val_loss: 0.0284
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0334
2928/6530 [============>.................] - ETA: 1s - loss: 0.0758
3200/6530 [=============>................] - ETA: 0s - loss: 0.0277
3296/6530 [==============>...............] - ETA: 1s - loss: 0.0722
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0275
6530/6530 [==============================] - 0s 18us/step - loss: 0.0274 - val_loss: 0.0252
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0304
3664/6530 [===============>..............] - ETA: 1s - loss: 0.0696
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0251
4048/6530 [=================>............] - ETA: 0s - loss: 0.0671
6528/6530 [============================>.] - ETA: 0s - loss: 0.0245
6530/6530 [==============================] - 0s 17us/step - loss: 0.0244 - val_loss: 0.0225
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0277
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0652
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0228
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0631
6400/6530 [============================>.] - ETA: 0s - loss: 0.0223
6530/6530 [==============================] - 0s 17us/step - loss: 0.0222 - val_loss: 0.0205
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0254
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0616
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0211
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0602
6400/6530 [============================>.] - ETA: 0s - loss: 0.0206
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0586
6530/6530 [==============================] - 0s 17us/step - loss: 0.0205 - val_loss: 0.0191
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0238
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0575
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0196
6530/6530 [==============================] - 0s 17us/step - loss: 0.0191 - val_loss: 0.0182

6530/6530 [==============================] - 2s 297us/step - loss: 0.0564 - val_loss: 0.0615
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0237
 416/6530 [>.............................] - ETA: 0s - loss: 0.0350
# training | RMSE: 0.2167, MAE: 0.1683
worker 0  xfile  [0, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12319328277512098}, 'layer_2_size': 40, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24037986348181428}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.21673060087786053, 'rmse': 0.21673060087786053, 'mae': 0.16826029025427527, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  52 | activation: tanh    | extras: batchnorm 
layer 2 | size:  93 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca41c5f60>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 2:09 - loss: 0.6807
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0363
 368/6530 [>.............................] - ETA: 6s - loss: 0.3941  
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0348
 704/6530 [==>...........................] - ETA: 3s - loss: 0.3259
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0349
1072/6530 [===>..........................] - ETA: 2s - loss: 0.2860
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0354
1408/6530 [=====>........................] - ETA: 1s - loss: 0.2669
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0349
1776/6530 [=======>......................] - ETA: 1s - loss: 0.2505
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0339
2160/6530 [========>.....................] - ETA: 1s - loss: 0.2440
3056/6530 [=============>................] - ETA: 0s - loss: 0.0336
2544/6530 [==========>...................] - ETA: 1s - loss: 0.2412
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0327
2928/6530 [============>.................] - ETA: 0s - loss: 0.2360
3808/6530 [================>.............] - ETA: 0s - loss: 0.0323
3312/6530 [==============>...............] - ETA: 0s - loss: 0.2311
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0319
3696/6530 [===============>..............] - ETA: 0s - loss: 0.2264
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0314
4080/6530 [=================>............] - ETA: 0s - loss: 0.2246
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0313
4448/6530 [===================>..........] - ETA: 0s - loss: 0.2219
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0309
4784/6530 [====================>.........] - ETA: 0s - loss: 0.2195
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0306
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2178
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0304
5472/6530 [========================>.....] - ETA: 0s - loss: 0.2151
# training | RMSE: 0.1342, MAE: 0.1057
worker 2  xfile  [2, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39690890797011347}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.1342301816013211, 'rmse': 0.1342301816013211, 'mae': 0.1056531888513479, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  77 | activation: tanh    | extras: batchnorm 
layer 2 | size:  23 | activation: relu    | extras: None 
layer 3 | size:   7 | activation: relu    | extras: None 
layer 4 | size:  47 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca41c5fd0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 16s - loss: 0.7270
6432/6530 [============================>.] - ETA: 0s - loss: 0.0302
5840/6530 [=========================>....] - ETA: 0s - loss: 0.2132
2688/6530 [===========>..................] - ETA: 0s - loss: 0.4626 
6530/6530 [==============================] - 1s 142us/step - loss: 0.0302 - val_loss: 0.0206

6176/6530 [===========================>..] - ETA: 0s - loss: 0.2114Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0307
5248/6530 [=======================>......] - ETA: 0s - loss: 0.3287
 368/6530 [>.............................] - ETA: 0s - loss: 0.0265
6530/6530 [==============================] - 0s 76us/step - loss: 0.2976 - val_loss: 0.2335
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1799
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0263
6530/6530 [==============================] - 1s 201us/step - loss: 0.2101 - val_loss: 0.1512
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1228
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1614
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0255
 368/6530 [>.............................] - ETA: 0s - loss: 0.1769
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1590
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0260
6530/6530 [==============================] - 0s 22us/step - loss: 0.1570 - val_loss: 0.1472
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1906
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1767
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0256
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1469
1072/6530 [===>..........................] - ETA: 0s - loss: 0.1755
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0257
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1427
1424/6530 [=====>........................] - ETA: 0s - loss: 0.1722
6530/6530 [==============================] - 0s 21us/step - loss: 0.1432 - val_loss: 0.2556
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1326
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0260
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1690
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1330
2960/6530 [============>.................] - ETA: 0s - loss: 0.0257
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1676
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1321
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0255
6530/6530 [==============================] - 0s 20us/step - loss: 0.1314 - val_loss: 0.3679

2496/6530 [==========>...................] - ETA: 0s - loss: 0.1694Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1817
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0254
2864/6530 [============>.................] - ETA: 0s - loss: 0.1691
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1240
4032/6530 [=================>............] - ETA: 0s - loss: 0.0254
3232/6530 [=============>................] - ETA: 0s - loss: 0.1684
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1241
6530/6530 [==============================] - 0s 20us/step - loss: 0.1238 - val_loss: 0.1649
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1398
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0256
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1694
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1172
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0254
3968/6530 [=================>............] - ETA: 0s - loss: 0.1694
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1164
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0254
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1688
6530/6530 [==============================] - 0s 20us/step - loss: 0.1159 - val_loss: 0.2550
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1287
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0252
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1688
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1124
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0254
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1684
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1124
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0254
6530/6530 [==============================] - 0s 21us/step - loss: 0.1109 - val_loss: 0.1804

5360/6530 [=======================>......] - ETA: 0s - loss: 0.1675
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1660
6530/6530 [==============================] - 1s 146us/step - loss: 0.0253 - val_loss: 0.0340
Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0173
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1657
 416/6530 [>.............................] - ETA: 0s - loss: 0.0280
6432/6530 [============================>.] - ETA: 0s - loss: 0.1651
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0242
6530/6530 [==============================] - 1s 149us/step - loss: 0.1651 - val_loss: 0.1372
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1096
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0244
 384/6530 [>.............................] - ETA: 0s - loss: 0.1392
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0237
 752/6530 [==>...........................] - ETA: 0s - loss: 0.1479
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0243
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1486
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0237
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1488
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0240
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1466
3088/6530 [=============>................] - ETA: 0s - loss: 0.0236
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1464
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0235
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1473
3856/6530 [================>.............] - ETA: 0s - loss: 0.0234
2976/6530 [============>.................] - ETA: 0s - loss: 0.1471
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0232
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1459
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0233
3712/6530 [================>.............] - ETA: 0s - loss: 0.1461
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0234
4080/6530 [=================>............] - ETA: 0s - loss: 0.1458
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0233
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1456
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0234
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1456
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0235
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1446
6480/6530 [============================>.] - ETA: 0s - loss: 0.0234
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1436
6530/6530 [==============================] - 1s 139us/step - loss: 0.0234 - val_loss: 0.0139
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0144
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1435
 400/6530 [>.............................] - ETA: 0s - loss: 0.0205
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1433
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0228
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0224
6530/6530 [==============================] - 1s 144us/step - loss: 0.1433 - val_loss: 0.1197
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1421
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0231
 384/6530 [>.............................] - ETA: 0s - loss: 0.1331
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0236
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1389
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0231
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1384
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0234
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1389
2976/6530 [============>.................] - ETA: 0s - loss: 0.0229
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1362
# training | RMSE: 0.2154, MAE: 0.1729
worker 2  xfile  [4, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.21542258642653675, 'rmse': 0.21542258642653675, 'mae': 0.17285472159747048, 'early_stop': True}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  13 | activation: tanh    | extras: batchnorm 
layer 2 | size:  13 | activation: sigmoid | extras: dropout - rate: 41.1% 
layer 3 | size:  17 | activation: relu    | extras: batchnorm 
layer 4 | size:  89 | activation: tanh    | extras: dropout - rate: 32.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca33e4eb8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 2:15 - loss: 1.8221
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0232
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1360
 448/6530 [=>............................] - ETA: 9s - loss: 1.2996  
3712/6530 [================>.............] - ETA: 0s - loss: 0.0235
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1365
 864/6530 [==>...........................] - ETA: 5s - loss: 0.9704
4064/6530 [=================>............] - ETA: 0s - loss: 0.0233
3008/6530 [============>.................] - ETA: 0s - loss: 0.1357
1312/6530 [=====>........................] - ETA: 3s - loss: 0.7554
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0230
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1344
1760/6530 [=======>......................] - ETA: 2s - loss: 0.6309
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0230
3728/6530 [================>.............] - ETA: 0s - loss: 0.1342
2176/6530 [========>.....................] - ETA: 1s - loss: 0.5529
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0230
4080/6530 [=================>............] - ETA: 0s - loss: 0.1338
2592/6530 [==========>...................] - ETA: 1s - loss: 0.4918
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0228
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1331
3008/6530 [============>.................] - ETA: 1s - loss: 0.4460
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0225
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1334
3424/6530 [==============>...............] - ETA: 0s - loss: 0.4111
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0227
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1322
3840/6530 [================>.............] - ETA: 0s - loss: 0.3818
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1313
6530/6530 [==============================] - 1s 145us/step - loss: 0.0227 - val_loss: 0.0239
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0171
4256/6530 [==================>...........] - ETA: 0s - loss: 0.3570
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1310
 368/6530 [>.............................] - ETA: 0s - loss: 0.0203
4704/6530 [====================>.........] - ETA: 0s - loss: 0.3359
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1309
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0207
5152/6530 [======================>.......] - ETA: 0s - loss: 0.3165
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0217
6530/6530 [==============================] - 1s 145us/step - loss: 0.1309 - val_loss: 0.1101
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1402
5600/6530 [========================>.....] - ETA: 0s - loss: 0.2994
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0218
 384/6530 [>.............................] - ETA: 0s - loss: 0.1183
6016/6530 [==========================>...] - ETA: 0s - loss: 0.2869
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0211
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1270
6432/6530 [============================>.] - ETA: 0s - loss: 0.2752
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0215
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1289
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0216
6530/6530 [==============================] - 2s 233us/step - loss: 0.2722 - val_loss: 0.0661
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0882
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1275
2928/6530 [============>.................] - ETA: 0s - loss: 0.0217
 416/6530 [>.............................] - ETA: 0s - loss: 0.1071
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1264
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0215
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1068
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1239
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0216
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1067
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1244
4048/6530 [=================>............] - ETA: 0s - loss: 0.0217
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1016
2832/6530 [============>.................] - ETA: 0s - loss: 0.1238
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0214
3184/6530 [=============>................] - ETA: 0s - loss: 0.1221
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1026
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0215
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1231
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1035
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0213
3872/6530 [================>.............] - ETA: 0s - loss: 0.1230
3008/6530 [============>.................] - ETA: 0s - loss: 0.1008
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0214
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1227
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1015
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0215
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1235
3840/6530 [================>.............] - ETA: 0s - loss: 0.1004
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0215
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1235
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0987
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1229
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0977
6530/6530 [==============================] - 1s 146us/step - loss: 0.0215 - val_loss: 0.0247
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0322
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1224
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0969
 368/6530 [>.............................] - ETA: 0s - loss: 0.0185
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1225
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0957
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0210
6368/6530 [============================>.] - ETA: 0s - loss: 0.1225
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0957
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0203
6530/6530 [==============================] - 1s 150us/step - loss: 0.1230 - val_loss: 0.1092

6368/6530 [============================>.] - ETA: 0s - loss: 0.0950Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1325
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0200
6530/6530 [==============================] - 1s 127us/step - loss: 0.0946 - val_loss: 0.0661
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0690
 368/6530 [>.............................] - ETA: 0s - loss: 0.1152
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0203
 480/6530 [=>............................] - ETA: 0s - loss: 0.0818
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1211
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0203
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0831
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1192
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0204
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0791
1424/6530 [=====>........................] - ETA: 0s - loss: 0.1204
2960/6530 [============>.................] - ETA: 0s - loss: 0.0202
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1198
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0777
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0201
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1169
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0779
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0200
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1179
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0788
4064/6530 [=================>............] - ETA: 0s - loss: 0.0206
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1182
3072/6530 [=============>................] - ETA: 0s - loss: 0.0777
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0202
3184/6530 [=============>................] - ETA: 0s - loss: 0.1175
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0203
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0774
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1178
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0203
3968/6530 [=================>............] - ETA: 0s - loss: 0.0772
3904/6530 [================>.............] - ETA: 0s - loss: 0.1175
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0205
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0769
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1170
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0205
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0762
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1173
6320/6530 [============================>.] - ETA: 0s - loss: 0.0206
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0759
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1167
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0760
6530/6530 [==============================] - 1s 143us/step - loss: 0.0206 - val_loss: 0.0136
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0216
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1165
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0755
 384/6530 [>.............................] - ETA: 0s - loss: 0.0215
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1161
6528/6530 [============================>.] - ETA: 0s - loss: 0.0747
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0212
6530/6530 [==============================] - 1s 124us/step - loss: 0.0746 - val_loss: 0.0583
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0759
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1161
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0200
 448/6530 [=>............................] - ETA: 0s - loss: 0.0666
6416/6530 [============================>.] - ETA: 0s - loss: 0.1162
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0199
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0710
6530/6530 [==============================] - 1s 150us/step - loss: 0.1166 - val_loss: 0.0976
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1181
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0203
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0680
 368/6530 [>.............................] - ETA: 0s - loss: 0.1086
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0203
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0658
 720/6530 [==>...........................] - ETA: 0s - loss: 0.1169
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0200
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0669
1072/6530 [===>..........................] - ETA: 0s - loss: 0.1177
2944/6530 [============>.................] - ETA: 0s - loss: 0.0208
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0658
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1175
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0209
2944/6530 [============>.................] - ETA: 0s - loss: 0.0656
1808/6530 [=======>......................] - ETA: 0s - loss: 0.1173
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0210
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0651
2160/6530 [========>.....................] - ETA: 0s - loss: 0.1153
4064/6530 [=================>............] - ETA: 0s - loss: 0.0212
3808/6530 [================>.............] - ETA: 0s - loss: 0.0645
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1148
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0211
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0641
2864/6530 [============>.................] - ETA: 0s - loss: 0.1148
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0210
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0634
3200/6530 [=============>................] - ETA: 0s - loss: 0.1140
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0212
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0630
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1141
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0211
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0624
3920/6530 [=================>............] - ETA: 0s - loss: 0.1138
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0211
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0627
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1136
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0210
6400/6530 [============================>.] - ETA: 0s - loss: 0.0624
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1138
6530/6530 [==============================] - 1s 126us/step - loss: 0.0620 - val_loss: 0.0580

6530/6530 [==============================] - 1s 146us/step - loss: 0.0209 - val_loss: 0.0137
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0658Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0299
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1136
 448/6530 [=>............................] - ETA: 0s - loss: 0.0645
 384/6530 [>.............................] - ETA: 0s - loss: 0.0217
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1136
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0660
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0224
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1130
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0645
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0224
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1133
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0626
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0224
6384/6530 [============================>.] - ETA: 0s - loss: 0.1131
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0626
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0225
6530/6530 [==============================] - 1s 150us/step - loss: 0.1137 - val_loss: 0.1017
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1182
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0625
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0219
 336/6530 [>.............................] - ETA: 0s - loss: 0.1041
2880/6530 [============>.................] - ETA: 0s - loss: 0.0617
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0219
 688/6530 [==>...........................] - ETA: 0s - loss: 0.1135
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0604
2912/6530 [============>.................] - ETA: 0s - loss: 0.0214
1040/6530 [===>..........................] - ETA: 0s - loss: 0.1134
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0212
3776/6530 [================>.............] - ETA: 0s - loss: 0.0601
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1138
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0211
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0603
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1142
3904/6530 [================>.............] - ETA: 0s - loss: 0.0209
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0602
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1118
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0210
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0598
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1122
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0209
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0597
2832/6530 [============>.................] - ETA: 0s - loss: 0.1119
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0208
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0593
3200/6530 [=============>................] - ETA: 0s - loss: 0.1108
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0208
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0589
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1113
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0206
6530/6530 [==============================] - 1s 128us/step - loss: 0.0583 - val_loss: 0.0505
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0753
3920/6530 [=================>............] - ETA: 0s - loss: 0.1110
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0205
 448/6530 [=>............................] - ETA: 0s - loss: 0.0551
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1108
6480/6530 [============================>.] - ETA: 0s - loss: 0.0205
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0575
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1111
6530/6530 [==============================] - 1s 147us/step - loss: 0.0205 - val_loss: 0.0139

1312/6530 [=====>........................] - ETA: 0s - loss: 0.0561
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1113
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0554
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1115
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1108
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0551
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0551
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1110
6416/6530 [============================>.] - ETA: 0s - loss: 0.1110
2976/6530 [============>.................] - ETA: 0s - loss: 0.0544
6530/6530 [==============================] - 1s 148us/step - loss: 0.1114 - val_loss: 0.0946
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1124
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0543
 400/6530 [>.............................] - ETA: 0s - loss: 0.1058
3872/6530 [================>.............] - ETA: 0s - loss: 0.0542
 784/6530 [==>...........................] - ETA: 0s - loss: 0.1100
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0544
1168/6530 [====>.........................] - ETA: 0s - loss: 0.1116
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0543
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1118
# training | RMSE: 0.1103, MAE: 0.0864
worker 1  xfile  [1, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4372703468506898}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.11031652359008501, 'rmse': 0.11031652359008501, 'mae': 0.08642147272716279, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  72 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca04a0ac8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 11s - loss: 1.2322
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0542
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1100
2816/6530 [===========>..................] - ETA: 0s - loss: 0.4299 
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0543
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1086
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2667
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0542
6530/6530 [==============================] - 0s 39us/step - loss: 0.2390 - val_loss: 0.0654
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0531
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1087
6496/6530 [============================>.] - ETA: 0s - loss: 0.0540
2880/6530 [============>.................] - ETA: 0s - loss: 0.0580
6530/6530 [==============================] - 1s 124us/step - loss: 0.0540 - val_loss: 0.0478
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0581
2896/6530 [============>.................] - ETA: 0s - loss: 0.1096
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0561
 480/6530 [=>............................] - ETA: 0s - loss: 0.0564
3248/6530 [=============>................] - ETA: 0s - loss: 0.1083
6530/6530 [==============================] - 0s 19us/step - loss: 0.0554 - val_loss: 0.0521
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0435
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0545
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1092
2880/6530 [============>.................] - ETA: 0s - loss: 0.0455
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0522
3952/6530 [=================>............] - ETA: 0s - loss: 0.1090
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0443
6530/6530 [==============================] - 0s 19us/step - loss: 0.0439 - val_loss: 0.0426
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0363
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0507
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1088
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0377
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0512
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1089
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0363
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0516
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1085
6530/6530 [==============================] - 0s 19us/step - loss: 0.0361 - val_loss: 0.0358
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0309
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1086
3008/6530 [============>.................] - ETA: 0s - loss: 0.0507
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0315
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1080
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0507
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0309
6530/6530 [==============================] - 0s 20us/step - loss: 0.0306 - val_loss: 0.0306
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0267
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1082
3872/6530 [================>.............] - ETA: 0s - loss: 0.0504
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0274
6384/6530 [============================>.] - ETA: 0s - loss: 0.1080
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0503
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0269
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0506
6530/6530 [==============================] - 1s 150us/step - loss: 0.1085 - val_loss: 0.1006

6530/6530 [==============================] - 0s 19us/step - loss: 0.0268 - val_loss: 0.0269
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0237
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0506
2944/6530 [============>.................] - ETA: 0s - loss: 0.0245
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0503
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0242
6530/6530 [==============================] - 0s 19us/step - loss: 0.0242 - val_loss: 0.0243
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0217
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0506
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0227
6368/6530 [============================>.] - ETA: 0s - loss: 0.0504
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0223
6530/6530 [==============================] - 1s 126us/step - loss: 0.0503 - val_loss: 0.0451
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0691
6530/6530 [==============================] - 0s 19us/step - loss: 0.0224 - val_loss: 0.0225
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0203
 480/6530 [=>............................] - ETA: 0s - loss: 0.0516
2880/6530 [============>.................] - ETA: 0s - loss: 0.0213
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0529
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0211
6530/6530 [==============================] - 0s 19us/step - loss: 0.0211 - val_loss: 0.0212

1344/6530 [=====>........................] - ETA: 0s - loss: 0.0513
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0500
# training | RMSE: 0.1210, MAE: 0.0931
worker 0  xfile  [3, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23163308861005694}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.12102923318746805, 'rmse': 0.12102923318746805, 'mae': 0.09309925629655103, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  50 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca04d4320>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 26s - loss: 0.6155
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0502
1472/6530 [=====>........................] - ETA: 0s - loss: 0.2527 
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0508
2912/6530 [============>.................] - ETA: 0s - loss: 0.2118
2976/6530 [============>.................] - ETA: 0s - loss: 0.0501
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1951
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0497
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1860
3744/6530 [================>.............] - ETA: 0s - loss: 0.0497
4128/6530 [=================>............] - ETA: 0s - loss: 0.0496
6530/6530 [==============================] - 0s 61us/step - loss: 0.1832 - val_loss: 0.1606
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1305
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0492
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1626
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0492
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1583
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0492
4128/6530 [=================>............] - ETA: 0s - loss: 0.1584
# training | RMSE: 0.1430, MAE: 0.1107
worker 1  xfile  [6, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 96, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25326331996434737}, 'layer_3_size': 9, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36533817719676887}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.14299154033401168, 'rmse': 0.14299154033401168, 'mae': 0.110746662193406, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  69 | activation: sigmoid | extras: dropout - rate: 47.0% 
layer 2 | size:  92 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc7aec8f28>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 21s - loss: 0.5426
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1600
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0490
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1136 
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0489
6530/6530 [==============================] - 0s 39us/step - loss: 0.1601 - val_loss: 0.1616

3840/6530 [================>.............] - ETA: 0s - loss: 0.0948Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.2044
6530/6530 [==============================] - 1s 127us/step - loss: 0.0485 - val_loss: 0.0447

5696/6530 [=========================>....] - ETA: 0s - loss: 0.0875
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1623Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0577
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1617
6530/6530 [==============================] - 0s 64us/step - loss: 0.0861 - val_loss: 0.0653
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0458
 480/6530 [=>............................] - ETA: 0s - loss: 0.0547
4000/6530 [=================>............] - ETA: 0s - loss: 0.1603
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0717
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0520
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1602
3776/6530 [================>.............] - ETA: 0s - loss: 0.0701
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0486
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0703
6530/6530 [==============================] - 0s 40us/step - loss: 0.1601 - val_loss: 0.1610
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1563
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0481
6530/6530 [==============================] - 0s 28us/step - loss: 0.0701 - val_loss: 0.0938
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1031
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1589
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0477
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0676
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1588
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0485
3712/6530 [================>.............] - ETA: 0s - loss: 0.0657
4096/6530 [=================>............] - ETA: 0s - loss: 0.1597
3104/6530 [=============>................] - ETA: 0s - loss: 0.0475
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0647
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1597
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0479
6530/6530 [==============================] - 0s 29us/step - loss: 0.0651 - val_loss: 0.0560
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0564
6530/6530 [==============================] - 0s 39us/step - loss: 0.1602 - val_loss: 0.1609
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1667
4000/6530 [=================>............] - ETA: 0s - loss: 0.0477
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0626
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1624
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0471
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0622
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1596
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0473
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0612
3936/6530 [=================>............] - ETA: 0s - loss: 0.1586
6530/6530 [==============================] - 0s 30us/step - loss: 0.0613 - val_loss: 0.1096
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0759
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0472
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1590
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0580
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0469
3712/6530 [================>.............] - ETA: 0s - loss: 0.0575
6530/6530 [==============================] - 0s 40us/step - loss: 0.1596 - val_loss: 0.1597

6048/6530 [==========================>...] - ETA: 0s - loss: 0.0469Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1842
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0575
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1662
6496/6530 [============================>.] - ETA: 0s - loss: 0.0466
6530/6530 [==============================] - 0s 28us/step - loss: 0.0571 - val_loss: 0.0513
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0670
6530/6530 [==============================] - 1s 124us/step - loss: 0.0465 - val_loss: 0.0490

2816/6530 [===========>..................] - ETA: 0s - loss: 0.1632
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0605
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1623
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0580
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1609
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0570
6530/6530 [==============================] - 0s 39us/step - loss: 0.1597 - val_loss: 0.1618
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1497
6530/6530 [==============================] - 0s 30us/step - loss: 0.0563 - val_loss: 0.0552
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0550
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1580
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0541
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1584
3904/6530 [================>.............] - ETA: 0s - loss: 0.0539
4000/6530 [=================>............] - ETA: 0s - loss: 0.1595
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0531
6530/6530 [==============================] - 0s 28us/step - loss: 0.0529 - val_loss: 0.0500
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0678
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1594
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0519
6530/6530 [==============================] - 0s 39us/step - loss: 0.1598 - val_loss: 0.1600
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1531
3904/6530 [================>.............] - ETA: 0s - loss: 0.0536
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1531
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0531
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1594
6530/6530 [==============================] - 0s 27us/step - loss: 0.0533 - val_loss: 0.0886
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0561
# training | RMSE: 0.2122, MAE: 0.1662
worker 2  xfile  [5, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41064930270372024}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32093918765125695}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.21221653588419181, 'rmse': 0.21221653588419181, 'mae': 0.16615364957745868, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  50 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcc741a6f98>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 4s - loss: 0.1962
4064/6530 [=================>............] - ETA: 0s - loss: 0.1611
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0521
6530/6530 [==============================] - 0s 40us/step - loss: 0.1452 - val_loss: 0.1119
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1168
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1601
3776/6530 [================>.............] - ETA: 0s - loss: 0.0517
6530/6530 [==============================] - 0s 6us/step - loss: 0.0904 - val_loss: 0.0740
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0642
6530/6530 [==============================] - 0s 40us/step - loss: 0.1594 - val_loss: 0.1621
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1981
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0516
6530/6530 [==============================] - 0s 6us/step - loss: 0.0634 - val_loss: 0.0556
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0548
6530/6530 [==============================] - 0s 29us/step - loss: 0.0514 - val_loss: 0.0658

6530/6530 [==============================] - 0s 6us/step - loss: 0.0501 - val_loss: 0.0466
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0517
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1599
6530/6530 [==============================] - 0s 6us/step - loss: 0.0440 - val_loss: 0.0430
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0426
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1621
6530/6530 [==============================] - 0s 6us/step - loss: 0.0415 - val_loss: 0.0412
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0399
4064/6530 [=================>............] - ETA: 0s - loss: 0.1597
6530/6530 [==============================] - 0s 6us/step - loss: 0.0404 - val_loss: 0.0405
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0463
6530/6530 [==============================] - 0s 6us/step - loss: 0.0400 - val_loss: 0.0405
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0361
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1585
6530/6530 [==============================] - 0s 6us/step - loss: 0.0398 - val_loss: 0.0402

6530/6530 [==============================] - 0s 38us/step - loss: 0.1590 - val_loss: 0.1608

# training | RMSE: 0.1991, MAE: 0.1597
worker 2  xfile  [9, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 100, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.1990555047972821, 'rmse': 0.1990555047972821, 'mae': 0.1596952889811967, 'early_stop': False}
vggnet done  2

# training | RMSE: 0.2049, MAE: 0.1588
worker 0  xfile  [7, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 63, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4799995916392109}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.20494544570691775, 'rmse': 0.20494544570691775, 'mae': 0.15877501372521877, 'early_stop': False}
vggnet done  0

# training | RMSE: 0.2558, MAE: 0.2106
worker 1  xfile  [8, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.470436768334743}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3326184702177891}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.25584288712309583, 'rmse': 0.25584288712309583, 'mae': 0.21064268319676593, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  24 | activation: relu    | extras: dropout - rate: 24.8% 
layer 2 | size:  98 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  42 | activation: tanh    | extras: dropout - rate: 34.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca04a0ba8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 19s - loss: 0.7269
1536/6530 [======>.......................] - ETA: 1s - loss: 0.6801 
2816/6530 [===========>..................] - ETA: 0s - loss: 0.5298
4224/6530 [==================>...........] - ETA: 0s - loss: 0.4085
5760/6530 [=========================>....] - ETA: 0s - loss: 0.3408
6530/6530 [==============================] - 1s 112us/step - loss: 0.3189 - val_loss: 0.1549
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1857
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1574
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1543
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1518
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1509
6530/6530 [==============================] - 0s 41us/step - loss: 0.1488 - val_loss: 0.1501
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1455
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1471
3200/6530 [=============>................] - ETA: 0s - loss: 0.1487
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1465
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1463
6530/6530 [==============================] - 0s 45us/step - loss: 0.1458 - val_loss: 0.1392
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1488
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1432
2944/6530 [============>.................] - ETA: 0s - loss: 0.1449
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1431
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1423
6530/6530 [==============================] - 0s 43us/step - loss: 0.1418 - val_loss: 0.1352
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1456
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1409
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1393
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1393
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1380
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1382
6530/6530 [==============================] - 0s 48us/step - loss: 0.1375 - val_loss: 0.1330
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1481
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1331
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1352
4096/6530 [=================>............] - ETA: 0s - loss: 0.1347
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1334
6530/6530 [==============================] - 0s 37us/step - loss: 0.1332 - val_loss: 0.1323
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1337
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1348
3072/6530 [=============>................] - ETA: 0s - loss: 0.1374
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1364
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1349
6530/6530 [==============================] - 0s 39us/step - loss: 0.1346 - val_loss: 0.1248
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1345
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1338
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1365
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1340
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1339
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1344
6530/6530 [==============================] - 0s 49us/step - loss: 0.1336 - val_loss: 0.1907
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.2157
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1379
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1368
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1335
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1343
6528/6530 [============================>.] - ETA: 0s - loss: 0.1335
6530/6530 [==============================] - 0s 43us/step - loss: 0.1335 - val_loss: 0.1231

# training | RMSE: 0.1629, MAE: 0.1164
worker 1  xfile  [10, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24846895451893564}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3480397037957643}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3528943621643851}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.16757928744849615}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.16288664424226157, 'rmse': 0.16288664424226157, 'mae': 0.11637489531343574, 'early_stop': False}
vggnet done  1
all of workers have been done
calculation finished
#0 epoch=9.0 loss={'loss': 0.21673060087786053, 'rmse': 0.21673060087786053, 'mae': 0.16826029025427527, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.12319328277512098}, 'layer_2_size': 40, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 79, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.24037986348181428}, 'layer_5_size': 45, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#2 epoch=9.0 loss={'loss': 0.1342301816013211, 'rmse': 0.1342301816013211, 'mae': 0.1056531888513479, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39690890797011347}, 'layer_4_size': 13, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#4 epoch=9.0 loss={'loss': 0.21542258642653675, 'rmse': 0.21542258642653675, 'mae': 0.17285472159747048, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#1 epoch=9.0 loss={'loss': 0.11031652359008501, 'rmse': 0.11031652359008501, 'mae': 0.08642147272716279, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4372703468506898}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#3 epoch=9.0 loss={'loss': 0.12102923318746805, 'rmse': 0.12102923318746805, 'mae': 0.09309925629655103, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23163308861005694}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#6 epoch=9.0 loss={'loss': 0.14299154033401168, 'rmse': 0.14299154033401168, 'mae': 0.110746662193406, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 72, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 96, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25326331996434737}, 'layer_3_size': 9, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.36533817719676887}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#5 epoch=9.0 loss={'loss': 0.21221653588419181, 'rmse': 0.21221653588419181, 'mae': 0.16615364957745868, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.41064930270372024}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32093918765125695}, 'layer_4_size': 89, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#8 epoch=9.0 loss={'loss': 0.25584288712309583, 'rmse': 0.25584288712309583, 'mae': 0.21064268319676593, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.470436768334743}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 92, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3326184702177891}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#9 epoch=9.0 loss={'loss': 0.1990555047972821, 'rmse': 0.1990555047972821, 'mae': 0.1596952889811967, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 100, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 62, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 36, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#7 epoch=9.0 loss={'loss': 0.20494544570691775, 'rmse': 0.20494544570691775, 'mae': 0.15877501372521877, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 63, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 70, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4799995916392109}, 'layer_5_size': 83, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#10 epoch=9.0 loss={'loss': 0.16288664424226157, 'rmse': 0.16288664424226157, 'mae': 0.11637489531343574, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.24846895451893564}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 98, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3480397037957643}, 'layer_3_size': 42, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3528943621643851}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.16757928744849615}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
get a list [results] of length 166
get a list [loss] of length 11
get a list [val_loss] of length 11
length of indices is [ 3  4  1  5 10  8  9  6  2  0  7]
length of indices is 11
length of T is 11
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23163308861005694}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [1, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [2, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4372703468506898}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]] 

*** 3.7777777777777777 configurations x 27.0 iterations each

11 | Thu Sep 27 22:16:00 2018 | lowest loss so far: 0.0970 (run 2)

{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  81 | activation: tanh    | extras: batchnorm 
layer 2 | size:  31 | activation: relu    | extras: None 
layer 3 | size:  76 | activation: relu    | extras: dropout - rate: 43.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7fcd19460208>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 6:19 - loss: 0.3507
 272/6530 [>.............................] - ETA: 22s - loss: 0.1679 
 544/6530 [=>............................] - ETA: 11s - loss: 0.1307
 816/6530 [==>...........................] - ETA: 7s - loss: 0.1155 {'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  77 | activation: tanh    | extras: batchnorm 
layer 2 | size:  23 | activation: relu    | extras: None 
layer 3 | size:   7 | activation: relu    | extras: None 
layer 4 | size:  47 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca41c25f8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 128/6530 [..............................] - ETA: 49s - loss: 0.7183
1152/6530 [====>.........................] - ETA: 5s - loss: 0.1005
2304/6530 [=========>....................] - ETA: 1s - loss: 0.4766 
1472/6530 [=====>........................] - ETA: 4s - loss: 0.0937
4608/6530 [====================>.........] - ETA: 0s - loss: 0.3380{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  52 | activation: tanh    | extras: batchnorm 
layer 2 | size:  93 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7fcca41c2588>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 7:23 - loss: 0.8512
1808/6530 [=======>......................] - ETA: 3s - loss: 0.0878
 336/6530 [>.............................] - ETA: 20s - loss: 0.5762 
6530/6530 [==============================] - 1s 179us/step - loss: 0.2939 - val_loss: 0.2532
Epoch 2/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1985
2176/6530 [========>.....................] - ETA: 2s - loss: 0.0835
 688/6530 [==>...........................] - ETA: 10s - loss: 0.4318
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1708
2544/6530 [==========>...................] - ETA: 2s - loss: 0.0791
1024/6530 [===>..........................] - ETA: 6s - loss: 0.3710 
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1677
2896/6530 [============>.................] - ETA: 1s - loss: 0.0760
1376/6530 [=====>........................] - ETA: 4s - loss: 0.3309
6530/6530 [==============================] - 0s 21us/step - loss: 0.1645 - val_loss: 0.2366
Epoch 3/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1673
3248/6530 [=============>................] - ETA: 1s - loss: 0.0726
1744/6530 [=======>......................] - ETA: 3s - loss: 0.3063
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1535
3616/6530 [===============>..............] - ETA: 1s - loss: 0.0697
2096/6530 [========>.....................] - ETA: 2s - loss: 0.2891
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1488
3968/6530 [=================>............] - ETA: 0s - loss: 0.0678
6530/6530 [==============================] - 0s 20us/step - loss: 0.1479 - val_loss: 0.2283
Epoch 4/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1643
2448/6530 [==========>...................] - ETA: 2s - loss: 0.2829
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0654
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1400
2800/6530 [===========>..................] - ETA: 1s - loss: 0.2762
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0636
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1371
3136/6530 [=============>................] - ETA: 1s - loss: 0.2681
6530/6530 [==============================] - 0s 21us/step - loss: 0.1370 - val_loss: 0.1378
Epoch 5/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1521
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0620
3488/6530 [===============>..............] - ETA: 1s - loss: 0.2621
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1317
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0604
3840/6530 [================>.............] - ETA: 1s - loss: 0.2565
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1301
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0592
4176/6530 [==================>...........] - ETA: 0s - loss: 0.2521
6530/6530 [==============================] - 0s 21us/step - loss: 0.1295 - val_loss: 0.2531
Epoch 6/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1399
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0580
4512/6530 [===================>..........] - ETA: 0s - loss: 0.2484
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1267
6480/6530 [============================>.] - ETA: 0s - loss: 0.0565
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2444
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1228
6530/6530 [==============================] - 0s 21us/step - loss: 0.1231 - val_loss: 0.1569
Epoch 7/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1287
5216/6530 [======================>.......] - ETA: 0s - loss: 0.2411
6530/6530 [==============================] - 2s 302us/step - loss: 0.0564 - val_loss: 0.0615
Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0237
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1166
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2374
 384/6530 [>.............................] - ETA: 0s - loss: 0.0351
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1180
5904/6530 [==========================>...] - ETA: 0s - loss: 0.2350
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0368
6530/6530 [==============================] - 0s 20us/step - loss: 0.1171 - val_loss: 0.2346
Epoch 8/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1012
6256/6530 [===========================>..] - ETA: 0s - loss: 0.2316
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0348
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1112
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0347
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1120
6530/6530 [==============================] - 2s 323us/step - loss: 0.2304 - val_loss: 0.1844

6530/6530 [==============================] - 0s 20us/step - loss: 0.1125 - val_loss: 0.1776
Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.2001Epoch 9/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1238
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0350
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1124
 384/6530 [>.............................] - ETA: 0s - loss: 0.1873
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0348
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1108
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1776
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0339
6530/6530 [==============================] - 0s 20us/step - loss: 0.1109 - val_loss: 0.3023

1104/6530 [====>.........................] - ETA: 0s - loss: 0.1757
2944/6530 [============>.................] - ETA: 0s - loss: 0.0340
# training | RMSE: 0.3288, MAE: 0.2953
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.3287804222816858, 'rmse': 0.3287804222816858, 'mae': 0.2952532798644083, 'early_stop': True}
vggnet done  1

1472/6530 [=====>........................] - ETA: 0s - loss: 0.1743
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0330
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1742
3712/6530 [================>.............] - ETA: 0s - loss: 0.0323
2224/6530 [=========>....................] - ETA: 0s - loss: 0.1722
4112/6530 [=================>............] - ETA: 0s - loss: 0.0320
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1734
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0317
2896/6530 [============>.................] - ETA: 0s - loss: 0.1731
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0313
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1713
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0312
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1712
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0308
4032/6530 [=================>............] - ETA: 0s - loss: 0.1707
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0305
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1693
6368/6530 [============================>.] - ETA: 0s - loss: 0.0302
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1694
6530/6530 [==============================] - 1s 141us/step - loss: 0.0302 - val_loss: 0.0206
Epoch 3/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0307
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1691
 416/6530 [>.............................] - ETA: 0s - loss: 0.0258
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1674
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0262
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1671
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0250
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1657
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0261
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0257
6530/6530 [==============================] - 1s 143us/step - loss: 0.1657 - val_loss: 0.1456
Epoch 3/27

  16/6530 [..............................] - ETA: 0s - loss: 0.1445
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0257
 384/6530 [>.............................] - ETA: 0s - loss: 0.1523
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0259
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1526
3104/6530 [=============>................] - ETA: 0s - loss: 0.0256
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1498
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0253
1520/6530 [=====>........................] - ETA: 0s - loss: 0.1469
3872/6530 [================>.............] - ETA: 0s - loss: 0.0256
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1453
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0257
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1451
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0255
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1452
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0252
2880/6530 [============>.................] - ETA: 0s - loss: 0.1448
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0253
3264/6530 [=============>................] - ETA: 0s - loss: 0.1446
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0255
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1444
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0253
4016/6530 [=================>............] - ETA: 0s - loss: 0.1444
6464/6530 [============================>.] - ETA: 0s - loss: 0.0253
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1438
6530/6530 [==============================] - 1s 138us/step - loss: 0.0253 - val_loss: 0.0340
Epoch 4/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0173
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1444
 400/6530 [>.............................] - ETA: 0s - loss: 0.0284
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1446
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0242
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1432
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0242
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1434
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0237
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1429
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0245
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0236
6530/6530 [==============================] - 1s 145us/step - loss: 0.1431 - val_loss: 0.1226
Epoch 4/27

  16/6530 [..............................] - ETA: 0s - loss: 0.1034
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0240
 400/6530 [>.............................] - ETA: 0s - loss: 0.1306
3056/6530 [=============>................] - ETA: 0s - loss: 0.0237
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1347
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0235
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1342
3856/6530 [================>.............] - ETA: 0s - loss: 0.0234
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1311
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0232
1904/6530 [=======>......................] - ETA: 0s - loss: 0.1302
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0233
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1296
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0234
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1306
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0233
3040/6530 [============>.................] - ETA: 0s - loss: 0.1302
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0234
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1304
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0234
3808/6530 [================>.............] - ETA: 0s - loss: 0.1302
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1307
6530/6530 [==============================] - 1s 137us/step - loss: 0.0234 - val_loss: 0.0139
Epoch 5/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0144
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1305
 416/6530 [>.............................] - ETA: 0s - loss: 0.0201
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1301
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0230
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1298
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0234
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1294
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0233
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1294
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0233
6464/6530 [============================>.] - ETA: 0s - loss: 0.1290
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0231
6530/6530 [==============================] - 1s 139us/step - loss: 0.1294 - val_loss: 0.1138
Epoch 5/27

  16/6530 [..............................] - ETA: 0s - loss: 0.1028
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0233
 400/6530 [>.............................] - ETA: 0s - loss: 0.1256
3184/6530 [=============>................] - ETA: 0s - loss: 0.0229
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1259
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0234
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1243
3952/6530 [=================>............] - ETA: 0s - loss: 0.0234
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1229
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0233
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1213
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0230
2256/6530 [=========>....................] - ETA: 0s - loss: 0.1205
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0230
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1221
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0228
2992/6530 [============>.................] - ETA: 0s - loss: 0.1217
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0227
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1216
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0227
3744/6530 [================>.............] - ETA: 0s - loss: 0.1215
6530/6530 [==============================] - 1s 134us/step - loss: 0.0227 - val_loss: 0.0239
Epoch 6/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0171
4128/6530 [=================>............] - ETA: 0s - loss: 0.1225
 400/6530 [>.............................] - ETA: 0s - loss: 0.0195
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1226
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0209
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1223
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0219
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1219
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0218
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1212
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0212
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1219
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0215
6336/6530 [============================>.] - ETA: 0s - loss: 0.1217
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0218
6530/6530 [==============================] - 1s 142us/step - loss: 0.1221 - val_loss: 0.1009
Epoch 6/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0956
3104/6530 [=============>................] - ETA: 0s - loss: 0.0217
 400/6530 [>.............................] - ETA: 0s - loss: 0.1128
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0216
 784/6530 [==>...........................] - ETA: 0s - loss: 0.1170
3904/6530 [================>.............] - ETA: 0s - loss: 0.0217
1168/6530 [====>.........................] - ETA: 0s - loss: 0.1169
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0215
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1166
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0214
1936/6530 [=======>......................] - ETA: 0s - loss: 0.1145
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0214
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1148
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0214
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1150
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0214
3088/6530 [=============>................] - ETA: 0s - loss: 0.1149
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0214
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1156
6530/6530 [==============================] - 1s 135us/step - loss: 0.0215 - val_loss: 0.0247
Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0322
3856/6530 [================>.............] - ETA: 0s - loss: 0.1160
 416/6530 [>.............................] - ETA: 0s - loss: 0.0181
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1161
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1161
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0207
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0200
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1155
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0203
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1158
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0204
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1155
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0203
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1153
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0202
6530/6530 [==============================] - 1s 138us/step - loss: 0.1159 - val_loss: 0.1162
Epoch 7/27

  16/6530 [..............................] - ETA: 0s - loss: 0.1024
3216/6530 [=============>................] - ETA: 0s - loss: 0.0203
 336/6530 [>.............................] - ETA: 0s - loss: 0.1117
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0200
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1143
3968/6530 [=================>............] - ETA: 0s - loss: 0.0206
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1121
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0204
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1125
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0204
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1129
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0204
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1104
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0205
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1115
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0205
2912/6530 [============>.................] - ETA: 0s - loss: 0.1117
6352/6530 [============================>.] - ETA: 0s - loss: 0.0206
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1113
6530/6530 [==============================] - 1s 133us/step - loss: 0.0206 - val_loss: 0.0136
Epoch 8/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0216
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1113
 416/6530 [>.............................] - ETA: 0s - loss: 0.0210
4048/6530 [=================>............] - ETA: 0s - loss: 0.1123
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0206
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1121
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0196
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1124
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0197
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1122
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0199
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1115
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0200
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1121
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0199
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1120
3120/6530 [=============>................] - ETA: 0s - loss: 0.0210
6530/6530 [==============================] - 1s 142us/step - loss: 0.1124 - val_loss: 0.1024
Epoch 8/27

  16/6530 [..............................] - ETA: 0s - loss: 0.1020
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0206
 384/6530 [>.............................] - ETA: 0s - loss: 0.1077
3904/6530 [================>.............] - ETA: 0s - loss: 0.0211
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1099
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0211
1104/6530 [====>.........................] - ETA: 0s - loss: 0.1119
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0212
1488/6530 [=====>........................] - ETA: 0s - loss: 0.1098
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0211
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1100
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0212
2256/6530 [=========>....................] - ETA: 0s - loss: 0.1089
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0211
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1104
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0210
3008/6530 [============>.................] - ETA: 0s - loss: 0.1097
6530/6530 [==============================] - 1s 135us/step - loss: 0.0210 - val_loss: 0.0132
Epoch 9/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0252
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1100
 400/6530 [>.............................] - ETA: 0s - loss: 0.0214
3760/6530 [================>.............] - ETA: 0s - loss: 0.1102
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0221
4128/6530 [=================>............] - ETA: 0s - loss: 0.1107
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0218
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1106
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0219
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1097
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0221
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1090
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0219
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1087
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0214
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1090
3104/6530 [=============>................] - ETA: 0s - loss: 0.0211
6368/6530 [============================>.] - ETA: 0s - loss: 0.1085
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0212
6530/6530 [==============================] - 1s 141us/step - loss: 0.1090 - val_loss: 0.1025
Epoch 9/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0987
3888/6530 [================>.............] - ETA: 0s - loss: 0.0209
 400/6530 [>.............................] - ETA: 0s - loss: 0.1017
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0210
 800/6530 [==>...........................] - ETA: 0s - loss: 0.1061
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0210
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1074
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0209
1520/6530 [=====>........................] - ETA: 0s - loss: 0.1071
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0209
1904/6530 [=======>......................] - ETA: 0s - loss: 0.1054
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0208
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1060
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0205
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1061
6530/6530 [==============================] - 1s 136us/step - loss: 0.0205 - val_loss: 0.0147
Epoch 10/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0078
3040/6530 [============>.................] - ETA: 0s - loss: 0.1056
 400/6530 [>.............................] - ETA: 0s - loss: 0.0195
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1057
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0204
3728/6530 [================>.............] - ETA: 0s - loss: 0.1057
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0204
4080/6530 [=================>............] - ETA: 0s - loss: 0.1067
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0212
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1066
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0211
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1066
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0208
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1060
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0202
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1053
3072/6530 [=============>................] - ETA: 0s - loss: 0.0200
5968/6530 [==========================>...] - ETA: 0s - loss: 0.1060
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0202
6336/6530 [============================>.] - ETA: 0s - loss: 0.1058
3824/6530 [================>.............] - ETA: 0s - loss: 0.0202
6530/6530 [==============================] - 1s 142us/step - loss: 0.1062 - val_loss: 0.0913
Epoch 10/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0839
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0202
 400/6530 [>.............................] - ETA: 0s - loss: 0.1019
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0202
 784/6530 [==>...........................] - ETA: 0s - loss: 0.1052
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0201
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1065
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0200
1520/6530 [=====>........................] - ETA: 0s - loss: 0.1058
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0200
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1047
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0199
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1047
6528/6530 [============================>.] - ETA: 0s - loss: 0.0199
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1054
6530/6530 [==============================] - 1s 137us/step - loss: 0.0199 - val_loss: 0.0140
Epoch 11/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0125
3040/6530 [============>.................] - ETA: 0s - loss: 0.1049
 416/6530 [>.............................] - ETA: 0s - loss: 0.0186
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1054
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0208
3792/6530 [================>.............] - ETA: 0s - loss: 0.1054
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0214
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1064
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0206
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1059
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0211
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1052
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0209
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1045
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0206
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1042
3136/6530 [=============>................] - ETA: 0s - loss: 0.0206
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1044
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0205
6384/6530 [============================>.] - ETA: 0s - loss: 0.1044
3920/6530 [=================>............] - ETA: 0s - loss: 0.0205
6530/6530 [==============================] - 1s 141us/step - loss: 0.1049 - val_loss: 0.0990
Epoch 11/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0824
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0205
 336/6530 [>.............................] - ETA: 1s - loss: 0.0985
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0208
 688/6530 [==>...........................] - ETA: 0s - loss: 0.1034
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0207
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1038
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0205
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1045
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0206
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1044
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0205
2160/6530 [========>.....................] - ETA: 0s - loss: 0.1025
6530/6530 [==============================] - 1s 137us/step - loss: 0.0204 - val_loss: 0.0189
Epoch 12/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0375
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1029
 416/6530 [>.............................] - ETA: 0s - loss: 0.0189
2928/6530 [============>.................] - ETA: 0s - loss: 0.1027
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0179
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1017
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0188
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1022
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0182
4000/6530 [=================>............] - ETA: 0s - loss: 0.1029
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0184
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1025
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0181
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1029
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0185
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1027
3072/6530 [=============>................] - ETA: 0s - loss: 0.0188
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1022
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0186
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1024
3872/6530 [================>.............] - ETA: 0s - loss: 0.0186
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1024
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0191
6530/6530 [==============================] - 1s 144us/step - loss: 0.1027 - val_loss: 0.1011
Epoch 12/27

  16/6530 [..............................] - ETA: 0s - loss: 0.1041
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0190
 384/6530 [>.............................] - ETA: 0s - loss: 0.1074
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0191
 752/6530 [==>...........................] - ETA: 0s - loss: 0.1073
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0191
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1086
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0192
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1060
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0191
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1041
6530/6530 [==============================] - 1s 137us/step - loss: 0.0191 - val_loss: 0.0109
Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0063
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1036
 416/6530 [>.............................] - ETA: 0s - loss: 0.0187
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1037
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0187
2976/6530 [============>.................] - ETA: 0s - loss: 0.1030
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0180
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1028
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0182
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1028
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0188
4016/6530 [=================>............] - ETA: 0s - loss: 0.1035
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0183
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1030
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0188
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1030
3024/6530 [============>.................] - ETA: 0s - loss: 0.0190
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1023
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0186
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1014
3792/6530 [================>.............] - ETA: 0s - loss: 0.0185
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1015
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0183
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1013
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0184
6530/6530 [==============================] - 1s 144us/step - loss: 0.1016 - val_loss: 0.0935
Epoch 13/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0895
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0188
 400/6530 [>.............................] - ETA: 0s - loss: 0.0989
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0190
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1012
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0190
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1029
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0191
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1022
6480/6530 [============================>.] - ETA: 0s - loss: 0.0192
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1003
6530/6530 [==============================] - 1s 138us/step - loss: 0.0192 - val_loss: 0.0140
Epoch 14/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0431
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0997
 400/6530 [>.............................] - ETA: 0s - loss: 0.0191
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1007
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0186
2944/6530 [============>.................] - ETA: 0s - loss: 0.1002
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0211
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1001
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0200
3696/6530 [===============>..............] - ETA: 0s - loss: 0.1000
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0196
4064/6530 [=================>............] - ETA: 0s - loss: 0.1011
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0194
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1009
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0196
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1008
3024/6530 [============>.................] - ETA: 0s - loss: 0.0200
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1004
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0198
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0998
3792/6530 [================>.............] - ETA: 0s - loss: 0.0200
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1000
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0198
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1000
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0197
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0196
6530/6530 [==============================] - 1s 144us/step - loss: 0.1004 - val_loss: 0.0992
Epoch 14/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0874
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0193
 384/6530 [>.............................] - ETA: 0s - loss: 0.1049
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0194
 752/6530 [==>...........................] - ETA: 0s - loss: 0.1053
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0197
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1053
6512/6530 [============================>.] - ETA: 0s - loss: 0.0197
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1026
6530/6530 [==============================] - 1s 138us/step - loss: 0.0196 - val_loss: 0.0125
Epoch 15/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0170
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1010
 384/6530 [>.............................] - ETA: 0s - loss: 0.0203
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1006
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0214
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1006
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0210
2944/6530 [============>.................] - ETA: 0s - loss: 0.1000
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0194
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0997
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0192
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0995
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0191
4064/6530 [=================>............] - ETA: 0s - loss: 0.1004
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0194
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1001
3072/6530 [=============>................] - ETA: 0s - loss: 0.0199
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0998
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0197
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0992
3856/6530 [================>.............] - ETA: 0s - loss: 0.0195
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0985
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0195
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0988
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0195
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0988
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0194
6530/6530 [==============================] - 1s 144us/step - loss: 0.0991 - val_loss: 0.0968

5376/6530 [=======================>......] - ETA: 0s - loss: 0.0194
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0194
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0194
6528/6530 [============================>.] - ETA: 0s - loss: 0.0191
6530/6530 [==============================] - 1s 138us/step - loss: 0.0191 - val_loss: 0.0166
Epoch 16/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0085
# training | RMSE: 0.1182, MAE: 0.0907
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23163308861005694}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.11815512399496497, 'rmse': 0.11815512399496497, 'mae': 0.09068005313107402, 'early_stop': True}
vggnet done  0

 432/6530 [>.............................] - ETA: 0s - loss: 0.0164
 848/6530 [==>...........................] - ETA: 0s - loss: 0.0163
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0169
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0176
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0178
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0179
2880/6530 [============>.................] - ETA: 0s - loss: 0.0178
3232/6530 [=============>................] - ETA: 0s - loss: 0.0178
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0176
3968/6530 [=================>............] - ETA: 0s - loss: 0.0179
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0179
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0177
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0179
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0178
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0177
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0178
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0178
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0179
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0178
6400/6530 [============================>.] - ETA: 0s - loss: 0.0179
6530/6530 [==============================] - 1s 174us/step - loss: 0.0179 - val_loss: 0.0125
Epoch 17/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0308
 192/6530 [..............................] - ETA: 1s - loss: 0.0235
 384/6530 [>.............................] - ETA: 1s - loss: 0.0192
 592/6530 [=>............................] - ETA: 1s - loss: 0.0180
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0187
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0189
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0194
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0193
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0198
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0193
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0193
2288/6530 [=========>....................] - ETA: 1s - loss: 0.0197
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0191
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0192
2928/6530 [============>.................] - ETA: 0s - loss: 0.0198
3136/6530 [=============>................] - ETA: 0s - loss: 0.0194
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0193
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0193
3776/6530 [================>.............] - ETA: 0s - loss: 0.0193
3968/6530 [=================>............] - ETA: 0s - loss: 0.0192
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0192
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0193
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0193
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0193
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0191
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0190
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0191
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0191
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0190
6400/6530 [============================>.] - ETA: 0s - loss: 0.0190
6530/6530 [==============================] - 2s 242us/step - loss: 0.0189 - val_loss: 0.0103
Epoch 18/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0285
 320/6530 [>.............................] - ETA: 1s - loss: 0.0165
 624/6530 [=>............................] - ETA: 0s - loss: 0.0185
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0186
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0186
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0183
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0189
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0186
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0185
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0186
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0184
2976/6530 [============>.................] - ETA: 0s - loss: 0.0184
3168/6530 [=============>................] - ETA: 0s - loss: 0.0184
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0184
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0183
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0184
3856/6530 [================>.............] - ETA: 0s - loss: 0.0184
4032/6530 [=================>............] - ETA: 0s - loss: 0.0182
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0182
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0180
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0180
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0181
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0183
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0182
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0182
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0181
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0180
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0180
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0180
6496/6530 [============================>.] - ETA: 0s - loss: 0.0180
6530/6530 [==============================] - 2s 244us/step - loss: 0.0180 - val_loss: 0.0117
Epoch 19/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0090
 192/6530 [..............................] - ETA: 1s - loss: 0.0183
 368/6530 [>.............................] - ETA: 1s - loss: 0.0167
 544/6530 [=>............................] - ETA: 1s - loss: 0.0150
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0150
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0156
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0155
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0159
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0159
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0159
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0164
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0165
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0165
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0164
2896/6530 [============>.................] - ETA: 0s - loss: 0.0165
3136/6530 [=============>................] - ETA: 0s - loss: 0.0166
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0167
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0170
3888/6530 [================>.............] - ETA: 0s - loss: 0.0171
4128/6530 [=================>............] - ETA: 0s - loss: 0.0170
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0172
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0171
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0172
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0173
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0173
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0173
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0174
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0175
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0175
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0175
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0174
6432/6530 [============================>.] - ETA: 0s - loss: 0.0174
6530/6530 [==============================] - 2s 260us/step - loss: 0.0174 - val_loss: 0.0113
Epoch 20/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0116
 224/6530 [>.............................] - ETA: 1s - loss: 0.0216
 448/6530 [=>............................] - ETA: 1s - loss: 0.0205
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0217
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0196
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0197
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0194
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0190
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0188
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0189
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0188
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0184
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0183
2560/6530 [==========>...................] - ETA: 1s - loss: 0.0179
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0179
3072/6530 [=============>................] - ETA: 0s - loss: 0.0178
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0176
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0178
3952/6530 [=================>............] - ETA: 0s - loss: 0.0174
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0173
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0173
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0175
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0176
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0176
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0176
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0176
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0176
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0175
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0175
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0173
6400/6530 [============================>.] - ETA: 0s - loss: 0.0173
6530/6530 [==============================] - 2s 259us/step - loss: 0.0173 - val_loss: 0.0144
Epoch 21/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0273
 192/6530 [..............................] - ETA: 1s - loss: 0.0184
 368/6530 [>.............................] - ETA: 1s - loss: 0.0176
 544/6530 [=>............................] - ETA: 1s - loss: 0.0172
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0175
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0174
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0174
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0172
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0174
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0168
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0169
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0171
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0170
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0170
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0173
2848/6530 [============>.................] - ETA: 1s - loss: 0.0173
3024/6530 [============>.................] - ETA: 0s - loss: 0.0174
3216/6530 [=============>................] - ETA: 0s - loss: 0.0174
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0176
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0175
3808/6530 [================>.............] - ETA: 0s - loss: 0.0176
4000/6530 [=================>............] - ETA: 0s - loss: 0.0176
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0176
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0176
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0174
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0174
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0175
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0175
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0176
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0176
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0175
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0174
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0175
6416/6530 [============================>.] - ETA: 0s - loss: 0.0177
6530/6530 [==============================] - 2s 279us/step - loss: 0.0178 - val_loss: 0.0114
Epoch 22/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0143
 192/6530 [..............................] - ETA: 1s - loss: 0.0185
 384/6530 [>.............................] - ETA: 1s - loss: 0.0177
 576/6530 [=>............................] - ETA: 1s - loss: 0.0160
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0152
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0152
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0158
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0164
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0163
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0167
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0162
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0160
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0165
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0167
2720/6530 [===========>..................] - ETA: 1s - loss: 0.0164
2896/6530 [============>.................] - ETA: 0s - loss: 0.0162
3088/6530 [=============>................] - ETA: 0s - loss: 0.0168
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0168
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0166
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0166
3856/6530 [================>.............] - ETA: 0s - loss: 0.0167
4032/6530 [=================>............] - ETA: 0s - loss: 0.0166
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0166
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0166
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0165
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0165
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0164
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0164
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0163
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0164
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0166
6464/6530 [============================>.] - ETA: 0s - loss: 0.0167
6530/6530 [==============================] - 2s 259us/step - loss: 0.0168 - val_loss: 0.0105

# training | RMSE: 0.0960, MAE: 0.0748
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4372703468506898}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.0959987479706642, 'rmse': 0.0959987479706642, 'mae': 0.07477357989414923, 'early_stop': True}
vggnet done  2
all of workers have been done
calculation finished
#1 epoch=27.0 loss={'loss': 0.3287804222816858, 'rmse': 0.3287804222816858, 'mae': 0.2952532798644083, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 47, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 56, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#0 epoch=27.0 loss={'loss': 0.11815512399496497, 'rmse': 0.11815512399496497, 'mae': 0.09068005313107402, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 93, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 78, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.23163308861005694}, 'layer_5_size': 9, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#2 epoch=27.0 loss={'loss': 0.0959987479706642, 'rmse': 0.0959987479706642, 'mae': 0.07477357989414923, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 81, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 31, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4372703468506898}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 93, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
get a list [results] of length 169
get a list [loss] of length 3
get a list [val_loss] of length 3
length of indices is [2 1 0]
length of indices is 3
length of T is 3
newly formed T structure is:[] 

*** 1.259259259259259 configurations x 81.0 iterations each

3 | Thu Sep 27 22:16:26 2018 | lowest loss so far: 0.0960 (run 2)

vggnet done  0
vggnet done  1
vggnet done  2
all of workers have been done
calculation finished
