loading data...
None
[0, 1, 2]
s=4
T is of size 81
T=[{'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47802181397194043}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23753366920597196}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4633799077258097}, 'layer_3_size': 71, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16999145104470526}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23702195861259667}, 'layer_1_size': 68, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.29478758241313924}, 'layer_3_size': 9, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.33428827796759364}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.282863249502904}, 'layer_3_size': 26, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2847538518782967}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2035119362983565}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48594345397869476}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39425337615491773}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 43, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 100, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 17, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.31824595404261413}, 'layer_3_size': 68, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 39, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.231036751147615}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12403564418814238}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 99, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18902062498118097}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.39060064397063343}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10996334638100778}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10952735483869187}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4326994328446677}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22735716747727375}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 88, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 75, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1576889091634152}, 'layer_2_size': 5, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30663997953070404}, 'layer_3_size': 87, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.25057276661770356}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10940858235669326}, 'layer_2_size': 28, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 9, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1234109170669142}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.34670527040571764}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2799181323299549}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23773996701446284}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32554561515669345}, 'layer_4_size': 30, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42334300316688633}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3864152965008092}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 85, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38569931652089273}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44153406687369323}, 'layer_4_size': 60, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18547798882111516}, 'layer_5_size': 22, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3798313841258728}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23157249654939427}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 46, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37667909807746214}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.24561458768767733}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4670441802312316}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 2, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 7, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4832891802493271}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4171010767899158}, 'layer_4_size': 87, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 21, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10103764488329858}, 'layer_3_size': 49, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28374513320726524}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1020118725516797}, 'layer_2_size': 71, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3228548507084892}, 'layer_3_size': 57, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1757581808044658}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43987587747758805}, 'layer_1_size': 47, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10179800915293612}, 'layer_2_size': 47, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 61, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2050947322336447}, 'layer_4_size': 41, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37487373657267964}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10079398389763844}, 'layer_2_size': 43, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25999363045229895}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.16639324870236827}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12485200845522315}, 'layer_4_size': 88, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11021378720415922}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4325646958140229}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4393538692076945}, 'layer_4_size': 46, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 9, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2968844105931647}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.379665317170057}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 19, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 7, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1857037786421014}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.25136261040079955}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 9, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.289478959192061}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1631506464809413}, 'layer_2_size': 52, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4142997896037961}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 94, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2420366468676569}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 14, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10289939797168778}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 71, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1296344458219014}, 'layer_2_size': 100, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4970884277939287}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1087460609141687}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1889310551921254}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3338165530239823}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20261337059472792}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10963218549386174}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17140315668636932}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 7, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17885634358977823}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 44, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37641506330630803}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28558516419712654}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3722775565240588}, 'layer_3_size': 9, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4108479697027453}, 'layer_5_size': 100, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46107233114729274}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10807125705702347}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44587267964897137}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21759894630158141}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30985642917020173}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 36, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 25, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 37, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10338216607501166}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3549833662281482}, 'layer_4_size': 4, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 8, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.32477920284882056}, 'layer_2_size': 38, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4711066283762235}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20852464738421236}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28393702145126665}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4273360035000433}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 63, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.48537797765244817}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.162244625320774}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36181969856349394}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45648493186748806}, 'layer_4_size': 48, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 98, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22558419861507129}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.25144933285848337}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.30975404693021946}, 'layer_2_size': 81, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35525074782080623}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2443185679023121}, 'layer_1_size': 80, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13727119846581481}, 'layer_2_size': 42, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.38348129314100576}, 'layer_4_size': 78, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15256004882486446}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43706453581122084}, 'layer_3_size': 21, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3149594268408259}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35245294857301446}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 45, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.14045255114533833}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3378174053984615}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3441548869484395}, 'layer_1_size': 58, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.25971825110197183}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27662952086894604}, 'layer_4_size': 79, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 87, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3527086162194488}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39459061959382125}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.360442390830502}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32952669768377524}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2597483729703344}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.211692527289565}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35135548822941043}, 'layer_1_size': 7, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 81, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 34, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3846968218925758}, 'layer_1_size': 28, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 72, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 11, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 42, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14970703358753235}, 'layer_3_size': 51, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.479652463853537}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21797867815066485}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4656804979841388}, 'layer_4_size': 43, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1935168503165503}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44411323388230295}, 'layer_4_size': 34, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2565654251079069}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28405862462018816}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4723512202998946}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.40650904959016443}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38345276572686826}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.43851058729984005}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 70, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2538091909685959}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]
newly formed T structure is:[[0, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47802181397194043}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [1, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23753366920597196}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4633799077258097}, 'layer_3_size': 71, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [2, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16999145104470526}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [3, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23702195861259667}, 'layer_1_size': 68, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.29478758241313924}, 'layer_3_size': 9, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.33428827796759364}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [4, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.282863249502904}, 'layer_3_size': 26, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2847538518782967}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [5, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2035119362983565}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48594345397869476}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [6, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39425337615491773}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 43, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 100, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [7, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [8, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [9, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [10, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 17, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}], [11, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.31824595404261413}, 'layer_3_size': 68, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 39, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [12, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.231036751147615}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12403564418814238}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [13, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 99, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18902062498118097}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.39060064397063343}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [14, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10996334638100778}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10952735483869187}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4326994328446677}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [15, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22735716747727375}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 88, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 75, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [16, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1576889091634152}, 'layer_2_size': 5, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30663997953070404}, 'layer_3_size': 87, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.25057276661770356}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [17, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10940858235669326}, 'layer_2_size': 28, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 9, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [18, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1234109170669142}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.34670527040571764}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2799181323299549}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [19, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23773996701446284}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32554561515669345}, 'layer_4_size': 30, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42334300316688633}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [20, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3864152965008092}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [21, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 85, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38569931652089273}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44153406687369323}, 'layer_4_size': 60, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18547798882111516}, 'layer_5_size': 22, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [22, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3798313841258728}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23157249654939427}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 46, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [23, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37667909807746214}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.24561458768767733}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4670441802312316}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [24, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 2, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [25, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 7, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4832891802493271}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4171010767899158}, 'layer_4_size': 87, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 21, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [26, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10103764488329858}, 'layer_3_size': 49, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [27, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28374513320726524}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [28, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1020118725516797}, 'layer_2_size': 71, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [29, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3228548507084892}, 'layer_3_size': 57, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1757581808044658}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [30, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43987587747758805}, 'layer_1_size': 47, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10179800915293612}, 'layer_2_size': 47, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 61, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2050947322336447}, 'layer_4_size': 41, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [31, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37487373657267964}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10079398389763844}, 'layer_2_size': 43, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25999363045229895}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.16639324870236827}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [32, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12485200845522315}, 'layer_4_size': 88, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11021378720415922}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [33, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4325646958140229}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4393538692076945}, 'layer_4_size': 46, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 9, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [34, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [35, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2968844105931647}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.379665317170057}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [36, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [37, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 19, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 7, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [38, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1857037786421014}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [39, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.25136261040079955}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 9, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [40, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.289478959192061}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1631506464809413}, 'layer_2_size': 52, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4142997896037961}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}], [41, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 94, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2420366468676569}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 14, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [42, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10289939797168778}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [43, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 71, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1296344458219014}, 'layer_2_size': 100, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [44, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4970884277939287}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1087460609141687}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1889310551921254}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [45, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [46, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3338165530239823}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [47, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20261337059472792}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10963218549386174}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [48, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17140315668636932}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 7, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17885634358977823}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [49, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 44, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [50, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37641506330630803}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [51, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28558516419712654}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3722775565240588}, 'layer_3_size': 9, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4108479697027453}, 'layer_5_size': 100, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [52, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46107233114729274}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10807125705702347}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}], [53, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44587267964897137}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [54, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21759894630158141}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30985642917020173}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [55, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 36, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 25, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 37, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10338216607501166}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [56, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3549833662281482}, 'layer_4_size': 4, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [57, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 8, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.32477920284882056}, 'layer_2_size': 38, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4711066283762235}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [58, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20852464738421236}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [59, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28393702145126665}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4273360035000433}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 63, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [60, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.48537797765244817}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.162244625320774}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [61, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36181969856349394}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45648493186748806}, 'layer_4_size': 48, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [62, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 98, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22558419861507129}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.25144933285848337}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [63, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.30975404693021946}, 'layer_2_size': 81, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35525074782080623}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [64, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2443185679023121}, 'layer_1_size': 80, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13727119846581481}, 'layer_2_size': 42, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.38348129314100576}, 'layer_4_size': 78, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [65, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15256004882486446}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43706453581122084}, 'layer_3_size': 21, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3149594268408259}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35245294857301446}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [66, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 45, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.14045255114533833}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [67, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3378174053984615}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}], [68, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3441548869484395}, 'layer_1_size': 58, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.25971825110197183}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27662952086894604}, 'layer_4_size': 79, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [69, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 87, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3527086162194488}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [70, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39459061959382125}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.360442390830502}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32952669768377524}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [71, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2597483729703344}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.211692527289565}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [72, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35135548822941043}, 'layer_1_size': 7, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 81, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [73, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 34, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [74, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3846968218925758}, 'layer_1_size': 28, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 72, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 11, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 42, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [75, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14970703358753235}, 'layer_3_size': 51, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.479652463853537}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [76, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21797867815066485}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4656804979841388}, 'layer_4_size': 43, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [77, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1935168503165503}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44411323388230295}, 'layer_4_size': 34, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [78, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2565654251079069}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28405862462018816}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4723512202998946}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.40650904959016443}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [79, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38345276572686826}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.43851058729984005}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [80, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 70, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2538091909685959}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]] 

*** 81 configurations x 1.0 iterations each

1 | Thu Sep 27 23:23:02 2018 | lowest loss so far: inf (run -1)

{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  12 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:20 - loss: 1.3253
 640/6530 [=>............................] - ETA: 7s - loss: 1.0822  {'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  67 | activation: relu    | extras: dropout - rate: 23.8% 
layer 2 | size:  88 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a797860>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:26 - loss: 0.6251
1168/6530 [====>.........................] - ETA: 4s - loss: 0.8761
 896/6530 [===>..........................] - ETA: 4s - loss: 0.2641  
1792/6530 [=======>......................] - ETA: 2s - loss: 0.7151
1920/6530 [=======>......................] - ETA: 1s - loss: 0.2284
2448/6530 [==========>...................] - ETA: 1s - loss: 0.5883
3008/6530 [============>.................] - ETA: 1s - loss: 0.2087
3136/6530 [=============>................] - ETA: 1s - loss: 0.4852
4096/6530 [=================>............] - ETA: 0s - loss: 0.1971
3824/6530 [================>.............] - ETA: 0s - loss: 0.4113
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1894
4528/6530 [===================>..........] - ETA: 0s - loss: 0.3567
6368/6530 [============================>.] - ETA: 0s - loss: 0.1839
5232/6530 [=======================>......] - ETA: 0s - loss: 0.3161
6530/6530 [==============================] - 1s 163us/step - loss: 0.1830 - val_loss: 0.2074

5872/6530 [=========================>....] - ETA: 0s - loss: 0.2877
6530/6530 [==============================] - 1s 202us/step - loss: 0.2639 - val_loss: 0.0527
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  24 | activation: relu    | extras: dropout - rate: 17.0% 
layer 2 | size:  11 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  64 | activation: sigmoid | extras: None 
layer 4 | size:  72 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:10 - loss: 0.7023
 672/6530 [==>...........................] - ETA: 8s - loss: 0.2659  
1312/6530 [=====>........................] - ETA: 4s - loss: 0.2359
2016/6530 [========>.....................] - ETA: 2s - loss: 0.2163
# training | RMSE: 0.2445, MAE: 0.2054
worker 1  xfile  [1, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23753366920597196}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4633799077258097}, 'layer_3_size': 71, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.24453143936765454, 'rmse': 0.24453143936765454, 'mae': 0.20536672667253278, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  68 | activation: relu    | extras: dropout - rate: 23.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 26s - loss: 1.1618
2720/6530 [===========>..................] - ETA: 1s - loss: 0.2041
1184/6530 [====>.........................] - ETA: 0s - loss: 0.6377 
3424/6530 [==============>...............] - ETA: 1s - loss: 0.1952
2336/6530 [=========>....................] - ETA: 0s - loss: 0.5169
4128/6530 [=================>............] - ETA: 0s - loss: 0.1889
3616/6530 [===============>..............] - ETA: 0s - loss: 0.4458
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1849
4896/6530 [=====================>........] - ETA: 0s - loss: 0.3983
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1801
6112/6530 [===========================>..] - ETA: 0s - loss: 0.3640
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1772
6530/6530 [==============================] - 0s 64us/step - loss: 0.3560 - val_loss: 0.2465

6530/6530 [==============================] - 1s 226us/step - loss: 0.1755 - val_loss: 0.1415

# training | RMSE: 0.1798, MAE: 0.1347
worker 2  xfile  [2, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16999145104470526}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.17978818761743376, 'rmse': 0.17978818761743376, 'mae': 0.13468790320719431, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  68 | activation: sigmoid | extras: None 
layer 2 | size:  51 | activation: sigmoid | extras: dropout - rate: 39.4% 
layer 3 | size:  43 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a797be0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 4s - loss: 2.3495
# training | RMSE: 0.2298, MAE: 0.1808
worker 0  xfile  [0, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47802181397194043}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.22981884488023502, 'rmse': 0.22981884488023502, 'mae': 0.180810010930137, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  52 | activation: sigmoid | extras: None 
layer 2 | size:  94 | activation: sigmoid | extras: None 
layer 3 | size:  26 | activation: relu    | extras: dropout - rate: 28.3% 
layer 4 | size:  28 | activation: tanh    | extras: batchnorm 
layer 5 | size:  44 | activation: relu    | extras: dropout - rate: 28.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a797a90>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:30 - loss: 6.3480
6530/6530 [==============================] - 0s 37us/step - loss: 0.3362 - val_loss: 0.0736

 544/6530 [=>............................] - ETA: 5s - loss: 2.9335  
# training | RMSE: 0.3016, MAE: 0.2354
worker 1  xfile  [3, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23702195861259667}, 'layer_1_size': 68, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.29478758241313924}, 'layer_3_size': 9, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.33428827796759364}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.3015827944873194, 'rmse': 0.3015827944873194, 'mae': 0.23539133187034625, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  72 | activation: tanh    | extras: batchnorm 
layer 2 | size:  27 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f77784b9400>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 1:52 - loss: 0.6285
1088/6530 [===>..........................] - ETA: 2s - loss: 1.8410
 448/6530 [=>............................] - ETA: 4s - loss: 0.2467  
1600/6530 [======>.......................] - ETA: 1s - loss: 1.3527
 896/6530 [===>..........................] - ETA: 2s - loss: 0.2113
2144/6530 [========>.....................] - ETA: 1s - loss: 1.0629
1344/6530 [=====>........................] - ETA: 1s - loss: 0.1940
2720/6530 [===========>..................] - ETA: 0s - loss: 0.8715
1776/6530 [=======>......................] - ETA: 1s - loss: 0.1850
3264/6530 [=============>................] - ETA: 0s - loss: 0.7487
2176/6530 [========>.....................] - ETA: 1s - loss: 0.1814
3744/6530 [================>.............] - ETA: 0s - loss: 0.6707
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1800
4288/6530 [==================>...........] - ETA: 0s - loss: 0.5993
3024/6530 [============>.................] - ETA: 0s - loss: 0.1756
4864/6530 [=====================>........] - ETA: 0s - loss: 0.5408
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1745
5440/6530 [=======================>......] - ETA: 0s - loss: 0.4957
3840/6530 [================>.............] - ETA: 0s - loss: 0.1735
5984/6530 [==========================>...] - ETA: 0s - loss: 0.4594
# training | RMSE: 0.2704, MAE: 0.2193
worker 2  xfile  [6, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39425337615491773}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 43, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 100, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2703680996932272, 'rmse': 0.2703680996932272, 'mae': 0.2192846483204795, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  58 | activation: tanh    | extras: None 
layer 2 | size:  53 | activation: relu    | extras: None 
layer 3 | size:  18 | activation: relu    | extras: None 
layer 4 | size:  40 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7771f08898>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 1:42 - loss: 0.7460
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1728
6528/6530 [============================>.] - ETA: 0s - loss: 0.4300
 416/6530 [>.............................] - ETA: 4s - loss: 0.4870  
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1724
6530/6530 [==============================] - 1s 172us/step - loss: 0.4300 - val_loss: 0.2184

5104/6530 [======================>.......] - ETA: 0s - loss: 0.1714
 800/6530 [==>...........................] - ETA: 2s - loss: 0.3570
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1700
1216/6530 [====>.........................] - ETA: 1s - loss: 0.3053
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1693
1632/6530 [======>.......................] - ETA: 1s - loss: 0.2808
6368/6530 [============================>.] - ETA: 0s - loss: 0.1681
2032/6530 [========>.....................] - ETA: 1s - loss: 0.2680
2464/6530 [==========>...................] - ETA: 0s - loss: 0.2577
6530/6530 [==============================] - 1s 171us/step - loss: 0.1678 - val_loss: 0.1519

2832/6530 [============>.................] - ETA: 0s - loss: 0.2532
3216/6530 [=============>................] - ETA: 0s - loss: 0.2473
3632/6530 [===============>..............] - ETA: 0s - loss: 0.2401
4064/6530 [=================>............] - ETA: 0s - loss: 0.2339
4512/6530 [===================>..........] - ETA: 0s - loss: 0.2291
4976/6530 [=====================>........] - ETA: 0s - loss: 0.2239
5424/6530 [=======================>......] - ETA: 0s - loss: 0.2192
5856/6530 [=========================>....] - ETA: 0s - loss: 0.2155
6288/6530 [===========================>..] - ETA: 0s - loss: 0.2120
6530/6530 [==============================] - 1s 167us/step - loss: 0.2108 - val_loss: 0.1673

# training | RMSE: 0.1901, MAE: 0.1502
worker 1  xfile  [5, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2035119362983565}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48594345397869476}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.19013196311052108, 'rmse': 0.19013196311052108, 'mae': 0.15021072128630594, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  87 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  19 | activation: sigmoid | extras: None 
layer 3 | size:  33 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f77697ca1d0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:10 - loss: 0.3277
 400/6530 [>.............................] - ETA: 5s - loss: 0.0777  
 784/6530 [==>...........................] - ETA: 3s - loss: 0.0653
# training | RMSE: 0.2080, MAE: 0.1669
worker 2  xfile  [7, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.20796408893906987, 'rmse': 0.20796408893906987, 'mae': 0.16689927612437072, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  62 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7771f08550>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 1:13 - loss: 2.1079
1168/6530 [====>.........................] - ETA: 2s - loss: 0.0574
 624/6530 [=>............................] - ETA: 2s - loss: 1.4377  
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0546
# training | RMSE: 0.4593, MAE: 0.3940
worker 0  xfile  [4, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.282863249502904}, 'layer_3_size': 26, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2847538518782967}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.4592967256295669, 'rmse': 0.4592967256295669, 'mae': 0.3940162329913608, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  17 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  45 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  83 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7779bac978>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 13s - loss: 1.0462
1216/6530 [====>.........................] - ETA: 1s - loss: 1.0769
1936/6530 [=======>......................] - ETA: 1s - loss: 0.0534
3840/6530 [================>.............] - ETA: 0s - loss: 0.4737 
1776/6530 [=======>......................] - ETA: 0s - loss: 0.8833
2288/6530 [=========>....................] - ETA: 1s - loss: 0.0524
2352/6530 [=========>....................] - ETA: 0s - loss: 0.7532
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0498
6530/6530 [==============================] - 1s 109us/step - loss: 0.3181 - val_loss: 0.1170

2960/6530 [============>.................] - ETA: 0s - loss: 0.6592
3024/6530 [============>.................] - ETA: 0s - loss: 0.0486
3520/6530 [===============>..............] - ETA: 0s - loss: 0.5983
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0475
4128/6530 [=================>............] - ETA: 0s - loss: 0.5461
3728/6530 [================>.............] - ETA: 0s - loss: 0.0461
4736/6530 [====================>.........] - ETA: 0s - loss: 0.5042
4096/6530 [=================>............] - ETA: 0s - loss: 0.0450
5328/6530 [=======================>......] - ETA: 0s - loss: 0.4703
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0441
5952/6530 [==========================>...] - ETA: 0s - loss: 0.4419
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0429
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0419
6530/6530 [==============================] - 1s 119us/step - loss: 0.4198 - val_loss: 0.2029

5568/6530 [========================>.....] - ETA: 0s - loss: 0.0413
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0402
6320/6530 [============================>.] - ETA: 0s - loss: 0.0398
6530/6530 [==============================] - 1s 196us/step - loss: 0.0395 - val_loss: 0.0342

# training | RMSE: 0.3419, MAE: 0.2974
worker 0  xfile  [8, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.34189232278098086, 'rmse': 0.34189232278098086, 'mae': 0.29742592207285884, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:   3 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  94 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7779c805c0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 16s - loss: 0.3615
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1954 
6530/6530 [==============================] - 0s 71us/step - loss: 0.1819 - val_loss: 0.1780

# training | RMSE: 0.2474, MAE: 0.1929
worker 2  xfile  [10, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 17, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.24736361295002976, 'rmse': 0.24736361295002976, 'mae': 0.19292735823140877, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  13 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f77701d0630>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 1:15 - loss: 5.2760
 656/6530 [==>...........................] - ETA: 2s - loss: 4.2737  
1328/6530 [=====>........................] - ETA: 1s - loss: 3.6720
2032/6530 [========>.....................] - ETA: 0s - loss: 3.1541
2688/6530 [===========>..................] - ETA: 0s - loss: 2.7441
3296/6530 [==============>...............] - ETA: 0s - loss: 2.4230
3920/6530 [=================>............] - ETA: 0s - loss: 2.1462
4576/6530 [====================>.........] - ETA: 0s - loss: 1.9039
5232/6530 [=======================>......] - ETA: 0s - loss: 1.7028
5840/6530 [=========================>....] - ETA: 0s - loss: 1.5451
6480/6530 [============================>.] - ETA: 0s - loss: 1.4062
6530/6530 [==============================] - 1s 113us/step - loss: 1.3965 - val_loss: 0.1079

# training | RMSE: 0.2321, MAE: 0.1761
worker 0  xfile  [11, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.31824595404261413}, 'layer_3_size': 68, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 39, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.23208593753600998, 'rmse': 0.23208593753600998, 'mae': 0.17614457473271905, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  97 | activation: relu    | extras: None 
layer 2 | size:   4 | activation: tanh    | extras: batchnorm 
layer 3 | size:  16 | activation: sigmoid | extras: dropout - rate: 11.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f77612ed470>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:46 - loss: 0.5576
 416/6530 [>.............................] - ETA: 6s - loss: 0.2704  
 832/6530 [==>...........................] - ETA: 3s - loss: 0.2167
1200/6530 [====>.........................] - ETA: 2s - loss: 0.1959
1568/6530 [======>.......................] - ETA: 1s - loss: 0.1839
# training | RMSE: 0.1843, MAE: 0.1464
worker 1  xfile  [9, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.18434819192210603, 'rmse': 0.18434819192210603, 'mae': 0.14640747054517594, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  85 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  99 | activation: tanh    | extras: batchnorm 
layer 3 | size:  89 | activation: tanh    | extras: dropout - rate: 18.9% 
layer 4 | size:  71 | activation: sigmoid | extras: None 
layer 5 | size:  25 | activation: tanh    | extras: dropout - rate: 39.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f77697ca048>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:47 - loss: 0.3176
1968/6530 [========>.....................] - ETA: 1s - loss: 0.1783
 576/6530 [=>............................] - ETA: 5s - loss: 0.4699  
2352/6530 [=========>....................] - ETA: 1s - loss: 0.1728
1120/6530 [====>.........................] - ETA: 3s - loss: 0.3540
2768/6530 [===========>..................] - ETA: 1s - loss: 0.1693
1664/6530 [======>.......................] - ETA: 2s - loss: 0.3037
3168/6530 [=============>................] - ETA: 0s - loss: 0.1640
2176/6530 [========>.....................] - ETA: 1s - loss: 0.2710
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1625
2720/6530 [===========>..................] - ETA: 1s - loss: 0.2491
3984/6530 [=================>............] - ETA: 0s - loss: 0.1600
3232/6530 [=============>................] - ETA: 0s - loss: 0.2315
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1582
3744/6530 [================>.............] - ETA: 0s - loss: 0.2186
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1571
4256/6530 [==================>...........] - ETA: 0s - loss: 0.2090
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1551
4768/6530 [====================>.........] - ETA: 0s - loss: 0.2021
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1531
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1947
5968/6530 [==========================>...] - ETA: 0s - loss: 0.1519
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1883
6320/6530 [============================>.] - ETA: 0s - loss: 0.1509
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1837
6530/6530 [==============================] - 1s 205us/step - loss: 0.1500 - val_loss: 0.1310

6530/6530 [==============================] - 1s 191us/step - loss: 0.1819 - val_loss: 0.2768

# training | RMSE: 0.3271, MAE: 0.2711
worker 2  xfile  [12, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.231036751147615}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12403564418814238}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.3271318448385201, 'rmse': 0.3271318448385201, 'mae': 0.2710531795741368, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  24 | activation: relu    | extras: dropout - rate: 22.7% 
layer 2 | size:  12 | activation: tanh    | extras: batchnorm 
layer 3 | size:  88 | activation: relu    | extras: batchnorm 
layer 4 | size:  20 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f77307526d8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:05 - loss: 0.4840
 960/6530 [===>..........................] - ETA: 4s - loss: 0.2866  
1792/6530 [=======>......................] - ETA: 1s - loss: 0.2467
2688/6530 [===========>..................] - ETA: 1s - loss: 0.2299
3328/6530 [==============>...............] - ETA: 0s - loss: 0.2194
3968/6530 [=================>............] - ETA: 0s - loss: 0.2151
4736/6530 [====================>.........] - ETA: 0s - loss: 0.2085
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2042
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1997
# training | RMSE: 0.1630, MAE: 0.1232
worker 0  xfile  [14, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10996334638100778}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10952735483869187}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4326994328446677}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.1629704997309757, 'rmse': 0.1629704997309757, 'mae': 0.1232071954065642, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  56 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f743c089320>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 8s - loss: 0.6060
6530/6530 [==============================] - 1s 178us/step - loss: 0.1979 - val_loss: 0.1485

6530/6530 [==============================] - 0s 66us/step - loss: 0.5243 - val_loss: 0.4585

# training | RMSE: 0.3040, MAE: 0.2791
worker 1  xfile  [13, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 99, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18902062498118097}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.39060064397063343}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.30402245211034745, 'rmse': 0.30402245211034745, 'mae': 0.2791034556288956, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  54 | activation: sigmoid | extras: batchnorm 
layer 2 | size:   5 | activation: tanh    | extras: dropout - rate: 15.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f74343f32b0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:01 - loss: 0.6577
 304/6530 [>.............................] - ETA: 10s - loss: 0.7005 
 624/6530 [=>............................] - ETA: 5s - loss: 0.5117 
 944/6530 [===>..........................] - ETA: 3s - loss: 0.4092
1264/6530 [====>.........................] - ETA: 2s - loss: 0.3543
1632/6530 [======>.......................] - ETA: 2s - loss: 0.3141
2000/6530 [========>.....................] - ETA: 1s - loss: 0.2892
2352/6530 [=========>....................] - ETA: 1s - loss: 0.2716
# training | RMSE: 0.6720, MAE: 0.6205
worker 0  xfile  [17, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10940858235669326}, 'layer_2_size': 28, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 9, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.6719629293368998, 'rmse': 0.6719629293368998, 'mae': 0.6204985710394874, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  34 | activation: relu    | extras: None 
layer 2 | size:  29 | activation: tanh    | extras: dropout - rate: 12.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f772c157898>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 1:49 - loss: 1.9243
2672/6530 [===========>..................] - ETA: 1s - loss: 0.2600
 608/6530 [=>............................] - ETA: 3s - loss: 0.9249  
3024/6530 [============>.................] - ETA: 1s - loss: 0.2465
1216/6530 [====>.........................] - ETA: 1s - loss: 0.6895
3360/6530 [==============>...............] - ETA: 0s - loss: 0.2386
1808/6530 [=======>......................] - ETA: 1s - loss: 0.5605
# training | RMSE: 0.1939, MAE: 0.1544
worker 2  xfile  [15, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22735716747727375}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 88, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 75, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.19389121898355996, 'rmse': 0.19389121898355996, 'mae': 0.15437244459142524, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  13 | activation: tanh    | extras: dropout - rate: 23.8% 
layer 2 | size:  74 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777804a7f0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 13s - loss: 0.8778
3696/6530 [===============>..............] - ETA: 0s - loss: 0.2319
2352/6530 [=========>....................] - ETA: 0s - loss: 0.4930
4224/6530 [==================>...........] - ETA: 0s - loss: 0.4553 
4016/6530 [=================>............] - ETA: 0s - loss: 0.2265
2944/6530 [============>.................] - ETA: 0s - loss: 0.4395
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2207
6530/6530 [==============================] - 0s 61us/step - loss: 0.3756 - val_loss: 0.1912

3536/6530 [===============>..............] - ETA: 0s - loss: 0.3994
4688/6530 [====================>.........] - ETA: 0s - loss: 0.2168
4160/6530 [==================>...........] - ETA: 0s - loss: 0.3678
5024/6530 [======================>.......] - ETA: 0s - loss: 0.2125
4752/6530 [====================>.........] - ETA: 0s - loss: 0.3455
5360/6530 [=======================>......] - ETA: 0s - loss: 0.2082
5360/6530 [=======================>......] - ETA: 0s - loss: 0.3260
5712/6530 [=========================>....] - ETA: 0s - loss: 0.2042
5936/6530 [==========================>...] - ETA: 0s - loss: 0.3121
6064/6530 [==========================>...] - ETA: 0s - loss: 0.2008
6496/6530 [============================>.] - ETA: 0s - loss: 0.2998
6432/6530 [============================>.] - ETA: 0s - loss: 0.1969
6530/6530 [==============================] - 1s 136us/step - loss: 0.2991 - val_loss: 0.1687

6530/6530 [==============================] - 2s 231us/step - loss: 0.1967 - val_loss: 0.1911

# training | RMSE: 0.2305, MAE: 0.1841
worker 2  xfile  [19, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23773996701446284}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32554561515669345}, 'layer_4_size': 30, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42334300316688633}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2305439993492947, 'rmse': 0.2305439993492947, 'mae': 0.18408498695680903, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  99 | activation: sigmoid | extras: None 
layer 2 | size:  19 | activation: sigmoid | extras: dropout - rate: 38.6% 
layer 3 | size:  18 | activation: sigmoid | extras: None 
layer 4 | size:  75 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f774c2346d8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:06 - loss: 0.6063
 992/6530 [===>..........................] - ETA: 2s - loss: 0.2349  
1952/6530 [=======>......................] - ETA: 1s - loss: 0.2254
2880/6530 [============>.................] - ETA: 0s - loss: 0.2193
3840/6530 [================>.............] - ETA: 0s - loss: 0.2168
4800/6530 [=====================>........] - ETA: 0s - loss: 0.2121
5696/6530 [=========================>....] - ETA: 0s - loss: 0.2075
# training | RMSE: 0.2030, MAE: 0.1595
worker 0  xfile  [18, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1234109170669142}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.34670527040571764}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2799181323299549}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2029772657212135, 'rmse': 0.2029772657212135, 'mae': 0.15950116535783682, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  15 | activation: relu    | extras: None 
layer 2 | size:  85 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f743c0167f0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 7s - loss: 0.7268
6528/6530 [============================>.] - ETA: 0s - loss: 0.2046
6530/6530 [==============================] - 0s 57us/step - loss: 0.6364 - val_loss: 0.5033

6530/6530 [==============================] - 1s 116us/step - loss: 0.2045 - val_loss: 0.1687

# training | RMSE: 0.5582, MAE: 0.4949
worker 0  xfile  [21, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 85, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38569931652089273}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44153406687369323}, 'layer_4_size': 60, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18547798882111516}, 'layer_5_size': 22, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.5582240750994912, 'rmse': 0.5582240750994912, 'mae': 0.494894945279168, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  23 | activation: relu    | extras: dropout - rate: 37.7% 
layer 2 | size:  95 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7436877358>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 17s - loss: 0.7422
# training | RMSE: 0.2284, MAE: 0.1873
worker 1  xfile  [16, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1576889091634152}, 'layer_2_size': 5, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30663997953070404}, 'layer_3_size': 87, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.25057276661770356}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.22840838859683818, 'rmse': 0.22840838859683818, 'mae': 0.18726963174741976, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  33 | activation: sigmoid | extras: dropout - rate: 38.0% 
layer 2 | size:  73 | activation: tanh    | extras: batchnorm 
layer 3 | size:  55 | activation: relu    | extras: None 
layer 4 | size:   8 | activation: tanh    | extras: dropout - rate: 23.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f74343f30b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 27s - loss: 0.8222
3072/6530 [=============>................] - ETA: 0s - loss: 0.4353 
1664/6530 [======>.......................] - ETA: 1s - loss: 0.5835 
5760/6530 [=========================>....] - ETA: 0s - loss: 0.3525
3328/6530 [==============>...............] - ETA: 0s - loss: 0.4906
6530/6530 [==============================] - 1s 80us/step - loss: 0.3378 - val_loss: 0.1858

5248/6530 [=======================>......] - ETA: 0s - loss: 0.4331
6530/6530 [==============================] - 1s 128us/step - loss: 0.4077 - val_loss: 0.4058

# training | RMSE: 0.2122, MAE: 0.1697
worker 2  xfile  [20, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3864152965008092}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.21218965526583347, 'rmse': 0.21218965526583347, 'mae': 0.1697150625577421, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  13 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  43 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  55 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7771ff8710>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:03 - loss: 0.6318
 176/6530 [..............................] - ETA: 28s - loss: 0.5518 
 368/6530 [>.............................] - ETA: 14s - loss: 0.3951
# training | RMSE: 0.4680, MAE: 0.4003
worker 1  xfile  [22, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3798313841258728}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23157249654939427}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 46, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.4679506962530148, 'rmse': 0.4679506962530148, 'mae': 0.40033384424683716, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  39 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f740772ed30>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 13s - loss: 2.8205
 544/6530 [=>............................] - ETA: 9s - loss: 0.3234 
4736/6530 [====================>.........] - ETA: 0s - loss: 0.7868 
 752/6530 [==>...........................] - ETA: 7s - loss: 0.2869
6530/6530 [==============================] - 0s 58us/step - loss: 0.6335 - val_loss: 0.1915

 960/6530 [===>..........................] - ETA: 5s - loss: 0.2622
1168/6530 [====>.........................] - ETA: 4s - loss: 0.2450
1392/6530 [=====>........................] - ETA: 4s - loss: 0.2338
1632/6530 [======>.......................] - ETA: 3s - loss: 0.2244
1872/6530 [=======>......................] - ETA: 3s - loss: 0.2180
2112/6530 [========>.....................] - ETA: 2s - loss: 0.2124
2352/6530 [=========>....................] - ETA: 2s - loss: 0.2075
2576/6530 [==========>...................] - ETA: 2s - loss: 0.2041
# training | RMSE: 0.2297, MAE: 0.1859
worker 0  xfile  [23, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37667909807746214}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.24561458768767733}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4670441802312316}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.22970952737823216, 'rmse': 0.22970952737823216, 'mae': 0.1858845957156062, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:   7 | activation: sigmoid | extras: None 
layer 2 | size:  34 | activation: relu    | extras: batchnorm 
layer 3 | size:  97 | activation: relu    | extras: dropout - rate: 48.3% 
layer 4 | size:  87 | activation: tanh    | extras: dropout - rate: 41.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f743655b2b0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 28s - loss: 1.0052
2800/6530 [===========>..................] - ETA: 1s - loss: 0.2006
2560/6530 [==========>...................] - ETA: 0s - loss: 0.2823 
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1982
3040/6530 [============>.................] - ETA: 1s - loss: 0.1986
3248/6530 [=============>................] - ETA: 1s - loss: 0.1977
# training | RMSE: 0.4442, MAE: 0.3603
worker 1  xfile  [26, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10103764488329858}, 'layer_3_size': 49, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.4442066380241941, 'rmse': 0.4442066380241941, 'mae': 0.3602868397254649, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  25 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f74071ff8d0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 14s - loss: 1.8651
6530/6530 [==============================] - 1s 118us/step - loss: 0.1747 - val_loss: 0.0648

3456/6530 [==============>...............] - ETA: 1s - loss: 0.1962
4736/6530 [====================>.........] - ETA: 0s - loss: 1.2073 
3680/6530 [===============>..............] - ETA: 1s - loss: 0.1945
6530/6530 [==============================] - 0s 62us/step - loss: 1.0751 - val_loss: 0.6319

3904/6530 [================>.............] - ETA: 1s - loss: 0.1920
4112/6530 [=================>............] - ETA: 1s - loss: 0.1908
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1899
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1889
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1877
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1861
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1852
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1849
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1838
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1831
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1824
6530/6530 [==============================] - 2s 361us/step - loss: 0.1818 - val_loss: 0.1627

# training | RMSE: 0.7818, MAE: 0.6327
worker 1  xfile  [27, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28374513320726524}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.7817742259961181, 'rmse': 0.7817742259961181, 'mae': 0.632745728207081, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  85 | activation: sigmoid | extras: None 
layer 2 | size:   9 | activation: tanh    | extras: None 
layer 3 | size:  57 | activation: sigmoid | extras: dropout - rate: 32.3% 
layer 4 | size:  83 | activation: tanh    | extras: dropout - rate: 17.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f740711b208>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:28 - loss: 0.3679
 704/6530 [==>...........................] - ETA: 4s - loss: 0.3220  
1472/6530 [=====>........................] - ETA: 1s - loss: 0.2457
2272/6530 [=========>....................] - ETA: 1s - loss: 0.2027
3136/6530 [=============>................] - ETA: 0s - loss: 0.1768
4032/6530 [=================>............] - ETA: 0s - loss: 0.1583
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1474
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1376
6530/6530 [==============================] - 1s 143us/step - loss: 0.1306 - val_loss: 0.0520

# training | RMSE: 0.2529, MAE: 0.2009
worker 0  xfile  [25, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 7, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4832891802493271}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4171010767899158}, 'layer_4_size': 87, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 21, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.25292151698758447, 'rmse': 0.25292151698758447, 'mae': 0.20087855995104414, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  13 | activation: relu    | extras: batchnorm 
layer 2 | size:  71 | activation: sigmoid | extras: dropout - rate: 10.2% 
layer 3 | size:  14 | activation: tanh    | extras: None 
layer 4 | size:  44 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f743655b1d0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:20 - loss: 0.8158
 416/6530 [>.............................] - ETA: 10s - loss: 0.2173 
 896/6530 [===>..........................] - ETA: 5s - loss: 0.1433 
1376/6530 [=====>........................] - ETA: 3s - loss: 0.1160
1856/6530 [=======>......................] - ETA: 2s - loss: 0.1033
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0935
3008/6530 [============>.................] - ETA: 1s - loss: 0.0864
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0820
4128/6530 [=================>............] - ETA: 0s - loss: 0.0781
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0751
# training | RMSE: 0.2064, MAE: 0.1602
worker 2  xfile  [24, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 2, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.206388120698654, 'rmse': 0.206388120698654, 'mae': 0.16016504906809928, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  47 | activation: sigmoid | extras: dropout - rate: 44.0% 
layer 2 | size:  47 | activation: tanh    | extras: dropout - rate: 10.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7771f82390>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 47s - loss: 1.2721
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0732
2048/6530 [========>.....................] - ETA: 1s - loss: 0.2298 
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0712
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1614
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0697
6530/6530 [==============================] - 1s 104us/step - loss: 0.1364 - val_loss: 0.0829

6530/6530 [==============================] - 1s 220us/step - loss: 0.0686 - val_loss: 0.0435

# training | RMSE: 0.2280, MAE: 0.1886
worker 1  xfile  [29, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3228548507084892}, 'layer_3_size': 57, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1757581808044658}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2280328406123311, 'rmse': 0.2280328406123311, 'mae': 0.18858816541670445, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  69 | activation: tanh    | extras: dropout - rate: 37.5% 
layer 2 | size:  43 | activation: relu    | extras: dropout - rate: 10.1% 
layer 3 | size:  76 | activation: relu    | extras: dropout - rate: 26.0% 
layer 4 | size:  31 | activation: relu    | extras: batchnorm 
layer 5 | size:  97 | activation: relu    | extras: dropout - rate: 16.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7406f90198>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 30s - loss: 0.7264
1792/6530 [=======>......................] - ETA: 1s - loss: 0.1734 
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1362
# training | RMSE: 0.2925, MAE: 0.2353
worker 2  xfile  [30, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43987587747758805}, 'layer_1_size': 47, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10179800915293612}, 'layer_2_size': 47, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 61, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2050947322336447}, 'layer_4_size': 41, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.29254136978708356, 'rmse': 0.29254136978708356, 'mae': 0.2353044067405342, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  31 | activation: sigmoid | extras: None 
layer 2 | size:  85 | activation: relu    | extras: None 
layer 3 | size:  41 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7406c15b70>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 19s - loss: 0.5726
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1159
3200/6530 [=============>................] - ETA: 0s - loss: 0.2500 
6530/6530 [==============================] - 1s 81us/step - loss: 0.1566 - val_loss: 0.0645

6530/6530 [==============================] - 1s 137us/step - loss: 0.1055 - val_loss: 0.0494

# training | RMSE: 0.2076, MAE: 0.1648
worker 0  xfile  [28, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1020118725516797}, 'layer_2_size': 71, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.20763999833543534, 'rmse': 0.20763999833543534, 'mae': 0.164791027732264, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  57 | activation: sigmoid | extras: dropout - rate: 43.3% 
layer 2 | size:  82 | activation: tanh    | extras: batchnorm 
layer 3 | size:  65 | activation: sigmoid | extras: None 
layer 4 | size:  46 | activation: tanh    | extras: dropout - rate: 43.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f74360e53c8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:27 - loss: 1.7722
 384/6530 [>.............................] - ETA: 12s - loss: 0.4871 
# training | RMSE: 0.2540, MAE: 0.2073
worker 2  xfile  [32, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12485200845522315}, 'layer_4_size': 88, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11021378720415922}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2539986770725802, 'rmse': 0.2539986770725802, 'mae': 0.20734261797176692, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  15 | activation: relu    | extras: None 
layer 2 | size:  27 | activation: relu    | extras: None 
layer 3 | size:  86 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f74069dc8d0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:26 - loss: 0.6959
 736/6530 [==>...........................] - ETA: 6s - loss: 0.3061 
 800/6530 [==>...........................] - ETA: 3s - loss: 0.5780  
1184/6530 [====>.........................] - ETA: 3s - loss: 0.2243
1696/6530 [======>.......................] - ETA: 1s - loss: 0.3903
1632/6530 [======>.......................] - ETA: 2s - loss: 0.1862
# training | RMSE: 0.2175, MAE: 0.1746
worker 1  xfile  [31, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37487373657267964}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10079398389763844}, 'layer_2_size': 43, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25999363045229895}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.16639324870236827}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.21749469057986529, 'rmse': 0.21749469057986529, 'mae': 0.1745610281530508, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  84 | activation: tanh    | extras: None 
layer 2 | size:  42 | activation: sigmoid | extras: dropout - rate: 29.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f74073727b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:58 - loss: 0.3301
2624/6530 [===========>..................] - ETA: 0s - loss: 0.3154
2112/6530 [========>.....................] - ETA: 2s - loss: 0.1621
 464/6530 [=>............................] - ETA: 6s - loss: 0.2160  
3552/6530 [===============>..............] - ETA: 0s - loss: 0.2751
2624/6530 [===========>..................] - ETA: 1s - loss: 0.1457
 928/6530 [===>..........................] - ETA: 3s - loss: 0.2007
4544/6530 [===================>..........] - ETA: 0s - loss: 0.2509
3136/6530 [=============>................] - ETA: 1s - loss: 0.1336
1408/6530 [=====>........................] - ETA: 2s - loss: 0.1904
5536/6530 [========================>.....] - ETA: 0s - loss: 0.2354
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1248
1872/6530 [=======>......................] - ETA: 1s - loss: 0.1845
6528/6530 [============================>.] - ETA: 0s - loss: 0.2244
4128/6530 [=================>............] - ETA: 0s - loss: 0.1178
2304/6530 [=========>....................] - ETA: 1s - loss: 0.1806
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1123
6530/6530 [==============================] - 1s 131us/step - loss: 0.2244 - val_loss: 0.1868

2784/6530 [===========>..................] - ETA: 1s - loss: 0.1775
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1081
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1744
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1052
3808/6530 [================>.............] - ETA: 0s - loss: 0.1718
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1022
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1704
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1694
6530/6530 [==============================] - 2s 235us/step - loss: 0.1008 - val_loss: 0.2707

5248/6530 [=======================>......] - ETA: 0s - loss: 0.1676
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1659
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1651
6530/6530 [==============================] - 1s 187us/step - loss: 0.1640 - val_loss: 0.1556

# training | RMSE: 0.2240, MAE: 0.1858
worker 2  xfile  [34, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.22397290188520158, 'rmse': 0.22397290188520158, 'mae': 0.18575801871307965, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  60 | activation: relu    | extras: None 
layer 2 | size:  32 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f74069dc7b8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 1:27 - loss: 0.6961
1152/6530 [====>.........................] - ETA: 2s - loss: 0.5325  
2304/6530 [=========>....................] - ETA: 0s - loss: 0.3730
3456/6530 [==============>...............] - ETA: 0s - loss: 0.3072
4512/6530 [===================>..........] - ETA: 0s - loss: 0.2758
5440/6530 [=======================>......] - ETA: 0s - loss: 0.2579
6464/6530 [============================>.] - ETA: 0s - loss: 0.2434
6530/6530 [==============================] - 1s 123us/step - loss: 0.2425 - val_loss: 0.1673

# training | RMSE: 0.1935, MAE: 0.1555
worker 1  xfile  [35, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2968844105931647}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.379665317170057}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.19348232517489047, 'rmse': 0.19348232517489047, 'mae': 0.15553961905879765, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  57 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f74067304e0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 2:45 - loss: 0.6572
 544/6530 [=>............................] - ETA: 5s - loss: 0.6602  
1072/6530 [===>..........................] - ETA: 2s - loss: 0.5641
1632/6530 [======>.......................] - ETA: 1s - loss: 0.4484
2224/6530 [=========>....................] - ETA: 1s - loss: 0.3743
2816/6530 [===========>..................] - ETA: 0s - loss: 0.3297
3440/6530 [==============>...............] - ETA: 0s - loss: 0.2988
4000/6530 [=================>............] - ETA: 0s - loss: 0.2800
4576/6530 [====================>.........] - ETA: 0s - loss: 0.2647
5168/6530 [======================>.......] - ETA: 0s - loss: 0.2533
5696/6530 [=========================>....] - ETA: 0s - loss: 0.2442
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2365
6530/6530 [==============================] - 1s 162us/step - loss: 0.2331 - val_loss: 0.1574

# training | RMSE: 0.2035, MAE: 0.1627
worker 2  xfile  [36, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.203466220305277, 'rmse': 0.203466220305277, 'mae': 0.16269092957024683, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  90 | activation: relu    | extras: dropout - rate: 25.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7406551b38>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 21s - loss: 0.7578
4352/6530 [==================>...........] - ETA: 0s - loss: 0.3705 
6530/6530 [==============================] - 1s 87us/step - loss: 0.3116 - val_loss: 0.1842

# training | RMSE: 0.5242, MAE: 0.4842
worker 0  xfile  [33, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4325646958140229}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4393538692076945}, 'layer_4_size': 46, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 9, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.5242321075875499, 'rmse': 0.5242321075875499, 'mae': 0.48423798182408473, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  19 | activation: sigmoid | extras: None 
layer 2 | size:   7 | activation: relu    | extras: batchnorm 
layer 3 | size:  19 | activation: tanh    | extras: None 
layer 4 | size:  77 | activation: relu    | extras: batchnorm 
layer 5 | size:  42 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f743559ee10>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 48s - loss: 0.6087
1664/6530 [======>.......................] - ETA: 3s - loss: 0.5286 
3328/6530 [==============>...............] - ETA: 1s - loss: 0.3234
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2302
6530/6530 [==============================] - 1s 195us/step - loss: 0.1860 - val_loss: 0.1897

# training | RMSE: 0.1939, MAE: 0.1541
worker 1  xfile  [38, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1857037786421014}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.1939112441802917, 'rmse': 0.1939112441802917, 'mae': 0.15412727361095965, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  75 | activation: tanh    | extras: dropout - rate: 28.9% 
layer 2 | size:  52 | activation: relu    | extras: dropout - rate: 16.3% 
layer 3 | size:  78 | activation: sigmoid | extras: dropout - rate: 41.4% 
layer 4 | size:  96 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f74064747b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:17 - loss: 0.7423
 448/6530 [=>............................] - ETA: 7s - loss: 0.2709  
 896/6530 [===>..........................] - ETA: 3s - loss: 0.2469
1312/6530 [=====>........................] - ETA: 2s - loss: 0.2371
1712/6530 [======>.......................] - ETA: 1s - loss: 0.2254
2128/6530 [========>.....................] - ETA: 1s - loss: 0.2182
# training | RMSE: 0.2298, MAE: 0.1842
worker 2  xfile  [39, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.25136261040079955}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 9, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2297716111453298, 'rmse': 0.2297716111453298, 'mae': 0.18421656170906123, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  94 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  47 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f740636deb8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 31s - loss: 0.3914
2480/6530 [==========>...................] - ETA: 1s - loss: 0.2131
2432/6530 [==========>...................] - ETA: 1s - loss: 0.2003 
2912/6530 [============>.................] - ETA: 1s - loss: 0.2093
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1814
3344/6530 [==============>...............] - ETA: 0s - loss: 0.2056
3776/6530 [================>.............] - ETA: 0s - loss: 0.2023
6530/6530 [==============================] - 1s 129us/step - loss: 0.1755 - val_loss: 0.2182

4192/6530 [==================>...........] - ETA: 0s - loss: 0.1999
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1973
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1952
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1936
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1922
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1913
6530/6530 [==============================] - 1s 216us/step - loss: 0.1913 - val_loss: 0.1780

# training | RMSE: 0.4349, MAE: 0.3829
worker 0  xfile  [37, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 19, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 7, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.434855610265058, 'rmse': 0.434855610265058, 'mae': 0.38285703871615684, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  49 | activation: relu    | extras: dropout - rate: 10.3% 
layer 2 | size:  65 | activation: tanh    | extras: batchnorm 
layer 3 | size:   6 | activation: tanh    | extras: None 
layer 4 | size:  97 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7434cc1c18>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:52 - loss: 1.1294
 256/6530 [>.............................] - ETA: 22s - loss: 0.4684 
 512/6530 [=>............................] - ETA: 11s - loss: 0.2694
 784/6530 [==>...........................] - ETA: 7s - loss: 0.1935 
1072/6530 [===>..........................] - ETA: 5s - loss: 0.1543
1360/6530 [=====>........................] - ETA: 4s - loss: 0.1292
1632/6530 [======>.......................] - ETA: 3s - loss: 0.1130
1888/6530 [=======>......................] - ETA: 3s - loss: 0.1031
2128/6530 [========>.....................] - ETA: 2s - loss: 0.0961
2368/6530 [=========>....................] - ETA: 2s - loss: 0.0898
2608/6530 [==========>...................] - ETA: 2s - loss: 0.0847
2848/6530 [============>.................] - ETA: 1s - loss: 0.0806
3072/6530 [=============>................] - ETA: 1s - loss: 0.0773
3328/6530 [==============>...............] - ETA: 1s - loss: 0.0745
3536/6530 [===============>..............] - ETA: 1s - loss: 0.0723
3760/6530 [================>.............] - ETA: 1s - loss: 0.0702
3984/6530 [=================>............] - ETA: 1s - loss: 0.0682
# training | RMSE: 0.2297, MAE: 0.1772
worker 1  xfile  [40, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.289478959192061}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1631506464809413}, 'layer_2_size': 52, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4142997896037961}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.22972479363363363, 'rmse': 0.22972479363363363, 'mae': 0.17717870280614356, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  17 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7406209f98>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 3:04 - loss: 0.5441
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0663
 560/6530 [=>............................] - ETA: 5s - loss: 0.3746  
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0646
1104/6530 [====>.........................] - ETA: 2s - loss: 0.2572
1648/6530 [======>.......................] - ETA: 1s - loss: 0.1954
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0628
2224/6530 [=========>....................] - ETA: 1s - loss: 0.1602
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0613
# training | RMSE: 0.2752, MAE: 0.2242
worker 2  xfile  [41, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 94, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2420366468676569}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 14, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.27522708780204774, 'rmse': 0.27522708780204774, 'mae': 0.22420943023667986, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  71 | activation: tanh    | extras: None 
layer 2 | size: 100 | activation: sigmoid | extras: dropout - rate: 13.0% 
layer 3 | size:  38 | activation: relu    | extras: batchnorm 
layer 4 | size:  61 | activation: sigmoid | extras: None 
layer 5 | size:  72 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7406181e48>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 20s - loss: 0.3723
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1414
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0598
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2531 
3264/6530 [=============>................] - ETA: 0s - loss: 0.1275
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0589
3840/6530 [================>.............] - ETA: 0s - loss: 0.1165
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0578
6530/6530 [==============================] - 1s 154us/step - loss: 0.2412 - val_loss: 0.2269

4400/6530 [===================>..........] - ETA: 0s - loss: 0.1083
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0566
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1022
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0556
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0971
6432/6530 [============================>.] - ETA: 0s - loss: 0.0547
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0928
6496/6530 [============================>.] - ETA: 0s - loss: 0.0892
6530/6530 [==============================] - 2s 366us/step - loss: 0.0543 - val_loss: 0.0333

6530/6530 [==============================] - 1s 176us/step - loss: 0.0889 - val_loss: 0.0462

# training | RMSE: 0.2780, MAE: 0.2256
worker 2  xfile  [43, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 71, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1296344458219014}, 'layer_2_size': 100, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2779897070458929, 'rmse': 0.2779897070458929, 'mae': 0.22556504214594336, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  96 | activation: tanh    | extras: batchnorm 
layer 2 | size:  33 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7406d6e5f8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 40s - loss: 1.3886
1792/6530 [=======>......................] - ETA: 2s - loss: 0.7181 
3456/6530 [==============>...............] - ETA: 0s - loss: 0.5394
5120/6530 [======================>.......] - ETA: 0s - loss: 0.4266
6530/6530 [==============================] - 1s 171us/step - loss: 0.3629 - val_loss: 0.1099

# training | RMSE: 0.2157, MAE: 0.1745
worker 1  xfile  [44, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4970884277939287}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1087460609141687}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1889310551921254}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.21571329051663193, 'rmse': 0.21571329051663193, 'mae': 0.17445429034054113, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  83 | activation: relu    | extras: batchnorm 
layer 2 | size:  75 | activation: tanh    | extras: None 
layer 3 | size:  44 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f74071ff588>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:22 - loss: 0.7613
 384/6530 [>.............................] - ETA: 12s - loss: 0.4089 
 800/6530 [==>...........................] - ETA: 5s - loss: 0.3137 
1280/6530 [====>.........................] - ETA: 3s - loss: 0.2674
1856/6530 [=======>......................] - ETA: 2s - loss: 0.2376
2432/6530 [==========>...................] - ETA: 1s - loss: 0.2217
3040/6530 [============>.................] - ETA: 1s - loss: 0.2095
3616/6530 [===============>..............] - ETA: 0s - loss: 0.2017
# training | RMSE: 0.1793, MAE: 0.1465
worker 0  xfile  [42, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10289939797168778}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.17928614827272552, 'rmse': 0.17928614827272552, 'mae': 0.14646403361905477, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  52 | activation: tanh    | extras: batchnorm 
layer 2 | size:  94 | activation: relu    | extras: None 
layer 3 | size:  32 | activation: relu    | extras: dropout - rate: 20.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7435405278>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:23 - loss: 2.5354
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1948
 320/6530 [>.............................] - ETA: 16s - loss: 0.6746 
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1895
 608/6530 [=>............................] - ETA: 8s - loss: 0.4186 
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1836
 896/6530 [===>..........................] - ETA: 5s - loss: 0.3095
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1795
1184/6530 [====>.........................] - ETA: 4s - loss: 0.2533
6528/6530 [============================>.] - ETA: 0s - loss: 0.1771
1504/6530 [=====>........................] - ETA: 3s - loss: 0.2139
1792/6530 [=======>......................] - ETA: 2s - loss: 0.1898
6530/6530 [==============================] - 1s 217us/step - loss: 0.1771 - val_loss: 0.1314

2064/6530 [========>.....................] - ETA: 2s - loss: 0.1723
2368/6530 [=========>....................] - ETA: 2s - loss: 0.1581
2672/6530 [===========>..................] - ETA: 1s - loss: 0.1474
2976/6530 [============>.................] - ETA: 1s - loss: 0.1374
3264/6530 [=============>................] - ETA: 1s - loss: 0.1292
3568/6530 [===============>..............] - ETA: 1s - loss: 0.1226
3872/6530 [================>.............] - ETA: 1s - loss: 0.1165
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1111
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1069
# training | RMSE: 0.3316, MAE: 0.2651
worker 2  xfile  [45, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.33155191092154523, 'rmse': 0.33155191092154523, 'mae': 0.2650666451777703, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  27 | activation: tanh    | extras: dropout - rate: 17.1% 
layer 2 | size:   7 | activation: relu    | extras: None 
layer 3 | size:  93 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7406d6ee10>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 15s - loss: 0.4826
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1025
6400/6530 [============================>.] - ETA: 0s - loss: 0.1793 
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0984
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0943
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0912
6530/6530 [==============================] - 1s 129us/step - loss: 0.1772 - val_loss: 0.0648

6208/6530 [===========================>..] - ETA: 0s - loss: 0.0886
6530/6530 [==============================] - 2s 306us/step - loss: 0.0863 - val_loss: 0.0302

# training | RMSE: 0.1794, MAE: 0.1388
worker 1  xfile  [46, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3338165530239823}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.17941231519464723, 'rmse': 0.17941231519464723, 'mae': 0.13877326622699315, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  17 | activation: relu    | extras: batchnorm 
layer 2 | size:  77 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7405e16358>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 34s - loss: 1.0758
2432/6530 [==========>...................] - ETA: 1s - loss: 0.5218 
4608/6530 [====================>.........] - ETA: 0s - loss: 0.3346
6530/6530 [==============================] - 1s 143us/step - loss: 0.2618 - val_loss: 0.0776

# training | RMSE: 0.2654, MAE: 0.2136
worker 2  xfile  [48, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17140315668636932}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 7, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17885634358977823}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.2653573533576449, 'rmse': 0.2653573533576449, 'mae': 0.21358090259756277, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  12 | activation: tanh    | extras: batchnorm 
layer 2 | size:  51 | activation: relu    | extras: dropout - rate: 37.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f74052c0390>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:13 - loss: 0.6707
1152/6530 [====>.........................] - ETA: 3s - loss: 0.4189  
2240/6530 [=========>....................] - ETA: 1s - loss: 0.3242
3456/6530 [==============>...............] - ETA: 0s - loss: 0.2793
4544/6530 [===================>..........] - ETA: 0s - loss: 0.2551
5440/6530 [=======================>......] - ETA: 0s - loss: 0.2428
6400/6530 [============================>.] - ETA: 0s - loss: 0.2326
6530/6530 [==============================] - 1s 180us/step - loss: 0.2311 - val_loss: 0.1794

# training | RMSE: 0.1638, MAE: 0.1286
worker 0  xfile  [47, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20261337059472792}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10963218549386174}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.16376715862711824, 'rmse': 0.16376715862711824, 'mae': 0.1286197792270614, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  60 | activation: tanh    | extras: dropout - rate: 28.6% 
layer 2 | size:  13 | activation: sigmoid | extras: batchnorm 
layer 3 | size:   9 | activation: relu    | extras: dropout - rate: 37.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7434ef0fd0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 42s - loss: 0.7249
1920/6530 [=======>......................] - ETA: 2s - loss: 0.6946 
3712/6530 [================>.............] - ETA: 0s - loss: 0.6650
5760/6530 [=========================>....] - ETA: 0s - loss: 0.6187
# training | RMSE: 0.2716, MAE: 0.2161
worker 1  xfile  [49, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 44, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.2716068847687774, 'rmse': 0.2716068847687774, 'mae': 0.21612191252892615, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  62 | activation: tanh    | extras: dropout - rate: 46.1% 
layer 2 | size:  73 | activation: sigmoid | extras: None 
layer 3 | size:  92 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7405e16208>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:00 - loss: 0.6646
1600/6530 [======>.......................] - ETA: 2s - loss: 0.2472  
6530/6530 [==============================] - 1s 174us/step - loss: 0.5983 - val_loss: 0.4701

3200/6530 [=============>................] - ETA: 0s - loss: 0.2248
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2116
6530/6530 [==============================] - 1s 135us/step - loss: 0.2033 - val_loss: 0.1673

# training | RMSE: 0.5420, MAE: 0.4698
worker 0  xfile  [51, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28558516419712654}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3722775565240588}, 'layer_3_size': 9, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4108479697027453}, 'layer_5_size': 100, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.5420126767232246, 'rmse': 0.5420126767232246, 'mae': 0.4697713594392751, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  13 | activation: relu    | extras: None 
layer 2 | size:  29 | activation: tanh    | extras: dropout - rate: 21.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7434322390>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 4:25 - loss: 1.5539
# training | RMSE: 0.2142, MAE: 0.1715
worker 2  xfile  [50, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37641506330630803}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2142235975093015, 'rmse': 0.2142235975093015, 'mae': 0.17152508405886077, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  33 | activation: relu    | extras: batchnorm 
layer 2 | size:  34 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7405788fd0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:07 - loss: 0.4699
 352/6530 [>.............................] - ETA: 12s - loss: 1.1926 
 272/6530 [>.............................] - ETA: 21s - loss: 0.4457 
 784/6530 [==>...........................] - ETA: 5s - loss: 0.9130 
 560/6530 [=>............................] - ETA: 10s - loss: 0.3273
1216/6530 [====>.........................] - ETA: 3s - loss: 0.7569
 848/6530 [==>...........................] - ETA: 7s - loss: 0.2506 
1680/6530 [======>.......................] - ETA: 2s - loss: 0.6553
1152/6530 [====>.........................] - ETA: 5s - loss: 0.2030
2192/6530 [=========>....................] - ETA: 1s - loss: 0.5770
1488/6530 [=====>........................] - ETA: 3s - loss: 0.1680
2656/6530 [===========>..................] - ETA: 1s - loss: 0.5280
1776/6530 [=======>......................] - ETA: 3s - loss: 0.1476
3120/6530 [=============>................] - ETA: 1s - loss: 0.4914
2080/6530 [========>.....................] - ETA: 2s - loss: 0.1332
3600/6530 [===============>..............] - ETA: 0s - loss: 0.4600
2400/6530 [==========>...................] - ETA: 2s - loss: 0.1225
4096/6530 [=================>............] - ETA: 0s - loss: 0.4319
2720/6530 [===========>..................] - ETA: 1s - loss: 0.1130
4592/6530 [====================>.........] - ETA: 0s - loss: 0.4117
3040/6530 [============>.................] - ETA: 1s - loss: 0.1044
5104/6530 [======================>.......] - ETA: 0s - loss: 0.3924
3376/6530 [==============>...............] - ETA: 1s - loss: 0.0974
5632/6530 [========================>.....] - ETA: 0s - loss: 0.3754
3712/6530 [================>.............] - ETA: 1s - loss: 0.0914
6128/6530 [===========================>..] - ETA: 0s - loss: 0.3619
4032/6530 [=================>............] - ETA: 0s - loss: 0.0868
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0826
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0794
6530/6530 [==============================] - 1s 225us/step - loss: 0.3522 - val_loss: 0.2021

4992/6530 [=====================>........] - ETA: 0s - loss: 0.0756
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0726
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0699
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0673
6320/6530 [============================>.] - ETA: 0s - loss: 0.0651
# training | RMSE: 0.2075, MAE: 0.1679
worker 1  xfile  [52, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46107233114729274}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10807125705702347}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.20750072965134064, 'rmse': 0.20750072965134064, 'mae': 0.16789709904726055, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  36 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  25 | activation: tanh    | extras: batchnorm 
layer 3 | size:  37 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7405479d30>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 49s - loss: 1.5542
1792/6530 [=======>......................] - ETA: 2s - loss: 0.9687 
6530/6530 [==============================] - 2s 322us/step - loss: 0.0637 - val_loss: 0.0166

3584/6530 [===============>..............] - ETA: 0s - loss: 0.6353
5376/6530 [=======================>......] - ETA: 0s - loss: 0.4952
6530/6530 [==============================] - 1s 198us/step - loss: 0.4415 - val_loss: 0.2045

# training | RMSE: 0.2530, MAE: 0.1997
worker 0  xfile  [54, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21759894630158141}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30985642917020173}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2530325565869371, 'rmse': 0.2530325565869371, 'mae': 0.1997325377097193, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  17 | activation: sigmoid | extras: None 
layer 2 | size:  80 | activation: tanh    | extras: None 
layer 3 | size:  60 | activation: sigmoid | extras: None 
layer 4 | size:   4 | activation: tanh    | extras: dropout - rate: 35.5% 
layer 5 | size:  37 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7407e107b8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 36s - loss: 0.7370
2688/6530 [===========>..................] - ETA: 1s - loss: 0.4584 
5504/6530 [========================>.....] - ETA: 0s - loss: 0.3534
6530/6530 [==============================] - 1s 151us/step - loss: 0.3352 - val_loss: 0.2578

# training | RMSE: 0.1215, MAE: 0.0938
worker 2  xfile  [53, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44587267964897137}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.12146222348912904, 'rmse': 0.12146222348912904, 'mae': 0.09379897876890517, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  68 | activation: relu    | extras: batchnorm 
layer 2 | size:   4 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f74048e0a58>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:40 - loss: 0.4341
 608/6530 [=>............................] - ETA: 8s - loss: 0.3132  
1248/6530 [====>.........................] - ETA: 3s - loss: 0.2574
1952/6530 [=======>......................] - ETA: 2s - loss: 0.2038
2592/6530 [==========>...................] - ETA: 1s - loss: 0.1728
3232/6530 [=============>................] - ETA: 1s - loss: 0.1496
3776/6530 [================>.............] - ETA: 0s - loss: 0.1353
# training | RMSE: 0.2578, MAE: 0.2050
worker 1  xfile  [55, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 36, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 25, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 37, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10338216607501166}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2578016958577526, 'rmse': 0.2578016958577526, 'mae': 0.20504840873011648, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:   8 | activation: tanh    | extras: batchnorm 
layer 2 | size:  38 | activation: relu    | extras: dropout - rate: 32.5% 
layer 3 | size:   5 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7405219630>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 49s - loss: 0.6089
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1222
2176/6530 [========>.....................] - ETA: 2s - loss: 0.5660 
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1117
4352/6530 [==================>...........] - ETA: 0s - loss: 0.4221
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1027
6528/6530 [============================>.] - ETA: 0s - loss: 0.3014
6530/6530 [==============================] - 1s 195us/step - loss: 0.3013 - val_loss: 0.0529

6530/6530 [==============================] - 1s 218us/step - loss: 0.0965 - val_loss: 0.0383

# training | RMSE: 0.3133, MAE: 0.2530
worker 0  xfile  [56, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3549833662281482}, 'layer_4_size': 4, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.31334830111756656, 'rmse': 0.31334830111756656, 'mae': 0.25302458231378955, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  48 | activation: relu    | extras: dropout - rate: 28.4% 
layer 2 | size:  49 | activation: sigmoid | extras: dropout - rate: 42.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7407c646d8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 44s - loss: 1.0608
3328/6530 [==============>...............] - ETA: 0s - loss: 0.3193 
6530/6530 [==============================] - 1s 167us/step - loss: 0.2001 - val_loss: 0.0579

# training | RMSE: 0.1923, MAE: 0.1517
worker 2  xfile  [58, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20852464738421236}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.19228274500827108, 'rmse': 0.19228274500827108, 'mae': 0.1516912434366689, 'early_stop': False}
{'batch_size': 128,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  35 | activation: relu    | extras: dropout - rate: 36.2% 
layer 2 | size:  78 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f74048e0860>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 36s - loss: 0.5826
3328/6530 [==============>...............] - ETA: 0s - loss: 0.2576 
6530/6530 [==============================] - 1s 143us/step - loss: 0.2357 - val_loss: 0.2113

# training | RMSE: 0.2205, MAE: 0.1744
worker 1  xfile  [57, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 8, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.32477920284882056}, 'layer_2_size': 38, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4711066283762235}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.2205200258328313, 'rmse': 0.2205200258328313, 'mae': 0.17442025063807803, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  46 | activation: relu    | extras: dropout - rate: 48.5% 
layer 2 | size:  43 | activation: relu    | extras: None 
layer 3 | size:  10 | activation: sigmoid | extras: batchnorm 
layer 4 | size:  83 | activation: tanh    | extras: dropout - rate: 16.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7404c16128>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:14 - loss: 0.6577
 272/6530 [>.............................] - ETA: 22s - loss: 0.6236 
 560/6530 [=>............................] - ETA: 10s - loss: 0.4757
 832/6530 [==>...........................] - ETA: 7s - loss: 0.3924 
1104/6530 [====>.........................] - ETA: 5s - loss: 0.3468
1344/6530 [=====>........................] - ETA: 4s - loss: 0.3168
# training | RMSE: 0.2380, MAE: 0.1945
worker 0  xfile  [59, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28393702145126665}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4273360035000433}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 63, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.23800536485265436, 'rmse': 0.23800536485265436, 'mae': 0.19452825998680257, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  98 | activation: sigmoid | extras: None 
layer 2 | size:  78 | activation: relu    | extras: dropout - rate: 22.6% 
layer 3 | size:  70 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f74078a4eb8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 39s - loss: 0.0963
1600/6530 [======>.......................] - ETA: 3s - loss: 0.2954
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0753 
1888/6530 [=======>......................] - ETA: 3s - loss: 0.2780
2224/6530 [=========>....................] - ETA: 2s - loss: 0.2639
2560/6530 [==========>...................] - ETA: 2s - loss: 0.2537
2864/6530 [============>.................] - ETA: 1s - loss: 0.2457
3104/6530 [=============>................] - ETA: 1s - loss: 0.2393
6530/6530 [==============================] - 1s 163us/step - loss: 0.0661 - val_loss: 0.0772

3328/6530 [==============>...............] - ETA: 1s - loss: 0.2347
3568/6530 [===============>..............] - ETA: 1s - loss: 0.2319
3872/6530 [================>.............] - ETA: 1s - loss: 0.2271
4176/6530 [==================>...........] - ETA: 0s - loss: 0.2236
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2217
4784/6530 [====================>.........] - ETA: 0s - loss: 0.2188
5088/6530 [======================>.......] - ETA: 0s - loss: 0.2159
5392/6530 [=======================>......] - ETA: 0s - loss: 0.2134
5728/6530 [=========================>....] - ETA: 0s - loss: 0.2103
6048/6530 [==========================>...] - ETA: 0s - loss: 0.2087
6384/6530 [============================>.] - ETA: 0s - loss: 0.2066
6530/6530 [==============================] - 2s 340us/step - loss: 0.2057 - val_loss: 0.2005

# training | RMSE: 0.2605, MAE: 0.2132
worker 2  xfile  [61, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36181969856349394}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45648493186748806}, 'layer_4_size': 48, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2604695798720141, 'rmse': 0.2604695798720141, 'mae': 0.21317823246040196, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  36 | activation: tanh    | extras: None 
layer 2 | size:  81 | activation: sigmoid | extras: dropout - rate: 31.0% 
layer 3 | size:  66 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7404517198>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 22s - loss: 0.5006
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1774 
6530/6530 [==============================] - 1s 164us/step - loss: 0.1655 - val_loss: 0.0853

# training | RMSE: 0.2762, MAE: 0.2285
worker 0  xfile  [62, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 98, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22558419861507129}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.25144933285848337}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2761632083847889, 'rmse': 0.2761632083847889, 'mae': 0.2284758487127777, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  80 | activation: relu    | extras: dropout - rate: 24.4% 
layer 2 | size:  42 | activation: sigmoid | extras: dropout - rate: 13.7% 
layer 3 | size:  60 | activation: relu    | extras: None 
layer 4 | size:  78 | activation: sigmoid | extras: dropout - rate: 38.3% 
layer 5 | size:  98 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7435b11630>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 23s - loss: 0.5573
4096/6530 [=================>............] - ETA: 0s - loss: 0.4438 
6530/6530 [==============================] - 1s 191us/step - loss: 0.4099 - val_loss: 0.2514

# training | RMSE: 0.2399, MAE: 0.1990
worker 1  xfile  [60, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.48537797765244817}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.162244625320774}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.23994867654155963, 'rmse': 0.23994867654155963, 'mae': 0.19902215218565492, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  64 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f74041feba8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 17s - loss: 0.7157
5888/6530 [==========================>...] - ETA: 0s - loss: 0.6331 
6530/6530 [==============================] - 1s 137us/step - loss: 0.6206 - val_loss: 0.4887

# training | RMSE: 0.2969, MAE: 0.2440
worker 2  xfile  [63, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.30975404693021946}, 'layer_2_size': 81, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35525074782080623}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2969436236765289, 'rmse': 0.2969436236765289, 'mae': 0.24399252143202824, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  45 | activation: relu    | extras: None 
layer 2 | size:  94 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  80 | activation: tanh    | extras: batchnorm 
layer 4 | size:  77 | activation: sigmoid | extras: dropout - rate: 14.0% 
layer 5 | size:  46 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7405428dd8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 28s - loss: 0.6402
3328/6530 [==============>...............] - ETA: 1s - loss: 0.1236 
# training | RMSE: 0.3048, MAE: 0.2484
worker 0  xfile  [64, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2443185679023121}, 'layer_1_size': 80, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13727119846581481}, 'layer_2_size': 42, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.38348129314100576}, 'layer_4_size': 78, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.3048061866646423, 'rmse': 0.3048061866646423, 'mae': 0.2483666817509191, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:   3 | activation: sigmoid | extras: None 
layer 2 | size:  55 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7406ff1358>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:19 - loss: 2.5838
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0826
2112/6530 [========>.....................] - ETA: 1s - loss: 0.3106  
3904/6530 [================>.............] - ETA: 0s - loss: 0.2035
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1628
6530/6530 [==============================] - 1s 217us/step - loss: 0.0796 - val_loss: 0.0516

6530/6530 [==============================] - 1s 167us/step - loss: 0.1485 - val_loss: 0.0658

# training | RMSE: 0.5432, MAE: 0.4810
worker 1  xfile  [65, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15256004882486446}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43706453581122084}, 'layer_3_size': 21, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3149594268408259}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35245294857301446}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.5432408153905975, 'rmse': 0.5432408153905975, 'mae': 0.4810232272629935, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  58 | activation: sigmoid | extras: dropout - rate: 34.4% 
layer 2 | size:  85 | activation: relu    | extras: dropout - rate: 26.0% 
layer 3 | size:  79 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f74041fe9e8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 21s - loss: 0.4789
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1785 
6530/6530 [==============================] - 1s 161us/step - loss: 0.1574 - val_loss: 0.0752

# training | RMSE: 0.2195, MAE: 0.1812
worker 2  xfile  [66, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 45, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.14045255114533833}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2194670829609276, 'rmse': 0.2194670829609276, 'mae': 0.18118711056415823, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  68 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7405428a20>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:36 - loss: 0.7104
 352/6530 [>.............................] - ETA: 15s - loss: 0.5641 
 800/6530 [==>...........................] - ETA: 6s - loss: 0.3532 
1296/6530 [====>.........................] - ETA: 3s - loss: 0.2452
1776/6530 [=======>......................] - ETA: 2s - loss: 0.1914
2288/6530 [=========>....................] - ETA: 2s - loss: 0.1579
# training | RMSE: 0.2594, MAE: 0.2100
worker 0  xfile  [67, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3378174053984615}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.25939941833429264, 'rmse': 0.25939941833429264, 'mae': 0.2100130397244522, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  53 | activation: sigmoid | extras: None 
layer 2 | size:  45 | activation: sigmoid | extras: dropout - rate: 39.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7406e1ef98>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:01 - loss: 0.2125
2816/6530 [===========>..................] - ETA: 1s - loss: 0.1364
 992/6530 [===>..........................] - ETA: 5s - loss: 0.1915  
3344/6530 [==============>...............] - ETA: 1s - loss: 0.1218
1984/6530 [========>.....................] - ETA: 2s - loss: 0.1837
3856/6530 [================>.............] - ETA: 0s - loss: 0.1124
3008/6530 [============>.................] - ETA: 1s - loss: 0.1741
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1045
4000/6530 [=================>............] - ETA: 0s - loss: 0.1705
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0977
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1694
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0925
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1678
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0882
6448/6530 [============================>.] - ETA: 0s - loss: 0.0844
# training | RMSE: 0.2753, MAE: 0.2226
worker 1  xfile  [68, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3441548869484395}, 'layer_1_size': 58, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.25971825110197183}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27662952086894604}, 'layer_4_size': 79, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.27526128321252674, 'rmse': 0.27526128321252674, 'mae': 0.22259993249369067, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  27 | activation: tanh    | extras: dropout - rate: 26.0% 
layer 2 | size:  27 | activation: relu    | extras: dropout - rate: 21.2% 
layer 3 | size:  54 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7404063358>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:03 - loss: 1.7404
6530/6530 [==============================] - 1s 208us/step - loss: 0.1667 - val_loss: 0.1587

 320/6530 [>.............................] - ETA: 18s - loss: 0.5580 
6530/6530 [==============================] - 2s 251us/step - loss: 0.0839 - val_loss: 0.0507

 672/6530 [==>...........................] - ETA: 8s - loss: 0.3465 
1008/6530 [===>..........................] - ETA: 5s - loss: 0.2777
1328/6530 [=====>........................] - ETA: 4s - loss: 0.2409
1616/6530 [======>.......................] - ETA: 3s - loss: 0.2180
1936/6530 [=======>......................] - ETA: 2s - loss: 0.2000
2192/6530 [=========>....................] - ETA: 2s - loss: 0.1869
2480/6530 [==========>...................] - ETA: 2s - loss: 0.1742
2816/6530 [===========>..................] - ETA: 1s - loss: 0.1632
3168/6530 [=============>................] - ETA: 1s - loss: 0.1541
3520/6530 [===============>..............] - ETA: 1s - loss: 0.1455
3872/6530 [================>.............] - ETA: 1s - loss: 0.1379
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1319
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1269
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1226
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1178
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1141
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1108
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1079
6528/6530 [============================>.] - ETA: 0s - loss: 0.1054
6530/6530 [==============================] - 2s 316us/step - loss: 0.1054 - val_loss: 0.0423

# training | RMSE: 0.1989, MAE: 0.1584
worker 0  xfile  [70, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39459061959382125}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.360442390830502}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32952669768377524}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.19889448432740472, 'rmse': 0.19889448432740472, 'mae': 0.15844496845395153, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:   7 | activation: relu    | extras: dropout - rate: 35.1% 
layer 2 | size:  86 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7406e1ee48>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 44s - loss: 0.7398
2944/6530 [============>.................] - ETA: 1s - loss: 0.4711 
6016/6530 [==========================>...] - ETA: 0s - loss: 0.3735
# training | RMSE: 0.2226, MAE: 0.1747
worker 2  xfile  [69, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 87, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3527086162194488}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.22263636402397796, 'rmse': 0.22263636402397796, 'mae': 0.17466156788068726, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  64 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7405428a58>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 3:03 - loss: 1.2037
 896/6530 [===>..........................] - ETA: 5s - loss: 0.8285  
6530/6530 [==============================] - 1s 170us/step - loss: 0.3637 - val_loss: 0.2067

1920/6530 [=======>......................] - ETA: 2s - loss: 0.6138
3008/6530 [============>.................] - ETA: 1s - loss: 0.4695
4192/6530 [==================>...........] - ETA: 0s - loss: 0.3930
5376/6530 [=======================>......] - ETA: 0s - loss: 0.3503
6530/6530 [==============================] - 1s 204us/step - loss: 0.3214 - val_loss: 0.1874

# training | RMSE: 0.2609, MAE: 0.2098
worker 0  xfile  [72, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35135548822941043}, 'layer_1_size': 7, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 81, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2609195428653303, 'rmse': 0.2609195428653303, 'mae': 0.20979038882269777, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  87 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f74069e0d30>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 5:38 - loss: 0.6089
 320/6530 [>.............................] - ETA: 17s - loss: 0.2766 
 672/6530 [==>...........................] - ETA: 8s - loss: 0.2400 
1024/6530 [===>..........................] - ETA: 5s - loss: 0.2259
1408/6530 [=====>........................] - ETA: 3s - loss: 0.2174
1808/6530 [=======>......................] - ETA: 2s - loss: 0.2090
2224/6530 [=========>....................] - ETA: 2s - loss: 0.2050
# training | RMSE: 0.2044, MAE: 0.1630
worker 1  xfile  [71, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2597483729703344}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.211692527289565}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.20441566491974772, 'rmse': 0.20441566491974772, 'mae': 0.16301288680630374, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  28 | activation: relu    | extras: dropout - rate: 38.5% 
layer 2 | size:  72 | activation: tanh    | extras: batchnorm 
layer 3 | size:  11 | activation: tanh    | extras: None 
layer 4 | size:  42 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f73ffec5e48>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 128/6530 [..............................] - ETA: 55s - loss: 1.6730
2624/6530 [===========>..................] - ETA: 1s - loss: 0.2006
1792/6530 [=======>......................] - ETA: 3s - loss: 1.4762 
3072/6530 [=============>................] - ETA: 1s - loss: 0.1963
3712/6530 [================>.............] - ETA: 0s - loss: 1.2596
3488/6530 [===============>..............] - ETA: 1s - loss: 0.1939
5632/6530 [========================>.....] - ETA: 0s - loss: 1.0426
3984/6530 [=================>............] - ETA: 0s - loss: 0.1911
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1877
# training | RMSE: 0.2378, MAE: 0.1884
worker 2  xfile  [73, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 34, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.23778209511594084, 'rmse': 0.23778209511594084, 'mae': 0.18835291786998254, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  93 | activation: relu    | extras: dropout - rate: 21.8% 
layer 2 | size:  23 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f73ff910390>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  16/6530 [..............................] - ETA: 6:06 - loss: 0.5330
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1858
 432/6530 [>.............................] - ETA: 13s - loss: 0.4151 
6530/6530 [==============================] - 1s 220us/step - loss: 0.9474 - val_loss: 0.2580

5504/6530 [========================>.....] - ETA: 0s - loss: 0.1833
 832/6530 [==>...........................] - ETA: 6s - loss: 0.3873 
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1813
1232/6530 [====>.........................] - ETA: 4s - loss: 0.3652
6528/6530 [============================>.] - ETA: 0s - loss: 0.1796
1632/6530 [======>.......................] - ETA: 3s - loss: 0.3491
2064/6530 [========>.....................] - ETA: 2s - loss: 0.3302
2464/6530 [==========>...................] - ETA: 1s - loss: 0.3228
6530/6530 [==============================] - 2s 266us/step - loss: 0.1796 - val_loss: 0.1623

2880/6530 [============>.................] - ETA: 1s - loss: 0.3157
3280/6530 [==============>...............] - ETA: 1s - loss: 0.3081
3664/6530 [===============>..............] - ETA: 1s - loss: 0.3013
4096/6530 [=================>............] - ETA: 0s - loss: 0.2931
4528/6530 [===================>..........] - ETA: 0s - loss: 0.2866
4912/6530 [=====================>........] - ETA: 0s - loss: 0.2817
5328/6530 [=======================>......] - ETA: 0s - loss: 0.2758
5696/6530 [=========================>....] - ETA: 0s - loss: 0.2719
6064/6530 [==========================>...] - ETA: 0s - loss: 0.2684
6448/6530 [============================>.] - ETA: 0s - loss: 0.2642
6530/6530 [==============================] - 2s 288us/step - loss: 0.2636 - val_loss: 0.2015

# training | RMSE: 0.3335, MAE: 0.2617
worker 1  xfile  [74, 1.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3846968218925758}, 'layer_1_size': 28, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 72, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 11, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 42, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.33346308371231076, 'rmse': 0.33346308371231076, 'mae': 0.2616693440611545, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  13 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f73ff3f8400>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 1:27 - loss: 0.5682
1472/6530 [=====>........................] - ETA: 3s - loss: 0.5212  
3200/6530 [=============>................] - ETA: 1s - loss: 0.3395
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2190
6530/6530 [==============================] - 1s 178us/step - loss: 0.1876 - val_loss: 0.0421

# training | RMSE: 0.2066, MAE: 0.1611
worker 0  xfile  [75, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14970703358753235}, 'layer_3_size': 51, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.479652463853537}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2066007020077189, 'rmse': 0.2066007020077189, 'mae': 0.16111057142201352, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  67 | activation: sigmoid | extras: dropout - rate: 25.7% 
layer 2 | size:  82 | activation: tanh    | extras: dropout - rate: 28.4% 
layer 3 | size:  76 | activation: tanh    | extras: dropout - rate: 47.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f74069e0b38>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/1

 256/6530 [>.............................] - ETA: 23s - loss: 0.5313
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1507 
6530/6530 [==============================] - 1s 176us/step - loss: 0.1345 - val_loss: 0.0702

# training | RMSE: 0.2660, MAE: 0.2173
worker 0  xfile  [78, 1.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2565654251079069}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28405862462018816}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4723512202998946}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.40650904959016443}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.26599660433603306, 'rmse': 0.26599660433603306, 'mae': 0.2173151185102597, 'early_stop': False}
vggnet done  0

# training | RMSE: 0.2406, MAE: 0.1986
worker 2  xfile  [76, 1.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21797867815066485}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4656804979841388}, 'layer_4_size': 43, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.24055626064246932, 'rmse': 0.24055626064246932, 'mae': 0.1985522556903441, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  23 | activation: tanh    | extras: dropout - rate: 38.3% 
layer 2 | size:  14 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f73ff7706a0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  32/6530 [..............................] - ETA: 2:59 - loss: 0.8513
 864/6530 [==>...........................] - ETA: 6s - loss: 0.7923  
1760/6530 [=======>......................] - ETA: 2s - loss: 0.6750
2720/6530 [===========>..................] - ETA: 1s - loss: 0.5884
3712/6530 [================>.............] - ETA: 0s - loss: 0.5207
4672/6530 [====================>.........] - ETA: 0s - loss: 0.4702
5536/6530 [========================>.....] - ETA: 0s - loss: 0.4361
6528/6530 [============================>.] - ETA: 0s - loss: 0.4047
6530/6530 [==============================] - 1s 211us/step - loss: 0.4047 - val_loss: 0.1672

# training | RMSE: 0.2099, MAE: 0.1649
worker 2  xfile  [79, 1.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38345276572686826}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.43851058729984005}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2098852580366011, 'rmse': 0.2098852580366011, 'mae': 0.16492021702112905, 'early_stop': False}
vggnet done  2

# training | RMSE: 0.2050, MAE: 0.1643
worker 1  xfile  [77, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1935168503165503}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44411323388230295}, 'layer_4_size': 34, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.20497731880572678, 'rmse': 0.20497731880572678, 'mae': 0.16431684987358292, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  70 | activation: relu    | extras: batchnorm 
layer 2 | size:  42 | activation: sigmoid | extras: dropout - rate: 25.4% 
layer 3 | size:  95 | activation: tanh    | extras: batchnorm 
layer 4 | size:  88 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f73ff3f82e8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/1

  64/6530 [..............................] - ETA: 2:04 - loss: 2.1341
 768/6530 [==>...........................] - ETA: 9s - loss: 1.1955  
1472/6530 [=====>........................] - ETA: 4s - loss: 0.7087
2240/6530 [=========>....................] - ETA: 2s - loss: 0.4915
3008/6530 [============>.................] - ETA: 1s - loss: 0.3861
3712/6530 [================>.............] - ETA: 1s - loss: 0.3262
4416/6530 [===================>..........] - ETA: 0s - loss: 0.2835
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2532
5824/6530 [=========================>....] - ETA: 0s - loss: 0.2297
6464/6530 [============================>.] - ETA: 0s - loss: 0.2126
6530/6530 [==============================] - 2s 294us/step - loss: 0.2109 - val_loss: 0.0398

# training | RMSE: 0.2006, MAE: 0.1627
worker 1  xfile  [80, 1.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 70, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2538091909685959}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.20064087418926235, 'rmse': 0.20064087418926235, 'mae': 0.1627302823809652, 'early_stop': False}
vggnet done  1
all of workers have been done
calculation finished
#1 epoch=1.0 loss={'loss': 0.24453143936765454, 'rmse': 0.24453143936765454, 'mae': 0.20536672667253278, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23753366920597196}, 'layer_1_size': 67, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4633799077258097}, 'layer_3_size': 71, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#0 epoch=1.0 loss={'loss': 0.22981884488023502, 'rmse': 0.22981884488023502, 'mae': 0.180810010930137, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.47802181397194043}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#3 epoch=1.0 loss={'loss': 0.3015827944873194, 'rmse': 0.3015827944873194, 'mae': 0.23539133187034625, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23702195861259667}, 'layer_1_size': 68, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.29478758241313924}, 'layer_3_size': 9, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.33428827796759364}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
#2 epoch=1.0 loss={'loss': 0.17978818761743376, 'rmse': 0.17978818761743376, 'mae': 0.13468790320719431, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16999145104470526}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#6 epoch=1.0 loss={'loss': 0.2703680996932272, 'rmse': 0.2703680996932272, 'mae': 0.2192846483204795, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39425337615491773}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 43, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 100, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#4 epoch=1.0 loss={'loss': 0.4592967256295669, 'rmse': 0.4592967256295669, 'mae': 0.3940162329913608, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.282863249502904}, 'layer_3_size': 26, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 28, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2847538518782967}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#5 epoch=1.0 loss={'loss': 0.19013196311052108, 'rmse': 0.19013196311052108, 'mae': 0.15021072128630594, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2035119362983565}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48594345397869476}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#7 epoch=1.0 loss={'loss': 0.20796408893906987, 'rmse': 0.20796408893906987, 'mae': 0.16689927612437072, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#8 epoch=1.0 loss={'loss': 0.34189232278098086, 'rmse': 0.34189232278098086, 'mae': 0.29742592207285884, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 45, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 48, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#10 epoch=1.0 loss={'loss': 0.24736361295002976, 'rmse': 0.24736361295002976, 'mae': 0.19292735823140877, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 62, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 17, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': True}
#9 epoch=1.0 loss={'loss': 0.18434819192210603, 'rmse': 0.18434819192210603, 'mae': 0.14640747054517594, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#11 epoch=1.0 loss={'loss': 0.23208593753600998, 'rmse': 0.23208593753600998, 'mae': 0.17614457473271905, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.31824595404261413}, 'layer_3_size': 68, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 39, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 26, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#12 epoch=1.0 loss={'loss': 0.3271318448385201, 'rmse': 0.3271318448385201, 'mae': 0.2710531795741368, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.231036751147615}, 'layer_2_size': 17, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12403564418814238}, 'layer_4_size': 87, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#13 epoch=1.0 loss={'loss': 0.30402245211034745, 'rmse': 0.30402245211034745, 'mae': 0.2791034556288956, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 99, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18902062498118097}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 71, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.39060064397063343}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#14 epoch=1.0 loss={'loss': 0.1629704997309757, 'rmse': 0.1629704997309757, 'mae': 0.1232071954065642, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10996334638100778}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10952735483869187}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4326994328446677}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#17 epoch=1.0 loss={'loss': 0.6719629293368998, 'rmse': 0.6719629293368998, 'mae': 0.6204985710394874, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 56, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10940858235669326}, 'layer_2_size': 28, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 9, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#15 epoch=1.0 loss={'loss': 0.19389121898355996, 'rmse': 0.19389121898355996, 'mae': 0.15437244459142524, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22735716747727375}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 88, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 75, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#19 epoch=1.0 loss={'loss': 0.2305439993492947, 'rmse': 0.2305439993492947, 'mae': 0.18408498695680903, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.23773996701446284}, 'layer_1_size': 13, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 71, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.32554561515669345}, 'layer_4_size': 30, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.42334300316688633}, 'layer_5_size': 94, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#18 epoch=1.0 loss={'loss': 0.2029772657212135, 'rmse': 0.2029772657212135, 'mae': 0.15950116535783682, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1234109170669142}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.34670527040571764}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2799181323299549}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#16 epoch=1.0 loss={'loss': 0.22840838859683818, 'rmse': 0.22840838859683818, 'mae': 0.18726963174741976, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1576889091634152}, 'layer_2_size': 5, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30663997953070404}, 'layer_3_size': 87, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.25057276661770356}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 96, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#21 epoch=1.0 loss={'loss': 0.5582240750994912, 'rmse': 0.5582240750994912, 'mae': 0.494894945279168, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 85, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38569931652089273}, 'layer_3_size': 68, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44153406687369323}, 'layer_4_size': 60, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18547798882111516}, 'layer_5_size': 22, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#20 epoch=1.0 loss={'loss': 0.21218965526583347, 'rmse': 0.21218965526583347, 'mae': 0.1697150625577421, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3864152965008092}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#23 epoch=1.0 loss={'loss': 0.22970952737823216, 'rmse': 0.22970952737823216, 'mae': 0.1858845957156062, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37667909807746214}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 24, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.24561458768767733}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4670441802312316}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#22 epoch=1.0 loss={'loss': 0.4679506962530148, 'rmse': 0.4679506962530148, 'mae': 0.40033384424683716, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3798313841258728}, 'layer_1_size': 33, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.23157249654939427}, 'layer_4_size': 8, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 46, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#26 epoch=1.0 loss={'loss': 0.4442066380241941, 'rmse': 0.4442066380241941, 'mae': 0.3602868397254649, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 39, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10103764488329858}, 'layer_3_size': 49, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 36, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#25 epoch=1.0 loss={'loss': 0.25292151698758447, 'rmse': 0.25292151698758447, 'mae': 0.20087855995104414, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 7, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4832891802493271}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4171010767899158}, 'layer_4_size': 87, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 21, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#27 epoch=1.0 loss={'loss': 0.7817742259961181, 'rmse': 0.7817742259961181, 'mae': 0.632745728207081, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 25, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28374513320726524}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 63, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#24 epoch=1.0 loss={'loss': 0.206388120698654, 'rmse': 0.206388120698654, 'mae': 0.16016504906809928, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 2, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#29 epoch=1.0 loss={'loss': 0.2280328406123311, 'rmse': 0.2280328406123311, 'mae': 0.18858816541670445, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 85, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 9, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3228548507084892}, 'layer_3_size': 57, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1757581808044658}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#30 epoch=1.0 loss={'loss': 0.29254136978708356, 'rmse': 0.29254136978708356, 'mae': 0.2353044067405342, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.43987587747758805}, 'layer_1_size': 47, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10179800915293612}, 'layer_2_size': 47, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 61, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2050947322336447}, 'layer_4_size': 41, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#28 epoch=1.0 loss={'loss': 0.20763999833543534, 'rmse': 0.20763999833543534, 'mae': 0.164791027732264, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1020118725516797}, 'layer_2_size': 71, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#32 epoch=1.0 loss={'loss': 0.2539986770725802, 'rmse': 0.2539986770725802, 'mae': 0.20734261797176692, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.12485200845522315}, 'layer_4_size': 88, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.11021378720415922}, 'layer_5_size': 29, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#31 epoch=1.0 loss={'loss': 0.21749469057986529, 'rmse': 0.21749469057986529, 'mae': 0.1745610281530508, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.37487373657267964}, 'layer_1_size': 69, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10079398389763844}, 'layer_2_size': 43, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25999363045229895}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 31, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.16639324870236827}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#34 epoch=1.0 loss={'loss': 0.22397290188520158, 'rmse': 0.22397290188520158, 'mae': 0.18575801871307965, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 57, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#33 epoch=1.0 loss={'loss': 0.5242321075875499, 'rmse': 0.5242321075875499, 'mae': 0.48423798182408473, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.4325646958140229}, 'layer_1_size': 57, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 82, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 65, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4393538692076945}, 'layer_4_size': 46, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 9, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#35 epoch=1.0 loss={'loss': 0.19348232517489047, 'rmse': 0.19348232517489047, 'mae': 0.15553961905879765, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2968844105931647}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.379665317170057}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#36 epoch=1.0 loss={'loss': 0.203466220305277, 'rmse': 0.203466220305277, 'mae': 0.16269092957024683, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#38 epoch=1.0 loss={'loss': 0.1939112441802917, 'rmse': 0.1939112441802917, 'mae': 0.15412727361095965, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1857037786421014}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#39 epoch=1.0 loss={'loss': 0.2297716111453298, 'rmse': 0.2297716111453298, 'mae': 0.18421656170906123, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.25136261040079955}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 9, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 94, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 73, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 99, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#37 epoch=1.0 loss={'loss': 0.434855610265058, 'rmse': 0.434855610265058, 'mae': 0.38285703871615684, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 19, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 7, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 42, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#41 epoch=1.0 loss={'loss': 0.27522708780204774, 'rmse': 0.27522708780204774, 'mae': 0.22420943023667986, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 94, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 47, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2420366468676569}, 'layer_3_size': 96, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 66, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 14, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#40 epoch=1.0 loss={'loss': 0.22972479363363363, 'rmse': 0.22972479363363363, 'mae': 0.17717870280614356, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.289478959192061}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1631506464809413}, 'layer_2_size': 52, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4142997896037961}, 'layer_3_size': 78, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 19, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': True}
#43 epoch=1.0 loss={'loss': 0.2779897070458929, 'rmse': 0.2779897070458929, 'mae': 0.22556504214594336, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 71, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1296344458219014}, 'layer_2_size': 100, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 38, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 72, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#44 epoch=1.0 loss={'loss': 0.21571329051663193, 'rmse': 0.21571329051663193, 'mae': 0.17445429034054113, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4970884277939287}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1087460609141687}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1889310551921254}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#42 epoch=1.0 loss={'loss': 0.17928614827272552, 'rmse': 0.17928614827272552, 'mae': 0.14646403361905477, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10289939797168778}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#45 epoch=1.0 loss={'loss': 0.33155191092154523, 'rmse': 0.33155191092154523, 'mae': 0.2650666451777703, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 96, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 72, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 82, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#46 epoch=1.0 loss={'loss': 0.17941231519464723, 'rmse': 0.17941231519464723, 'mae': 0.13877326622699315, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3338165530239823}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#48 epoch=1.0 loss={'loss': 0.2653573533576449, 'rmse': 0.2653573533576449, 'mae': 0.21358090259756277, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.17140315668636932}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 7, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.17885634358977823}, 'layer_4_size': 76, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 17, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#47 epoch=1.0 loss={'loss': 0.16376715862711824, 'rmse': 0.16376715862711824, 'mae': 0.1286197792270614, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20261337059472792}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10963218549386174}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#49 epoch=1.0 loss={'loss': 0.2716068847687774, 'rmse': 0.2716068847687774, 'mae': 0.21612191252892615, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 44, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 75, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
#50 epoch=1.0 loss={'loss': 0.2142235975093015, 'rmse': 0.2142235975093015, 'mae': 0.17152508405886077, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37641506330630803}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#51 epoch=1.0 loss={'loss': 0.5420126767232246, 'rmse': 0.5420126767232246, 'mae': 0.4697713594392751, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28558516419712654}, 'layer_1_size': 60, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3722775565240588}, 'layer_3_size': 9, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4108479697027453}, 'layer_5_size': 100, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#52 epoch=1.0 loss={'loss': 0.20750072965134064, 'rmse': 0.20750072965134064, 'mae': 0.16789709904726055, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46107233114729274}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10807125705702347}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}
#54 epoch=1.0 loss={'loss': 0.2530325565869371, 'rmse': 0.2530325565869371, 'mae': 0.1997325377097193, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21759894630158141}, 'layer_2_size': 29, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30985642917020173}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 12, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#55 epoch=1.0 loss={'loss': 0.2578016958577526, 'rmse': 0.2578016958577526, 'mae': 0.20504840873011648, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 36, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 25, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 37, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 61, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10338216607501166}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#53 epoch=1.0 loss={'loss': 0.12146222348912904, 'rmse': 0.12146222348912904, 'mae': 0.09379897876890517, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44587267964897137}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#56 epoch=1.0 loss={'loss': 0.31334830111756656, 'rmse': 0.31334830111756656, 'mae': 0.25302458231378955, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 80, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3549833662281482}, 'layer_4_size': 4, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#57 epoch=1.0 loss={'loss': 0.2205200258328313, 'rmse': 0.2205200258328313, 'mae': 0.17442025063807803, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 8, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.32477920284882056}, 'layer_2_size': 38, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 5, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 54, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4711066283762235}, 'layer_5_size': 61, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#58 epoch=1.0 loss={'loss': 0.19228274500827108, 'rmse': 0.19228274500827108, 'mae': 0.1516912434366689, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20852464738421236}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#59 epoch=1.0 loss={'loss': 0.23800536485265436, 'rmse': 0.23800536485265436, 'mae': 0.19452825998680257, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.28393702145126665}, 'layer_1_size': 48, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4273360035000433}, 'layer_2_size': 49, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 63, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 72, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#61 epoch=1.0 loss={'loss': 0.2604695798720141, 'rmse': 0.2604695798720141, 'mae': 0.21317823246040196, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.36181969856349394}, 'layer_1_size': 35, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.45648493186748806}, 'layer_4_size': 48, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 30, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#62 epoch=1.0 loss={'loss': 0.2761632083847889, 'rmse': 0.2761632083847889, 'mae': 0.2284758487127777, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 98, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.22558419861507129}, 'layer_2_size': 78, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 70, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.25144933285848337}, 'layer_4_size': 62, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 78, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#60 epoch=1.0 loss={'loss': 0.23994867654155963, 'rmse': 0.23994867654155963, 'mae': 0.19902215218565492, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.48537797765244817}, 'layer_1_size': 46, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.162244625320774}, 'layer_4_size': 83, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 91, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#63 epoch=1.0 loss={'loss': 0.2969436236765289, 'rmse': 0.2969436236765289, 'mae': 0.24399252143202824, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 36, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.30975404693021946}, 'layer_2_size': 81, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35525074782080623}, 'layer_5_size': 87, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#64 epoch=1.0 loss={'loss': 0.3048061866646423, 'rmse': 0.3048061866646423, 'mae': 0.2483666817509191, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2443185679023121}, 'layer_1_size': 80, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13727119846581481}, 'layer_2_size': 42, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 60, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.38348129314100576}, 'layer_4_size': 78, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 98, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#65 epoch=1.0 loss={'loss': 0.5432408153905975, 'rmse': 0.5432408153905975, 'mae': 0.4810232272629935, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.15256004882486446}, 'layer_2_size': 72, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43706453581122084}, 'layer_3_size': 21, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3149594268408259}, 'layer_4_size': 31, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.35245294857301446}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#66 epoch=1.0 loss={'loss': 0.2194670829609276, 'rmse': 0.2194670829609276, 'mae': 0.18118711056415823, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 45, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 94, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 80, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.14045255114533833}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 46, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#67 epoch=1.0 loss={'loss': 0.25939941833429264, 'rmse': 0.25939941833429264, 'mae': 0.2100130397244522, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 55, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3378174053984615}, 'layer_4_size': 85, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'RobustScaler', 'shuffle': False}
#68 epoch=1.0 loss={'loss': 0.27526128321252674, 'rmse': 0.27526128321252674, 'mae': 0.22259993249369067, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3441548869484395}, 'layer_1_size': 58, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.25971825110197183}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 79, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27662952086894604}, 'layer_4_size': 79, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 44, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#70 epoch=1.0 loss={'loss': 0.19889448432740472, 'rmse': 0.19889448432740472, 'mae': 0.15844496845395153, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39459061959382125}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.360442390830502}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32952669768377524}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#69 epoch=1.0 loss={'loss': 0.22263636402397796, 'rmse': 0.22263636402397796, 'mae': 0.17466156788068726, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 87, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3527086162194488}, 'layer_5_size': 40, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#71 epoch=1.0 loss={'loss': 0.20441566491974772, 'rmse': 0.20441566491974772, 'mae': 0.16301288680630374, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2597483729703344}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.211692527289565}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#72 epoch=1.0 loss={'loss': 0.2609195428653303, 'rmse': 0.2609195428653303, 'mae': 0.20979038882269777, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.35135548822941043}, 'layer_1_size': 7, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 86, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 41, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 61, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 81, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#73 epoch=1.0 loss={'loss': 0.23778209511594084, 'rmse': 0.23778209511594084, 'mae': 0.18835291786998254, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 89, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 34, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#74 epoch=1.0 loss={'loss': 0.33346308371231076, 'rmse': 0.33346308371231076, 'mae': 0.2616693440611545, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3846968218925758}, 'layer_1_size': 28, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 72, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 11, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 42, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 67, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#75 epoch=1.0 loss={'loss': 0.2066007020077189, 'rmse': 0.2066007020077189, 'mae': 0.16111057142201352, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14970703358753235}, 'layer_3_size': 51, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.479652463853537}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#76 epoch=1.0 loss={'loss': 0.24055626064246932, 'rmse': 0.24055626064246932, 'mae': 0.1985522556903441, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.21797867815066485}, 'layer_1_size': 93, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4656804979841388}, 'layer_4_size': 43, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#77 epoch=1.0 loss={'loss': 0.20497731880572678, 'rmse': 0.20497731880572678, 'mae': 0.16431684987358292, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1935168503165503}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44411323388230295}, 'layer_4_size': 34, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#78 epoch=1.0 loss={'loss': 0.26599660433603306, 'rmse': 0.26599660433603306, 'mae': 0.2173151185102597, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2565654251079069}, 'layer_1_size': 67, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28405862462018816}, 'layer_2_size': 82, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4723512202998946}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.40650904959016443}, 'layer_4_size': 12, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#79 epoch=1.0 loss={'loss': 0.2098852580366011, 'rmse': 0.2098852580366011, 'mae': 0.16492021702112905, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38345276572686826}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.43851058729984005}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#80 epoch=1.0 loss={'loss': 0.20064087418926235, 'rmse': 0.20064087418926235, 'mae': 0.1627302823809652, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 70, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2538091909685959}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
get a list [results] of length 81
get a list [loss] of length 81
get a list [val_loss] of length 81
length of indices is (53, 14, 47, 42, 46, 2, 9, 5, 58, 35, 15, 38, 70, 80, 18, 36, 71, 77, 24, 75, 52, 28, 7, 79, 20, 50, 44, 31, 66, 57, 69, 34, 29, 16, 23, 40, 39, 0, 19, 11, 73, 59, 60, 76, 1, 10, 25, 54, 32, 55, 67, 61, 72, 48, 78, 6, 49, 41, 68, 62, 43, 30, 63, 3, 13, 64, 56, 12, 45, 74, 8, 37, 26, 4, 22, 33, 51, 65, 21, 17, 27)
length of indices is 81
length of T is 81
newly formed T structure is:[[0, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44587267964897137}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [1, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10996334638100778}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10952735483869187}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4326994328446677}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [2, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20261337059472792}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10963218549386174}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [3, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10289939797168778}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [4, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3338165530239823}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [5, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16999145104470526}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [6, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [7, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2035119362983565}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48594345397869476}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [8, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20852464738421236}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [9, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2968844105931647}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.379665317170057}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [10, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22735716747727375}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 88, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 75, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [11, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1857037786421014}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [12, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39459061959382125}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.360442390830502}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32952669768377524}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [13, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 70, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2538091909685959}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [14, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1234109170669142}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.34670527040571764}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2799181323299549}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [15, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}], [16, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2597483729703344}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.211692527289565}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [17, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1935168503165503}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44411323388230295}, 'layer_4_size': 34, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [18, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 2, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [19, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14970703358753235}, 'layer_3_size': 51, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.479652463853537}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}], [20, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46107233114729274}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10807125705702347}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}], [21, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1020118725516797}, 'layer_2_size': 71, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}], [22, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}], [23, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38345276572686826}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.43851058729984005}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [24, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3864152965008092}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}], [25, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37641506330630803}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [26, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4970884277939287}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1087460609141687}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1889310551921254}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]] 

*** 27.0 configurations x 3.0 iterations each

81 | Thu Sep 27 23:23:53 2018 | lowest loss so far: 0.1215 (run 53)

{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  97 | activation: relu    | extras: None 
layer 2 | size:   4 | activation: tanh    | extras: batchnorm 
layer 3 | size:  16 | activation: sigmoid | extras: dropout - rate: 11.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 6:28 - loss: 0.3581
 288/6530 [>.............................] - ETA: 21s - loss: 0.2115 {'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  33 | activation: relu    | extras: batchnorm 
layer 2 | size:  34 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a748128>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 7:06 - loss: 0.5152
 608/6530 [=>............................] - ETA: 10s - loss: 0.1890
 320/6530 [>.............................] - ETA: 21s - loss: 0.4819 
 944/6530 [===>..........................] - ETA: 6s - loss: 0.1758 
 672/6530 [==>...........................] - ETA: 10s - loss: 0.3615
1328/6530 [=====>........................] - ETA: 4s - loss: 0.1650{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  52 | activation: tanh    | extras: batchnorm 
layer 2 | size:  94 | activation: relu    | extras: None 
layer 3 | size:  32 | activation: relu    | extras: dropout - rate: 20.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a748278>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 7:05 - loss: 1.9850
1024/6530 [===>..........................] - ETA: 6s - loss: 0.2725 
1712/6530 [======>.......................] - ETA: 3s - loss: 0.1586
 336/6530 [>.............................] - ETA: 20s - loss: 0.6272 
1376/6530 [=====>........................] - ETA: 4s - loss: 0.2171
2096/6530 [========>.....................] - ETA: 2s - loss: 0.1544
 656/6530 [==>...........................] - ETA: 10s - loss: 0.3820
1728/6530 [======>.......................] - ETA: 3s - loss: 0.1824
2464/6530 [==========>...................] - ETA: 2s - loss: 0.1533
 992/6530 [===>..........................] - ETA: 6s - loss: 0.2817 
2080/6530 [========>.....................] - ETA: 2s - loss: 0.1605
2864/6530 [============>.................] - ETA: 1s - loss: 0.1502
1312/6530 [=====>........................] - ETA: 4s - loss: 0.2307
2432/6530 [==========>...................] - ETA: 2s - loss: 0.1444
3248/6530 [=============>................] - ETA: 1s - loss: 0.1471
1632/6530 [======>.......................] - ETA: 3s - loss: 0.1982
2784/6530 [===========>..................] - ETA: 1s - loss: 0.1313
3632/6530 [===============>..............] - ETA: 1s - loss: 0.1465
1952/6530 [=======>......................] - ETA: 3s - loss: 0.1763
3136/6530 [=============>................] - ETA: 1s - loss: 0.1204
4032/6530 [=================>............] - ETA: 0s - loss: 0.1451
2288/6530 [=========>....................] - ETA: 2s - loss: 0.1583
3488/6530 [===============>..............] - ETA: 1s - loss: 0.1123
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1435
2624/6530 [===========>..................] - ETA: 2s - loss: 0.1456
3840/6530 [================>.............] - ETA: 1s - loss: 0.1051
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1431
2944/6530 [============>.................] - ETA: 1s - loss: 0.1355
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0989
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1419
3264/6530 [=============>................] - ETA: 1s - loss: 0.1271
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0935
3584/6530 [===============>..............] - ETA: 1s - loss: 0.1202
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1406
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0886
3920/6530 [=================>............] - ETA: 1s - loss: 0.1144
5968/6530 [==========================>...] - ETA: 0s - loss: 0.1401
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0844
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1094
6352/6530 [============================>.] - ETA: 0s - loss: 0.1393
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0807
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1049
6530/6530 [==============================] - 2s 292us/step - loss: 0.1388 - val_loss: 0.1167
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0967
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0778
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1013
 368/6530 [>.............................] - ETA: 0s - loss: 0.1131
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0751
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0972
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1183
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0939
6530/6530 [==============================] - 2s 315us/step - loss: 0.0730 - val_loss: 0.0183
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0159
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1199
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0912
 384/6530 [>.............................] - ETA: 0s - loss: 0.0196
1552/6530 [======>.......................] - ETA: 0s - loss: 0.1189
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0884
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0210
1936/6530 [=======>......................] - ETA: 0s - loss: 0.1195
6496/6530 [============================>.] - ETA: 0s - loss: 0.0862
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0219
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1199
6530/6530 [==============================] - 2s 327us/step - loss: 0.0861 - val_loss: 0.0351
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0416
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0214
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1214
 352/6530 [>.............................] - ETA: 0s - loss: 0.0412
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0207
3104/6530 [=============>................] - ETA: 0s - loss: 0.1194
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0401
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0208
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1204
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0388
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0208
3872/6530 [================>.............] - ETA: 0s - loss: 0.1203
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0379
2880/6530 [============>.................] - ETA: 0s - loss: 0.0205
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1199
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0372
3232/6530 [=============>................] - ETA: 0s - loss: 0.0202
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1205
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0203
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0353
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1207
3936/6530 [=================>............] - ETA: 0s - loss: 0.0201
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0354
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1204
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0200
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0353
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1198
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0199
3024/6530 [============>.................] - ETA: 0s - loss: 0.0348
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1201
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0198
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0347
6530/6530 [==============================] - 1s 137us/step - loss: 0.1198 - val_loss: 0.1087
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0806
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0345
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0199
 416/6530 [>.............................] - ETA: 0s - loss: 0.1070
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0196
4016/6530 [=================>............] - ETA: 0s - loss: 0.0345
 800/6530 [==>...........................] - ETA: 0s - loss: 0.1123
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0196
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0345
1200/6530 [====>.........................] - ETA: 0s - loss: 0.1120
6432/6530 [============================>.] - ETA: 0s - loss: 0.0195
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0344
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1125
6530/6530 [==============================] - 1s 148us/step - loss: 0.0195 - val_loss: 0.0129
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0054
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0342
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1131
 368/6530 [>.............................] - ETA: 0s - loss: 0.0154
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0338
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1140
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0165
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0335
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1143
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0175
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0332
3200/6530 [=============>................] - ETA: 0s - loss: 0.1127
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0171
6352/6530 [============================>.] - ETA: 0s - loss: 0.0329
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1137
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0168
6530/6530 [==============================] - 1s 160us/step - loss: 0.0330 - val_loss: 0.0239
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0316
3984/6530 [=================>............] - ETA: 0s - loss: 0.1135
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0167
 352/6530 [>.............................] - ETA: 0s - loss: 0.0283
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1133
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0168
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0295
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1144
2912/6530 [============>.................] - ETA: 0s - loss: 0.0166
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0288
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1141
3232/6530 [=============>................] - ETA: 0s - loss: 0.0164
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0292
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1141
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0166
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0287
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1137
3952/6530 [=================>............] - ETA: 0s - loss: 0.0167
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0274
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1140
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0166
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0273
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0168
6530/6530 [==============================] - 1s 137us/step - loss: 0.1137 - val_loss: 0.1037

2608/6530 [==========>...................] - ETA: 0s - loss: 0.0274
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0166
2960/6530 [============>.................] - ETA: 0s - loss: 0.0270
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0167
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0269
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0165
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0268
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0166
3968/6530 [=================>............] - ETA: 0s - loss: 0.0269
6448/6530 [============================>.] - ETA: 0s - loss: 0.0166
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0268
6530/6530 [==============================] - 1s 148us/step - loss: 0.0166 - val_loss: 0.0114

4656/6530 [====================>.........] - ETA: 0s - loss: 0.0269
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0267
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0263
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0261
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0260
6432/6530 [============================>.] - ETA: 0s - loss: 0.0258
6530/6530 [==============================] - 1s 157us/step - loss: 0.0260 - val_loss: 0.0181

# training | RMSE: 0.1357, MAE: 0.0996
worker 1  xfile  [1, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10996334638100778}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10952735483869187}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4326994328446677}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.13570671940727044, 'rmse': 0.13570671940727044, 'mae': 0.09958555935190071, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  49 | activation: relu    | extras: dropout - rate: 10.3% 
layer 2 | size:  65 | activation: tanh    | extras: batchnorm 
layer 3 | size:   6 | activation: tanh    | extras: None 
layer 4 | size:  97 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f777803aef0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:50 - loss: 0.0800
 192/6530 [..............................] - ETA: 15s - loss: 0.0718 
# training | RMSE: 0.0990, MAE: 0.0774
worker 0  xfile  [0, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44587267964897137}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.09895041638632798, 'rmse': 0.09895041638632798, 'mae': 0.07736684639394083, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  83 | activation: relu    | extras: batchnorm 
layer 2 | size:  75 | activation: tanh    | extras: None 
layer 3 | size:  44 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a748400>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:15 - loss: 0.9890
 384/6530 [>.............................] - ETA: 8s - loss: 0.0588 
 544/6530 [=>............................] - ETA: 4s - loss: 0.4721  
 624/6530 [=>............................] - ETA: 5s - loss: 0.0521
1120/6530 [====>.........................] - ETA: 2s - loss: 0.3423
 880/6530 [===>..........................] - ETA: 4s - loss: 0.0485
1696/6530 [======>.......................] - ETA: 1s - loss: 0.2913
1152/6530 [====>.........................] - ETA: 3s - loss: 0.0465
2272/6530 [=========>....................] - ETA: 1s - loss: 0.2636
1424/6530 [=====>........................] - ETA: 2s - loss: 0.0446
2880/6530 [============>.................] - ETA: 0s - loss: 0.2447
1696/6530 [======>.......................] - ETA: 2s - loss: 0.0436
# training | RMSE: 0.1256, MAE: 0.0988
worker 2  xfile  [2, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20261337059472792}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10963218549386174}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1255554590761898, 'rmse': 0.1255554590761898, 'mae': 0.09876236904757964, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  24 | activation: relu    | extras: dropout - rate: 17.0% 
layer 2 | size:  11 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  64 | activation: sigmoid | extras: None 
layer 4 | size:  72 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a748518>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:10 - loss: 0.6591
3488/6530 [===============>..............] - ETA: 0s - loss: 0.2315
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0427
 704/6530 [==>...........................] - ETA: 3s - loss: 0.2579  
4064/6530 [=================>............] - ETA: 0s - loss: 0.2218
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0416
1376/6530 [=====>........................] - ETA: 1s - loss: 0.2264
4672/6530 [====================>.........] - ETA: 0s - loss: 0.2137
2512/6530 [==========>...................] - ETA: 1s - loss: 0.0408
2048/6530 [========>.....................] - ETA: 1s - loss: 0.2079
5280/6530 [=======================>......] - ETA: 0s - loss: 0.2076
2784/6530 [===========>..................] - ETA: 1s - loss: 0.0404
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1989
5856/6530 [=========================>....] - ETA: 0s - loss: 0.2025
3008/6530 [============>.................] - ETA: 1s - loss: 0.0407
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1915
6432/6530 [============================>.] - ETA: 0s - loss: 0.1983
3264/6530 [=============>................] - ETA: 1s - loss: 0.0399
3936/6530 [=================>............] - ETA: 0s - loss: 0.1870
6530/6530 [==============================] - 1s 153us/step - loss: 0.1978 - val_loss: 0.2332

3536/6530 [===============>..............] - ETA: 0s - loss: 0.0396Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2216
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1839
3808/6530 [================>.............] - ETA: 0s - loss: 0.0392
 576/6530 [=>............................] - ETA: 0s - loss: 0.1624
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1801
4064/6530 [=================>............] - ETA: 0s - loss: 0.0385
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1532
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1774
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0382
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1485
6528/6530 [============================>.] - ETA: 0s - loss: 0.1749
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0378
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1443
6530/6530 [==============================] - 1s 141us/step - loss: 0.1748 - val_loss: 0.1630
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1434
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0372
3008/6530 [============>.................] - ETA: 0s - loss: 0.1416
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1469
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0368
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1399
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1448
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0367
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1394
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1456
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0362
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1390
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1465
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0356
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1375
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1454
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1370
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0354
3968/6530 [=================>............] - ETA: 0s - loss: 0.1453
6448/6530 [============================>.] - ETA: 0s - loss: 0.0350
6530/6530 [==============================] - 1s 88us/step - loss: 0.1363 - val_loss: 0.1854
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1815
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1458
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1354
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1453
6530/6530 [==============================] - 2s 274us/step - loss: 0.0351 - val_loss: 0.0329
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0514
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1304
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1453
 288/6530 [>.............................] - ETA: 1s - loss: 0.0283
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1275
6530/6530 [==============================] - 1s 81us/step - loss: 0.1449 - val_loss: 0.1278
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1267
 560/6530 [=>............................] - ETA: 1s - loss: 0.0269
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1247
 640/6530 [=>............................] - ETA: 0s - loss: 0.1358
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0267
3040/6530 [============>.................] - ETA: 0s - loss: 0.1221
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0265
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1349
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1212
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0279
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1363
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1210
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0272
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1386
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1211
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0270
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1379
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1202
4000/6530 [=================>............] - ETA: 0s - loss: 0.1383
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0274
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1202
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1391
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0276
6530/6530 [==============================] - 1s 88us/step - loss: 0.1199 - val_loss: 0.1122

5344/6530 [=======================>......] - ETA: 0s - loss: 0.1388
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0273
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1393
2992/6530 [============>.................] - ETA: 0s - loss: 0.0273
3232/6530 [=============>................] - ETA: 0s - loss: 0.0278
6530/6530 [==============================] - 1s 80us/step - loss: 0.1389 - val_loss: 0.1229

3520/6530 [===============>..............] - ETA: 0s - loss: 0.0277
3792/6530 [================>.............] - ETA: 0s - loss: 0.0276
4064/6530 [=================>............] - ETA: 0s - loss: 0.0275
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0277
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0282
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0281
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0277
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0277
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0275
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0273
6384/6530 [============================>.] - ETA: 0s - loss: 0.0270
6530/6530 [==============================] - 1s 190us/step - loss: 0.0271 - val_loss: 0.0240
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0180
 304/6530 [>.............................] - ETA: 1s - loss: 0.0219
 608/6530 [=>............................] - ETA: 1s - loss: 0.0234
# training | RMSE: 0.1425, MAE: 0.1125
worker 0  xfile  [4, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3338165530239823}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.14248664904814243, 'rmse': 0.14248664904814243, 'mae': 0.1125212293789281, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  87 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  19 | activation: sigmoid | extras: None 
layer 3 | size:  33 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7779b1dd68>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:15 - loss: 0.2655
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0254
# training | RMSE: 0.1580, MAE: 0.1183
worker 2  xfile  [5, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16999145104470526}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.15795342653018213, 'rmse': 0.15795342653018213, 'mae': 0.11825145708938387, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  72 | activation: tanh    | extras: batchnorm 
layer 2 | size:  27 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7778139a20>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:57 - loss: 0.6057
 368/6530 [>.............................] - ETA: 6s - loss: 0.0692  
 432/6530 [>.............................] - ETA: 4s - loss: 0.2434  
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0252
 736/6530 [==>...........................] - ETA: 3s - loss: 0.0618
 864/6530 [==>...........................] - ETA: 2s - loss: 0.2094
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0251
1104/6530 [====>.........................] - ETA: 2s - loss: 0.0579
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1945
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0262
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0544
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1858
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0269
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0540
2112/6530 [========>.....................] - ETA: 1s - loss: 0.1809
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0263
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0521
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1789
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0259
2544/6530 [==========>...................] - ETA: 1s - loss: 0.0501
2944/6530 [============>.................] - ETA: 0s - loss: 0.1756
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0251
2928/6530 [============>.................] - ETA: 0s - loss: 0.0496
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1732
3056/6530 [=============>................] - ETA: 0s - loss: 0.0249
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0482
3776/6530 [================>.............] - ETA: 0s - loss: 0.1723
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0248
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0471
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1717
4032/6530 [=================>............] - ETA: 0s - loss: 0.0457
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0247
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1711
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0445
3872/6530 [================>.............] - ETA: 0s - loss: 0.0244
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1703
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0435
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0246
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1690
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0425
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0246
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1679
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0417
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0249
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1667
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0409
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0247
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0398
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0244
6530/6530 [==============================] - 1s 176us/step - loss: 0.1661 - val_loss: 0.1472
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1860
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0242
 432/6530 [>.............................] - ETA: 0s - loss: 0.1587
6530/6530 [==============================] - 1s 200us/step - loss: 0.0394 - val_loss: 0.0396

5792/6530 [=========================>....] - ETA: 0s - loss: 0.0242Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0431
 848/6530 [==>...........................] - ETA: 0s - loss: 0.1589
 368/6530 [>.............................] - ETA: 0s - loss: 0.0249
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0241
1232/6530 [====>.........................] - ETA: 0s - loss: 0.1556
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0276
6320/6530 [============================>.] - ETA: 0s - loss: 0.0243
1648/6530 [======>.......................] - ETA: 0s - loss: 0.1529
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0263
6530/6530 [==============================] - 1s 195us/step - loss: 0.0244 - val_loss: 0.0395

2032/6530 [========>.....................] - ETA: 0s - loss: 0.1519
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0263
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1519
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0260
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1515
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0263
3168/6530 [=============>................] - ETA: 0s - loss: 0.1496
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0266
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1510
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0264
4000/6530 [=================>............] - ETA: 0s - loss: 0.1513
3072/6530 [=============>................] - ETA: 0s - loss: 0.0262
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1514
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0260
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1516
3824/6530 [================>.............] - ETA: 0s - loss: 0.0263
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1513
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0258
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1506
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0258
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1504
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0257
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0257
6530/6530 [==============================] - 1s 130us/step - loss: 0.1499 - val_loss: 0.1378
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1803
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0256
 448/6530 [=>............................] - ETA: 0s - loss: 0.1512
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0255
 864/6530 [==>...........................] - ETA: 0s - loss: 0.1497
6384/6530 [============================>.] - ETA: 0s - loss: 0.0253
1264/6530 [====>.........................] - ETA: 0s - loss: 0.1488
6530/6530 [==============================] - 1s 150us/step - loss: 0.0253 - val_loss: 0.0218
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0247
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1465
 384/6530 [>.............................] - ETA: 0s - loss: 0.0249
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1452
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0220
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1458
# training | RMSE: 0.1933, MAE: 0.1664
worker 1  xfile  [3, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10289939797168778}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.19325496950642462, 'rmse': 0.19325496950642462, 'mae': 0.1663764504175135, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  68 | activation: relu    | extras: batchnorm 
layer 2 | size:   4 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f775438f278>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 59s - loss: 0.6531
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0222
2976/6530 [============>.................] - ETA: 0s - loss: 0.1448
 832/6530 [==>...........................] - ETA: 2s - loss: 0.3537 
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0221
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1445
1600/6530 [======>.......................] - ETA: 1s - loss: 0.2458
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0222
3792/6530 [================>.............] - ETA: 0s - loss: 0.1448
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1886
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0231
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1452
3168/6530 [=============>................] - ETA: 0s - loss: 0.1582
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0228
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1457
3936/6530 [=================>............] - ETA: 0s - loss: 0.1390
2912/6530 [============>.................] - ETA: 0s - loss: 0.0227
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1457
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1237
3264/6530 [=============>................] - ETA: 0s - loss: 0.0227
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1457
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1124
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0227
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1452
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1039
3936/6530 [=================>............] - ETA: 0s - loss: 0.0227
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1450
6530/6530 [==============================] - 1s 118us/step - loss: 0.1016 - val_loss: 0.0395
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0337
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0229
6530/6530 [==============================] - 1s 128us/step - loss: 0.1446 - val_loss: 0.1322

 832/6530 [==>...........................] - ETA: 0s - loss: 0.0407
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0230
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0414
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0228
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0408
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0226
3232/6530 [=============>................] - ETA: 0s - loss: 0.0394
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0224
4032/6530 [=================>............] - ETA: 0s - loss: 0.0387
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0225
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0381
6464/6530 [============================>.] - ETA: 0s - loss: 0.0228
6530/6530 [==============================] - 1s 147us/step - loss: 0.0227 - val_loss: 0.0233

5664/6530 [=========================>....] - ETA: 0s - loss: 0.0373
6530/6530 [==============================] - 0s 65us/step - loss: 0.0374 - val_loss: 0.0305
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0523
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0337
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0331
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0319
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0325
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0317
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0314
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0309
# training | RMSE: 0.1682, MAE: 0.1330
worker 2  xfile  [7, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2035119362983565}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48594345397869476}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.1682280462529784, 'rmse': 0.1682280462529784, 'mae': 0.13302461932561335, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  84 | activation: tanh    | extras: None 
layer 2 | size:  42 | activation: sigmoid | extras: dropout - rate: 29.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f773436c4a8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:32 - loss: 0.4850
 480/6530 [=>............................] - ETA: 3s - loss: 0.2511  
6530/6530 [==============================] - 0s 65us/step - loss: 0.0308 - val_loss: 0.0239

1056/6530 [===>..........................] - ETA: 1s - loss: 0.2082
1584/6530 [======>.......................] - ETA: 1s - loss: 0.1918
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1856
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1830
3200/6530 [=============>................] - ETA: 0s - loss: 0.1766
3760/6530 [================>.............] - ETA: 0s - loss: 0.1754
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1724
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1716
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1695
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1683
6352/6530 [============================>.] - ETA: 0s - loss: 0.1665
# training | RMSE: 0.1507, MAE: 0.1183
worker 1  xfile  [8, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20852464738421236}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.1507027701675847, 'rmse': 0.1507027701675847, 'mae': 0.11825425102531353, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  57 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f773c75d6a0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:23 - loss: 0.6580
6530/6530 [==============================] - 1s 138us/step - loss: 0.1659 - val_loss: 0.1603
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.2090
 624/6530 [=>............................] - ETA: 2s - loss: 0.6531  
 496/6530 [=>............................] - ETA: 0s - loss: 0.1513
1232/6530 [====>.........................] - ETA: 1s - loss: 0.5309
 992/6530 [===>..........................] - ETA: 0s - loss: 0.1459
1872/6530 [=======>......................] - ETA: 0s - loss: 0.4143
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1414
2512/6530 [==========>...................] - ETA: 0s - loss: 0.3523
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1407
3152/6530 [=============>................] - ETA: 0s - loss: 0.3119
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1395
3744/6530 [================>.............] - ETA: 0s - loss: 0.2890
3040/6530 [============>.................] - ETA: 0s - loss: 0.1371
4336/6530 [==================>...........] - ETA: 0s - loss: 0.2721
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1370
4976/6530 [=====================>........] - ETA: 0s - loss: 0.2583
4048/6530 [=================>............] - ETA: 0s - loss: 0.1355
5616/6530 [========================>.....] - ETA: 0s - loss: 0.2469
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1343
# training | RMSE: 0.1481, MAE: 0.1183
worker 0  xfile  [6, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.14805786090369236, 'rmse': 0.14805786090369236, 'mae': 0.11833462240540302, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  24 | activation: relu    | extras: dropout - rate: 22.7% 
layer 2 | size:  12 | activation: tanh    | extras: batchnorm 
layer 3 | size:  88 | activation: relu    | extras: batchnorm 
layer 4 | size:  20 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7779b1da90>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 1:01 - loss: 0.9313
6224/6530 [===========================>..] - ETA: 0s - loss: 0.2389
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1334
 896/6530 [===>..........................] - ETA: 4s - loss: 0.7346  
6530/6530 [==============================] - 1s 120us/step - loss: 0.2348 - val_loss: 0.1596
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1791
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1322
1728/6530 [======>.......................] - ETA: 1s - loss: 0.5881
 608/6530 [=>............................] - ETA: 0s - loss: 0.1608
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1317
2560/6530 [==========>...................] - ETA: 1s - loss: 0.4836
1200/6530 [====>.........................] - ETA: 0s - loss: 0.1541
3392/6530 [==============>...............] - ETA: 0s - loss: 0.4164
6530/6530 [==============================] - 1s 105us/step - loss: 0.1307 - val_loss: 0.1434
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1417
1808/6530 [=======>......................] - ETA: 0s - loss: 0.1530
4224/6530 [==================>...........] - ETA: 0s - loss: 0.3727
 512/6530 [=>............................] - ETA: 0s - loss: 0.1195
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1523
4992/6530 [=====================>........] - ETA: 0s - loss: 0.3444
 976/6530 [===>..........................] - ETA: 0s - loss: 0.1154
3024/6530 [============>.................] - ETA: 0s - loss: 0.1501
5824/6530 [=========================>....] - ETA: 0s - loss: 0.3195
1456/6530 [=====>........................] - ETA: 0s - loss: 0.1126
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1495
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1131
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1480
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1126
6530/6530 [==============================] - 1s 167us/step - loss: 0.3040 - val_loss: 0.1826
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1654
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1473
2848/6530 [============>.................] - ETA: 0s - loss: 0.1122
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1622
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1458
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1109
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1638
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1446
3840/6530 [================>.............] - ETA: 0s - loss: 0.1108
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1625
6530/6530 [==============================] - 1s 88us/step - loss: 0.1435 - val_loss: 0.1309
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1323
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1096
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1611
 608/6530 [=>............................] - ETA: 0s - loss: 0.1255
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1098
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1611
1200/6530 [====>.........................] - ETA: 0s - loss: 0.1213
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1090
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1600
1808/6530 [=======>......................] - ETA: 0s - loss: 0.1223
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1082
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1590
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1231
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1082
6530/6530 [==============================] - 0s 65us/step - loss: 0.1588 - val_loss: 0.1446
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1344
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1235
6530/6530 [==============================] - 1s 111us/step - loss: 0.1077 - val_loss: 0.1209

 896/6530 [===>..........................] - ETA: 0s - loss: 0.1431
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1231
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1451
4000/6530 [=================>............] - ETA: 0s - loss: 0.1228
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1434
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1223
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1435
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1217
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1430
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1213
6448/6530 [============================>.] - ETA: 0s - loss: 0.1210
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1427
6530/6530 [==============================] - 1s 90us/step - loss: 0.1208 - val_loss: 0.1181

5952/6530 [==========================>...] - ETA: 0s - loss: 0.1419
6530/6530 [==============================] - 0s 65us/step - loss: 0.1420 - val_loss: 0.1339

# training | RMSE: 0.1463, MAE: 0.1135
worker 2  xfile  [9, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2968844105931647}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.379665317170057}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.14633873827520127, 'rmse': 0.14633873827520127, 'mae': 0.1134831026715265, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  53 | activation: sigmoid | extras: None 
layer 2 | size:  45 | activation: sigmoid | extras: dropout - rate: 39.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7734359cf8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 44s - loss: 0.3535
1088/6530 [===>..........................] - ETA: 1s - loss: 0.2336 
2208/6530 [=========>....................] - ETA: 0s - loss: 0.2108
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1999
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1941
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1882
6530/6530 [==============================] - 1s 86us/step - loss: 0.1832 - val_loss: 0.1610
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1715
# training | RMSE: 0.1646, MAE: 0.1306
worker 0  xfile  [10, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22735716747727375}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 88, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 75, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.16462337835185933, 'rmse': 0.16462337835185933, 'mae': 0.13061356650237213, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  34 | activation: relu    | extras: None 
layer 2 | size:  29 | activation: tanh    | extras: dropout - rate: 12.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7734794160>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:56 - loss: 0.9772
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1587
 608/6530 [=>............................] - ETA: 3s - loss: 0.4784  
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1586
1232/6530 [====>.........................] - ETA: 1s - loss: 0.3788
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1589
1824/6530 [=======>......................] - ETA: 1s - loss: 0.3338
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1589
2400/6530 [==========>...................] - ETA: 0s - loss: 0.3060
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1601
2960/6530 [============>.................] - ETA: 0s - loss: 0.2879
6530/6530 [==============================] - 0s 45us/step - loss: 0.1604 - val_loss: 0.1612
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1712
3552/6530 [===============>..............] - ETA: 0s - loss: 0.2720
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1564
4128/6530 [=================>............] - ETA: 0s - loss: 0.2599
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1570
4688/6530 [====================>.........] - ETA: 0s - loss: 0.2501
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1595
5264/6530 [=======================>......] - ETA: 0s - loss: 0.2413
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1600
5856/6530 [=========================>....] - ETA: 0s - loss: 0.2337
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1586
6530/6530 [==============================] - 0s 45us/step - loss: 0.1596 - val_loss: 0.1612

6432/6530 [============================>.] - ETA: 0s - loss: 0.2266
6530/6530 [==============================] - 1s 139us/step - loss: 0.2252 - val_loss: 0.1566
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1191
 592/6530 [=>............................] - ETA: 0s - loss: 0.1498
# training | RMSE: 0.1474, MAE: 0.1117
worker 1  xfile  [11, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1857037786421014}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.14739248738999622, 'rmse': 0.14739248738999622, 'mae': 0.11167727746345076, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  70 | activation: relu    | extras: batchnorm 
layer 2 | size:  42 | activation: sigmoid | extras: dropout - rate: 25.4% 
layer 3 | size:  95 | activation: tanh    | extras: batchnorm 
layer 4 | size:  88 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f773c765400>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 1:01 - loss: 0.8911
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1480
1024/6530 [===>..........................] - ETA: 3s - loss: 0.3294  
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1487
1984/6530 [========>.....................] - ETA: 1s - loss: 0.2062
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1468
2944/6530 [============>.................] - ETA: 0s - loss: 0.1590
3008/6530 [============>.................] - ETA: 0s - loss: 0.1463
3904/6530 [================>.............] - ETA: 0s - loss: 0.1336
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1450
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1180
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1425
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1063
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1417
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1409
# training | RMSE: 0.2065, MAE: 0.1590
worker 2  xfile  [12, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39459061959382125}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.360442390830502}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32952669768377524}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2064989482033038, 'rmse': 0.2064989482033038, 'mae': 0.1589884206256287, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  60 | activation: relu    | extras: None 
layer 2 | size:  32 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777822deb8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 47s - loss: 0.6919
6530/6530 [==============================] - 1s 157us/step - loss: 0.1007 - val_loss: 0.0358
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0509
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1394
1152/6530 [====>.........................] - ETA: 1s - loss: 0.5399 
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0504
6530/6530 [==============================] - 1s 89us/step - loss: 0.1380 - val_loss: 0.1380
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1186
2304/6530 [=========>....................] - ETA: 0s - loss: 0.3687
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0487
 592/6530 [=>............................] - ETA: 0s - loss: 0.1297
3424/6530 [==============>...............] - ETA: 0s - loss: 0.3022
3072/6530 [=============>................] - ETA: 0s - loss: 0.0474
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1233
4544/6530 [===================>..........] - ETA: 0s - loss: 0.2686
4096/6530 [=================>............] - ETA: 0s - loss: 0.0478
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1221
5664/6530 [=========================>....] - ETA: 0s - loss: 0.2480
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0472
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1231
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0466
6530/6530 [==============================] - 1s 87us/step - loss: 0.2370 - val_loss: 0.1648
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1793
2896/6530 [============>.................] - ETA: 0s - loss: 0.1217
6530/6530 [==============================] - 0s 54us/step - loss: 0.0463 - val_loss: 0.0332
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0464
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1587
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1202
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0451
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1565
4032/6530 [=================>............] - ETA: 0s - loss: 0.1187
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0436
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1562
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1182
3072/6530 [=============>................] - ETA: 0s - loss: 0.0434
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1569
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1177
4032/6530 [=================>............] - ETA: 0s - loss: 0.0432
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1571
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1164
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0428
6530/6530 [==============================] - 0s 48us/step - loss: 0.1570 - val_loss: 0.1594
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1740
6320/6530 [============================>.] - ETA: 0s - loss: 0.1153
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0430
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1520
6530/6530 [==============================] - 1s 92us/step - loss: 0.1154 - val_loss: 0.1135

6530/6530 [==============================] - 0s 55us/step - loss: 0.0426 - val_loss: 0.0299

2304/6530 [=========>....................] - ETA: 0s - loss: 0.1507
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1505
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1506
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1505
6530/6530 [==============================] - 0s 47us/step - loss: 0.1503 - val_loss: 0.1521

# training | RMSE: 0.1362, MAE: 0.1058
worker 0  xfile  [14, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1234109170669142}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.34670527040571764}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2799181323299549}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.13622542310641958, 'rmse': 0.13622542310641958, 'mae': 0.10576451188443767, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  13 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f771c1b54e0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 25s - loss: 0.5169
1728/6530 [======>.......................] - ETA: 0s - loss: 0.4946 
3648/6530 [===============>..............] - ETA: 0s - loss: 0.2951
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2091
6530/6530 [==============================] - 0s 71us/step - loss: 0.1828 - val_loss: 0.0420
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0546
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0409
# training | RMSE: 0.1686, MAE: 0.1310
worker 1  xfile  [13, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 70, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2538091909685959}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.16855163559017308, 'rmse': 0.16855163559017308, 'mae': 0.13098123019597174, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  27 | activation: tanh    | extras: dropout - rate: 26.0% 
layer 2 | size:  27 | activation: relu    | extras: dropout - rate: 21.2% 
layer 3 | size:  54 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777007cd68>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:19 - loss: 1.1083
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0415
 384/6530 [>.............................] - ETA: 6s - loss: 0.3662  
6530/6530 [==============================] - 0s 25us/step - loss: 0.0411 - val_loss: 0.0419
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0394
 768/6530 [==>...........................] - ETA: 3s - loss: 0.2413
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0399
1152/6530 [====>.........................] - ETA: 2s - loss: 0.1931
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0402
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1703
6530/6530 [==============================] - 0s 24us/step - loss: 0.0409 - val_loss: 0.0411

1920/6530 [=======>......................] - ETA: 1s - loss: 0.1525
2320/6530 [=========>....................] - ETA: 1s - loss: 0.1389
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1279
3120/6530 [=============>................] - ETA: 0s - loss: 0.1205
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1151
3920/6530 [=================>............] - ETA: 0s - loss: 0.1099
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1050
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1017
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0981
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0949
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0924
# training | RMSE: 0.1867, MAE: 0.1459
worker 2  xfile  [15, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1866506904410619, 'rmse': 0.1866506904410619, 'mae': 0.1458956782085537, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  13 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  43 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  55 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7778054fd0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 4:08 - loss: 1.0179
# training | RMSE: 0.2025, MAE: 0.1627
worker 0  xfile  [17, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1935168503165503}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44411323388230295}, 'layer_4_size': 34, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.2024581741107015, 'rmse': 0.2024581741107015, 'mae': 0.1626775954232455, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  87 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f771c2dc0f0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:51 - loss: 1.2135
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0900
 240/6530 [>.............................] - ETA: 17s - loss: 0.5679 
 592/6530 [=>............................] - ETA: 3s - loss: 0.4333  
 480/6530 [=>............................] - ETA: 9s - loss: 0.4018 
1152/6530 [====>.........................] - ETA: 1s - loss: 0.3238
6530/6530 [==============================] - 1s 194us/step - loss: 0.0885 - val_loss: 0.0482
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0405
 720/6530 [==>...........................] - ETA: 6s - loss: 0.3242
1728/6530 [======>.......................] - ETA: 1s - loss: 0.2767
 400/6530 [>.............................] - ETA: 0s - loss: 0.0504
 960/6530 [===>..........................] - ETA: 4s - loss: 0.2866
2288/6530 [=========>....................] - ETA: 0s - loss: 0.2522
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0492
1184/6530 [====>.........................] - ETA: 3s - loss: 0.2621
2848/6530 [============>.................] - ETA: 0s - loss: 0.2363
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0489
1408/6530 [=====>........................] - ETA: 3s - loss: 0.2467
3408/6530 [==============>...............] - ETA: 0s - loss: 0.2252
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0482
1648/6530 [======>.......................] - ETA: 2s - loss: 0.2343
3984/6530 [=================>............] - ETA: 0s - loss: 0.2166
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0483
1888/6530 [=======>......................] - ETA: 2s - loss: 0.2284
4528/6530 [===================>..........] - ETA: 0s - loss: 0.2103
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0493
2128/6530 [========>.....................] - ETA: 2s - loss: 0.2214
5072/6530 [======================>.......] - ETA: 0s - loss: 0.2062
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0492
2368/6530 [=========>....................] - ETA: 1s - loss: 0.2160
5648/6530 [========================>.....] - ETA: 0s - loss: 0.2014
3056/6530 [=============>................] - ETA: 0s - loss: 0.0495
2608/6530 [==========>...................] - ETA: 1s - loss: 0.2113
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1980
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0501
2832/6530 [============>.................] - ETA: 1s - loss: 0.2091
3808/6530 [================>.............] - ETA: 0s - loss: 0.0498
6530/6530 [==============================] - 1s 140us/step - loss: 0.1961 - val_loss: 0.1615
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1529
3072/6530 [=============>................] - ETA: 1s - loss: 0.2057
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0496
 576/6530 [=>............................] - ETA: 0s - loss: 0.1662
3312/6530 [==============>...............] - ETA: 1s - loss: 0.2029
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0492
1168/6530 [====>.........................] - ETA: 0s - loss: 0.1612
3552/6530 [===============>..............] - ETA: 1s - loss: 0.2003
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0490
1744/6530 [=======>......................] - ETA: 0s - loss: 0.1632
3792/6530 [================>.............] - ETA: 1s - loss: 0.1981
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0490
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1633
4032/6530 [=================>............] - ETA: 0s - loss: 0.1971
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0489
2896/6530 [============>.................] - ETA: 0s - loss: 0.1630
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1948
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0488
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1637
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1941
6448/6530 [============================>.] - ETA: 0s - loss: 0.0489
4032/6530 [=================>............] - ETA: 0s - loss: 0.1644
6530/6530 [==============================] - 1s 139us/step - loss: 0.0491 - val_loss: 0.0411
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0546
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1933
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1648
 400/6530 [>.............................] - ETA: 0s - loss: 0.0452
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1922
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1650
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0440
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1903
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1642
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0448
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1894
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1638
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0444
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1888
6530/6530 [==============================] - 1s 93us/step - loss: 0.1637 - val_loss: 0.1670
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.1420
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0442
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1871
 560/6530 [=>............................] - ETA: 0s - loss: 0.1615
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0442
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1870
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1628
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0433
6432/6530 [============================>.] - ETA: 0s - loss: 0.1865
1744/6530 [=======>......................] - ETA: 0s - loss: 0.1615
3104/6530 [=============>................] - ETA: 0s - loss: 0.0441
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1634
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0434
2960/6530 [============>.................] - ETA: 0s - loss: 0.1658
6530/6530 [==============================] - 2s 328us/step - loss: 0.1861 - val_loss: 0.1636
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1454
3872/6530 [================>.............] - ETA: 0s - loss: 0.0435
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1665
 256/6530 [>.............................] - ETA: 1s - loss: 0.1453
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0433
4048/6530 [=================>............] - ETA: 0s - loss: 0.1654
 480/6530 [=>............................] - ETA: 1s - loss: 0.1546
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0429
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1652
 720/6530 [==>...........................] - ETA: 1s - loss: 0.1572
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0432
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1642
 944/6530 [===>..........................] - ETA: 1s - loss: 0.1585
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0428
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1639
1168/6530 [====>.........................] - ETA: 1s - loss: 0.1580
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0425
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1633
1392/6530 [=====>........................] - ETA: 1s - loss: 0.1581
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0426
6530/6530 [==============================] - 1s 94us/step - loss: 0.1636 - val_loss: 0.1913

1632/6530 [======>.......................] - ETA: 1s - loss: 0.1609
6480/6530 [============================>.] - ETA: 0s - loss: 0.0427
6530/6530 [==============================] - 1s 139us/step - loss: 0.0426 - val_loss: 0.0340

1856/6530 [=======>......................] - ETA: 1s - loss: 0.1632
2080/6530 [========>.....................] - ETA: 1s - loss: 0.1647
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1654
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1656
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1663
3056/6530 [=============>................] - ETA: 0s - loss: 0.1661
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1658
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1666
3712/6530 [================>.............] - ETA: 0s - loss: 0.1660
3888/6530 [================>.............] - ETA: 0s - loss: 0.1661
4112/6530 [=================>............] - ETA: 0s - loss: 0.1664
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1667
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1672
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1679
# training | RMSE: 0.2485, MAE: 0.1901
worker 0  xfile  [19, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14970703358753235}, 'layer_3_size': 51, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.479652463853537}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.24853578962963374, 'rmse': 0.24853578962963374, 'mae': 0.19014996871275835, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  62 | activation: tanh    | extras: dropout - rate: 46.1% 
layer 2 | size:  73 | activation: sigmoid | extras: None 
layer 3 | size:  92 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f771c2dc390>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 32s - loss: 0.7091
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1674
1984/6530 [========>.....................] - ETA: 0s - loss: 0.2449 
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1669
4032/6530 [=================>............] - ETA: 0s - loss: 0.2189
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1668
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2047
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1659
6530/6530 [==============================] - 1s 82us/step - loss: 0.2026 - val_loss: 0.1639

6048/6530 [==========================>...] - ETA: 0s - loss: 0.1658Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1833
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1674
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1664
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1669
6528/6530 [============================>.] - ETA: 0s - loss: 0.1662
6530/6530 [==============================] - 1s 229us/step - loss: 0.1662 - val_loss: 0.1598

6336/6530 [============================>.] - ETA: 0s - loss: 0.1674Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1667
6530/6530 [==============================] - 0s 25us/step - loss: 0.1672 - val_loss: 0.1628
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1736
 224/6530 [>.............................] - ETA: 1s - loss: 0.1462
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1638
 464/6530 [=>............................] - ETA: 1s - loss: 0.1556
4032/6530 [=================>............] - ETA: 0s - loss: 0.1637
 720/6530 [==>...........................] - ETA: 1s - loss: 0.1556
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1649
6530/6530 [==============================] - 0s 27us/step - loss: 0.1644 - val_loss: 0.1623

 976/6530 [===>..........................] - ETA: 1s - loss: 0.1542
1216/6530 [====>.........................] - ETA: 1s - loss: 0.1542
1456/6530 [=====>........................] - ETA: 1s - loss: 0.1552
# training | RMSE: 0.1804, MAE: 0.1432
worker 1  xfile  [16, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2597483729703344}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.211692527289565}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.1803710112315717, 'rmse': 0.1803710112315717, 'mae': 0.14316317192503317, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  13 | activation: relu    | extras: batchnorm 
layer 2 | size:  71 | activation: sigmoid | extras: dropout - rate: 10.2% 
layer 3 | size:  14 | activation: tanh    | extras: None 
layer 4 | size:  44 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f77300ebb38>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 2:04 - loss: 0.3363
1696/6530 [======>.......................] - ETA: 1s - loss: 0.1548
 608/6530 [=>............................] - ETA: 6s - loss: 0.1219  
1920/6530 [=======>......................] - ETA: 1s - loss: 0.1549
1152/6530 [====>.........................] - ETA: 3s - loss: 0.1056
2160/6530 [========>.....................] - ETA: 0s - loss: 0.1555
1696/6530 [======>.......................] - ETA: 2s - loss: 0.0932
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1565
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0874
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1553
2816/6530 [===========>..................] - ETA: 1s - loss: 0.0809
2928/6530 [============>.................] - ETA: 0s - loss: 0.1545
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0766
3184/6530 [=============>................] - ETA: 0s - loss: 0.1555
3936/6530 [=================>............] - ETA: 0s - loss: 0.0735
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1562
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0711
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1562
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0689
3872/6530 [================>.............] - ETA: 0s - loss: 0.1552
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0672
4112/6530 [=================>............] - ETA: 0s - loss: 0.1557
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0664
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1550
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1547
6530/6530 [==============================] - 1s 199us/step - loss: 0.0647 - val_loss: 0.0446
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0647
4848/6530 [=====================>........] - ETA: 0s - loss: 0.1548
# training | RMSE: 0.2031, MAE: 0.1613
worker 0  xfile  [20, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46107233114729274}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10807125705702347}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.20310310937806159, 'rmse': 0.20310310937806159, 'mae': 0.16133077587231456, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  58 | activation: tanh    | extras: None 
layer 2 | size:  53 | activation: relu    | extras: None 
layer 3 | size:  18 | activation: relu    | extras: None 
layer 4 | size:  40 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7436953e10>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:41 - loss: 0.8297
 544/6530 [=>............................] - ETA: 0s - loss: 0.0477
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1541
 416/6530 [>.............................] - ETA: 6s - loss: 0.6661  
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0471
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1539
 816/6530 [==>...........................] - ETA: 3s - loss: 0.4706
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0473
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1536
1216/6530 [====>.........................] - ETA: 2s - loss: 0.3877
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0465
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1535
1600/6530 [======>.......................] - ETA: 1s - loss: 0.3464
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0467
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1536
1984/6530 [========>.....................] - ETA: 1s - loss: 0.3204
3200/6530 [=============>................] - ETA: 0s - loss: 0.0450
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1531
2368/6530 [=========>....................] - ETA: 1s - loss: 0.3033
3712/6530 [================>.............] - ETA: 0s - loss: 0.0448
6512/6530 [============================>.] - ETA: 0s - loss: 0.1531
2768/6530 [===========>..................] - ETA: 1s - loss: 0.2909
6530/6530 [==============================] - 1s 222us/step - loss: 0.1532 - val_loss: 0.1400

4256/6530 [==================>...........] - ETA: 0s - loss: 0.0443
3168/6530 [=============>................] - ETA: 0s - loss: 0.2803
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0445
3552/6530 [===============>..............] - ETA: 0s - loss: 0.2728
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0443
3952/6530 [=================>............] - ETA: 0s - loss: 0.2672
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0438
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2618
6400/6530 [============================>.] - ETA: 0s - loss: 0.0436
4736/6530 [====================>.........] - ETA: 0s - loss: 0.2566
6530/6530 [==============================] - 1s 100us/step - loss: 0.0433 - val_loss: 0.0344
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0452
5136/6530 [======================>.......] - ETA: 0s - loss: 0.2511
 576/6530 [=>............................] - ETA: 0s - loss: 0.0407
5552/6530 [========================>.....] - ETA: 0s - loss: 0.2446
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0392
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2407
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0395
6368/6530 [============================>.] - ETA: 0s - loss: 0.2363
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0392
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0388
6530/6530 [==============================] - 1s 200us/step - loss: 0.2346 - val_loss: 0.1674
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1557
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0375
 400/6530 [>.............................] - ETA: 0s - loss: 0.1634
3808/6530 [================>.............] - ETA: 0s - loss: 0.0369
 784/6530 [==>...........................] - ETA: 0s - loss: 0.1639
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0366
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1646
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0368
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1674
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0367
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1673
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0365
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1688
6432/6530 [============================>.] - ETA: 0s - loss: 0.0361
2832/6530 [============>.................] - ETA: 0s - loss: 0.1678
6530/6530 [==============================] - 1s 99us/step - loss: 0.0359 - val_loss: 0.0292

# training | RMSE: 0.1741, MAE: 0.1407
worker 2  xfile  [18, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 2, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.17413270889721255, 'rmse': 0.17413270889721255, 'mae': 0.14067670974534638, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  23 | activation: tanh    | extras: dropout - rate: 38.3% 
layer 2 | size:  14 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f74273f2940>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:02 - loss: 0.7842
3248/6530 [=============>................] - ETA: 0s - loss: 0.1678
1120/6530 [====>.........................] - ETA: 1s - loss: 0.7031  
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1660
2208/6530 [=========>....................] - ETA: 0s - loss: 0.6090
4080/6530 [=================>............] - ETA: 0s - loss: 0.1671
3264/6530 [=============>................] - ETA: 0s - loss: 0.5382
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1669
4352/6530 [==================>...........] - ETA: 0s - loss: 0.4814
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1662
5440/6530 [=======================>......] - ETA: 0s - loss: 0.4413
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1662
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1656
6530/6530 [==============================] - 1s 102us/step - loss: 0.4091 - val_loss: 0.1864
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2393
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1647
1152/6530 [====>.........................] - ETA: 0s - loss: 0.2313
6528/6530 [============================>.] - ETA: 0s - loss: 0.1640
6530/6530 [==============================] - 1s 130us/step - loss: 0.1640 - val_loss: 0.1607

2240/6530 [=========>....................] - ETA: 0s - loss: 0.2215Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1132
3264/6530 [=============>................] - ETA: 0s - loss: 0.2152
 352/6530 [>.............................] - ETA: 0s - loss: 0.1634
4384/6530 [===================>..........] - ETA: 0s - loss: 0.2126
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1557
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2099
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1553
1552/6530 [======>.......................] - ETA: 0s - loss: 0.1531
6530/6530 [==============================] - 0s 48us/step - loss: 0.2075 - val_loss: 0.1702
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1649
1968/6530 [========>.....................] - ETA: 0s - loss: 0.1506
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1915
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1486
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1870
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1482
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1856
3120/6530 [=============>................] - ETA: 0s - loss: 0.1452
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1841
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1444
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1841
3936/6530 [=================>............] - ETA: 0s - loss: 0.1433
6530/6530 [==============================] - 0s 47us/step - loss: 0.1836 - val_loss: 0.1591

# training | RMSE: 0.1654, MAE: 0.1326
worker 1  xfile  [21, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1020118725516797}, 'layer_2_size': 71, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1654379522730005, 'rmse': 0.1654379522730005, 'mae': 0.13261302180019188, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  99 | activation: sigmoid | extras: None 
layer 2 | size:  19 | activation: sigmoid | extras: dropout - rate: 38.6% 
layer 3 | size:  18 | activation: sigmoid | extras: None 
layer 4 | size:  75 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f743755d438>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:23 - loss: 0.8652
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1412
1024/6530 [===>..........................] - ETA: 2s - loss: 0.2432  
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1397
1952/6530 [=======>......................] - ETA: 1s - loss: 0.2297
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1397
2848/6530 [============>.................] - ETA: 0s - loss: 0.2290
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1397
3808/6530 [================>.............] - ETA: 0s - loss: 0.2254
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1391
4800/6530 [=====================>........] - ETA: 0s - loss: 0.2214
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1394
5824/6530 [=========================>....] - ETA: 0s - loss: 0.2164
6530/6530 [==============================] - 1s 135us/step - loss: 0.1394 - val_loss: 0.1341

6530/6530 [==============================] - 1s 126us/step - loss: 0.2128 - val_loss: 0.2390
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2498
 864/6530 [==>...........................] - ETA: 0s - loss: 0.1817
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1813
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1826
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1803
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1801
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1790
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1774
6530/6530 [==============================] - 0s 62us/step - loss: 0.1771 - val_loss: 0.1608
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1756
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1698
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1735
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1732
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1718
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1712
# training | RMSE: 0.2049, MAE: 0.1575
worker 2  xfile  [23, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38345276572686826}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.43851058729984005}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20493379096338202, 'rmse': 0.20493379096338202, 'mae': 0.157467961019968, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  12 | activation: tanh    | extras: batchnorm 
layer 2 | size:  51 | activation: relu    | extras: dropout - rate: 37.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7427579b70>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 50s - loss: 1.0488
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1699
1344/6530 [=====>........................] - ETA: 2s - loss: 0.6897 
6496/6530 [============================>.] - ETA: 0s - loss: 0.1694
2624/6530 [===========>..................] - ETA: 0s - loss: 0.4994
6530/6530 [==============================] - 0s 57us/step - loss: 0.1694 - val_loss: 0.1894

4032/6530 [=================>............] - ETA: 0s - loss: 0.3949
# training | RMSE: 0.1680, MAE: 0.1296
worker 0  xfile  [22, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.16799949597260586, 'rmse': 0.16799949597260586, 'mae': 0.12961184659604363, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  17 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f74366c9438>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:11 - loss: 0.6319
5376/6530 [=======================>......] - ETA: 0s - loss: 0.3416
 560/6530 [=>............................] - ETA: 3s - loss: 0.3526  
1120/6530 [====>.........................] - ETA: 2s - loss: 0.2416
1712/6530 [======>.......................] - ETA: 1s - loss: 0.1832
6530/6530 [==============================] - 1s 126us/step - loss: 0.3129 - val_loss: 0.1817

# training | RMSE: 0.2431, MAE: 0.1888
worker 1  xfile  [24, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3864152965008092}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.24307967999874627, 'rmse': 0.24307967999874627, 'mae': 0.1887506375342203, 'early_stop': False}
vggnet done  1
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1821
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1527
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1645
2880/6530 [============>.................] - ETA: 0s - loss: 0.1324
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1642
3408/6530 [==============>...............] - ETA: 0s - loss: 0.1198
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1627
4016/6530 [=================>............] - ETA: 0s - loss: 0.1095
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1607
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1028
6530/6530 [==============================] - 0s 40us/step - loss: 0.1608 - val_loss: 0.1654
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1530
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0970
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1490
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0923
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1490
6352/6530 [============================>.] - ETA: 0s - loss: 0.0881
4096/6530 [=================>............] - ETA: 0s - loss: 0.1485
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1472
6530/6530 [==============================] - 1s 148us/step - loss: 0.0870 - val_loss: 0.0468
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0370
6530/6530 [==============================] - 0s 39us/step - loss: 0.1474 - val_loss: 0.1551

 624/6530 [=>............................] - ETA: 0s - loss: 0.0454
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0455
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0458
# training | RMSE: 0.1898, MAE: 0.1496
worker 2  xfile  [25, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37641506330630803}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.18979099148565645, 'rmse': 0.18979099148565645, 'mae': 0.14961579759972965, 'early_stop': False}
vggnet done  2

2384/6530 [=========>....................] - ETA: 0s - loss: 0.0449
2992/6530 [============>.................] - ETA: 0s - loss: 0.0453
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0448
4080/6530 [=================>............] - ETA: 0s - loss: 0.0446
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0441
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0438
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0436
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0433
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0435
6530/6530 [==============================] - 1s 107us/step - loss: 0.0433 - val_loss: 0.0442
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0345
 368/6530 [>.............................] - ETA: 0s - loss: 0.0407
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0404
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0399
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0408
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0416
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0418
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0412
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0415
3008/6530 [============>.................] - ETA: 0s - loss: 0.0417
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0415
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0419
3904/6530 [================>.............] - ETA: 0s - loss: 0.0413
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0411
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0408
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0408
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0406
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0407
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0409
6320/6530 [============================>.] - ETA: 0s - loss: 0.0410
6530/6530 [==============================] - 1s 159us/step - loss: 0.0409 - val_loss: 0.0414

# training | RMSE: 0.2017, MAE: 0.1606
worker 0  xfile  [26, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4970884277939287}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1087460609141687}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1889310551921254}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2016808869003276, 'rmse': 0.2016808869003276, 'mae': 0.16059129199216207, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#1 epoch=3.0 loss={'loss': 0.13570671940727044, 'rmse': 0.13570671940727044, 'mae': 0.09958555935190071, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10996334638100778}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10952735483869187}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4326994328446677}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#0 epoch=3.0 loss={'loss': 0.09895041638632798, 'rmse': 0.09895041638632798, 'mae': 0.07736684639394083, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44587267964897137}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#2 epoch=3.0 loss={'loss': 0.1255554590761898, 'rmse': 0.1255554590761898, 'mae': 0.09876236904757964, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20261337059472792}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10963218549386174}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#4 epoch=3.0 loss={'loss': 0.14248664904814243, 'rmse': 0.14248664904814243, 'mae': 0.1125212293789281, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3338165530239823}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#5 epoch=3.0 loss={'loss': 0.15795342653018213, 'rmse': 0.15795342653018213, 'mae': 0.11825145708938387, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.16999145104470526}, 'layer_1_size': 24, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 72, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#3 epoch=3.0 loss={'loss': 0.19325496950642462, 'rmse': 0.19325496950642462, 'mae': 0.1663764504175135, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10289939797168778}, 'layer_1_size': 49, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 6, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 97, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#7 epoch=3.0 loss={'loss': 0.1682280462529784, 'rmse': 0.1682280462529784, 'mae': 0.13302461932561335, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 72, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2035119362983565}, 'layer_3_size': 28, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.48594345397869476}, 'layer_4_size': 97, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 35, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#6 epoch=3.0 loss={'loss': 0.14805786090369236, 'rmse': 0.14805786090369236, 'mae': 0.11833462240540302, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
#8 epoch=3.0 loss={'loss': 0.1507027701675847, 'rmse': 0.1507027701675847, 'mae': 0.11825425102531353, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20852464738421236}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#9 epoch=3.0 loss={'loss': 0.14633873827520127, 'rmse': 0.14633873827520127, 'mae': 0.1134831026715265, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2968844105931647}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.379665317170057}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#11 epoch=3.0 loss={'loss': 0.14739248738999622, 'rmse': 0.14739248738999622, 'mae': 0.11167727746345076, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1857037786421014}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#10 epoch=3.0 loss={'loss': 0.16462337835185933, 'rmse': 0.16462337835185933, 'mae': 0.13061356650237213, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.22735716747727375}, 'layer_1_size': 24, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 12, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 88, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 75, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#12 epoch=3.0 loss={'loss': 0.2064989482033038, 'rmse': 0.2064989482033038, 'mae': 0.1589884206256287, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.39459061959382125}, 'layer_2_size': 45, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 99, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.360442390830502}, 'layer_4_size': 11, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.32952669768377524}, 'layer_5_size': 18, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#13 epoch=3.0 loss={'loss': 0.16855163559017308, 'rmse': 0.16855163559017308, 'mae': 0.13098123019597174, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 70, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2538091909685959}, 'layer_2_size': 42, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 28, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#14 epoch=3.0 loss={'loss': 0.13622542310641958, 'rmse': 0.13622542310641958, 'mae': 0.10576451188443767, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1234109170669142}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.34670527040571764}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2799181323299549}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#15 epoch=3.0 loss={'loss': 0.1866506904410619, 'rmse': 0.1866506904410619, 'mae': 0.1458956782085537, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 32, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 45, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 36, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 93, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': None, 'shuffle': False}
#17 epoch=3.0 loss={'loss': 0.2024581741107015, 'rmse': 0.2024581741107015, 'mae': 0.1626775954232455, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 13, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 6, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.1935168503165503}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44411323388230295}, 'layer_4_size': 34, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#19 epoch=3.0 loss={'loss': 0.24853578962963374, 'rmse': 0.24853578962963374, 'mae': 0.19014996871275835, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14970703358753235}, 'layer_3_size': 51, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.479652463853537}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 82, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': True}
#16 epoch=3.0 loss={'loss': 0.1803710112315717, 'rmse': 0.1803710112315717, 'mae': 0.14316317192503317, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2597483729703344}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.211692527289565}, 'layer_2_size': 27, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 54, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 22, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#20 epoch=3.0 loss={'loss': 0.20310310937806159, 'rmse': 0.20310310937806159, 'mae': 0.16133077587231456, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.46107233114729274}, 'layer_1_size': 62, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 73, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10807125705702347}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MinMaxScaler', 'shuffle': False}
#18 epoch=3.0 loss={'loss': 0.17413270889721255, 'rmse': 0.17413270889721255, 'mae': 0.14067670974534638, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 43, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 55, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 2, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 89, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#21 epoch=3.0 loss={'loss': 0.1654379522730005, 'rmse': 0.1654379522730005, 'mae': 0.13261302180019188, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1020118725516797}, 'layer_2_size': 71, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 44, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': False}
#23 epoch=3.0 loss={'loss': 0.20493379096338202, 'rmse': 0.20493379096338202, 'mae': 0.157467961019968, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38345276572686826}, 'layer_1_size': 23, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 14, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 13, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.43851058729984005}, 'layer_4_size': 50, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#22 epoch=3.0 loss={'loss': 0.16799949597260586, 'rmse': 0.16799949597260586, 'mae': 0.12961184659604363, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 58, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 53, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 40, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 45, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': True}
#24 epoch=3.0 loss={'loss': 0.24307967999874627, 'rmse': 0.24307967999874627, 'mae': 0.1887506375342203, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 99, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3864152965008092}, 'layer_2_size': 19, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 18, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 77, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': True}
#25 epoch=3.0 loss={'loss': 0.18979099148565645, 'rmse': 0.18979099148565645, 'mae': 0.14961579759972965, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 12, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.37641506330630803}, 'layer_2_size': 51, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 32, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 38, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 86, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#26 epoch=3.0 loss={'loss': 0.2016808869003276, 'rmse': 0.2016808869003276, 'mae': 0.16059129199216207, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 99, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4970884277939287}, 'layer_3_size': 36, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.1087460609141687}, 'layer_4_size': 20, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1889310551921254}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
get a list [results] of length 108
get a list [loss] of length 27
get a list [val_loss] of length 27
length of indices is (0, 2, 1, 14, 4, 9, 11, 6, 8, 5, 10, 21, 22, 7, 13, 18, 16, 15, 25, 3, 26, 17, 20, 23, 12, 24, 19)
length of indices is 27
length of T is 27
newly formed T structure is:[[0, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44587267964897137}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [1, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20261337059472792}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10963218549386174}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [2, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10996334638100778}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10952735483869187}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4326994328446677}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [3, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1234109170669142}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.34670527040571764}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2799181323299549}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [4, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3338165530239823}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [5, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2968844105931647}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.379665317170057}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [6, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1857037786421014}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [7, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}], [8, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20852464738421236}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]] 

*** 9.0 configurations x 9.0 iterations each

27 | Thu Sep 27 23:24:21 2018 | lowest loss so far: 0.0990 (run 0)

{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  97 | activation: relu    | extras: None 
layer 2 | size:   4 | activation: tanh    | extras: batchnorm 
layer 3 | size:  16 | activation: sigmoid | extras: dropout - rate: 11.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 5:55 - loss: 0.3581
 288/6530 [>.............................] - ETA: 20s - loss: 0.2115 
 640/6530 [=>............................] - ETA: 8s - loss: 0.1869 {'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  33 | activation: relu    | extras: batchnorm 
layer 2 | size:  34 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a76fef0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 7:05 - loss: 0.5152
 992/6530 [===>..........................] - ETA: 5s - loss: 0.1754
 352/6530 [>.............................] - ETA: 19s - loss: 0.4738 
1360/6530 [=====>........................] - ETA: 4s - loss: 0.1638{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  52 | activation: tanh    | extras: batchnorm 
layer 2 | size:  94 | activation: relu    | extras: None 
layer 3 | size:  32 | activation: relu    | extras: dropout - rate: 20.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a76ff98>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 6:25 - loss: 1.9850
 688/6530 [==>...........................] - ETA: 9s - loss: 0.3559 
1744/6530 [=======>......................] - ETA: 3s - loss: 0.1583
 320/6530 [>.............................] - ETA: 19s - loss: 0.6492 
1040/6530 [===>..........................] - ETA: 6s - loss: 0.2695
2112/6530 [========>.....................] - ETA: 2s - loss: 0.1545
 640/6530 [=>............................] - ETA: 9s - loss: 0.3895 
1408/6530 [=====>........................] - ETA: 4s - loss: 0.2132
2480/6530 [==========>...................] - ETA: 2s - loss: 0.1533
 960/6530 [===>..........................] - ETA: 6s - loss: 0.2875
1760/6530 [=======>......................] - ETA: 3s - loss: 0.1798
2816/6530 [===========>..................] - ETA: 1s - loss: 0.1510
1280/6530 [====>.........................] - ETA: 4s - loss: 0.2344
2112/6530 [========>.....................] - ETA: 2s - loss: 0.1590
3200/6530 [=============>................] - ETA: 1s - loss: 0.1474
1600/6530 [======>.......................] - ETA: 3s - loss: 0.2008
2448/6530 [==========>...................] - ETA: 2s - loss: 0.1437
3584/6530 [===============>..............] - ETA: 1s - loss: 0.1468
1920/6530 [=======>......................] - ETA: 3s - loss: 0.1779
2816/6530 [===========>..................] - ETA: 1s - loss: 0.1302
3952/6530 [=================>............] - ETA: 0s - loss: 0.1452
2240/6530 [=========>....................] - ETA: 2s - loss: 0.1607
3184/6530 [=============>................] - ETA: 1s - loss: 0.1193
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1438
2576/6530 [==========>...................] - ETA: 2s - loss: 0.1473
3536/6530 [===============>..............] - ETA: 1s - loss: 0.1111
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1432
2912/6530 [============>.................] - ETA: 1s - loss: 0.1364
3904/6530 [================>.............] - ETA: 1s - loss: 0.1039
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1426
3248/6530 [=============>................] - ETA: 1s - loss: 0.1274
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0979
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1413
3584/6530 [===============>..............] - ETA: 1s - loss: 0.1202
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0929
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1401
3920/6530 [=================>............] - ETA: 1s - loss: 0.1144
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0880
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1397
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1101
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0841
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1055
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0804
6530/6530 [==============================] - 2s 281us/step - loss: 0.1388 - val_loss: 0.1167
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0967
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1019
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0770
 384/6530 [>.............................] - ETA: 0s - loss: 0.1138
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0978
6400/6530 [============================>.] - ETA: 0s - loss: 0.0741
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1183
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0945
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1192
6530/6530 [==============================] - 2s 314us/step - loss: 0.0730 - val_loss: 0.0183
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0159
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0914
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1191
 352/6530 [>.............................] - ETA: 0s - loss: 0.0194
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0889
1904/6530 [=======>......................] - ETA: 0s - loss: 0.1194
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0209
6432/6530 [============================>.] - ETA: 0s - loss: 0.0866
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1199
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0213
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1213
6530/6530 [==============================] - 2s 314us/step - loss: 0.0861 - val_loss: 0.0351
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0416
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0213
3056/6530 [=============>................] - ETA: 0s - loss: 0.1196
 352/6530 [>.............................] - ETA: 0s - loss: 0.0412
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0206
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1203
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0401
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0208
3824/6530 [================>.............] - ETA: 0s - loss: 0.1202
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0388
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0207
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1198
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0379
2848/6530 [============>.................] - ETA: 0s - loss: 0.0206
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1207
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0372
3216/6530 [=============>................] - ETA: 0s - loss: 0.0203
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1206
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0353
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0203
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1204
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0355
3936/6530 [=================>............] - ETA: 0s - loss: 0.0201
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1199
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0354
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0200
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1201
2992/6530 [============>.................] - ETA: 0s - loss: 0.0348
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0200
6528/6530 [============================>.] - ETA: 0s - loss: 0.1198
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0347
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0198
6530/6530 [==============================] - 1s 138us/step - loss: 0.1198 - val_loss: 0.1087
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0807
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0346
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0199
 400/6530 [>.............................] - ETA: 0s - loss: 0.1074
3984/6530 [=================>............] - ETA: 0s - loss: 0.0346
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0197
 800/6530 [==>...........................] - ETA: 0s - loss: 0.1123
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0344
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0196
1168/6530 [====>.........................] - ETA: 0s - loss: 0.1125
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0345
6432/6530 [============================>.] - ETA: 0s - loss: 0.0195
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1121
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0342
6530/6530 [==============================] - 1s 148us/step - loss: 0.0195 - val_loss: 0.0129
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0054
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1132
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0338
 352/6530 [>.............................] - ETA: 0s - loss: 0.0154
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1132
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0335
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0166
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1145
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0334
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0170
3040/6530 [============>.................] - ETA: 0s - loss: 0.1130
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0330
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0170
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1138
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0165
6530/6530 [==============================] - 1s 162us/step - loss: 0.0330 - val_loss: 0.0239

3840/6530 [================>.............] - ETA: 0s - loss: 0.1138Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0316
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0167
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1132
 336/6530 [>.............................] - ETA: 0s - loss: 0.0284
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0168
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1143
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0295
2864/6530 [============>.................] - ETA: 0s - loss: 0.0166
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1143
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0289
3216/6530 [=============>................] - ETA: 0s - loss: 0.0165
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1143
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0292
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0165
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1138
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0288
3888/6530 [================>.............] - ETA: 0s - loss: 0.0167
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1140
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0274
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0166
6512/6530 [============================>.] - ETA: 0s - loss: 0.1137
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0273
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0168
6530/6530 [==============================] - 1s 139us/step - loss: 0.1137 - val_loss: 0.1037
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0754
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0274
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0166
 416/6530 [>.............................] - ETA: 0s - loss: 0.1022
2944/6530 [============>.................] - ETA: 0s - loss: 0.0270
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0167
 800/6530 [==>...........................] - ETA: 0s - loss: 0.1081
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0269
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0166
1200/6530 [====>.........................] - ETA: 0s - loss: 0.1080
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0268
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0166
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1083
3920/6530 [=================>............] - ETA: 0s - loss: 0.0269
6352/6530 [============================>.] - ETA: 0s - loss: 0.0166
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1086
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0268
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1090
6530/6530 [==============================] - 1s 150us/step - loss: 0.0166 - val_loss: 0.0114
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0049
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0270
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1103
 368/6530 [>.............................] - ETA: 0s - loss: 0.0135
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0268
3120/6530 [=============>................] - ETA: 0s - loss: 0.1085
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0147
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0264
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1093
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0159
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0261
3904/6530 [================>.............] - ETA: 0s - loss: 0.1092
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0156
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0261
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1092
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0152
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0258
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1098
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0153
6496/6530 [============================>.] - ETA: 0s - loss: 0.0259
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1102
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0154
6530/6530 [==============================] - 1s 164us/step - loss: 0.0260 - val_loss: 0.0181
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0228
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1102
2912/6530 [============>.................] - ETA: 0s - loss: 0.0152
 352/6530 [>.............................] - ETA: 0s - loss: 0.0232
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1098
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0151
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0247
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1100
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0154
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0242
4016/6530 [=================>............] - ETA: 0s - loss: 0.0155
6530/6530 [==============================] - 1s 135us/step - loss: 0.1097 - val_loss: 0.1004

1344/6530 [=====>........................] - ETA: 0s - loss: 0.0247Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0722
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0153
 400/6530 [>.............................] - ETA: 0s - loss: 0.0997
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0245
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0155
 784/6530 [==>...........................] - ETA: 0s - loss: 0.1047
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0232
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0154
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1053
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0235
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0154
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1050
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0234
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0153
1936/6530 [=======>......................] - ETA: 0s - loss: 0.1054
3008/6530 [============>.................] - ETA: 0s - loss: 0.0230
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0154
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1057
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0228
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1069
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0228
6530/6530 [==============================] - 1s 146us/step - loss: 0.0153 - val_loss: 0.0106
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0055
3088/6530 [=============>................] - ETA: 0s - loss: 0.1052
4000/6530 [=================>............] - ETA: 0s - loss: 0.0228
 368/6530 [>.............................] - ETA: 0s - loss: 0.0125
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1061
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0227
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0138
3872/6530 [================>.............] - ETA: 0s - loss: 0.1062
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0228
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0151
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1060
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0226
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0149
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1067
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0223
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0145
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1071
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0221
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0144
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1071
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0222
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0146
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1064
6336/6530 [============================>.] - ETA: 0s - loss: 0.0220
2864/6530 [============>.................] - ETA: 0s - loss: 0.0144
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1069
6530/6530 [==============================] - 1s 160us/step - loss: 0.0223 - val_loss: 0.0151
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0213
3232/6530 [=============>................] - ETA: 0s - loss: 0.0142
6530/6530 [==============================] - 1s 135us/step - loss: 0.1066 - val_loss: 0.0979
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0718
 352/6530 [>.............................] - ETA: 0s - loss: 0.0205
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0145
 400/6530 [>.............................] - ETA: 0s - loss: 0.0975
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0226
3936/6530 [=================>............] - ETA: 0s - loss: 0.0146
 784/6530 [==>...........................] - ETA: 0s - loss: 0.1026
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0219
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0145
1168/6530 [====>.........................] - ETA: 0s - loss: 0.1033
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0223
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0146
1552/6530 [======>.......................] - ETA: 0s - loss: 0.1026
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0220
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0145
1936/6530 [=======>......................] - ETA: 0s - loss: 0.1029
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0206
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0146
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1032
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0209
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0145
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1041
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0211
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0145
3072/6530 [=============>................] - ETA: 0s - loss: 0.1027
2960/6530 [============>.................] - ETA: 0s - loss: 0.0207
6416/6530 [============================>.] - ETA: 0s - loss: 0.0144
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1036
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0205
6530/6530 [==============================] - 1s 149us/step - loss: 0.0144 - val_loss: 0.0100
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0055
3824/6530 [================>.............] - ETA: 0s - loss: 0.1036
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0204
 384/6530 [>.............................] - ETA: 0s - loss: 0.0121
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1034
3936/6530 [=================>............] - ETA: 0s - loss: 0.0205
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0131
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1044
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0205
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0143
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1043
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0205
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0140
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1045
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0204
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0138
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1039
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0201
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0139
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1041
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0199
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0139
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0200
2928/6530 [============>.................] - ETA: 0s - loss: 0.0136
6530/6530 [==============================] - 1s 137us/step - loss: 0.1041 - val_loss: 0.0960
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0685
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0199
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0135
 400/6530 [>.............................] - ETA: 0s - loss: 0.0957
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0138
 800/6530 [==>...........................] - ETA: 0s - loss: 0.1017
6530/6530 [==============================] - 1s 161us/step - loss: 0.0201 - val_loss: 0.0132
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0153
4016/6530 [=================>............] - ETA: 0s - loss: 0.0139
1200/6530 [====>.........................] - ETA: 0s - loss: 0.1012
 352/6530 [>.............................] - ETA: 0s - loss: 0.0178
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0138
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1011
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0202
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0139
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1009
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0198
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0138
2384/6530 [=========>....................] - ETA: 0s - loss: 0.1014
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0205
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0138
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1017
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0204
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0139
3184/6530 [=============>................] - ETA: 0s - loss: 0.1002
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0191
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0139
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1015
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0196
3968/6530 [=================>............] - ETA: 0s - loss: 0.1012
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0195
6530/6530 [==============================] - 1s 146us/step - loss: 0.0138 - val_loss: 0.0095
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0045
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1012
3008/6530 [============>.................] - ETA: 0s - loss: 0.0193
 384/6530 [>.............................] - ETA: 0s - loss: 0.0117
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1021
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0192
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0125
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1024
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0191
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0137
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1024
3968/6530 [=================>............] - ETA: 0s - loss: 0.0192
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0136
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1017
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0191
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0132
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1022
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0191
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0131
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0190
6530/6530 [==============================] - 1s 135us/step - loss: 0.1020 - val_loss: 0.0942
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0673
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0132
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0187
 416/6530 [>.............................] - ETA: 0s - loss: 0.0931
2832/6530 [============>.................] - ETA: 0s - loss: 0.0131
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0186
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0992
3200/6530 [=============>................] - ETA: 0s - loss: 0.0128
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0187
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0999
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0130
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0186
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0988
3904/6530 [================>.............] - ETA: 0s - loss: 0.0131
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0993
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0131
6530/6530 [==============================] - 1s 161us/step - loss: 0.0188 - val_loss: 0.0132
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0154
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0994
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0132
 352/6530 [>.............................] - ETA: 0s - loss: 0.0170
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1001
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0132
 640/6530 [=>............................] - ETA: 0s - loss: 0.0189
3088/6530 [=============>................] - ETA: 0s - loss: 0.0984
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0133
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0189
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0995
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0132
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0193
3888/6530 [================>.............] - ETA: 0s - loss: 0.0994
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0132
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0192
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0995
6416/6530 [============================>.] - ETA: 0s - loss: 0.0132
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0181
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1001
6530/6530 [==============================] - 1s 149us/step - loss: 0.0132 - val_loss: 0.0091
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0038
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0182
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1004
 368/6530 [>.............................] - ETA: 0s - loss: 0.0112
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0184
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1006
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0122
2928/6530 [============>.................] - ETA: 0s - loss: 0.0182
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1000
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0134
3248/6530 [=============>................] - ETA: 0s - loss: 0.0181
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1003
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0131
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0181
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0129
3904/6530 [================>.............] - ETA: 0s - loss: 0.0181
6530/6530 [==============================] - 1s 137us/step - loss: 0.1002 - val_loss: 0.0928
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0676
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0129
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0181
 416/6530 [>.............................] - ETA: 0s - loss: 0.0920
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0129
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0182
 816/6530 [==>...........................] - ETA: 0s - loss: 0.0980
2912/6530 [============>.................] - ETA: 0s - loss: 0.0126
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0181
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0983
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0124
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0178
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0978
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0126
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0176
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0974
4000/6530 [=================>............] - ETA: 0s - loss: 0.0127
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0177
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0984
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0125
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0177
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0981
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0127
3200/6530 [=============>................] - ETA: 0s - loss: 0.0968
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0126
6530/6530 [==============================] - 1s 164us/step - loss: 0.0180 - val_loss: 0.0125
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0149
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0978
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0127
 352/6530 [>.............................] - ETA: 0s - loss: 0.0168
3952/6530 [=================>............] - ETA: 0s - loss: 0.0980
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0127
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0187
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0978
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0127
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0181
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0988
6528/6530 [============================>.] - ETA: 0s - loss: 0.0127
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0185
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0988
6530/6530 [==============================] - 1s 149us/step - loss: 0.0127 - val_loss: 0.0090
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0043
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0183
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0989
 368/6530 [>.............................] - ETA: 0s - loss: 0.0109
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0174
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0984
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0117
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0174
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0990
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0129
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0177
6530/6530 [==============================] - 1s 137us/step - loss: 0.0987 - val_loss: 0.0914

1440/6530 [=====>........................] - ETA: 0s - loss: 0.0128
2896/6530 [============>.................] - ETA: 0s - loss: 0.0175
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0124
3216/6530 [=============>................] - ETA: 0s - loss: 0.0173
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0124
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0172
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0124
3888/6530 [================>.............] - ETA: 0s - loss: 0.0174
2896/6530 [============>.................] - ETA: 0s - loss: 0.0121
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0174
3248/6530 [=============>................] - ETA: 0s - loss: 0.0120
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0174
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0122
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0173
3968/6530 [=================>............] - ETA: 0s - loss: 0.0121
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0170
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0121
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0169
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0122
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0170
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0122
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0170
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0123
6528/6530 [============================>.] - ETA: 0s - loss: 0.0172
# training | RMSE: 0.1156, MAE: 0.0851
worker 2  xfile  [2, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10996334638100778}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10952735483869187}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4326994328446677}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.11559877278530636, 'rmse': 0.11559877278530636, 'mae': 0.08507184825296418, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  34 | activation: relu    | extras: None 
layer 2 | size:  29 | activation: tanh    | extras: dropout - rate: 12.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a7bb390>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 55s - loss: 0.9984
6530/6530 [==============================] - 1s 163us/step - loss: 0.0172 - val_loss: 0.0118
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0131
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0122
 592/6530 [=>............................] - ETA: 1s - loss: 0.5384 
 320/6530 [>.............................] - ETA: 1s - loss: 0.0154
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0123
1184/6530 [====>.........................] - ETA: 1s - loss: 0.4204
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0175
6448/6530 [============================>.] - ETA: 0s - loss: 0.0122
1776/6530 [=======>......................] - ETA: 0s - loss: 0.3643
6530/6530 [==============================] - 1s 148us/step - loss: 0.0123 - val_loss: 0.0090

 960/6530 [===>..........................] - ETA: 0s - loss: 0.0172
2384/6530 [=========>....................] - ETA: 0s - loss: 0.3288
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0178
2960/6530 [============>.................] - ETA: 0s - loss: 0.3063
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0175
3552/6530 [===============>..............] - ETA: 0s - loss: 0.2863
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0166
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2717
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0167
4784/6530 [====================>.........] - ETA: 0s - loss: 0.2584
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0169
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2488
2864/6530 [============>.................] - ETA: 0s - loss: 0.0167
6000/6530 [==========================>...] - ETA: 0s - loss: 0.2395
3184/6530 [=============>................] - ETA: 0s - loss: 0.0165
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0166
6530/6530 [==============================] - 1s 111us/step - loss: 0.2334 - val_loss: 0.1695
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1406
3872/6530 [================>.............] - ETA: 0s - loss: 0.0166
 656/6530 [==>...........................] - ETA: 0s - loss: 0.1628
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0166
1232/6530 [====>.........................] - ETA: 0s - loss: 0.1540
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0166
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1517
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0164
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1495
3120/6530 [=============>................] - ETA: 0s - loss: 0.1469
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0162
3760/6530 [================>.............] - ETA: 0s - loss: 0.1445
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0161
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1422
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0162
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1407
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0161
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1393
6530/6530 [==============================] - 1s 160us/step - loss: 0.0164 - val_loss: 0.0117

6256/6530 [===========================>..] - ETA: 0s - loss: 0.1373
6530/6530 [==============================] - 1s 84us/step - loss: 0.1371 - val_loss: 0.1285
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1086
 656/6530 [==>...........................] - ETA: 0s - loss: 0.1232
# training | RMSE: 0.0849, MAE: 0.0661
worker 0  xfile  [0, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44587267964897137}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.08486242945415225, 'rmse': 0.08486242945415225, 'mae': 0.06613285819775877, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  83 | activation: relu    | extras: batchnorm 
layer 2 | size:  75 | activation: tanh    | extras: None 
layer 3 | size:  44 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a7bb208>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 1:15 - loss: 0.9890
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1225
 640/6530 [=>............................] - ETA: 3s - loss: 0.4370  
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1226
1248/6530 [====>.........................] - ETA: 2s - loss: 0.3284
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1209
1920/6530 [=======>......................] - ETA: 1s - loss: 0.2787
3040/6530 [============>.................] - ETA: 0s - loss: 0.1202
2560/6530 [==========>...................] - ETA: 0s - loss: 0.2547
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1186
3168/6530 [=============>................] - ETA: 0s - loss: 0.2374
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1174
3808/6530 [================>.............] - ETA: 0s - loss: 0.2260
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1160
4448/6530 [===================>..........] - ETA: 0s - loss: 0.2167
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1155
5088/6530 [======================>.......] - ETA: 0s - loss: 0.2093
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1148
5728/6530 [=========================>....] - ETA: 0s - loss: 0.2032
# training | RMSE: 0.1003, MAE: 0.0789
worker 1  xfile  [1, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20261337059472792}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10963218549386174}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.1003262358143578, 'rmse': 0.1003262358143578, 'mae': 0.07892231308730584, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  84 | activation: tanh    | extras: None 
layer 2 | size:  42 | activation: sigmoid | extras: dropout - rate: 29.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a7bb278>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 1:13 - loss: 1.1818
6530/6530 [==============================] - 1s 87us/step - loss: 0.1146 - val_loss: 0.1558
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1460
6368/6530 [============================>.] - ETA: 0s - loss: 0.1986
 512/6530 [=>............................] - ETA: 2s - loss: 0.7496  
 640/6530 [=>............................] - ETA: 0s - loss: 0.1067
6530/6530 [==============================] - 1s 146us/step - loss: 0.1978 - val_loss: 0.2332
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.2216
 992/6530 [===>..........................] - ETA: 1s - loss: 0.4811
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1058
 640/6530 [=>............................] - ETA: 0s - loss: 0.1608
1488/6530 [=====>........................] - ETA: 1s - loss: 0.3792
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1073
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1517
1984/6530 [========>.....................] - ETA: 0s - loss: 0.3268
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1061
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1479
2448/6530 [==========>...................] - ETA: 0s - loss: 0.2959
3072/6530 [=============>................] - ETA: 0s - loss: 0.1065
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1438
2944/6530 [============>.................] - ETA: 0s - loss: 0.2723
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1056
3472/6530 [==============>...............] - ETA: 0s - loss: 0.2559
3136/6530 [=============>................] - ETA: 0s - loss: 0.1413
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1051
3984/6530 [=================>............] - ETA: 0s - loss: 0.2434
3776/6530 [================>.............] - ETA: 0s - loss: 0.1399
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1054
4496/6530 [===================>..........] - ETA: 0s - loss: 0.2338
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1390
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1051
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2263
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1384
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1043
5488/6530 [========================>.....] - ETA: 0s - loss: 0.2199
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1368
6530/6530 [==============================] - 1s 87us/step - loss: 0.1034 - val_loss: 0.1175
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1215
6000/6530 [==========================>...] - ETA: 0s - loss: 0.2144
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1366
 624/6530 [=>............................] - ETA: 0s - loss: 0.0973
6530/6530 [==============================] - 1s 86us/step - loss: 0.1363 - val_loss: 0.1854

6496/6530 [============================>.] - ETA: 0s - loss: 0.2092Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1815
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0973
6530/6530 [==============================] - 1s 136us/step - loss: 0.2088 - val_loss: 0.1646
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.2189
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1354
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0970
 512/6530 [=>............................] - ETA: 0s - loss: 0.1507
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1300
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0971
1008/6530 [===>..........................] - ETA: 0s - loss: 0.1475
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1275
3008/6530 [============>.................] - ETA: 0s - loss: 0.0966
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1440
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1247
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0964
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1430
3072/6530 [=============>................] - ETA: 0s - loss: 0.1224
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0968
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1398
3712/6530 [================>.............] - ETA: 0s - loss: 0.1213
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0966
3024/6530 [============>.................] - ETA: 0s - loss: 0.1377
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1206
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0961
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1378
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1210
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0961
4048/6530 [=================>............] - ETA: 0s - loss: 0.1364
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1200
6530/6530 [==============================] - 1s 88us/step - loss: 0.0959 - val_loss: 0.1079
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0881
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1354
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1201
 640/6530 [=>............................] - ETA: 0s - loss: 0.0943
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1346
6530/6530 [==============================] - 1s 88us/step - loss: 0.1199 - val_loss: 0.1122
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1283
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0951
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1333
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1158
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0937
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1325
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1159
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0943
6530/6530 [==============================] - 1s 104us/step - loss: 0.1314 - val_loss: 0.1333
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1399
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1125
3040/6530 [============>.................] - ETA: 0s - loss: 0.0932
 528/6530 [=>............................] - ETA: 0s - loss: 0.1180
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1120
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0932
1040/6530 [===>..........................] - ETA: 0s - loss: 0.1167
3136/6530 [=============>................] - ETA: 0s - loss: 0.1115
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0922
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1147
3744/6530 [================>.............] - ETA: 0s - loss: 0.1107
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0914
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1148
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1105
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0909
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1140
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1105
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0909
3056/6530 [=============>................] - ETA: 0s - loss: 0.1117
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1099
6530/6530 [==============================] - 1s 87us/step - loss: 0.0908 - val_loss: 0.0952
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0738
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1118
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1101
 624/6530 [=>............................] - ETA: 0s - loss: 0.0859
4048/6530 [=================>............] - ETA: 0s - loss: 0.1114
6530/6530 [==============================] - 1s 86us/step - loss: 0.1101 - val_loss: 0.1526
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1516
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0867
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1107
 640/6530 [=>............................] - ETA: 0s - loss: 0.1145
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0866
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1107
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1090
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0869
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1101
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1054
3024/6530 [============>.................] - ETA: 0s - loss: 0.0865
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1098
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1047
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0864
6512/6530 [============================>.] - ETA: 0s - loss: 0.1092
3008/6530 [============>.................] - ETA: 0s - loss: 0.1043
6530/6530 [==============================] - 1s 106us/step - loss: 0.1092 - val_loss: 0.1285
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1130
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0863
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1035
 512/6530 [=>............................] - ETA: 0s - loss: 0.1023
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0862
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1031
1008/6530 [===>..........................] - ETA: 0s - loss: 0.1030
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0865
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1029
1520/6530 [=====>........................] - ETA: 0s - loss: 0.1024
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0866
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1025
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1031
6530/6530 [==============================] - 1s 89us/step - loss: 0.0862 - val_loss: 0.0929
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0701
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1026
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1027
 624/6530 [=>............................] - ETA: 0s - loss: 0.0840
6530/6530 [==============================] - 1s 87us/step - loss: 0.1029 - val_loss: 0.1369
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1605
3024/6530 [============>.................] - ETA: 0s - loss: 0.1013
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0844
 640/6530 [=>............................] - ETA: 0s - loss: 0.1082
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1013
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0841
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1014
4048/6530 [=================>............] - ETA: 0s - loss: 0.1008
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0842
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0991
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1001
3008/6530 [============>.................] - ETA: 0s - loss: 0.0838
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0986
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1002
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0831
3104/6530 [=============>................] - ETA: 0s - loss: 0.0981
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0996
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0832
3712/6530 [================>.............] - ETA: 0s - loss: 0.0979
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0995
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0832
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0977
6530/6530 [==============================] - 1s 105us/step - loss: 0.0990 - val_loss: 0.1276
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0828
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0833
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0978
 512/6530 [=>............................] - ETA: 0s - loss: 0.0946
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0832
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0969
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0947
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0979
6530/6530 [==============================] - 1s 88us/step - loss: 0.0832 - val_loss: 0.0956
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0781
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0941
 560/6530 [=>............................] - ETA: 0s - loss: 0.0790
6530/6530 [==============================] - 1s 88us/step - loss: 0.0982 - val_loss: 0.1261
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1011
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0943
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0814
 640/6530 [=>............................] - ETA: 0s - loss: 0.1033
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0949
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0811
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1006
2992/6530 [============>.................] - ETA: 0s - loss: 0.0939
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0814
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0964
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0937
2976/6530 [============>.................] - ETA: 0s - loss: 0.0819
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0956
4016/6530 [=================>............] - ETA: 0s - loss: 0.0937
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0819
3200/6530 [=============>................] - ETA: 0s - loss: 0.0945
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0931
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0820
3808/6530 [================>.............] - ETA: 0s - loss: 0.0945
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0933
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0816
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0940
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0928
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0812
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0939
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0927
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0805
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0934
6530/6530 [==============================] - 1s 105us/step - loss: 0.0923 - val_loss: 0.1074
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0695
6530/6530 [==============================] - 1s 88us/step - loss: 0.0804 - val_loss: 0.0943

6272/6530 [===========================>..] - ETA: 0s - loss: 0.0941
 544/6530 [=>............................] - ETA: 0s - loss: 0.0873
6530/6530 [==============================] - 1s 86us/step - loss: 0.0944 - val_loss: 0.0914
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0891
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0888
 640/6530 [=>............................] - ETA: 0s - loss: 0.0985
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0888
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0955
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0888
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0928
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0894
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0922
3056/6530 [=============>................] - ETA: 0s - loss: 0.0880
3136/6530 [=============>................] - ETA: 0s - loss: 0.0912
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0883
3776/6530 [================>.............] - ETA: 0s - loss: 0.0905
4032/6530 [=================>............] - ETA: 0s - loss: 0.0887
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0900
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0883
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0900
# training | RMSE: 0.1180, MAE: 0.0927
worker 2  xfile  [3, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1234109170669142}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.34670527040571764}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2799181323299549}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.11797287301213853, 'rmse': 0.11797287301213853, 'mae': 0.09265434814164587, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  57 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777808f940>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 1:04 - loss: 0.6570
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0885
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0894
 640/6530 [=>............................] - ETA: 1s - loss: 0.6411  
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0881
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0901
1264/6530 [====>.........................] - ETA: 1s - loss: 0.5166
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0878
6530/6530 [==============================] - 1s 85us/step - loss: 0.0906 - val_loss: 0.1294
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1200
1856/6530 [=======>......................] - ETA: 0s - loss: 0.4095
 608/6530 [=>............................] - ETA: 0s - loss: 0.0958
6530/6530 [==============================] - 1s 105us/step - loss: 0.0873 - val_loss: 0.1007
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0598
2464/6530 [==========>...................] - ETA: 0s - loss: 0.3499
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0913
 496/6530 [=>............................] - ETA: 0s - loss: 0.0834
3056/6530 [=============>................] - ETA: 0s - loss: 0.3120
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0917
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0844
3680/6530 [===============>..............] - ETA: 0s - loss: 0.2867
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0900
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0844
4288/6530 [==================>...........] - ETA: 0s - loss: 0.2690
3008/6530 [============>.................] - ETA: 0s - loss: 0.0897
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0847
4848/6530 [=====================>........] - ETA: 0s - loss: 0.2570
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0892
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0857
5360/6530 [=======================>......] - ETA: 0s - loss: 0.2479
4096/6530 [=================>............] - ETA: 0s - loss: 0.0889
2912/6530 [============>.................] - ETA: 0s - loss: 0.0845
5904/6530 [==========================>...] - ETA: 0s - loss: 0.2394
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0885
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0839
6448/6530 [============================>.] - ETA: 0s - loss: 0.2325
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0882
3840/6530 [================>.............] - ETA: 0s - loss: 0.0841
6530/6530 [==============================] - 1s 117us/step - loss: 0.2313 - val_loss: 0.1571
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1686
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0878
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0837
 608/6530 [=>............................] - ETA: 0s - loss: 0.1556
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0886
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0844
6530/6530 [==============================] - 1s 92us/step - loss: 0.0888 - val_loss: 0.1164

1232/6530 [====>.........................] - ETA: 0s - loss: 0.1497
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0841
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1469
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0836
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1469
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0836
3120/6530 [=============>................] - ETA: 0s - loss: 0.1446
6530/6530 [==============================] - 1s 109us/step - loss: 0.0834 - val_loss: 0.1091
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0538
3728/6530 [================>.............] - ETA: 0s - loss: 0.1438
 544/6530 [=>............................] - ETA: 0s - loss: 0.0802
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1429
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0821
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1423
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0821
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1407
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0822
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1402
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0825
6530/6530 [==============================] - 1s 87us/step - loss: 0.1394 - val_loss: 0.1281
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1285
3088/6530 [=============>................] - ETA: 0s - loss: 0.0809
 592/6530 [=>............................] - ETA: 0s - loss: 0.1211
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0809
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1180
4112/6530 [=================>............] - ETA: 0s - loss: 0.0808
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1188
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0806
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1206
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0808
3152/6530 [=============>................] - ETA: 0s - loss: 0.1192
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0805
3808/6530 [================>.............] - ETA: 0s - loss: 0.1192
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0806
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1185
6530/6530 [==============================] - 1s 102us/step - loss: 0.0804 - val_loss: 0.0976
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0482
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1184
# training | RMSE: 0.1380, MAE: 0.1084
worker 0  xfile  [4, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3338165530239823}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.13801947661363823, 'rmse': 0.13801947661363823, 'mae': 0.10841153289730386, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  87 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  19 | activation: sigmoid | extras: None 
layer 3 | size:  33 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f77781981d0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 2:16 - loss: 0.2655
 496/6530 [=>............................] - ETA: 0s - loss: 0.0779
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1174
 384/6530 [>.............................] - ETA: 6s - loss: 0.0691  
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0788
6368/6530 [============================>.] - ETA: 0s - loss: 0.1177
 736/6530 [==>...........................] - ETA: 3s - loss: 0.0618
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0796
6530/6530 [==============================] - 1s 83us/step - loss: 0.1173 - val_loss: 0.1146
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1021
1088/6530 [===>..........................] - ETA: 2s - loss: 0.0582
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0792
 624/6530 [=>............................] - ETA: 0s - loss: 0.1063
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0545
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0801
1232/6530 [====>.........................] - ETA: 0s - loss: 0.1049
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0542
2912/6530 [============>.................] - ETA: 0s - loss: 0.0792
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1067
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0521
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0786
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1083
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0501
3888/6530 [================>.............] - ETA: 0s - loss: 0.0783
2960/6530 [============>.................] - ETA: 0s - loss: 0.1079
2832/6530 [============>.................] - ETA: 0s - loss: 0.0496
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0776
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1074
3200/6530 [=============>................] - ETA: 0s - loss: 0.0486
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0782
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1065
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0473
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0781
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1066
3888/6530 [================>.............] - ETA: 0s - loss: 0.0464
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0779
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1063
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0450
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0778
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1059
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0440
6530/6530 [==============================] - 1s 110us/step - loss: 0.0776 - val_loss: 0.1046

6530/6530 [==============================] - 1s 88us/step - loss: 0.1056 - val_loss: 0.1068
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0877
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0431
 640/6530 [=>............................] - ETA: 0s - loss: 0.0981
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0422
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0969
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0412
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0987
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0403
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1007
6432/6530 [============================>.] - ETA: 0s - loss: 0.0394
3200/6530 [=============>................] - ETA: 0s - loss: 0.0995
3856/6530 [================>.............] - ETA: 0s - loss: 0.0995
6530/6530 [==============================] - 1s 204us/step - loss: 0.0394 - val_loss: 0.0396
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0431
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0987
 368/6530 [>.............................] - ETA: 0s - loss: 0.0249
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0990
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0271
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0984
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0268
6336/6530 [============================>.] - ETA: 0s - loss: 0.0986
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0260
6530/6530 [==============================] - 1s 84us/step - loss: 0.0983 - val_loss: 0.1015
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0793
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0261
 592/6530 [=>............................] - ETA: 0s - loss: 0.0927
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0267
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0926
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0264
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0937
3024/6530 [============>.................] - ETA: 0s - loss: 0.0262
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0951
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0260
3008/6530 [============>.................] - ETA: 0s - loss: 0.0948
# training | RMSE: 0.1284, MAE: 0.1005
worker 1  xfile  [5, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2968844105931647}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.379665317170057}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.1283521082395272, 'rmse': 0.1283521082395272, 'mae': 0.10054313275576332, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  68 | activation: relu    | extras: batchnorm 
layer 2 | size:   4 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7755e6cf28>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 1:03 - loss: 0.4279
3792/6530 [================>.............] - ETA: 0s - loss: 0.0263
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0949
 800/6530 [==>...........................] - ETA: 2s - loss: 0.3281  
4128/6530 [=================>............] - ETA: 0s - loss: 0.0258
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0940
1568/6530 [======>.......................] - ETA: 1s - loss: 0.2400
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0259
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0942
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1943
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0257
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0942
3040/6530 [============>.................] - ETA: 0s - loss: 0.1684
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0257
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0938
3808/6530 [================>.............] - ETA: 0s - loss: 0.1464
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0257
6480/6530 [============================>.] - ETA: 0s - loss: 0.0938
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1301
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0256
6530/6530 [==============================] - 1s 90us/step - loss: 0.0937 - val_loss: 0.0980
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0728
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1179
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0254
 624/6530 [=>............................] - ETA: 0s - loss: 0.0884
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1087
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0896
6530/6530 [==============================] - 1s 145us/step - loss: 0.0253 - val_loss: 0.0218
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0247
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0907
6530/6530 [==============================] - 1s 121us/step - loss: 0.1055 - val_loss: 0.0381
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0402
 384/6530 [>.............................] - ETA: 0s - loss: 0.0249
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0921
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0368
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0217
3040/6530 [============>.................] - ETA: 0s - loss: 0.0917
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0364
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0221
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0919
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0373
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0223
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0909
3136/6530 [=============>................] - ETA: 0s - loss: 0.0363
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0222
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0912
3936/6530 [=================>............] - ETA: 0s - loss: 0.0362
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0230
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0912
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0364
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0229
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0911
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0358
2896/6530 [============>.................] - ETA: 0s - loss: 0.0227
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0347
6530/6530 [==============================] - 1s 87us/step - loss: 0.0909 - val_loss: 0.0971
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0714
3248/6530 [=============>................] - ETA: 0s - loss: 0.0226
6530/6530 [==============================] - 0s 68us/step - loss: 0.0345 - val_loss: 0.0276
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0316
 624/6530 [=>............................] - ETA: 0s - loss: 0.0888
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0227
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0265
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0898
3952/6530 [=================>............] - ETA: 0s - loss: 0.0226
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0269
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0894
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0228
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0279
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0903
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0229
3008/6530 [============>.................] - ETA: 0s - loss: 0.0274
2912/6530 [============>.................] - ETA: 0s - loss: 0.0907
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0227
3776/6530 [================>.............] - ETA: 0s - loss: 0.0273
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0903
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0225
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0278
4128/6530 [=================>............] - ETA: 0s - loss: 0.0896
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0223
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0272
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0893
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0225
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0267
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0896
6480/6530 [============================>.] - ETA: 0s - loss: 0.0227
6530/6530 [==============================] - 0s 71us/step - loss: 0.0268 - val_loss: 0.0220
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0359
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0892
6530/6530 [==============================] - 1s 149us/step - loss: 0.0227 - val_loss: 0.0233
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0361
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0232
6448/6530 [============================>.] - ETA: 0s - loss: 0.0893
 384/6530 [>.............................] - ETA: 0s - loss: 0.0219
6530/6530 [==============================] - 1s 90us/step - loss: 0.0892 - val_loss: 0.0941
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0673
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0224
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0209
 624/6530 [=>............................] - ETA: 0s - loss: 0.0867
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0222
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0217
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0879
3232/6530 [=============>................] - ETA: 0s - loss: 0.0225
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0223
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0883
4032/6530 [=================>............] - ETA: 0s - loss: 0.0223
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0222
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0892
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0220
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0219
3072/6530 [=============>................] - ETA: 0s - loss: 0.0883
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0219
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0214
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0884
6336/6530 [============================>.] - ETA: 0s - loss: 0.0221
2928/6530 [============>.................] - ETA: 0s - loss: 0.0215
6530/6530 [==============================] - 0s 68us/step - loss: 0.0222 - val_loss: 0.0200
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0265
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0875
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0216
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0238
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0878
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0213
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0218
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0878
4032/6530 [=================>............] - ETA: 0s - loss: 0.0214
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0218
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0879
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0213
3232/6530 [=============>................] - ETA: 0s - loss: 0.0213
6530/6530 [==============================] - 1s 86us/step - loss: 0.0876 - val_loss: 0.0936

4784/6530 [====================>.........] - ETA: 0s - loss: 0.0212
4064/6530 [=================>............] - ETA: 0s - loss: 0.0214
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0210
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0214
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0209
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0211
# training | RMSE: 0.1163, MAE: 0.0850
worker 2  xfile  [6, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1857037786421014}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.11629261346087715, 'rmse': 0.11629261346087715, 'mae': 0.08501122348887294, 'early_stop': False}
vggnet done  2

5856/6530 [=========================>....] - ETA: 0s - loss: 0.0210
6432/6530 [============================>.] - ETA: 0s - loss: 0.0209
6530/6530 [==============================] - 0s 67us/step - loss: 0.0209 - val_loss: 0.0174
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0240
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0211
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0201
6530/6530 [==============================] - 1s 144us/step - loss: 0.0211 - val_loss: 0.0143
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0107
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0190
 400/6530 [>.............................] - ETA: 0s - loss: 0.0192
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0194
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0201
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0191
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0196
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0191
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0195
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0188
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0201
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0186
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0201
6496/6530 [============================>.] - ETA: 0s - loss: 0.0184
6530/6530 [==============================] - 0s 66us/step - loss: 0.0184 - val_loss: 0.0148
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0070
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0199
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0154
3040/6530 [============>.................] - ETA: 0s - loss: 0.0201
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0155
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0199
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0156
3808/6530 [================>.............] - ETA: 0s - loss: 0.0200
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0158
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0201
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0160
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0202
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0161
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0201
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0162
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0199
6530/6530 [==============================] - 0s 62us/step - loss: 0.0162 - val_loss: 0.0142
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0155
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0200
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0157
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0200
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0163
6530/6530 [==============================] - 1s 138us/step - loss: 0.0201 - val_loss: 0.0144

2560/6530 [==========>...................] - ETA: 0s - loss: 0.0156Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0238
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0158
 400/6530 [>.............................] - ETA: 0s - loss: 0.0217
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0153
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0214
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0151
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0215
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0150
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0210
6530/6530 [==============================] - 0s 63us/step - loss: 0.0150 - val_loss: 0.0149
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0130
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0206
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0144
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0202
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0202
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0143
3120/6530 [=============>................] - ETA: 0s - loss: 0.0201
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0148
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0204
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0148
3856/6530 [================>.............] - ETA: 0s - loss: 0.0204
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0146
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0200
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0145
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0197
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0146
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0195
6530/6530 [==============================] - 0s 64us/step - loss: 0.0146 - val_loss: 0.0131

5360/6530 [=======================>......] - ETA: 0s - loss: 0.0194
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0193
# training | RMSE: 0.1096, MAE: 0.0853
worker 1  xfile  [8, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20852464738421236}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.1095743516568681, 'rmse': 0.1095743516568681, 'mae': 0.08526961670443206, 'early_stop': False}
vggnet done  1

6112/6530 [===========================>..] - ETA: 0s - loss: 0.0193
6512/6530 [============================>.] - ETA: 0s - loss: 0.0192
6530/6530 [==============================] - 1s 138us/step - loss: 0.0192 - val_loss: 0.0352
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0169
 432/6530 [>.............................] - ETA: 0s - loss: 0.0192
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0182
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0187
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0180
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0181
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0184
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0186
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0183
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0183
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0185
3008/6530 [============>.................] - ETA: 0s - loss: 0.0183
3216/6530 [=============>................] - ETA: 0s - loss: 0.0184
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0184
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0186
3936/6530 [=================>............] - ETA: 0s - loss: 0.0187
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0185
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0184
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0183
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0184
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0183
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0182
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0182
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0182
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0182
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0183
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0182
6416/6530 [============================>.] - ETA: 0s - loss: 0.0181
6530/6530 [==============================] - 1s 226us/step - loss: 0.0180 - val_loss: 0.0146
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0219
 208/6530 [..............................] - ETA: 1s - loss: 0.0150
 416/6530 [>.............................] - ETA: 1s - loss: 0.0177
 624/6530 [=>............................] - ETA: 1s - loss: 0.0176
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0191
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0183
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0185
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0189
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0188
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0185
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0189
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0185
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0182
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0181
2880/6530 [============>.................] - ETA: 0s - loss: 0.0177
3152/6530 [=============>................] - ETA: 0s - loss: 0.0177
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0180
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0178
3968/6530 [=================>............] - ETA: 0s - loss: 0.0178
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0177
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0178
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0176
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0175
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0177
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0177
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0178
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0179
6480/6530 [============================>.] - ETA: 0s - loss: 0.0178
6530/6530 [==============================] - 1s 222us/step - loss: 0.0178 - val_loss: 0.0320
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0171
 320/6530 [>.............................] - ETA: 1s - loss: 0.0167
 592/6530 [=>............................] - ETA: 1s - loss: 0.0189
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0170
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0176
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0177
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0175
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0175
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0173
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0170
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0171
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0176
2832/6530 [============>.................] - ETA: 0s - loss: 0.0177
3072/6530 [=============>................] - ETA: 0s - loss: 0.0179
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0177
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0178
3888/6530 [================>.............] - ETA: 0s - loss: 0.0177
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0180
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0180
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0180
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0180
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0179
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0180
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0179
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0180
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0179
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0181
6352/6530 [============================>.] - ETA: 0s - loss: 0.0181
6528/6530 [============================>.] - ETA: 0s - loss: 0.0180
6530/6530 [==============================] - 2s 234us/step - loss: 0.0180 - val_loss: 0.0194

# training | RMSE: 0.1335, MAE: 0.1091
worker 0  xfile  [7, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.13348192591765856, 'rmse': 0.13348192591765856, 'mae': 0.10914567104378854, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#2 epoch=9.0 loss={'loss': 0.11559877278530636, 'rmse': 0.11559877278530636, 'mae': 0.08507184825296418, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10996334638100778}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10952735483869187}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4326994328446677}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#0 epoch=9.0 loss={'loss': 0.08486242945415225, 'rmse': 0.08486242945415225, 'mae': 0.06613285819775877, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44587267964897137}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#1 epoch=9.0 loss={'loss': 0.1003262358143578, 'rmse': 0.1003262358143578, 'mae': 0.07892231308730584, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20261337059472792}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10963218549386174}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#3 epoch=9.0 loss={'loss': 0.11797287301213853, 'rmse': 0.11797287301213853, 'mae': 0.09265434814164587, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 34, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1234109170669142}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.34670527040571764}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2799181323299549}, 'layer_5_size': 85, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#4 epoch=9.0 loss={'loss': 0.13801947661363823, 'rmse': 0.13801947661363823, 'mae': 0.10841153289730386, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 75, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 44, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 89, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.3338165530239823}, 'layer_5_size': 70, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#5 epoch=9.0 loss={'loss': 0.1283521082395272, 'rmse': 0.1283521082395272, 'mae': 0.10054313275576332, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 84, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2968844105931647}, 'layer_2_size': 42, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 70, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.379665317170057}, 'layer_5_size': 80, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#6 epoch=9.0 loss={'loss': 0.11629261346087715, 'rmse': 0.11629261346087715, 'mae': 0.08501122348887294, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 57, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 74, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 74, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1857037786421014}, 'layer_5_size': 50, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#8 epoch=9.0 loss={'loss': 0.1095743516568681, 'rmse': 0.1095743516568681, 'mae': 0.08526961670443206, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 68, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 96, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20852464738421236}, 'layer_5_size': 52, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#7 epoch=9.0 loss={'loss': 0.13348192591765856, 'rmse': 0.13348192591765856, 'mae': 0.10914567104378854, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 87, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 19, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 33, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': True}
get a list [results] of length 117
get a list [loss] of length 9
get a list [val_loss] of length 9
length of indices is (0, 1, 8, 2, 6, 3, 5, 7, 4)
length of indices is 9
length of T is 9
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44587267964897137}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [1, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20261337059472792}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10963218549386174}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [2, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10996334638100778}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10952735483869187}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4326994328446677}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]] 

*** 3.0 configurations x 27.0 iterations each

8 | Thu Sep 27 23:24:49 2018 | lowest loss so far: 0.0849 (run 0)

{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  97 | activation: relu    | extras: None 
layer 2 | size:   4 | activation: tanh    | extras: batchnorm 
layer 3 | size:  16 | activation: sigmoid | extras: dropout - rate: 11.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 7:36 - loss: 0.3581{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  33 | activation: relu    | extras: batchnorm 
layer 2 | size:  34 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a74b8d0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 7:41 - loss: 0.5152
 288/6530 [>.............................] - ETA: 25s - loss: 0.2115 
 304/6530 [>.............................] - ETA: 24s - loss: 0.4865 {'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  52 | activation: tanh    | extras: batchnorm 
layer 2 | size:  94 | activation: relu    | extras: None 
layer 3 | size:  32 | activation: relu    | extras: dropout - rate: 20.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a74b978>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 7:35 - loss: 1.9850
 624/6530 [=>............................] - ETA: 11s - loss: 0.1872
 624/6530 [=>............................] - ETA: 11s - loss: 0.3788
 288/6530 [>.............................] - ETA: 25s - loss: 0.6897 
 992/6530 [===>..........................] - ETA: 7s - loss: 0.1754 
 944/6530 [===>..........................] - ETA: 7s - loss: 0.2901 
1328/6530 [=====>........................] - ETA: 5s - loss: 0.1650
 608/6530 [=>............................] - ETA: 11s - loss: 0.4056
1280/6530 [====>.........................] - ETA: 5s - loss: 0.2297
1696/6530 [======>.......................] - ETA: 3s - loss: 0.1587
 912/6530 [===>..........................] - ETA: 7s - loss: 0.2989 
1632/6530 [======>.......................] - ETA: 4s - loss: 0.1901
2064/6530 [========>.....................] - ETA: 3s - loss: 0.1552
1232/6530 [====>.........................] - ETA: 5s - loss: 0.2402
1968/6530 [========>.....................] - ETA: 3s - loss: 0.1672
2448/6530 [==========>...................] - ETA: 2s - loss: 0.1535
1536/6530 [======>.......................] - ETA: 4s - loss: 0.2060
2304/6530 [=========>....................] - ETA: 2s - loss: 0.1498
2832/6530 [============>.................] - ETA: 1s - loss: 0.1508
1856/6530 [=======>......................] - ETA: 3s - loss: 0.1819
2640/6530 [===========>..................] - ETA: 2s - loss: 0.1366
3200/6530 [=============>................] - ETA: 1s - loss: 0.1474
2176/6530 [========>.....................] - ETA: 2s - loss: 0.1636
2992/6530 [============>.................] - ETA: 1s - loss: 0.1246
3584/6530 [===============>..............] - ETA: 1s - loss: 0.1468
2496/6530 [==========>...................] - ETA: 2s - loss: 0.1495
3344/6530 [==============>...............] - ETA: 1s - loss: 0.1155
3968/6530 [=================>............] - ETA: 1s - loss: 0.1450
2816/6530 [===========>..................] - ETA: 2s - loss: 0.1391
3680/6530 [===============>..............] - ETA: 1s - loss: 0.1081
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1436
3136/6530 [=============>................] - ETA: 1s - loss: 0.1304
4032/6530 [=================>............] - ETA: 1s - loss: 0.1017
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1434
3456/6530 [==============>...............] - ETA: 1s - loss: 0.1227
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0956
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1425
3776/6530 [================>.............] - ETA: 1s - loss: 0.1167
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0909
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1411
4096/6530 [=================>............] - ETA: 1s - loss: 0.1115
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0864
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1401
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1073
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0826
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1397
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1032
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0791
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0995
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0761
6530/6530 [==============================] - 2s 320us/step - loss: 0.1388 - val_loss: 0.1167
Epoch 2/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0967
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0957
6496/6530 [============================>.] - ETA: 0s - loss: 0.0733
 400/6530 [>.............................] - ETA: 0s - loss: 0.1146
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0924
6530/6530 [==============================] - 2s 333us/step - loss: 0.0730 - val_loss: 0.0183
Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0159
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1183
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0896
 384/6530 [>.............................] - ETA: 0s - loss: 0.0196
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1199
6352/6530 [============================>.] - ETA: 0s - loss: 0.0871
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0209
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1191
6530/6530 [==============================] - 2s 343us/step - loss: 0.0861 - val_loss: 0.0351

1104/6530 [====>.........................] - ETA: 0s - loss: 0.0218Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0416
1936/6530 [=======>......................] - ETA: 0s - loss: 0.1195
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0212
 352/6530 [>.............................] - ETA: 0s - loss: 0.0412
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1197
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0405
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0209
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1214
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0387
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0208
3120/6530 [=============>................] - ETA: 0s - loss: 0.1194
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0382
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0208
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1202
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0372
2944/6530 [============>.................] - ETA: 0s - loss: 0.0204
3888/6530 [================>.............] - ETA: 0s - loss: 0.1201
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0355
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0203
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1199
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0354
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0202
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1204
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0354
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1206
4048/6530 [=================>............] - ETA: 0s - loss: 0.0202
2960/6530 [============>.................] - ETA: 0s - loss: 0.0350
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1204
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0199
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0348
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1200
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0200
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1202
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0344
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0198
3920/6530 [=================>............] - ETA: 0s - loss: 0.0347
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0198
6530/6530 [==============================] - 1s 136us/step - loss: 0.1198 - val_loss: 0.1087
Epoch 3/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0806
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0344
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0196
 384/6530 [>.............................] - ETA: 0s - loss: 0.1066
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0345
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0196
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1110
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0344
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1131
6530/6530 [==============================] - 1s 147us/step - loss: 0.0195 - val_loss: 0.0129
Epoch 3/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0054
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0339
1520/6530 [=====>........................] - ETA: 0s - loss: 0.1120
 368/6530 [>.............................] - ETA: 0s - loss: 0.0154
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0336
1904/6530 [=======>......................] - ETA: 0s - loss: 0.1127
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0165
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0334
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1130
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0173
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0331
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1146
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0170
6528/6530 [============================>.] - ETA: 0s - loss: 0.0330
3072/6530 [=============>................] - ETA: 0s - loss: 0.1130
6530/6530 [==============================] - 1s 162us/step - loss: 0.0330 - val_loss: 0.0239
Epoch 3/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0316
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0166
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1138
 336/6530 [>.............................] - ETA: 0s - loss: 0.0284
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0166
3872/6530 [================>.............] - ETA: 0s - loss: 0.1138
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0295
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0168
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1134
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0288
2864/6530 [============>.................] - ETA: 0s - loss: 0.0166
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1141
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0292
3232/6530 [=============>................] - ETA: 0s - loss: 0.0164
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1144
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0286
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0166
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1143
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0273
3968/6530 [=================>............] - ETA: 0s - loss: 0.0167
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1137
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0275
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0166
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1140
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0274
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0168
2944/6530 [============>.................] - ETA: 0s - loss: 0.0270
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0166
6530/6530 [==============================] - 1s 137us/step - loss: 0.1137 - val_loss: 0.1037
Epoch 4/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0754
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0269
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0167
 416/6530 [>.............................] - ETA: 0s - loss: 0.1022
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0268
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0165
 816/6530 [==>...........................] - ETA: 0s - loss: 0.1074
3920/6530 [=================>............] - ETA: 0s - loss: 0.0269
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0166
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1082
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0268
6512/6530 [============================>.] - ETA: 0s - loss: 0.0166
1616/6530 [======>.......................] - ETA: 0s - loss: 0.1084
6530/6530 [==============================] - 1s 147us/step - loss: 0.0166 - val_loss: 0.0114
Epoch 4/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0049
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0269
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1087
 384/6530 [>.............................] - ETA: 0s - loss: 0.0136
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0268
2384/6530 [=========>....................] - ETA: 0s - loss: 0.1093
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0146
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0264
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1099
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0158
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0261
3184/6530 [=============>................] - ETA: 0s - loss: 0.1083
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0155
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0261
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1094
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0152
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0259
3984/6530 [=================>............] - ETA: 0s - loss: 0.1092
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0153
6528/6530 [============================>.] - ETA: 0s - loss: 0.0260
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1089
6530/6530 [==============================] - 1s 162us/step - loss: 0.0260 - val_loss: 0.0181
Epoch 4/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0228
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0154
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1102
 336/6530 [>.............................] - ETA: 0s - loss: 0.0233
2880/6530 [============>.................] - ETA: 0s - loss: 0.0152
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1101
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0247
3248/6530 [=============>................] - ETA: 0s - loss: 0.0151
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1099
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0242
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0153
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1096
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0247
3984/6530 [=================>............] - ETA: 0s - loss: 0.0155
6320/6530 [============================>.] - ETA: 0s - loss: 0.1099
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0245
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0153
6530/6530 [==============================] - 1s 134us/step - loss: 0.1097 - val_loss: 0.1004
Epoch 5/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0722
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0233
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0155
 400/6530 [>.............................] - ETA: 0s - loss: 0.0997
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0234
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0154
 816/6530 [==>...........................] - ETA: 0s - loss: 0.1049
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0234
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0154
1200/6530 [====>.........................] - ETA: 0s - loss: 0.1051
2960/6530 [============>.................] - ETA: 0s - loss: 0.0231
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0153
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1054
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0228
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0154
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1054
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0228
6480/6530 [============================>.] - ETA: 0s - loss: 0.0153
2384/6530 [=========>....................] - ETA: 0s - loss: 0.1060
6530/6530 [==============================] - 1s 147us/step - loss: 0.0153 - val_loss: 0.0106
Epoch 5/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0055
3968/6530 [=================>............] - ETA: 0s - loss: 0.0228
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1067
 352/6530 [>.............................] - ETA: 0s - loss: 0.0126
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0228
3152/6530 [=============>................] - ETA: 0s - loss: 0.1051
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0137
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0228
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1059
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0149
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0227
3920/6530 [=================>............] - ETA: 0s - loss: 0.1060
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0148
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0224
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1061
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0144
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0222
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1066
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0144
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0222
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1071
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0145
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0220
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1069
2880/6530 [============>.................] - ETA: 0s - loss: 0.0143
6530/6530 [==============================] - 1s 160us/step - loss: 0.0223 - val_loss: 0.0151
Epoch 5/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0213
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1064
3232/6530 [=============>................] - ETA: 0s - loss: 0.0142
 304/6530 [>.............................] - ETA: 1s - loss: 0.0207
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1068
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0145
 640/6530 [=>............................] - ETA: 0s - loss: 0.0222
3968/6530 [=================>............] - ETA: 0s - loss: 0.0145
6530/6530 [==============================] - 1s 136us/step - loss: 0.1066 - val_loss: 0.0979
Epoch 6/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0718
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0219
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0145
 416/6530 [>.............................] - ETA: 0s - loss: 0.0971
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0223
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0146
 784/6530 [==>...........................] - ETA: 0s - loss: 0.1026
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0221
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0145
1168/6530 [====>.........................] - ETA: 0s - loss: 0.1033
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0209
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0146
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1027
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0209
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0145
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1033
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0211
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0145
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1034
2880/6530 [============>.................] - ETA: 0s - loss: 0.0207
6448/6530 [============================>.] - ETA: 0s - loss: 0.0144
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1042
3216/6530 [=============>................] - ETA: 0s - loss: 0.0206
6530/6530 [==============================] - 1s 148us/step - loss: 0.0144 - val_loss: 0.0100
Epoch 6/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0055
3136/6530 [=============>................] - ETA: 0s - loss: 0.1024
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0205
 384/6530 [>.............................] - ETA: 0s - loss: 0.0121
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1034
3872/6530 [================>.............] - ETA: 0s - loss: 0.0205
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0130
3920/6530 [=================>............] - ETA: 0s - loss: 0.1035
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0205
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0142
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1037
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0205
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0140
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1041
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0205
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0139
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1045
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0201
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0138
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1046
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0199
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0139
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1040
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0200
2896/6530 [============>.................] - ETA: 0s - loss: 0.0136
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1044
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0199
3264/6530 [=============>................] - ETA: 0s - loss: 0.0135
6528/6530 [============================>.] - ETA: 0s - loss: 0.0201
6530/6530 [==============================] - 1s 136us/step - loss: 0.1041 - val_loss: 0.0959
Epoch 7/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0697
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0138
6530/6530 [==============================] - 1s 162us/step - loss: 0.0201 - val_loss: 0.0132
Epoch 6/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0153
 400/6530 [>.............................] - ETA: 0s - loss: 0.0956
3984/6530 [=================>............] - ETA: 0s - loss: 0.0139
 352/6530 [>.............................] - ETA: 0s - loss: 0.0178
 784/6530 [==>...........................] - ETA: 0s - loss: 0.1009
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0137
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0200
1168/6530 [====>.........................] - ETA: 0s - loss: 0.1016
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0139
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0198
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1007
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0138
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0205
1968/6530 [========>.....................] - ETA: 0s - loss: 0.1010
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0139
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0203
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1011
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0138
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0192
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1020
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0138
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0195
3104/6530 [=============>................] - ETA: 0s - loss: 0.1003
6528/6530 [============================>.] - ETA: 0s - loss: 0.0138
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0196
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1015
6530/6530 [==============================] - 1s 149us/step - loss: 0.0138 - val_loss: 0.0095
Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0045
2928/6530 [============>.................] - ETA: 0s - loss: 0.0193
3856/6530 [================>.............] - ETA: 0s - loss: 0.1013
 368/6530 [>.............................] - ETA: 0s - loss: 0.0116
3264/6530 [=============>................] - ETA: 0s - loss: 0.0192
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1012
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0126
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0192
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1019
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0139
3936/6530 [=================>............] - ETA: 0s - loss: 0.0192
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1024
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0136
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0191
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1025
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0133
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0192
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1017
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0133
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0190
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1021
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0133
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0187
2928/6530 [============>.................] - ETA: 0s - loss: 0.0129
6530/6530 [==============================] - 1s 137us/step - loss: 0.1019 - val_loss: 0.0942
Epoch 8/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0686
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0186
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0128
 384/6530 [>.............................] - ETA: 0s - loss: 0.0930
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0187
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0131
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0992
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0187
4000/6530 [=================>............] - ETA: 0s - loss: 0.0132
1168/6530 [====>.........................] - ETA: 0s - loss: 0.1000
6530/6530 [==============================] - 1s 162us/step - loss: 0.0188 - val_loss: 0.0132
Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0154
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0130
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0989
 336/6530 [>.............................] - ETA: 0s - loss: 0.0168
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0132
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0994
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0190
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0132
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0995
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0187
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0132
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1002
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0192
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0132
3072/6530 [=============>................] - ETA: 0s - loss: 0.0986
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0190
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0132
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0997
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0179
6480/6530 [============================>.] - ETA: 0s - loss: 0.0132
3872/6530 [================>.............] - ETA: 0s - loss: 0.0996
6530/6530 [==============================] - 1s 148us/step - loss: 0.0132 - val_loss: 0.0091
Epoch 8/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0038
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0181
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0995
 368/6530 [>.............................] - ETA: 0s - loss: 0.0112
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0184
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1004
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0121
2928/6530 [============>.................] - ETA: 0s - loss: 0.0182
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1004
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0133
3232/6530 [=============>................] - ETA: 0s - loss: 0.0181
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1006
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0132
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0181
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1000
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0129
3904/6530 [================>.............] - ETA: 0s - loss: 0.0181
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1002
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0127
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0181
6528/6530 [============================>.] - ETA: 0s - loss: 0.1002
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0128
6530/6530 [==============================] - 1s 139us/step - loss: 0.1002 - val_loss: 0.0927
Epoch 9/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0679
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0182
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0127
 400/6530 [>.............................] - ETA: 0s - loss: 0.0924
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0181
3184/6530 [=============>................] - ETA: 0s - loss: 0.0124
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0178
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0982
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0125
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0176
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0985
3904/6530 [================>.............] - ETA: 0s - loss: 0.0126
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0976
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0177
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0126
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0979
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0177
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0127
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0978
6464/6530 [============================>.] - ETA: 0s - loss: 0.0178
6530/6530 [==============================] - 1s 164us/step - loss: 0.0180 - val_loss: 0.0125
Epoch 8/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0149
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0126
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0985
 352/6530 [>.............................] - ETA: 0s - loss: 0.0168
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0128
3088/6530 [=============>................] - ETA: 0s - loss: 0.0969
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0187
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0126
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0980
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0181
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0127
3872/6530 [================>.............] - ETA: 0s - loss: 0.0981
6432/6530 [============================>.] - ETA: 0s - loss: 0.0127
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0185
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0978
6530/6530 [==============================] - 1s 148us/step - loss: 0.0127 - val_loss: 0.0090

1648/6530 [======>.......................] - ETA: 0s - loss: 0.0183Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0043
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0988
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0173
 384/6530 [>.............................] - ETA: 0s - loss: 0.0109
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0988
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0175
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0117
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0991
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0177
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0130
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0985
2944/6530 [============>.................] - ETA: 0s - loss: 0.0174
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0127
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0987
3264/6530 [=============>................] - ETA: 0s - loss: 0.0173
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0124
6496/6530 [============================>.] - ETA: 0s - loss: 0.0985
6530/6530 [==============================] - 1s 139us/step - loss: 0.0987 - val_loss: 0.0913
Epoch 10/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0656
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0173
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0123
 400/6530 [>.............................] - ETA: 0s - loss: 0.0911
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0124
3920/6530 [=================>............] - ETA: 0s - loss: 0.0174
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0975
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0122
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0174
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0970
3184/6530 [=============>................] - ETA: 0s - loss: 0.0120
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0174
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0964
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0121
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0173
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0962
3888/6530 [================>.............] - ETA: 0s - loss: 0.0121
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0170
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0965
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0121
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0169
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0972
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0122
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0170
3136/6530 [=============>................] - ETA: 0s - loss: 0.0954
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0122
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0170
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0964
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0123
6530/6530 [==============================] - 1s 163us/step - loss: 0.0172 - val_loss: 0.0118
Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0131
3904/6530 [================>.............] - ETA: 0s - loss: 0.0965
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0122
 352/6530 [>.............................] - ETA: 0s - loss: 0.0157
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0968
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0123
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0175
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0971
6416/6530 [============================>.] - ETA: 0s - loss: 0.0122
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0172
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0975
6530/6530 [==============================] - 1s 150us/step - loss: 0.0123 - val_loss: 0.0090
Epoch 10/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0039
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0178
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0978
 384/6530 [>.............................] - ETA: 0s - loss: 0.0106
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0175
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0972
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0113
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0165
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0974
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0124
6512/6530 [============================>.] - ETA: 0s - loss: 0.0973
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0167
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0122
6530/6530 [==============================] - 1s 138us/step - loss: 0.0973 - val_loss: 0.0900
Epoch 11/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0678
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0169
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0120
 400/6530 [>.............................] - ETA: 0s - loss: 0.0903
2912/6530 [============>.................] - ETA: 0s - loss: 0.0168
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0118
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0958
3216/6530 [=============>................] - ETA: 0s - loss: 0.0165
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0120
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0966
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0165
2848/6530 [============>.................] - ETA: 0s - loss: 0.0118
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0953
3888/6530 [================>.............] - ETA: 0s - loss: 0.0166
3216/6530 [=============>................] - ETA: 0s - loss: 0.0116
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0955
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0166
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0118
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0955
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0166
3920/6530 [=================>............] - ETA: 0s - loss: 0.0118
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0961
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0164
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0117
3136/6530 [=============>................] - ETA: 0s - loss: 0.0944
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0162
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0119
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0954
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0160
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0118
3904/6530 [================>.............] - ETA: 0s - loss: 0.0955
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0161
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0118
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0958
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0161
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0118
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0961
6448/6530 [============================>.] - ETA: 0s - loss: 0.0162
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0119
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0967
6530/6530 [==============================] - 1s 165us/step - loss: 0.0164 - val_loss: 0.0117
Epoch 10/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0117
6336/6530 [============================>.] - ETA: 0s - loss: 0.0118
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0968
 352/6530 [>.............................] - ETA: 0s - loss: 0.0149
6530/6530 [==============================] - 1s 150us/step - loss: 0.0119 - val_loss: 0.0088
Epoch 11/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0037
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0961
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0169
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0966
 368/6530 [>.............................] - ETA: 0s - loss: 0.0106
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0164
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0112
6530/6530 [==============================] - 1s 136us/step - loss: 0.0963 - val_loss: 0.0903
Epoch 12/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0726
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0170
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0121
 400/6530 [>.............................] - ETA: 0s - loss: 0.0891
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0168
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0119
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0959
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0158
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0116
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0954
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0161
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0115
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0948
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0163
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0116
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0943
2960/6530 [============>.................] - ETA: 0s - loss: 0.0160
2880/6530 [============>.................] - ETA: 0s - loss: 0.0114
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0948
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0159
3248/6530 [=============>................] - ETA: 0s - loss: 0.0113
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0948
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0159
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0115
3136/6530 [=============>................] - ETA: 0s - loss: 0.0933
3936/6530 [=================>............] - ETA: 0s - loss: 0.0159
3920/6530 [=================>............] - ETA: 0s - loss: 0.0115
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0943
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0159
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0114
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0160
3920/6530 [=================>............] - ETA: 0s - loss: 0.0946
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0115
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0159
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0948
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0115
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0156
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0952
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0115
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0956
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0155
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0115
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0156
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0957
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0115
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0951
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0157
6352/6530 [============================>.] - ETA: 0s - loss: 0.0115
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0955
6528/6530 [============================>.] - ETA: 0s - loss: 0.0159
6530/6530 [==============================] - 1s 162us/step - loss: 0.0159 - val_loss: 0.0117

6530/6530 [==============================] - 1s 150us/step - loss: 0.0116 - val_loss: 0.0085
Epoch 11/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0126Epoch 12/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0037
6530/6530 [==============================] - 1s 137us/step - loss: 0.0953 - val_loss: 0.0880
Epoch 13/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0685
 352/6530 [>.............................] - ETA: 0s - loss: 0.0137
 336/6530 [>.............................] - ETA: 1s - loss: 0.0104
 400/6530 [>.............................] - ETA: 0s - loss: 0.0879
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0160
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0109
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0951
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0157
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0117
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0947
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0161
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0116
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0937
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0112
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0160
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0935
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0112
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0150
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0939
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0114
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0154
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0941
2848/6530 [============>.................] - ETA: 0s - loss: 0.0112
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0155
3040/6530 [============>.................] - ETA: 0s - loss: 0.0926
3184/6530 [=============>................] - ETA: 0s - loss: 0.0110
2960/6530 [============>.................] - ETA: 0s - loss: 0.0153
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0933
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0112
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0152
3792/6530 [================>.............] - ETA: 0s - loss: 0.0933
3904/6530 [================>.............] - ETA: 0s - loss: 0.0112
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0152
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0938
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0112
3936/6530 [=================>............] - ETA: 0s - loss: 0.0152
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0945
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0113
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0151
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0945
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0112
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0152
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0949
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0113
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0151
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0944
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0112
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0149
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0943
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0113
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0148
6528/6530 [============================>.] - ETA: 0s - loss: 0.0944
6400/6530 [============================>.] - ETA: 0s - loss: 0.0113
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0149
6530/6530 [==============================] - 1s 139us/step - loss: 0.0943 - val_loss: 0.0875
Epoch 14/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0683
6530/6530 [==============================] - 1s 149us/step - loss: 0.0113 - val_loss: 0.0084
Epoch 13/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0037
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0150
 400/6530 [>.............................] - ETA: 0s - loss: 0.0873
 384/6530 [>.............................] - ETA: 0s - loss: 0.0102
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0939
6530/6530 [==============================] - 1s 163us/step - loss: 0.0152 - val_loss: 0.0114
Epoch 12/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0119
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0108
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0941
 336/6530 [>.............................] - ETA: 0s - loss: 0.0134
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0116
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0927
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0159
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0114
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0929
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0156
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0111
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0931
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0160
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0111
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0935
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0157
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0111
3104/6530 [=============>................] - ETA: 0s - loss: 0.0917
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0150
2928/6530 [============>.................] - ETA: 0s - loss: 0.0109
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0927
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0152
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0109
3872/6530 [================>.............] - ETA: 0s - loss: 0.0929
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0152
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0110
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0931
2976/6530 [============>.................] - ETA: 0s - loss: 0.0150
3984/6530 [=================>............] - ETA: 0s - loss: 0.0111
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0935
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0149
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0109
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0940
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0149
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0111
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0940
3968/6530 [=================>............] - ETA: 0s - loss: 0.0149
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0110
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0934
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0148
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0111
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0937
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0148
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0111
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0147
6530/6530 [==============================] - 1s 137us/step - loss: 0.0935 - val_loss: 0.0864
Epoch 15/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0697
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0111
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0146
 368/6530 [>.............................] - ETA: 0s - loss: 0.0869
6496/6530 [============================>.] - ETA: 0s - loss: 0.0111
6530/6530 [==============================] - 1s 147us/step - loss: 0.0111 - val_loss: 0.0084
Epoch 14/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0037
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0145
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0934
 352/6530 [>.............................] - ETA: 0s - loss: 0.0101
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0146
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0947
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0106
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0147
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0926
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0112
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0925
6530/6530 [==============================] - 1s 161us/step - loss: 0.0149 - val_loss: 0.0114
Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0099
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0112
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0924
 352/6530 [>.............................] - ETA: 0s - loss: 0.0134
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0109
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0930
 624/6530 [=>............................] - ETA: 0s - loss: 0.0154
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0107
3056/6530 [=============>................] - ETA: 0s - loss: 0.0915
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0154
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0109
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0925
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0158
2864/6530 [============>.................] - ETA: 0s - loss: 0.0107
3840/6530 [================>.............] - ETA: 0s - loss: 0.0925
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0155
3216/6530 [=============>................] - ETA: 0s - loss: 0.0107
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0924
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0146
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0109
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0933
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0147
3952/6530 [=================>............] - ETA: 0s - loss: 0.0108
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0933
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0148
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0108
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0937
2864/6530 [============>.................] - ETA: 0s - loss: 0.0146
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0109
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0931
3200/6530 [=============>................] - ETA: 0s - loss: 0.0145
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0109
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0931
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0146
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0109
3856/6530 [================>.............] - ETA: 0s - loss: 0.0146
6530/6530 [==============================] - 1s 139us/step - loss: 0.0931 - val_loss: 0.0862
Epoch 16/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0710
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0108
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0145
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0109
 384/6530 [>.............................] - ETA: 0s - loss: 0.0853
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0145
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0922
6368/6530 [============================>.] - ETA: 0s - loss: 0.0109
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0145
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0935
6530/6530 [==============================] - 1s 150us/step - loss: 0.0110 - val_loss: 0.0082
Epoch 15/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0048
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0143
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0913
 384/6530 [>.............................] - ETA: 0s - loss: 0.0099
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0141
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0912
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0105
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0142
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0915
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0113
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0142
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0920
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0111
6448/6530 [============================>.] - ETA: 0s - loss: 0.0143
3072/6530 [=============>................] - ETA: 0s - loss: 0.0907
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0108
6530/6530 [==============================] - 1s 166us/step - loss: 0.0144 - val_loss: 0.0109
Epoch 14/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0109
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0916
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0108
 336/6530 [>.............................] - ETA: 0s - loss: 0.0129
3856/6530 [================>.............] - ETA: 0s - loss: 0.0916
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0108
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0151
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0916
2912/6530 [============>.................] - ETA: 0s - loss: 0.0106
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0149
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0924
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0106
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0152
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0926
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0107
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0150
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0930
4000/6530 [=================>............] - ETA: 0s - loss: 0.0108
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0142
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0921
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0107
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0145
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0927
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0108
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0145
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0108
6530/6530 [==============================] - 1s 137us/step - loss: 0.0924 - val_loss: 0.0856
Epoch 17/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0704
2960/6530 [============>.................] - ETA: 0s - loss: 0.0143
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0108
 416/6530 [>.............................] - ETA: 0s - loss: 0.0852
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0141
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0108
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0922
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0142
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0108
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0925
3936/6530 [=================>............] - ETA: 0s - loss: 0.0142
6480/6530 [============================>.] - ETA: 0s - loss: 0.0108
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0911
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0142
6530/6530 [==============================] - 1s 147us/step - loss: 0.0109 - val_loss: 0.0080
Epoch 16/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0051
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0907
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0142
 384/6530 [>.............................] - ETA: 0s - loss: 0.0097
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0910
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0141
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0103
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0916
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0138
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0108
3056/6530 [=============>................] - ETA: 0s - loss: 0.0901
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0137
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0110
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0911
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0138
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0107
3824/6530 [================>.............] - ETA: 0s - loss: 0.0910
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0139
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0105
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0912
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0106
6530/6530 [==============================] - 1s 162us/step - loss: 0.0140 - val_loss: 0.0104
Epoch 15/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0092
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0919
2832/6530 [============>.................] - ETA: 0s - loss: 0.0105
 336/6530 [>.............................] - ETA: 0s - loss: 0.0127
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0921
3184/6530 [=============>................] - ETA: 0s - loss: 0.0104
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0148
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0924
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0106
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0145
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0916
3888/6530 [================>.............] - ETA: 0s - loss: 0.0106
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0148
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0920
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0105
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0146
6530/6530 [==============================] - 1s 137us/step - loss: 0.0918 - val_loss: 0.0844
Epoch 18/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0684
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0106
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0138
 416/6530 [>.............................] - ETA: 0s - loss: 0.0851
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0106
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0141
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0927
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0107
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0141
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0923
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0106
2976/6530 [============>.................] - ETA: 0s - loss: 0.0139
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0915
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0106
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0138
6400/6530 [============================>.] - ETA: 0s - loss: 0.0106
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0907
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0137
6530/6530 [==============================] - 1s 149us/step - loss: 0.0107 - val_loss: 0.0081

2384/6530 [=========>....................] - ETA: 0s - loss: 0.0912Epoch 17/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0051
3968/6530 [=================>............] - ETA: 0s - loss: 0.0137
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0911
 384/6530 [>.............................] - ETA: 0s - loss: 0.0096
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0136
3168/6530 [=============>................] - ETA: 0s - loss: 0.0897
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0102
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0137
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0905
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0109
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0136
3920/6530 [=================>............] - ETA: 0s - loss: 0.0908
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0108
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0134
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0912
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0106
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0133
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0914
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0104
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0134
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0918
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0105
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0135
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0918
2896/6530 [============>.................] - ETA: 0s - loss: 0.0103
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0911
6530/6530 [==============================] - 1s 161us/step - loss: 0.0136 - val_loss: 0.0105
Epoch 16/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0094
3248/6530 [=============>................] - ETA: 0s - loss: 0.0103
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0915
 352/6530 [>.............................] - ETA: 0s - loss: 0.0124
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0105
3952/6530 [=================>............] - ETA: 0s - loss: 0.0104
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0139
6530/6530 [==============================] - 1s 136us/step - loss: 0.0914 - val_loss: 0.0842
Epoch 19/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0681
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0138
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0104
 416/6530 [>.............................] - ETA: 0s - loss: 0.0846
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0105
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0140
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0919
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0105
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0139
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0917
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0105
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0131
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0905
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0105
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0135
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0902
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0105
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0134
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0905
2960/6530 [============>.................] - ETA: 0s - loss: 0.0133
6496/6530 [============================>.] - ETA: 0s - loss: 0.0105
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0907
6530/6530 [==============================] - 1s 147us/step - loss: 0.0106 - val_loss: 0.0080
Epoch 18/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0051
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0132
3120/6530 [=============>................] - ETA: 0s - loss: 0.0889
 368/6530 [>.............................] - ETA: 0s - loss: 0.0095
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0132
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0898
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0101
3952/6530 [=================>............] - ETA: 0s - loss: 0.0132
3888/6530 [================>.............] - ETA: 0s - loss: 0.0901
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0107
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0132
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0904
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0105
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0132
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0907
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0103
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0131
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0912
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0103
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0130
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0914
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0103
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0129
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0907
2928/6530 [============>.................] - ETA: 0s - loss: 0.0101
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0130
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0910
3248/6530 [=============>................] - ETA: 0s - loss: 0.0101
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0131
6530/6530 [==============================] - 1s 136us/step - loss: 0.0908 - val_loss: 0.0839
Epoch 20/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0685
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0103
6530/6530 [==============================] - 1s 161us/step - loss: 0.0132 - val_loss: 0.0105
Epoch 17/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0098
 400/6530 [>.............................] - ETA: 0s - loss: 0.0844
3984/6530 [=================>............] - ETA: 0s - loss: 0.0103
 336/6530 [>.............................] - ETA: 1s - loss: 0.0118
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0911
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0102
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0139
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0913
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0103
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0135
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0899
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0103
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0138
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0894
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0104
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0136
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0896
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0103
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0129
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0901
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0104
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0131
3072/6530 [=============>................] - ETA: 0s - loss: 0.0887
6448/6530 [============================>.] - ETA: 0s - loss: 0.0104
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0132
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0896
6530/6530 [==============================] - 1s 148us/step - loss: 0.0104 - val_loss: 0.0082
Epoch 19/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0058
2944/6530 [============>.................] - ETA: 0s - loss: 0.0130
3856/6530 [================>.............] - ETA: 0s - loss: 0.0896
 352/6530 [>.............................] - ETA: 0s - loss: 0.0095
3264/6530 [=============>................] - ETA: 0s - loss: 0.0130
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0896
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0097
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0130
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0903
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0104
3920/6530 [=================>............] - ETA: 0s - loss: 0.0130
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0906
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0104
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0130
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0909
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0103
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0903
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0130
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0100
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0129
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0903
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0101
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0127
2848/6530 [============>.................] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 1s 138us/step - loss: 0.0903 - val_loss: 0.0829
Epoch 21/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0686
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0126
3200/6530 [=============>................] - ETA: 0s - loss: 0.0099
 400/6530 [>.............................] - ETA: 0s - loss: 0.0843
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0127
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0102
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0909
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0127
3920/6530 [=================>............] - ETA: 0s - loss: 0.0102
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0911
6480/6530 [============================>.] - ETA: 0s - loss: 0.0128
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0101
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0900
6530/6530 [==============================] - 1s 165us/step - loss: 0.0129 - val_loss: 0.0102
Epoch 18/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0089
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0102
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0895
 336/6530 [>.............................] - ETA: 1s - loss: 0.0117
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0102
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0898
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0138
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0102
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0900
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0134
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0102
3120/6530 [=============>................] - ETA: 0s - loss: 0.0882
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0135
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0102
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0891
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0133
6416/6530 [============================>.] - ETA: 0s - loss: 0.0102
3904/6530 [================>.............] - ETA: 0s - loss: 0.0893
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0126
6530/6530 [==============================] - 1s 149us/step - loss: 0.0103 - val_loss: 0.0081
Epoch 20/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0052
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0898
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0129
 368/6530 [>.............................] - ETA: 0s - loss: 0.0093
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0900
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0129
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0097
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0905
2992/6530 [============>.................] - ETA: 0s - loss: 0.0127
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0103
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0906
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0126
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0103
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0900
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0126
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0101
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0902
3984/6530 [=================>............] - ETA: 0s - loss: 0.0127
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 1s 135us/step - loss: 0.0900 - val_loss: 0.0827
Epoch 22/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0688
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0126
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0101
 400/6530 [>.............................] - ETA: 0s - loss: 0.0837
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0126
2880/6530 [============>.................] - ETA: 0s - loss: 0.0099
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0904
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0125
3232/6530 [=============>................] - ETA: 0s - loss: 0.0099
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0907
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0124
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0101
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0890
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0123
3856/6530 [================>.............] - ETA: 0s - loss: 0.0101
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0885
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0124
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0100
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0886
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0125
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0101
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0894
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0101
6530/6530 [==============================] - 1s 163us/step - loss: 0.0126 - val_loss: 0.0100
Epoch 19/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0095
3008/6530 [============>.................] - ETA: 0s - loss: 0.0880
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0101
 336/6530 [>.............................] - ETA: 1s - loss: 0.0115
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0880
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0100
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0134
3760/6530 [================>.............] - ETA: 0s - loss: 0.0883
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0101
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0131
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0889
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0101
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0133
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0895
6530/6530 [==============================] - 1s 153us/step - loss: 0.0102 - val_loss: 0.0081
Epoch 21/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0054
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0130
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0893
 352/6530 [>.............................] - ETA: 0s - loss: 0.0093
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0124
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0896
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0095
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0126
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0895
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0101
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0126
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0892
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0102
2912/6530 [============>.................] - ETA: 0s - loss: 0.0125
6416/6530 [============================>.] - ETA: 0s - loss: 0.0893
6530/6530 [==============================] - 1s 140us/step - loss: 0.0894 - val_loss: 0.0826

1776/6530 [=======>......................] - ETA: 0s - loss: 0.0099Epoch 23/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0683
3248/6530 [=============>................] - ETA: 0s - loss: 0.0123
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0098
 384/6530 [>.............................] - ETA: 0s - loss: 0.0832
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0123
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0099
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0897
3904/6530 [================>.............] - ETA: 0s - loss: 0.0123
2848/6530 [============>.................] - ETA: 0s - loss: 0.0098
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0905
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0123
3200/6530 [=============>................] - ETA: 0s - loss: 0.0098
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0889
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0124
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0099
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0883
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0123
3904/6530 [================>.............] - ETA: 0s - loss: 0.0099
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0887
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0121
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0099
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0889
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0120
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0100
3088/6530 [=============>................] - ETA: 0s - loss: 0.0875
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0121
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0100
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0883
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0121
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0100
3888/6530 [================>.............] - ETA: 0s - loss: 0.0884
6512/6530 [============================>.] - ETA: 0s - loss: 0.0123
6530/6530 [==============================] - 1s 164us/step - loss: 0.0123 - val_loss: 0.0098

5696/6530 [=========================>....] - ETA: 0s - loss: 0.0100Epoch 20/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0087
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0888
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0101
 336/6530 [>.............................] - ETA: 1s - loss: 0.0111
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0892
6416/6530 [============================>.] - ETA: 0s - loss: 0.0100
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0130
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0894
6530/6530 [==============================] - 1s 149us/step - loss: 0.0101 - val_loss: 0.0081
Epoch 22/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0052
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0127
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0897
 384/6530 [>.............................] - ETA: 0s - loss: 0.0094
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0128
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0889
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0096
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0126
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0892
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0101
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0120
6530/6530 [==============================] - 1s 137us/step - loss: 0.0891 - val_loss: 0.0816
Epoch 24/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0686
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0100
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0121
 400/6530 [>.............................] - ETA: 0s - loss: 0.0835
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0099
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0122
 784/6530 [==>...........................] - ETA: 0s - loss: 0.0899
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0099
2944/6530 [============>.................] - ETA: 0s - loss: 0.0120
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0902
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0099
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0119
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0890
2912/6530 [============>.................] - ETA: 0s - loss: 0.0097
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0119
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0879
3264/6530 [=============>................] - ETA: 0s - loss: 0.0097
3936/6530 [=================>............] - ETA: 0s - loss: 0.0119
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0883
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0099
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0119
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0886
4000/6530 [=================>............] - ETA: 0s - loss: 0.0099
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0119
3152/6530 [=============>................] - ETA: 0s - loss: 0.0871
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0098
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0118
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0878
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0099
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0117
3936/6530 [=================>............] - ETA: 0s - loss: 0.0881
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0100
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0117
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0884
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0099
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0118
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0889
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0099
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0119
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0891
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0100
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0890
6530/6530 [==============================] - 1s 162us/step - loss: 0.0120 - val_loss: 0.0099
Epoch 21/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0076
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0885
6530/6530 [==============================] - 1s 146us/step - loss: 0.0101 - val_loss: 0.0082

 320/6530 [>.............................] - ETA: 1s - loss: 0.0104
6320/6530 [============================>.] - ETA: 0s - loss: 0.0889
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0124
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0121
6530/6530 [==============================] - 1s 138us/step - loss: 0.0888 - val_loss: 0.0813
Epoch 25/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0685
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0123
 400/6530 [>.............................] - ETA: 0s - loss: 0.0829
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0122
# training | RMSE: 0.0809, MAE: 0.0632
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44587267964897137}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.08093001304924163, 'rmse': 0.08093001304924163, 'mae': 0.06321846383088192, 'early_stop': True}
vggnet done  0

 800/6530 [==>...........................] - ETA: 0s - loss: 0.0898
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0116
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0891
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0117
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0886
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0117
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0878
2928/6530 [============>.................] - ETA: 0s - loss: 0.0116
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0884
3248/6530 [=============>................] - ETA: 0s - loss: 0.0115
2864/6530 [============>.................] - ETA: 0s - loss: 0.0876
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0115
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0867
3952/6530 [=================>............] - ETA: 0s - loss: 0.0115
3712/6530 [================>.............] - ETA: 0s - loss: 0.0873
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0115
4128/6530 [=================>............] - ETA: 0s - loss: 0.0880
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0115
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0884
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0114
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0885
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0113
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0889
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0113
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0883
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0114
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0884
6336/6530 [============================>.] - ETA: 0s - loss: 0.0114
6530/6530 [==============================] - 1s 128us/step - loss: 0.0883 - val_loss: 0.0807
Epoch 26/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0681
6530/6530 [==============================] - 1s 158us/step - loss: 0.0116 - val_loss: 0.0098
Epoch 22/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0087
 432/6530 [>.............................] - ETA: 0s - loss: 0.0835
 368/6530 [>.............................] - ETA: 0s - loss: 0.0113
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0889
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0122
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0889
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0123
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0879
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0121
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0870
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0117
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0883
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0115
2944/6530 [============>.................] - ETA: 0s - loss: 0.0869
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0115
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0863
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0114
3744/6530 [================>.............] - ETA: 0s - loss: 0.0870
3136/6530 [=============>................] - ETA: 0s - loss: 0.0113
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0875
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0113
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0882
3824/6530 [================>.............] - ETA: 0s - loss: 0.0112
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0883
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0112
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0887
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0112
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0879
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0111
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0882
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0110
6530/6530 [==============================] - 1s 126us/step - loss: 0.0880 - val_loss: 0.0802
Epoch 27/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0684
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0109
 432/6530 [>.............................] - ETA: 0s - loss: 0.0831
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0111
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0890
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0112
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0884
6530/6530 [==============================] - 1s 154us/step - loss: 0.0113 - val_loss: 0.0098
Epoch 23/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0084
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0881
 336/6530 [>.............................] - ETA: 0s - loss: 0.0102
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0868
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0120
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0875
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0116
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0873
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0118
3168/6530 [=============>................] - ETA: 0s - loss: 0.0860
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0115
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0866
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0110
3952/6530 [=================>............] - ETA: 0s - loss: 0.0869
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0110
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0873
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0110
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0875
3040/6530 [============>.................] - ETA: 0s - loss: 0.0109
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0877
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0109
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0877
3728/6530 [================>.............] - ETA: 0s - loss: 0.0109
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0875
4080/6530 [=================>............] - ETA: 0s - loss: 0.0109
6448/6530 [============================>.] - ETA: 0s - loss: 0.0875
6530/6530 [==============================] - 1s 132us/step - loss: 0.0876 - val_loss: 0.0799

4416/6530 [===================>..........] - ETA: 0s - loss: 0.0108
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0109
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0108
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0107
# training | RMSE: 0.0971, MAE: 0.0722
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10996334638100778}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10952735483869187}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4326994328446677}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.09713239021932966, 'rmse': 0.09713239021932966, 'mae': 0.07223138809065786, 'early_stop': False}
vggnet done  2

5744/6530 [=========================>....] - ETA: 0s - loss: 0.0107
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0108
6464/6530 [============================>.] - ETA: 0s - loss: 0.0109
6530/6530 [==============================] - 1s 156us/step - loss: 0.0110 - val_loss: 0.0099
Epoch 24/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0085
 336/6530 [>.............................] - ETA: 0s - loss: 0.0100
 640/6530 [=>............................] - ETA: 0s - loss: 0.0114
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0112
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0112
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0113
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0111
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0106
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0107
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0107
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0107
2976/6530 [============>.................] - ETA: 0s - loss: 0.0106
3248/6530 [=============>................] - ETA: 0s - loss: 0.0106
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0107
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0106
3904/6530 [================>.............] - ETA: 0s - loss: 0.0106
4080/6530 [=================>............] - ETA: 0s - loss: 0.0107
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0106
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0106
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0106
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0106
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0105
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0104
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0104
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0104
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0105
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0105
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0106
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0106
6480/6530 [============================>.] - ETA: 0s - loss: 0.0106
6530/6530 [==============================] - 2s 246us/step - loss: 0.0107 - val_loss: 0.0099
Epoch 25/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0092
 176/6530 [..............................] - ETA: 2s - loss: 0.0094
 368/6530 [>.............................] - ETA: 1s - loss: 0.0102
 544/6530 [=>............................] - ETA: 1s - loss: 0.0105
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0111
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0108
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0109
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0109
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0107
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0107
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0103
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0103
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0103
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0103
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0103
2816/6530 [===========>..................] - ETA: 1s - loss: 0.0103
2992/6530 [============>.................] - ETA: 0s - loss: 0.0102
3168/6530 [=============>................] - ETA: 0s - loss: 0.0102
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0102
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0102
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0102
3888/6530 [================>.............] - ETA: 0s - loss: 0.0102
4064/6530 [=================>............] - ETA: 0s - loss: 0.0103
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0103
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0102
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0102
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0102
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0101
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0100
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0100
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0101
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0102
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0102
6528/6530 [============================>.] - ETA: 0s - loss: 0.0103
6530/6530 [==============================] - 2s 272us/step - loss: 0.0103 - val_loss: 0.0098
Epoch 26/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0081
 288/6530 [>.............................] - ETA: 1s - loss: 0.0098
 576/6530 [=>............................] - ETA: 1s - loss: 0.0106
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0105
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0108
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0107
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0105
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0102
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0103
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0102
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0103
3024/6530 [============>.................] - ETA: 0s - loss: 0.0102
3264/6530 [=============>................] - ETA: 0s - loss: 0.0101
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0102
3712/6530 [================>.............] - ETA: 0s - loss: 0.0101
3936/6530 [=================>............] - ETA: 0s - loss: 0.0102
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0102
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0101
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0101
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0101
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0100
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0100
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0099
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0099
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0099
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0100
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0100
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0100
6320/6530 [============================>.] - ETA: 0s - loss: 0.0101
6512/6530 [============================>.] - ETA: 0s - loss: 0.0102
6530/6530 [==============================] - 2s 238us/step - loss: 0.0102 - val_loss: 0.0099

# training | RMSE: 0.0877, MAE: 0.0683
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20261337059472792}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10963218549386174}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.08772726075700307, 'rmse': 0.08772726075700307, 'mae': 0.06829993111023042, 'early_stop': True}
vggnet done  1
all of workers have been done
calculation finished
#0 epoch=27.0 loss={'loss': 0.08093001304924163, 'rmse': 0.08093001304924163, 'mae': 0.06321846383088192, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44587267964897137}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#2 epoch=27.0 loss={'loss': 0.09713239021932966, 'rmse': 0.09713239021932966, 'mae': 0.07223138809065786, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 97, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 4, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10996334638100778}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.10952735483869187}, 'layer_4_size': 17, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4326994328446677}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#1 epoch=27.0 loss={'loss': 0.08772726075700307, 'rmse': 0.08772726075700307, 'mae': 0.06829993111023042, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20261337059472792}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10963218549386174}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
get a list [results] of length 120
get a list [loss] of length 3
get a list [val_loss] of length 3
length of indices is (0, 1, 2)
length of indices is 3
length of T is 3
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20261337059472792}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10963218549386174}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]] 

*** 1.0 configurations x 81.0 iterations each

2 | Thu Sep 27 23:25:21 2018 | lowest loss so far: 0.0809 (run 0)

vggnet done  1
vggnet done  2
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  52 | activation: tanh    | extras: batchnorm 
layer 2 | size:  94 | activation: relu    | extras: None 
layer 3 | size:  32 | activation: relu    | extras: dropout - rate: 20.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a74bc18>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 6:12 - loss: 1.9850
 240/6530 [>.............................] - ETA: 25s - loss: 0.7814 
 448/6530 [=>............................] - ETA: 13s - loss: 0.5125
 624/6530 [=>............................] - ETA: 10s - loss: 0.3971
 800/6530 [==>...........................] - ETA: 8s - loss: 0.3297 
 960/6530 [===>..........................] - ETA: 6s - loss: 0.2875
1120/6530 [====>.........................] - ETA: 5s - loss: 0.2585
1280/6530 [====>.........................] - ETA: 5s - loss: 0.2344
1440/6530 [=====>........................] - ETA: 4s - loss: 0.2161
1616/6530 [======>.......................] - ETA: 4s - loss: 0.1991
1792/6530 [=======>......................] - ETA: 3s - loss: 0.1855
1968/6530 [========>.....................] - ETA: 3s - loss: 0.1753
2160/6530 [========>.....................] - ETA: 3s - loss: 0.1646
2336/6530 [=========>....................] - ETA: 2s - loss: 0.1564
2480/6530 [==========>...................] - ETA: 2s - loss: 0.1503
2640/6530 [===========>..................] - ETA: 2s - loss: 0.1452
2816/6530 [===========>..................] - ETA: 2s - loss: 0.1391
2992/6530 [============>.................] - ETA: 2s - loss: 0.1340
3168/6530 [=============>................] - ETA: 1s - loss: 0.1296
3328/6530 [==============>...............] - ETA: 1s - loss: 0.1257
3504/6530 [===============>..............] - ETA: 1s - loss: 0.1216
3664/6530 [===============>..............] - ETA: 1s - loss: 0.1189
3824/6530 [================>.............] - ETA: 1s - loss: 0.1160
3984/6530 [=================>............] - ETA: 1s - loss: 0.1134
4160/6530 [==================>...........] - ETA: 1s - loss: 0.1106
4320/6530 [==================>...........] - ETA: 1s - loss: 0.1084
4496/6530 [===================>..........] - ETA: 1s - loss: 0.1060
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1040
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1023
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1002
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0984
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0966
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0948
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0928
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0912
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0896
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0882
6416/6530 [============================>.] - ETA: 0s - loss: 0.0866
6530/6530 [==============================] - 3s 460us/step - loss: 0.0861 - val_loss: 0.0351
Epoch 2/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0416
 208/6530 [..............................] - ETA: 1s - loss: 0.0440
 432/6530 [>.............................] - ETA: 1s - loss: 0.0422
 640/6530 [=>............................] - ETA: 1s - loss: 0.0405
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0393
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0385
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0384
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0382
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0373
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0367
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0351
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0355
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0353
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0352
2976/6530 [============>.................] - ETA: 0s - loss: 0.0349
3168/6530 [=============>................] - ETA: 0s - loss: 0.0350
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0347
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0345
3760/6530 [================>.............] - ETA: 0s - loss: 0.0345
3936/6530 [=================>............] - ETA: 0s - loss: 0.0346
4112/6530 [=================>............] - ETA: 0s - loss: 0.0344
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0344
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0346
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0344
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0344
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0342
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0338
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0337
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0336
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0334
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0334
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0331
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0330
6496/6530 [============================>.] - ETA: 0s - loss: 0.0330
6530/6530 [==============================] - 2s 275us/step - loss: 0.0330 - val_loss: 0.0239
Epoch 3/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0316
 192/6530 [..............................] - ETA: 1s - loss: 0.0304
 368/6530 [>.............................] - ETA: 1s - loss: 0.0285
 544/6530 [=>............................] - ETA: 1s - loss: 0.0282
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0295
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0292
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0293
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0292
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0292
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0287
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0288
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0283
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0274
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0272
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0275
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0273
2688/6530 [===========>..................] - ETA: 1s - loss: 0.0273
2848/6530 [============>.................] - ETA: 1s - loss: 0.0272
3008/6530 [============>.................] - ETA: 1s - loss: 0.0270
3184/6530 [=============>................] - ETA: 1s - loss: 0.0270
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0269
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0267
3712/6530 [================>.............] - ETA: 0s - loss: 0.0267
3888/6530 [================>.............] - ETA: 0s - loss: 0.0270
4080/6530 [=================>............] - ETA: 0s - loss: 0.0268
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0269
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0269
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0269
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0269
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0266
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0264
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0263
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0261
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0260
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0260
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0258
6496/6530 [============================>.] - ETA: 0s - loss: 0.0259
6530/6530 [==============================] - 2s 298us/step - loss: 0.0260 - val_loss: 0.0181
Epoch 4/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0228
 272/6530 [>.............................] - ETA: 1s - loss: 0.0244
 544/6530 [=>............................] - ETA: 1s - loss: 0.0233
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0248
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0246
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0248
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0243
1712/6530 [======>.......................] - ETA: 1s - loss: 0.0244
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0235
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0232
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0233
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0236
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0235
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0232
3040/6530 [============>.................] - ETA: 0s - loss: 0.0230
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0229
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0228
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0228
3872/6530 [================>.............] - ETA: 0s - loss: 0.0228
4080/6530 [=================>............] - ETA: 0s - loss: 0.0228
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0228
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0228
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0228
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0227
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0226
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0225
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0223
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0222
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0222
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0222
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0222
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0220
6320/6530 [============================>.] - ETA: 0s - loss: 0.0220
6496/6530 [============================>.] - ETA: 0s - loss: 0.0222
6530/6530 [==============================] - 2s 277us/step - loss: 0.0223 - val_loss: 0.0151
Epoch 5/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0213
 208/6530 [..............................] - ETA: 1s - loss: 0.0210
 384/6530 [>.............................] - ETA: 1s - loss: 0.0208
 560/6530 [=>............................] - ETA: 1s - loss: 0.0211
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0223
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0219
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0222
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0222
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0221
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0212
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0208
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0210
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0211
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0208
3072/6530 [=============>................] - ETA: 0s - loss: 0.0207
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0205
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0205
3792/6530 [================>.............] - ETA: 0s - loss: 0.0203
4032/6530 [=================>............] - ETA: 0s - loss: 0.0206
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0205
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0205
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0204
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0204
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0201
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0200
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0199
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0200
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0199
6480/6530 [============================>.] - ETA: 0s - loss: 0.0200
6530/6530 [==============================] - 2s 232us/step - loss: 0.0201 - val_loss: 0.0132
Epoch 6/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0153
 256/6530 [>.............................] - ETA: 1s - loss: 0.0186
 448/6530 [=>............................] - ETA: 1s - loss: 0.0198
 608/6530 [=>............................] - ETA: 1s - loss: 0.0197
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0200
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0198
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0205
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0206
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0205
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0203
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0199
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0191
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0195
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0197
2688/6530 [===========>..................] - ETA: 1s - loss: 0.0195
2912/6530 [============>.................] - ETA: 0s - loss: 0.0194
3088/6530 [=============>................] - ETA: 0s - loss: 0.0193
3248/6530 [=============>................] - ETA: 0s - loss: 0.0192
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0192
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0191
3808/6530 [================>.............] - ETA: 0s - loss: 0.0191
3984/6530 [=================>............] - ETA: 0s - loss: 0.0192
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0192
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0191
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0191
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0191
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0191
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0189
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0187
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0186
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0186
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0187
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0186
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0186
6496/6530 [============================>.] - ETA: 0s - loss: 0.0188
6530/6530 [==============================] - 2s 283us/step - loss: 0.0188 - val_loss: 0.0132
Epoch 7/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0154
 224/6530 [>.............................] - ETA: 1s - loss: 0.0175
 416/6530 [>.............................] - ETA: 1s - loss: 0.0182
 592/6530 [=>............................] - ETA: 1s - loss: 0.0184
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0189
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0190
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0195
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0193
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0192
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0192
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0187
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0179
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0182
2352/6530 [=========>....................] - ETA: 1s - loss: 0.0183
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0184
2720/6530 [===========>..................] - ETA: 1s - loss: 0.0184
2912/6530 [============>.................] - ETA: 1s - loss: 0.0183
3088/6530 [=============>................] - ETA: 0s - loss: 0.0182
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0181
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0182
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0181
3824/6530 [================>.............] - ETA: 0s - loss: 0.0182
3984/6530 [=================>............] - ETA: 0s - loss: 0.0182
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0182
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0180
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0182
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0181
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0181
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0180
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0178
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0176
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0177
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0178
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0177
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0177
6464/6530 [============================>.] - ETA: 0s - loss: 0.0178
6530/6530 [==============================] - 2s 293us/step - loss: 0.0179 - val_loss: 0.0130
Epoch 8/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0150
 192/6530 [..............................] - ETA: 1s - loss: 0.0179
 368/6530 [>.............................] - ETA: 1s - loss: 0.0173
 544/6530 [=>............................] - ETA: 1s - loss: 0.0175
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0188
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0182
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0185
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0183
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0184
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0182
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0179
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0174
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0172
2256/6530 [=========>....................] - ETA: 1s - loss: 0.0173
2416/6530 [==========>...................] - ETA: 1s - loss: 0.0175
2576/6530 [==========>...................] - ETA: 1s - loss: 0.0176
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0175
2896/6530 [============>.................] - ETA: 1s - loss: 0.0174
3072/6530 [=============>................] - ETA: 1s - loss: 0.0173
3248/6530 [=============>................] - ETA: 0s - loss: 0.0172
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0173
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0172
3776/6530 [================>.............] - ETA: 0s - loss: 0.0171
3936/6530 [=================>............] - ETA: 0s - loss: 0.0173
4096/6530 [=================>............] - ETA: 0s - loss: 0.0173
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0172
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0173
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0172
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0172
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0171
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0169
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0169
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0168
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0169
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0169
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0169
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0169
6496/6530 [============================>.] - ETA: 0s - loss: 0.0171
6530/6530 [==============================] - 2s 304us/step - loss: 0.0172 - val_loss: 0.0120
Epoch 9/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0139
 176/6530 [..............................] - ETA: 1s - loss: 0.0167
 336/6530 [>.............................] - ETA: 2s - loss: 0.0151
 512/6530 [=>............................] - ETA: 1s - loss: 0.0167
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0173
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0173
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0176
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0175
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0177
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0175
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0173
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0168
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0166
2256/6530 [=========>....................] - ETA: 1s - loss: 0.0168
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0170
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0170
2816/6530 [===========>..................] - ETA: 1s - loss: 0.0169
2992/6530 [============>.................] - ETA: 1s - loss: 0.0167
3184/6530 [=============>................] - ETA: 1s - loss: 0.0166
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0167
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0166
3712/6530 [================>.............] - ETA: 0s - loss: 0.0165
3888/6530 [================>.............] - ETA: 0s - loss: 0.0167
4080/6530 [=================>............] - ETA: 0s - loss: 0.0167
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0166
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0166
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0166
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0165
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0164
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0163
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0163
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0161
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0162
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0163
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0163
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0163
6432/6530 [============================>.] - ETA: 0s - loss: 0.0164
6530/6530 [==============================] - 2s 304us/step - loss: 0.0165 - val_loss: 0.0118
Epoch 10/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0116
 176/6530 [..............................] - ETA: 2s - loss: 0.0150
 352/6530 [>.............................] - ETA: 1s - loss: 0.0145
 528/6530 [=>............................] - ETA: 1s - loss: 0.0156
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0168
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0164
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0167
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0169
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0168
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0167
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0165
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0160
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0158
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0160
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0161
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0161
2704/6530 [===========>..................] - ETA: 1s - loss: 0.0160
2880/6530 [============>.................] - ETA: 1s - loss: 0.0160
3072/6530 [=============>................] - ETA: 1s - loss: 0.0159
3248/6530 [=============>................] - ETA: 1s - loss: 0.0158
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0160
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0158
3808/6530 [================>.............] - ETA: 0s - loss: 0.0158
3968/6530 [=================>............] - ETA: 0s - loss: 0.0159
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0160
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0158
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0159
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0159
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0158
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0158
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0156
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0156
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0154
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0156
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0157
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0157
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0157
6432/6530 [============================>.] - ETA: 0s - loss: 0.0157
6530/6530 [==============================] - 2s 314us/step - loss: 0.0159 - val_loss: 0.0115
Epoch 11/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0119
 192/6530 [..............................] - ETA: 1s - loss: 0.0147
 384/6530 [>.............................] - ETA: 1s - loss: 0.0148
 576/6530 [=>............................] - ETA: 1s - loss: 0.0154
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0161
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0159
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0165
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0163
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0162
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0161
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0156
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0152
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0156
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0155
2560/6530 [==========>...................] - ETA: 1s - loss: 0.0157
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0156
2912/6530 [============>.................] - ETA: 1s - loss: 0.0155
3088/6530 [=============>................] - ETA: 0s - loss: 0.0154
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0153
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0154
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0153
3840/6530 [================>.............] - ETA: 0s - loss: 0.0155
4032/6530 [=================>............] - ETA: 0s - loss: 0.0155
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0154
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0154
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0155
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0154
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0154
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0153
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0152
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0151
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0151
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0152
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0152
6496/6530 [============================>.] - ETA: 0s - loss: 0.0153
6530/6530 [==============================] - 2s 280us/step - loss: 0.0154 - val_loss: 0.0113
Epoch 12/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0100
 256/6530 [>.............................] - ETA: 1s - loss: 0.0139
 480/6530 [=>............................] - ETA: 1s - loss: 0.0152
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0156
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0155
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0160
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0158
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0158
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0155
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0148
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0151
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0152
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0151
2832/6530 [============>.................] - ETA: 0s - loss: 0.0150
3024/6530 [============>.................] - ETA: 0s - loss: 0.0149
3200/6530 [=============>................] - ETA: 0s - loss: 0.0148
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0148
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0149
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0148
3824/6530 [================>.............] - ETA: 0s - loss: 0.0149
4000/6530 [=================>............] - ETA: 0s - loss: 0.0149
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0149
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0149
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0149
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0147
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0147
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0145
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0147
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0147
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0147
6464/6530 [============================>.] - ETA: 0s - loss: 0.0148
6530/6530 [==============================] - 2s 252us/step - loss: 0.0149 - val_loss: 0.0113
Epoch 13/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0101
 208/6530 [..............................] - ETA: 1s - loss: 0.0132
 432/6530 [>.............................] - ETA: 1s - loss: 0.0146
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0152
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0149
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0155
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0152
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0149
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0144
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0145
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0146
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0145
3040/6530 [============>.................] - ETA: 0s - loss: 0.0143
3248/6530 [=============>................] - ETA: 0s - loss: 0.0143
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0145
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0143
3808/6530 [================>.............] - ETA: 0s - loss: 0.0143
3984/6530 [=================>............] - ETA: 0s - loss: 0.0144
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0144
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0143
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0144
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0144
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0143
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0142
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0142
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0140
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0141
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0142
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0141
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0142
6384/6530 [============================>.] - ETA: 0s - loss: 0.0141
6530/6530 [==============================] - 2s 258us/step - loss: 0.0143 - val_loss: 0.0112
Epoch 14/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0085
 192/6530 [..............................] - ETA: 1s - loss: 0.0131
 368/6530 [>.............................] - ETA: 1s - loss: 0.0134
 544/6530 [=>............................] - ETA: 1s - loss: 0.0140
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0148
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0145
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0150
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0149
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0147
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0145
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0139
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0141
2400/6530 [==========>...................] - ETA: 1s - loss: 0.0141
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0141
2896/6530 [============>.................] - ETA: 0s - loss: 0.0140
3120/6530 [=============>................] - ETA: 0s - loss: 0.0139
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0139
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0140
3760/6530 [================>.............] - ETA: 0s - loss: 0.0139
3984/6530 [=================>............] - ETA: 0s - loss: 0.0140
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0140
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0139
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0139
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0139
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0139
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0138
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0137
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0136
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0136
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0137
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0138
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0137
6336/6530 [============================>.] - ETA: 0s - loss: 0.0138
6512/6530 [============================>.] - ETA: 0s - loss: 0.0139
6530/6530 [==============================] - 2s 277us/step - loss: 0.0139 - val_loss: 0.0113
Epoch 15/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0082
 176/6530 [..............................] - ETA: 2s - loss: 0.0124
 352/6530 [>.............................] - ETA: 1s - loss: 0.0128
 544/6530 [=>............................] - ETA: 1s - loss: 0.0139
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0148
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0143
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0145
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0145
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0144
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0144
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0141
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0136
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0137
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0137
2720/6530 [===========>..................] - ETA: 1s - loss: 0.0137
2992/6530 [============>.................] - ETA: 0s - loss: 0.0135
3232/6530 [=============>................] - ETA: 0s - loss: 0.0135
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0137
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0136
3872/6530 [================>.............] - ETA: 0s - loss: 0.0137
4096/6530 [=================>............] - ETA: 0s - loss: 0.0137
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0136
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0136
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0137
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0137
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0136
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0135
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0134
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0134
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0134
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0135
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0135
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0135
6384/6530 [============================>.] - ETA: 0s - loss: 0.0135
6530/6530 [==============================] - 2s 281us/step - loss: 0.0136 - val_loss: 0.0110
Epoch 16/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0089
 208/6530 [..............................] - ETA: 1s - loss: 0.0122
 400/6530 [>.............................] - ETA: 1s - loss: 0.0130
 576/6530 [=>............................] - ETA: 1s - loss: 0.0137
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0140
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0137
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0139
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0140
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0137
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0138
1856/6530 [=======>......................] - ETA: 1s - loss: 0.0134
2016/6530 [========>.....................] - ETA: 1s - loss: 0.0131
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0134
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0132
2592/6530 [==========>...................] - ETA: 1s - loss: 0.0134
2800/6530 [===========>..................] - ETA: 1s - loss: 0.0133
3008/6530 [============>.................] - ETA: 0s - loss: 0.0132
3216/6530 [=============>................] - ETA: 0s - loss: 0.0132
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0132
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0132
3792/6530 [================>.............] - ETA: 0s - loss: 0.0132
3968/6530 [=================>............] - ETA: 0s - loss: 0.0133
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0133
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0132
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0132
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0132
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0132
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0131
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0130
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0130
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0129
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0130
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0130
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0130
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0130
6464/6530 [============================>.] - ETA: 0s - loss: 0.0131
6530/6530 [==============================] - 2s 295us/step - loss: 0.0132 - val_loss: 0.0107
Epoch 17/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0091
 208/6530 [..............................] - ETA: 1s - loss: 0.0123
 400/6530 [>.............................] - ETA: 1s - loss: 0.0129
 592/6530 [=>............................] - ETA: 1s - loss: 0.0136
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0138
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0134
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0137
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0136
1520/6530 [=====>........................] - ETA: 1s - loss: 0.0133
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0133
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0128
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0128
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0128
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0129
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0130
2848/6530 [============>.................] - ETA: 1s - loss: 0.0128
3056/6530 [=============>................] - ETA: 0s - loss: 0.0127
3248/6530 [=============>................] - ETA: 0s - loss: 0.0127
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0129
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0129
3856/6530 [================>.............] - ETA: 0s - loss: 0.0130
4048/6530 [=================>............] - ETA: 0s - loss: 0.0130
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0129
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0129
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0128
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0128
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0128
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0127
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0126
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0126
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0127
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0127
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0127
6384/6530 [============================>.] - ETA: 0s - loss: 0.0127
6530/6530 [==============================] - 2s 281us/step - loss: 0.0128 - val_loss: 0.0107
Epoch 18/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0066
 192/6530 [..............................] - ETA: 1s - loss: 0.0115
 352/6530 [>.............................] - ETA: 1s - loss: 0.0118
 544/6530 [=>............................] - ETA: 1s - loss: 0.0128
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0133
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0128
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0132
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0130
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0129
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0127
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0123
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0123
2288/6530 [=========>....................] - ETA: 1s - loss: 0.0124
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0125
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0125
2816/6530 [===========>..................] - ETA: 1s - loss: 0.0124
3008/6530 [============>.................] - ETA: 0s - loss: 0.0123
3216/6530 [=============>................] - ETA: 0s - loss: 0.0123
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0124
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0124
3808/6530 [================>.............] - ETA: 0s - loss: 0.0124
4032/6530 [=================>............] - ETA: 0s - loss: 0.0125
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0125
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0124
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0124
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0123
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0123
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0122
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0122
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0123
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0123
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0123
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0123
6496/6530 [============================>.] - ETA: 0s - loss: 0.0123
6530/6530 [==============================] - 2s 274us/step - loss: 0.0124 - val_loss: 0.0103
Epoch 19/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0079
 240/6530 [>.............................] - ETA: 1s - loss: 0.0112
 448/6530 [=>............................] - ETA: 1s - loss: 0.0130
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0131
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0126
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0127
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0128
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0124
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0122
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0119
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0120
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0121
2880/6530 [============>.................] - ETA: 0s - loss: 0.0119
3120/6530 [=============>................] - ETA: 0s - loss: 0.0118
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0119
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0120
3776/6530 [================>.............] - ETA: 0s - loss: 0.0119
3984/6530 [=================>............] - ETA: 0s - loss: 0.0121
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0121
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0120
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0121
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0121
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0120
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0120
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0119
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0118
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0119
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0119
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0120
6320/6530 [============================>.] - ETA: 0s - loss: 0.0120
6530/6530 [==============================] - 2s 248us/step - loss: 0.0121 - val_loss: 0.0101
Epoch 20/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0082
 192/6530 [..............................] - ETA: 1s - loss: 0.0109
 368/6530 [>.............................] - ETA: 1s - loss: 0.0116
 560/6530 [=>............................] - ETA: 1s - loss: 0.0120
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0127
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0125
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0126
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0125
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0123
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0122
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0120
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0117
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0117
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0118
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0117
2560/6530 [==========>...................] - ETA: 1s - loss: 0.0118
2752/6530 [===========>..................] - ETA: 1s - loss: 0.0118
2928/6530 [============>.................] - ETA: 1s - loss: 0.0117
3104/6530 [=============>................] - ETA: 1s - loss: 0.0116
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0116
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0117
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0116
3808/6530 [================>.............] - ETA: 0s - loss: 0.0117
3984/6530 [=================>............] - ETA: 0s - loss: 0.0118
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0118
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0117
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0117
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0117
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0117
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0116
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0115
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0115
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0116
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0116
6400/6530 [============================>.] - ETA: 0s - loss: 0.0116
6530/6530 [==============================] - 2s 283us/step - loss: 0.0117 - val_loss: 0.0102
Epoch 21/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0080
 288/6530 [>.............................] - ETA: 1s - loss: 0.0108
 528/6530 [=>............................] - ETA: 1s - loss: 0.0120
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0124
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0120
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0122
1424/6530 [=====>........................] - ETA: 1s - loss: 0.0120
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0120
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0116
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0113
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0115
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0114
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0115
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0115
2896/6530 [============>.................] - ETA: 0s - loss: 0.0114
3088/6530 [=============>................] - ETA: 0s - loss: 0.0113
3264/6530 [=============>................] - ETA: 0s - loss: 0.0113
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0114
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0114
3776/6530 [================>.............] - ETA: 0s - loss: 0.0114
3936/6530 [=================>............] - ETA: 0s - loss: 0.0115
4096/6530 [=================>............] - ETA: 0s - loss: 0.0115
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0115
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0114
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0115
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0115
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0115
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0114
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0113
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0113
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0113
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0114
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0113
6336/6530 [============================>.] - ETA: 0s - loss: 0.0114
6530/6530 [==============================] - 2s 282us/step - loss: 0.0115 - val_loss: 0.0099
Epoch 22/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0082
 240/6530 [>.............................] - ETA: 1s - loss: 0.0104
 480/6530 [=>............................] - ETA: 1s - loss: 0.0120
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0122
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0116
1232/6530 [====>.........................] - ETA: 1s - loss: 0.0119
1440/6530 [=====>........................] - ETA: 1s - loss: 0.0116
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0115
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0113
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0110
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0111
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0111
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0112
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0112
2928/6530 [============>.................] - ETA: 0s - loss: 0.0111
3104/6530 [=============>................] - ETA: 0s - loss: 0.0110
3264/6530 [=============>................] - ETA: 0s - loss: 0.0110
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0111
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0111
3728/6530 [================>.............] - ETA: 0s - loss: 0.0111
3888/6530 [================>.............] - ETA: 0s - loss: 0.0112
4032/6530 [=================>............] - ETA: 0s - loss: 0.0113
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0112
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0112
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0112
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0112
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0111
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0110
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0110
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0109
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0109
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0110
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0110
6384/6530 [============================>.] - ETA: 0s - loss: 0.0110
6530/6530 [==============================] - 2s 274us/step - loss: 0.0111 - val_loss: 0.0097
Epoch 23/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0081
 224/6530 [>.............................] - ETA: 1s - loss: 0.0099
 448/6530 [=>............................] - ETA: 1s - loss: 0.0116
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0119
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0113
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0114
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0112
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0111
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0108
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0109
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0109
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0109
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0109
2928/6530 [============>.................] - ETA: 0s - loss: 0.0108
3104/6530 [=============>................] - ETA: 0s - loss: 0.0107
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0106
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0108
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0107
3808/6530 [================>.............] - ETA: 0s - loss: 0.0108
4000/6530 [=================>............] - ETA: 0s - loss: 0.0109
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0109
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0108
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0108
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0108
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0108
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0107
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0107
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0106
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0107
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0107
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0107
6352/6530 [============================>.] - ETA: 0s - loss: 0.0107
6530/6530 [==============================] - 2s 266us/step - loss: 0.0108 - val_loss: 0.0095
Epoch 24/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0083
 208/6530 [..............................] - ETA: 1s - loss: 0.0101
 384/6530 [>.............................] - ETA: 1s - loss: 0.0108
 544/6530 [=>............................] - ETA: 1s - loss: 0.0112
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0117
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0114
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0115
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0112
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0109
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0107
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0105
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0105
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0105
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0106
2672/6530 [===========>..................] - ETA: 1s - loss: 0.0106
2832/6530 [============>.................] - ETA: 1s - loss: 0.0105
2992/6530 [============>.................] - ETA: 0s - loss: 0.0105
3168/6530 [=============>................] - ETA: 0s - loss: 0.0104
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0104
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0105
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0105
3872/6530 [================>.............] - ETA: 0s - loss: 0.0106
4080/6530 [=================>............] - ETA: 0s - loss: 0.0107
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0106
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0106
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0106
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0106
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0106
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0105
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0105
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0104
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0105
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0105
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0105
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0105
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0105
6480/6530 [============================>.] - ETA: 0s - loss: 0.0105
6530/6530 [==============================] - 2s 302us/step - loss: 0.0106 - val_loss: 0.0098
Epoch 25/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0078
 224/6530 [>.............................] - ETA: 1s - loss: 0.0099
 448/6530 [=>............................] - ETA: 1s - loss: 0.0113
 640/6530 [=>............................] - ETA: 1s - loss: 0.0116
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0112
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0111
1264/6530 [====>.........................] - ETA: 1s - loss: 0.0111
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0107
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0106
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0102
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0102
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0103
2512/6530 [==========>...................] - ETA: 1s - loss: 0.0103
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0103
2848/6530 [============>.................] - ETA: 0s - loss: 0.0102
3008/6530 [============>.................] - ETA: 0s - loss: 0.0102
3168/6530 [=============>................] - ETA: 0s - loss: 0.0101
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0101
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0102
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0102
3824/6530 [================>.............] - ETA: 0s - loss: 0.0104
4016/6530 [=================>............] - ETA: 0s - loss: 0.0103
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0103
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0103
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0103
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0103
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0102
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0102
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0101
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0102
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0102
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0102
6336/6530 [============================>.] - ETA: 0s - loss: 0.0102
6528/6530 [============================>.] - ETA: 0s - loss: 0.0103
6530/6530 [==============================] - 2s 274us/step - loss: 0.0103 - val_loss: 0.0097
Epoch 26/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0083
 224/6530 [>.............................] - ETA: 1s - loss: 0.0096
 432/6530 [>.............................] - ETA: 1s - loss: 0.0109
 624/6530 [=>............................] - ETA: 1s - loss: 0.0111
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0111
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0107
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0107
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0106
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0103
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0102
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0100
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0099
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0100
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0100
2720/6530 [===========>..................] - ETA: 1s - loss: 0.0100
2992/6530 [============>.................] - ETA: 0s - loss: 0.0099
3248/6530 [=============>................] - ETA: 0s - loss: 0.0099
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0100
3728/6530 [================>.............] - ETA: 0s - loss: 0.0100
3952/6530 [=================>............] - ETA: 0s - loss: 0.0101
4128/6530 [=================>............] - ETA: 0s - loss: 0.0101
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0101
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0101
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0101
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0101
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0100
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0100
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0099
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0099
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0100
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0100
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0100
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0100
6448/6530 [============================>.] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 2s 276us/step - loss: 0.0101 - val_loss: 0.0099
Epoch 27/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0092
 224/6530 [>.............................] - ETA: 1s - loss: 0.0094
 416/6530 [>.............................] - ETA: 1s - loss: 0.0106
 640/6530 [=>............................] - ETA: 1s - loss: 0.0112
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0106
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0107
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0107
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0104
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0102
1952/6530 [=======>......................] - ETA: 1s - loss: 0.0099
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0099
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0099
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0100
2880/6530 [============>.................] - ETA: 0s - loss: 0.0098
3072/6530 [=============>................] - ETA: 0s - loss: 0.0098
3264/6530 [=============>................] - ETA: 0s - loss: 0.0097
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0098
3728/6530 [================>.............] - ETA: 0s - loss: 0.0098
3968/6530 [=================>............] - ETA: 0s - loss: 0.0099
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0099
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0099
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0099
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0099
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0098
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0098
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0098
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0097
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0098
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0098
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0098
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0098
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0098
6464/6530 [============================>.] - ETA: 0s - loss: 0.0099
6530/6530 [==============================] - 2s 267us/step - loss: 0.0099 - val_loss: 0.0101
Epoch 28/81

  16/6530 [..............................] - ETA: 2s - loss: 0.0089
 192/6530 [..............................] - ETA: 1s - loss: 0.0099
 400/6530 [>.............................] - ETA: 1s - loss: 0.0105
 592/6530 [=>............................] - ETA: 1s - loss: 0.0108
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0109
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0105
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0107
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0104
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0101
1728/6530 [======>.......................] - ETA: 1s - loss: 0.0099
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0097
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0097
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0096
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0097
2592/6530 [==========>...................] - ETA: 1s - loss: 0.0097
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0097
2944/6530 [============>.................] - ETA: 1s - loss: 0.0096
3120/6530 [=============>................] - ETA: 0s - loss: 0.0095
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0095
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0096
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0096
3856/6530 [================>.............] - ETA: 0s - loss: 0.0097
4080/6530 [=================>............] - ETA: 0s - loss: 0.0097
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0097
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0097
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0097
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0096
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0096
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0095
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0095
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0096
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0096
6464/6530 [============================>.] - ETA: 0s - loss: 0.0096
6530/6530 [==============================] - 2s 264us/step - loss: 0.0097 - val_loss: 0.0097

# training | RMSE: 0.0880, MAE: 0.0686
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20261337059472792}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10963218549386174}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.08800967493615067, 'rmse': 0.08800967493615067, 'mae': 0.0685854007524324, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=81.0 loss={'loss': 0.08800967493615067, 'rmse': 0.08800967493615067, 'mae': 0.0685854007524324, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 52, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 94, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.20261337059472792}, 'layer_3_size': 32, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.10963218549386174}, 'layer_5_size': 100, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
get a list [results] of length 121
get a list [loss] of length 1
get a list [val_loss] of length 1
length of indices is (0,)
length of indices is 1
length of T is 1
s=3
T is of size 34
T=[{'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2683168763307201}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31497950775664185}, 'layer_4_size': 29, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38012202189334443}, 'layer_3_size': 39, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 97, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.36520987335416155}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24309239614529404}, 'layer_2_size': 99, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.475456178893257}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39915504322999096}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45526819660442785}, 'layer_1_size': 79, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.20112903734462156}, 'layer_4_size': 90, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42072789870156346}, 'layer_4_size': 63, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3578978424837119}, 'layer_2_size': 25, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 87, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3994301914875815}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17384346890971408}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.494814807006017}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.32905590204389745}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2658416628015635}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14283571375707005}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 35, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28252178251643656}, 'layer_5_size': 42, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2597606869601959}, 'layer_4_size': 82, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 74, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4358447939016047}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38572840251416507}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.280536333095057}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4399885133326492}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.44607532308855846}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1382648397930245}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.19275704376715105}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19448349833074535}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1752856684537518}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15886729165116187}, 'layer_1_size': 26, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 5, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 24, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39631621748974977}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3898051523657077}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2324361715500335}, 'layer_3_size': 78, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41762675118712356}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23052081187472862}, 'layer_3_size': 22, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 49, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 23, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40910277421326435}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3257062982984602}, 'layer_1_size': 6, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 32, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.29332416198982303}, 'layer_1_size': 11, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 40, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3170982998056019}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34939336786908626}, 'layer_2_size': 39, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2352907000867381}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4599186566416881}, 'layer_4_size': 32, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13515921791326946}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4273031903618001}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3015218909611247}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 22, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 71, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 72, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2877377074137758}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 6, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2022834813979455}, 'layer_4_size': 9, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 59, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2677434157431451}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]
newly formed T structure is:[[0, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2683168763307201}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31497950775664185}, 'layer_4_size': 29, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [1, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38012202189334443}, 'layer_3_size': 39, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}], [2, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 97, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.36520987335416155}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [3, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24309239614529404}, 'layer_2_size': 99, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.475456178893257}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39915504322999096}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [4, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45526819660442785}, 'layer_1_size': 79, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}], [5, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.20112903734462156}, 'layer_4_size': 90, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [6, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42072789870156346}, 'layer_4_size': 63, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}], [7, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3578978424837119}, 'layer_2_size': 25, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 87, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [8, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3994301914875815}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [9, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17384346890971408}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.494814807006017}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [10, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.32905590204389745}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2658416628015635}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [11, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14283571375707005}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 35, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28252178251643656}, 'layer_5_size': 42, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [12, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2597606869601959}, 'layer_4_size': 82, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [13, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 74, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [14, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4358447939016047}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [15, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38572840251416507}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.280536333095057}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4399885133326492}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [16, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [17, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.44607532308855846}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1382648397930245}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [18, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.19275704376715105}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19448349833074535}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [19, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1752856684537518}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [20, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15886729165116187}, 'layer_1_size': 26, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 5, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 24, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [21, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39631621748974977}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3898051523657077}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2324361715500335}, 'layer_3_size': 78, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41762675118712356}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [22, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23052081187472862}, 'layer_3_size': 22, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 49, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 23, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}], [23, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40910277421326435}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [24, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3257062982984602}, 'layer_1_size': 6, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 32, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [25, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.29332416198982303}, 'layer_1_size': 11, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 40, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [26, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3170982998056019}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34939336786908626}, 'layer_2_size': 39, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2352907000867381}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4599186566416881}, 'layer_4_size': 32, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [27, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13515921791326946}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [28, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4273031903618001}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3015218909611247}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [29, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [30, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 22, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 71, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [31, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 72, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2877377074137758}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}], [32, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 6, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2022834813979455}, 'layer_4_size': 9, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}], [33, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 59, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2677434157431451}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]] 

*** 34 configurations x 3.0 iterations each

1 | Thu Sep 27 23:26:14 2018 | lowest loss so far: 0.0809 (run 0)

{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  18 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a74be80>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 1:16 - loss: 0.7033
1536/6530 [======>.......................] - ETA: 2s - loss: 0.6959  
3136/6530 [=============>................] - ETA: 0s - loss: 0.6765
4736/6530 [====================>.........] - ETA: 0s - loss: 0.6310
6530/6530 [==============================] - 1s 149us/step - loss: 0.5652 - val_loss: 0.3090
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2973
2624/6530 [===========>..................] - ETA: 0s - loss: 0.2134
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1871
6530/6530 [==============================] - 0s 20us/step - loss: 0.1819 - val_loss: 0.1614
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1730
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1613
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1612
6464/6530 [============================>.] - ETA: 0s - loss: 0.1614
6530/6530 [==============================] - 0s 25us/step - loss: 0.1613 - val_loss: 0.1610
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  29 | activation: tanh    | extras: dropout - rate: 26.8% 
layer 2 | size:  11 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  19 | activation: relu    | extras: None 
layer 4 | size:  29 | activation: sigmoid | extras: dropout - rate: 31.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 6:55 - loss: 0.4449{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  97 | activation: relu    | extras: batchnorm 
layer 2 | size:  41 | activation: relu    | extras: dropout - rate: 36.5% 
layer 3 | size:  46 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 1:44 - loss: 1.3897
 272/6530 [>.............................] - ETA: 24s - loss: 0.4178 
1152/6530 [====>.........................] - ETA: 5s - loss: 0.7639  
 512/6530 [=>............................] - ETA: 13s - loss: 0.3419
2176/6530 [========>.....................] - ETA: 2s - loss: 0.5974
 800/6530 [==>...........................] - ETA: 8s - loss: 0.2949 
3264/6530 [=============>................] - ETA: 1s - loss: 0.4995
1104/6530 [====>.........................] - ETA: 6s - loss: 0.2693
4352/6530 [==================>...........] - ETA: 0s - loss: 0.4348
1392/6530 [=====>........................] - ETA: 4s - loss: 0.2541
5568/6530 [========================>.....] - ETA: 0s - loss: 0.3841
1680/6530 [======>.......................] - ETA: 3s - loss: 0.2415
# training | RMSE: 0.2038, MAE: 0.1603
worker 1  xfile  [1, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38012202189334443}, 'layer_3_size': 39, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.20379584298538703, 'rmse': 0.20379584298538703, 'mae': 0.1602556482816008, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  60 | activation: sigmoid | extras: None 
layer 2 | size:  99 | activation: relu    | extras: dropout - rate: 24.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 33s - loss: 0.4868
1968/6530 [========>.....................] - ETA: 3s - loss: 0.2331
6530/6530 [==============================] - 1s 211us/step - loss: 0.3522 - val_loss: 0.0931
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1324
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0944 
2256/6530 [=========>....................] - ETA: 2s - loss: 0.2255
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1614
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0835
2544/6530 [==========>...................] - ETA: 2s - loss: 0.2199
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1464
2912/6530 [============>.................] - ETA: 0s - loss: 0.0782
2832/6530 [============>.................] - ETA: 2s - loss: 0.2141
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1383
3872/6530 [================>.............] - ETA: 0s - loss: 0.0750
3120/6530 [=============>................] - ETA: 1s - loss: 0.2113
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1314
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0741
3408/6530 [==============>...............] - ETA: 1s - loss: 0.2075
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1270
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0732
3680/6530 [===============>..............] - ETA: 1s - loss: 0.2044
6530/6530 [==============================] - 0s 45us/step - loss: 0.1236 - val_loss: 0.0601
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0937
6530/6530 [==============================] - 1s 82us/step - loss: 0.0722 - val_loss: 0.0805
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0855
3968/6530 [=================>............] - ETA: 1s - loss: 0.2024
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0925
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0702
4272/6530 [==================>...........] - ETA: 0s - loss: 0.2005
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0900
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0717
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1988
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0885
2912/6530 [============>.................] - ETA: 0s - loss: 0.0667
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1969
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0854
3904/6530 [================>.............] - ETA: 0s - loss: 0.0635
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1947
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0842
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0633
6530/6530 [==============================] - 0s 46us/step - loss: 0.0827 - val_loss: 0.0521

5456/6530 [========================>.....] - ETA: 0s - loss: 0.1934
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0624
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1919
6530/6530 [==============================] - 0s 54us/step - loss: 0.0614 - val_loss: 0.1082
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1086
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1903
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0567
6352/6530 [============================>.] - ETA: 0s - loss: 0.1895
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0592
3040/6530 [============>.................] - ETA: 0s - loss: 0.0549
6530/6530 [==============================] - 2s 345us/step - loss: 0.1891 - val_loss: 0.1593
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1545
4000/6530 [=================>............] - ETA: 0s - loss: 0.0541
 304/6530 [>.............................] - ETA: 1s - loss: 0.1666
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0537
 624/6530 [=>............................] - ETA: 1s - loss: 0.1643
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0535
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1643
6530/6530 [==============================] - 0s 52us/step - loss: 0.0530 - val_loss: 0.1002

1232/6530 [====>.........................] - ETA: 0s - loss: 0.1629
# training | RMSE: 0.2250, MAE: 0.1769
worker 2  xfile  [2, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 97, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.36520987335416155}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.22497816161445844, 'rmse': 0.22497816161445844, 'mae': 0.17691561320360472, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  79 | activation: sigmoid | extras: dropout - rate: 45.5% 
layer 2 | size:  60 | activation: relu    | extras: None 
layer 3 | size:  76 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a7c00f0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 18s - loss: 1.3119
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1624
2304/6530 [=========>....................] - ETA: 0s - loss: 0.3090 
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1620
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2106
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1617
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1629
6530/6530 [==============================] - 0s 56us/step - loss: 0.1716 - val_loss: 0.1182
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0934
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1628
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0861
3008/6530 [============>.................] - ETA: 0s - loss: 0.1629
4032/6530 [=================>............] - ETA: 0s - loss: 0.0826
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1630
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0807
6530/6530 [==============================] - 0s 26us/step - loss: 0.0799 - val_loss: 0.1006
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0932
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1619
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0722
3920/6530 [=================>............] - ETA: 0s - loss: 0.1627
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0716
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1628
6336/6530 [============================>.] - ETA: 0s - loss: 0.0706
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1626
6530/6530 [==============================] - 0s 26us/step - loss: 0.0706 - val_loss: 0.0916

4832/6530 [=====================>........] - ETA: 0s - loss: 0.1634
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1633
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1634
# training | RMSE: 0.3151, MAE: 0.2688
worker 1  xfile  [3, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24309239614529404}, 'layer_2_size': 99, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.475456178893257}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39915504322999096}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.31510035236999556, 'rmse': 0.31510035236999556, 'mae': 0.26879114859380004, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  27 | activation: relu    | extras: batchnorm 
layer 2 | size:   3 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f77784879b0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 15s - loss: 0.6880
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1638
3328/6530 [==============>...............] - ETA: 0s - loss: 0.5726 
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1639
6530/6530 [==============================] - 0s 68us/step - loss: 0.5188 - val_loss: 0.4125
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.3975
6336/6530 [============================>.] - ETA: 0s - loss: 0.1634
3200/6530 [=============>................] - ETA: 0s - loss: 0.3681
6530/6530 [==============================] - 1s 176us/step - loss: 0.1634 - val_loss: 0.1563
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1501
6272/6530 [===========================>..] - ETA: 0s - loss: 0.3414
6530/6530 [==============================] - 0s 17us/step - loss: 0.3390 - val_loss: 0.2635
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2593
 320/6530 [>.............................] - ETA: 1s - loss: 0.1553
3200/6530 [=============>................] - ETA: 0s - loss: 0.2451
 624/6530 [=>............................] - ETA: 0s - loss: 0.1594
# training | RMSE: 0.3040, MAE: 0.2523
worker 2  xfile  [4, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45526819660442785}, 'layer_1_size': 79, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.30396724932766467, 'rmse': 0.30396724932766467, 'mae': 0.25232013808562925, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  53 | activation: tanh    | extras: None 
layer 2 | size:  93 | activation: sigmoid | extras: None 
layer 3 | size:  42 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777868a358>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 36s - loss: 0.9872
6400/6530 [============================>.] - ETA: 0s - loss: 0.2258
 912/6530 [===>..........................] - ETA: 0s - loss: 0.1592
6530/6530 [==============================] - 0s 18us/step - loss: 0.2254 - val_loss: 0.1903

1120/6530 [====>.........................] - ETA: 1s - loss: 0.1859 
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1591
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1242
1488/6530 [=====>........................] - ETA: 0s - loss: 0.1614
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0972
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1616
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0834
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1592
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0745
2384/6530 [=========>....................] - ETA: 0s - loss: 0.1585
6530/6530 [==============================] - 1s 78us/step - loss: 0.0696 - val_loss: 0.0480
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0526
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1590
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0434
2992/6530 [============>.................] - ETA: 0s - loss: 0.1592
# training | RMSE: 0.2416, MAE: 0.1888
worker 1  xfile  [5, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.20112903734462156}, 'layer_4_size': 90, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.24156119538008497, 'rmse': 0.24156119538008497, 'mae': 0.1887513662701486, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  79 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777800e438>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 7s - loss: 0.5816
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0413
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1599
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2539
3136/6530 [=============>................] - ETA: 0s - loss: 0.0416
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1597
6530/6530 [==============================] - 0s 36us/step - loss: 0.2190 - val_loss: 0.0529
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0584
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0416
3888/6530 [================>.............] - ETA: 0s - loss: 0.1598
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0479
6530/6530 [==============================] - 0s 10us/step - loss: 0.0475 - val_loss: 0.0473
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0514
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0414
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1593
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0445
6368/6530 [============================>.] - ETA: 0s - loss: 0.0416
6530/6530 [==============================] - 0s 11us/step - loss: 0.0443 - val_loss: 0.0451

4496/6530 [===================>..........] - ETA: 0s - loss: 0.1591
6530/6530 [==============================] - 0s 50us/step - loss: 0.0415 - val_loss: 0.0450
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.0745
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1596
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0414
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1589
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0408
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1584
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0403
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1580
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0406
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1580
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0414
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1580
6530/6530 [==============================] - 0s 48us/step - loss: 0.0414 - val_loss: 0.0514

6512/6530 [============================>.] - ETA: 0s - loss: 0.1585
6530/6530 [==============================] - 1s 179us/step - loss: 0.1585 - val_loss: 0.1526

# training | RMSE: 0.2075, MAE: 0.1673
worker 1  xfile  [7, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3578978424837119}, 'layer_2_size': 25, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 87, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.2074859341048814, 'rmse': 0.2074859341048814, 'mae': 0.16734264898711884, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  78 | activation: tanh    | extras: None 
layer 2 | size:  54 | activation: relu    | extras: dropout - rate: 39.9% 
layer 3 | size:  83 | activation: tanh    | extras: None 
layer 4 | size:  46 | activation: relu    | extras: batchnorm 
layer 5 | size:  68 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7760185780>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:49 - loss: 3.9014
# training | RMSE: 0.2241, MAE: 0.1717
worker 2  xfile  [6, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42072789870156346}, 'layer_4_size': 63, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2240687772566506, 'rmse': 0.2240687772566506, 'mae': 0.17167759980235475, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  64 | activation: relu    | extras: batchnorm 
layer 2 | size:  37 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777801a3c8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 28s - loss: 2.0309
 288/6530 [>.............................] - ETA: 10s - loss: 1.5423 
1600/6530 [======>.......................] - ETA: 1s - loss: 0.2783 
 576/6530 [=>............................] - ETA: 5s - loss: 1.1165 
3200/6530 [=============>................] - ETA: 0s - loss: 0.1836
 880/6530 [===>..........................] - ETA: 3s - loss: 0.9312
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1472
1184/6530 [====>.........................] - ETA: 2s - loss: 0.8098
6528/6530 [============================>.] - ETA: 0s - loss: 0.1266
# training | RMSE: 0.1927, MAE: 0.1465
worker 0  xfile  [0, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2683168763307201}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31497950775664185}, 'layer_4_size': 29, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.1926673153342449, 'rmse': 0.1926673153342449, 'mae': 0.146488326691823, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  39 | activation: relu    | extras: None 
layer 2 | size:  62 | activation: tanh    | extras: dropout - rate: 32.9% 
layer 3 | size:  90 | activation: sigmoid | extras: None 
layer 4 | size:  63 | activation: sigmoid | extras: dropout - rate: 26.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a7c0080>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 14s - loss: 0.5912
6530/6530 [==============================] - 1s 81us/step - loss: 0.1266 - val_loss: 0.0593
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0702
1504/6530 [=====>........................] - ETA: 2s - loss: 0.7195
3072/6530 [=============>................] - ETA: 0s - loss: 0.1176 
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0557
1808/6530 [=======>......................] - ETA: 1s - loss: 0.6635
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0881
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0528
2112/6530 [========>.....................] - ETA: 1s - loss: 0.6120
6530/6530 [==============================] - 0s 67us/step - loss: 0.0839 - val_loss: 0.0484
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0622
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0516
2400/6530 [==========>...................] - ETA: 1s - loss: 0.5798
2944/6530 [============>.................] - ETA: 0s - loss: 0.0465
2704/6530 [===========>..................] - ETA: 1s - loss: 0.5499
6530/6530 [==============================] - 0s 33us/step - loss: 0.0506 - val_loss: 0.0568
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0689
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0441
6530/6530 [==============================] - 0s 19us/step - loss: 0.0433 - val_loss: 0.0394
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0454
2992/6530 [============>.................] - ETA: 1s - loss: 0.5191
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0437
2944/6530 [============>.................] - ETA: 0s - loss: 0.0414
3296/6530 [==============>...............] - ETA: 0s - loss: 0.4924
3264/6530 [=============>................] - ETA: 0s - loss: 0.0416
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0408
3600/6530 [===============>..............] - ETA: 0s - loss: 0.4713
6530/6530 [==============================] - 0s 19us/step - loss: 0.0405 - val_loss: 0.0350

4928/6530 [=====================>........] - ETA: 0s - loss: 0.0412
3904/6530 [================>.............] - ETA: 0s - loss: 0.4548
6528/6530 [============================>.] - ETA: 0s - loss: 0.0408
6530/6530 [==============================] - 0s 33us/step - loss: 0.0408 - val_loss: 0.0391

4176/6530 [==================>...........] - ETA: 0s - loss: 0.4379
4496/6530 [===================>..........] - ETA: 0s - loss: 0.4219
4832/6530 [=====================>........] - ETA: 0s - loss: 0.4073
5168/6530 [======================>.......] - ETA: 0s - loss: 0.3925
5456/6530 [========================>.....] - ETA: 0s - loss: 0.3819
5728/6530 [=========================>....] - ETA: 0s - loss: 0.3720
6000/6530 [==========================>...] - ETA: 0s - loss: 0.3635
6304/6530 [===========================>..] - ETA: 0s - loss: 0.3534
# training | RMSE: 0.1837, MAE: 0.1472
worker 0  xfile  [10, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.32905590204389745}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2658416628015635}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.18366187597835126, 'rmse': 0.18366187597835126, 'mae': 0.14721898313333556, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  15 | activation: tanh    | extras: None 
layer 2 | size:  15 | activation: sigmoid | extras: None 
layer 3 | size:  98 | activation: tanh    | extras: dropout - rate: 14.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7750110940>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:28 - loss: 0.6476
6530/6530 [==============================] - 2s 248us/step - loss: 0.3469 - val_loss: 0.0924
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.2213
# training | RMSE: 0.1765, MAE: 0.1406
worker 2  xfile  [9, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17384346890971408}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.494814807006017}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.17651724691758222, 'rmse': 0.17651724691758222, 'mae': 0.14058402312658005, 'early_stop': False}
{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  21 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777801a320>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 1:08 - loss: 0.8222
 560/6530 [=>............................] - ETA: 2s - loss: 0.1368  
 320/6530 [>.............................] - ETA: 1s - loss: 0.1661
 704/6530 [==>...........................] - ETA: 1s - loss: 0.3514  
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1068
 624/6530 [=>............................] - ETA: 1s - loss: 0.1729
1408/6530 [=====>........................] - ETA: 0s - loss: 0.2232
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0901
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1673
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1773
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0813
1232/6530 [====>.........................] - ETA: 0s - loss: 0.1618
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1512
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0757
3440/6530 [==============>...............] - ETA: 0s - loss: 0.1349
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1552
3184/6530 [=============>................] - ETA: 0s - loss: 0.0697
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1221
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1521
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0665
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1131
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1512
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0635
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1058
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1494
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0612
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0997
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1479
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0593
3040/6530 [============>.................] - ETA: 0s - loss: 0.1456
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0576
6530/6530 [==============================] - 1s 107us/step - loss: 0.0969 - val_loss: 0.0487
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0657
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1439
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0563
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0511
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1422
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0488
6530/6530 [==============================] - 1s 138us/step - loss: 0.0555 - val_loss: 0.0404
Epoch 2/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0476
3936/6530 [=================>............] - ETA: 0s - loss: 0.1402
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0480
 544/6530 [=>............................] - ETA: 0s - loss: 0.0420
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1391
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0475
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0414
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1379
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0463
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0394
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1362
4000/6530 [=================>............] - ETA: 0s - loss: 0.0460
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0397
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1353
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0456
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0406
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1345
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0454
3136/6530 [=============>................] - ETA: 0s - loss: 0.0394
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1338
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0448
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0402
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1327
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0401
6530/6530 [==============================] - 1s 80us/step - loss: 0.0445 - val_loss: 0.0432
Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0498
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1315
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0402
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0434
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0403
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0414
6530/6530 [==============================] - 1s 178us/step - loss: 0.1305 - val_loss: 0.0601
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0961
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0402
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0416
 304/6530 [>.............................] - ETA: 1s - loss: 0.1099
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0403
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0419
 608/6530 [=>............................] - ETA: 1s - loss: 0.1100
6530/6530 [==============================] - 1s 102us/step - loss: 0.0401 - val_loss: 0.0397

3280/6530 [==============>...............] - ETA: 0s - loss: 0.0409Epoch 3/3

  16/6530 [..............................] - ETA: 0s - loss: 0.0469
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1063
3968/6530 [=================>............] - ETA: 0s - loss: 0.0413
 544/6530 [=>............................] - ETA: 0s - loss: 0.0418
1200/6530 [====>.........................] - ETA: 0s - loss: 0.1063
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0414
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0412
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1062
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0415
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0390
1808/6530 [=======>......................] - ETA: 0s - loss: 0.1054
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0415
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0393
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1025
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0402
2384/6530 [=========>....................] - ETA: 0s - loss: 0.1002
6530/6530 [==============================] - 1s 80us/step - loss: 0.0413 - val_loss: 0.0430

3152/6530 [=============>................] - ETA: 0s - loss: 0.0390
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0987
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0395
2976/6530 [============>.................] - ETA: 0s - loss: 0.0989
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0395
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0976
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0396
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0971
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0396
3872/6530 [================>.............] - ETA: 0s - loss: 0.0968
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0395
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0964
6336/6530 [============================>.] - ETA: 0s - loss: 0.0396
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0956
6530/6530 [==============================] - 1s 100us/step - loss: 0.0394 - val_loss: 0.0414

4784/6530 [====================>.........] - ETA: 0s - loss: 0.0946
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0943
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0943
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0936
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0930
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0924
6530/6530 [==============================] - 1s 179us/step - loss: 0.0918 - val_loss: 0.0501

# training | RMSE: 0.2061, MAE: 0.1689
worker 2  xfile  [12, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2597606869601959}, 'layer_4_size': 82, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.20610084846090226, 'rmse': 0.20610084846090226, 'mae': 0.1688538869804835, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  77 | activation: relu    | extras: batchnorm 
layer 2 | size:  48 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777874af28>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 10s - loss: 3.8334
5632/6530 [========================>.....] - ETA: 0s - loss: 1.2794 
# training | RMSE: 0.2028, MAE: 0.1669
worker 0  xfile  [11, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14283571375707005}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 35, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28252178251643656}, 'layer_5_size': 42, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20275045750390155, 'rmse': 0.20275045750390155, 'mae': 0.16694152676556454, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  66 | activation: tanh    | extras: None 
layer 2 | size:  52 | activation: sigmoid | extras: dropout - rate: 43.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777814c4a8>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 11s - loss: 0.4543
6530/6530 [==============================] - 1s 80us/step - loss: 1.1409 - val_loss: 0.2567

3840/6530 [================>.............] - ETA: 0s - loss: 0.2642 Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2477
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1928
6530/6530 [==============================] - 0s 52us/step - loss: 0.2319 - val_loss: 0.1721
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1784
6530/6530 [==============================] - 0s 10us/step - loss: 0.1851 - val_loss: 0.1526
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.1455
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1628
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1332
6530/6530 [==============================] - 0s 10us/step - loss: 0.1318 - val_loss: 0.1235

6530/6530 [==============================] - 0s 13us/step - loss: 0.1625 - val_loss: 0.1616
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1639
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1628
6530/6530 [==============================] - 0s 12us/step - loss: 0.1619 - val_loss: 0.1613

# training | RMSE: 0.2202, MAE: 0.1768
worker 1  xfile  [8, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3994301914875815}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.22015346539745975, 'rmse': 0.22015346539745975, 'mae': 0.1768167083729206, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  42 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  67 | activation: tanh    | extras: dropout - rate: 38.6% 
layer 3 | size:  15 | activation: relu    | extras: dropout - rate: 28.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f775877f3c8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 2:52 - loss: 0.6272
 304/6530 [>.............................] - ETA: 9s - loss: 0.6589  
# training | RMSE: 0.2029, MAE: 0.1602
worker 0  xfile  [14, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4358447939016047}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20288699719811606, 'rmse': 0.20288699719811606, 'mae': 0.1601991649080949, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:   9 | activation: relu    | extras: dropout - rate: 44.6% 
layer 2 | size:  29 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7748066080>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 16s - loss: 0.8177
 608/6530 [=>............................] - ETA: 5s - loss: 0.5450
3584/6530 [===============>..............] - ETA: 0s - loss: 0.5681 
 912/6530 [===>..........................] - ETA: 3s - loss: 0.4706
1248/6530 [====>.........................] - ETA: 2s - loss: 0.4226
6530/6530 [==============================] - 0s 71us/step - loss: 0.4242 - val_loss: 0.2007
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2540
1600/6530 [======>.......................] - ETA: 2s - loss: 0.3876
3968/6530 [=================>............] - ETA: 0s - loss: 0.2237
1952/6530 [=======>......................] - ETA: 1s - loss: 0.3587
6530/6530 [==============================] - 0s 14us/step - loss: 0.2205 - val_loss: 0.2001
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.2302
2320/6530 [=========>....................] - ETA: 1s - loss: 0.3329
3968/6530 [=================>............] - ETA: 0s - loss: 0.2167
6530/6530 [==============================] - 0s 14us/step - loss: 0.2140 - val_loss: 0.1997

2672/6530 [===========>..................] - ETA: 1s - loss: 0.3132
3040/6530 [============>.................] - ETA: 1s - loss: 0.2979
3408/6530 [==============>...............] - ETA: 0s - loss: 0.2862
3792/6530 [================>.............] - ETA: 0s - loss: 0.2752
# training | RMSE: 0.3354, MAE: 0.2675
worker 2  xfile  [13, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 74, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.33536852317420474, 'rmse': 0.33536852317420474, 'mae': 0.2674671414075762, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  48 | activation: relu    | extras: None 
layer 2 | size:  54 | activation: relu    | extras: batchnorm 
layer 3 | size:  23 | activation: relu    | extras: batchnorm 
layer 4 | size:  33 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f74343082e8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 53s - loss: 0.5765
4128/6530 [=================>............] - ETA: 0s - loss: 0.2672
 896/6530 [===>..........................] - ETA: 3s - loss: 0.4991 
4416/6530 [===================>..........] - ETA: 0s - loss: 0.2630
2048/6530 [========>.....................] - ETA: 1s - loss: 0.4148
4736/6530 [====================>.........] - ETA: 0s - loss: 0.2574
3200/6530 [=============>................] - ETA: 0s - loss: 0.3327
5024/6530 [======================>.......] - ETA: 0s - loss: 0.2530
4288/6530 [==================>...........] - ETA: 0s - loss: 0.2637
5344/6530 [=======================>......] - ETA: 0s - loss: 0.2490
5312/6530 [=======================>......] - ETA: 0s - loss: 0.2197
5680/6530 [=========================>....] - ETA: 0s - loss: 0.2450
6464/6530 [============================>.] - ETA: 0s - loss: 0.1844
6032/6530 [==========================>...] - ETA: 0s - loss: 0.2412
6368/6530 [============================>.] - ETA: 0s - loss: 0.2378
6530/6530 [==============================] - 1s 142us/step - loss: 0.1827 - val_loss: 0.0414
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0305
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0211
6530/6530 [==============================] - 2s 231us/step - loss: 0.2363 - val_loss: 0.1670
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1241
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0192
 352/6530 [>.............................] - ETA: 0s - loss: 0.1695
3136/6530 [=============>................] - ETA: 0s - loss: 0.0192
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1754
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0184
1040/6530 [===>..........................] - ETA: 0s - loss: 0.1735
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0181
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1743
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0172
6530/6530 [==============================] - 0s 53us/step - loss: 0.0171 - val_loss: 0.0256

1760/6530 [=======>......................] - ETA: 0s - loss: 0.1738Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0170
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1729
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0135
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1730
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0133
2864/6530 [============>.................] - ETA: 0s - loss: 0.1732
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0132
# training | RMSE: 0.2465, MAE: 0.2019
worker 0  xfile  [17, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.44607532308855846}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1382648397930245}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.24653444441817327, 'rmse': 0.24653444441817327, 'mae': 0.20186190801805987, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  31 | activation: relu    | extras: None 
layer 2 | size:  22 | activation: relu    | extras: batchnorm 
layer 3 | size:  16 | activation: sigmoid | extras: dropout - rate: 19.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7734711630>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 54s - loss: 3.6320
3152/6530 [=============>................] - ETA: 0s - loss: 0.1728
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0130
1344/6530 [=====>........................] - ETA: 2s - loss: 3.0852 
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1722
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0131
2560/6530 [==========>...................] - ETA: 0s - loss: 2.7146
3824/6530 [================>.............] - ETA: 0s - loss: 0.1728
6464/6530 [============================>.] - ETA: 0s - loss: 0.0127
6530/6530 [==============================] - 0s 51us/step - loss: 0.0127 - val_loss: 0.0120

3840/6530 [================>.............] - ETA: 0s - loss: 2.3521
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1732
5184/6530 [======================>.......] - ETA: 0s - loss: 2.0292
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1734
6464/6530 [============================>.] - ETA: 0s - loss: 1.7768
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1730
6530/6530 [==============================] - 1s 131us/step - loss: 1.7647 - val_loss: 0.5169
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.5802
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1732
1408/6530 [=====>........................] - ETA: 0s - loss: 0.4581
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1732
2816/6530 [===========>..................] - ETA: 0s - loss: 0.3799
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1732
4160/6530 [==================>...........] - ETA: 0s - loss: 0.3133
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1731
5440/6530 [=======================>......] - ETA: 0s - loss: 0.2679
6530/6530 [==============================] - 1s 152us/step - loss: 0.1730 - val_loss: 0.1574
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.1336
6530/6530 [==============================] - 0s 41us/step - loss: 0.2376 - val_loss: 0.0705
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.0864
 368/6530 [>.............................] - ETA: 0s - loss: 0.1604
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0679
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1606
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0653
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1620
4032/6530 [=================>............] - ETA: 0s - loss: 0.0612
1424/6530 [=====>........................] - ETA: 0s - loss: 0.1637
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0581
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1633
6530/6530 [==============================] - 0s 41us/step - loss: 0.0560 - val_loss: 0.0413

2080/6530 [========>.....................] - ETA: 0s - loss: 0.1649
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1649
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1646
3104/6530 [=============>................] - ETA: 0s - loss: 0.1634
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1608
3792/6530 [================>.............] - ETA: 0s - loss: 0.1610
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1606
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1599
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1589
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1584
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1583
# training | RMSE: 0.2082, MAE: 0.1678
worker 0  xfile  [18, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.19275704376715105}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19448349833074535}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.20816220175744168, 'rmse': 0.20816220175744168, 'mae': 0.1677958824886727, 'early_stop': False}
{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  26 | activation: sigmoid | extras: dropout - rate: 15.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f77340b07f0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 26s - loss: 1.5332
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1579
2432/6530 [==========>...................] - ETA: 0s - loss: 1.1244 
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1581
4864/6530 [=====================>........] - ETA: 0s - loss: 0.7311
6336/6530 [============================>.] - ETA: 0s - loss: 0.1575
6530/6530 [==============================] - 1s 159us/step - loss: 0.1569 - val_loss: 0.1465

6530/6530 [==============================] - 0s 67us/step - loss: 0.5880 - val_loss: 0.1642
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1851
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1625
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1616
6530/6530 [==============================] - 0s 21us/step - loss: 0.1609 - val_loss: 0.1645
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1759
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1597
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1596
6530/6530 [==============================] - 0s 23us/step - loss: 0.1597 - val_loss: 0.1660

# training | RMSE: 0.0978, MAE: 0.0764
worker 2  xfile  [16, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.09780316914090126, 'rmse': 0.09780316914090126, 'mae': 0.07636453041303178, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  58 | activation: relu    | extras: dropout - rate: 17.5% 
layer 2 | size:  33 | activation: tanh    | extras: batchnorm 
layer 3 | size:  64 | activation: relu    | extras: batchnorm 
layer 4 | size:  75 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7730704390>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 2:58 - loss: 0.8654
 480/6530 [=>............................] - ETA: 11s - loss: 0.7029 
 928/6530 [===>..........................] - ETA: 5s - loss: 0.5897 
1376/6530 [=====>........................] - ETA: 3s - loss: 0.5208
1856/6530 [=======>......................] - ETA: 2s - loss: 0.4608
2304/6530 [=========>....................] - ETA: 2s - loss: 0.4219
2752/6530 [===========>..................] - ETA: 1s - loss: 0.3943
3200/6530 [=============>................] - ETA: 1s - loss: 0.3717
3616/6530 [===============>..............] - ETA: 1s - loss: 0.3544
# training | RMSE: 0.2050, MAE: 0.1651
worker 0  xfile  [20, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15886729165116187}, 'layer_1_size': 26, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 5, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 24, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20502684216008746, 'rmse': 0.20502684216008746, 'mae': 0.1651218371635373, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  13 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f77346f79b0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 6s - loss: 1.0711
# training | RMSE: 0.1824, MAE: 0.1423
worker 1  xfile  [15, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38572840251416507}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.280536333095057}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4399885133326492}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.18243719850255627, 'rmse': 0.18243719850255627, 'mae': 0.14227164753558205, 'early_stop': False}
{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  31 | activation: relu    | extras: dropout - rate: 39.6% 
layer 2 | size:  74 | activation: relu    | extras: dropout - rate: 39.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777808da58>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 14s - loss: 0.8882
4096/6530 [=================>............] - ETA: 0s - loss: 0.3394
3968/6530 [=================>............] - ETA: 0s - loss: 0.3486 
6530/6530 [==============================] - 0s 49us/step - loss: 0.8918 - val_loss: 0.7235
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.7274
4512/6530 [===================>..........] - ETA: 0s - loss: 0.3276
6530/6530 [==============================] - 0s 5us/step - loss: 0.6497 - val_loss: 0.5366
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.5439
6530/6530 [==============================] - 0s 5us/step - loss: 0.4806 - val_loss: 0.3942

4960/6530 [=====================>........] - ETA: 0s - loss: 0.3174
6530/6530 [==============================] - 0s 63us/step - loss: 0.2597 - val_loss: 0.0742
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1039
5408/6530 [=======================>......] - ETA: 0s - loss: 0.3088
3840/6530 [================>.............] - ETA: 0s - loss: 0.0938
6530/6530 [==============================] - 0s 14us/step - loss: 0.0878 - val_loss: 0.0527
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0756
5824/6530 [=========================>....] - ETA: 0s - loss: 0.3016
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0740
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2939
6530/6530 [==============================] - 0s 13us/step - loss: 0.0719 - val_loss: 0.0475

6530/6530 [==============================] - 2s 266us/step - loss: 0.2899 - val_loss: 0.1748
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2162
 480/6530 [=>............................] - ETA: 0s - loss: 0.2085
 928/6530 [===>..........................] - ETA: 0s - loss: 0.2029
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1987
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1947
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1921
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1904
2848/6530 [============>.................] - ETA: 0s - loss: 0.1865
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1856
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1842
4032/6530 [=================>............] - ETA: 0s - loss: 0.1835
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1826
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1817
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1800
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1798
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1781
6496/6530 [============================>.] - ETA: 0s - loss: 0.1777
6530/6530 [==============================] - 1s 135us/step - loss: 0.1777 - val_loss: 0.1333
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.1654
 544/6530 [=>............................] - ETA: 0s - loss: 0.1636
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1672
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1642
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1635
# training | RMSE: 0.2188, MAE: 0.1802
worker 1  xfile  [21, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39631621748974977}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3898051523657077}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2324361715500335}, 'layer_3_size': 78, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41762675118712356}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.21876428941521325, 'rmse': 0.21876428941521325, 'mae': 0.18019539923676572, 'early_stop': False}
{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:   6 | activation: sigmoid | extras: dropout - rate: 32.6% 
layer 2 | size:  32 | activation: tanh    | extras: batchnorm 
layer 3 | size:  76 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7752395668>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 12s - loss: 0.6579
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1618
4864/6530 [=====================>........] - ETA: 0s - loss: 0.3318 
2848/6530 [============>.................] - ETA: 0s - loss: 0.1625
# training | RMSE: 0.6376, MAE: 0.5329
worker 0  xfile  [22, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23052081187472862}, 'layer_3_size': 22, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 49, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 23, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.6376169534348238, 'rmse': 0.6376169534348238, 'mae': 0.5329034024942529, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:   3 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  15 | activation: tanh    | extras: batchnorm 
layer 3 | size:  86 | activation: sigmoid | extras: None 
layer 4 | size:  69 | activation: relu    | extras: None 
layer 5 | size:  13 | activation: sigmoid | extras: dropout - rate: 40.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7416a3d518>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 36s - loss: 0.4578
6530/6530 [==============================] - 1s 100us/step - loss: 0.3020 - val_loss: 0.2148
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2120
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1609
2176/6530 [========>.....................] - ETA: 1s - loss: 0.2656 
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2176
3776/6530 [================>.............] - ETA: 0s - loss: 0.1606
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1790
6530/6530 [==============================] - 0s 12us/step - loss: 0.2170 - val_loss: 0.2146
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2125
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1601
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1385
4608/6530 [====================>.........] - ETA: 0s - loss: 0.2178
6530/6530 [==============================] - 0s 13us/step - loss: 0.2171 - val_loss: 0.2143

4704/6530 [====================>.........] - ETA: 0s - loss: 0.1604
6530/6530 [==============================] - 1s 149us/step - loss: 0.1347 - val_loss: 0.0506
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0593
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1593
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0497
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1577
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0474
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1567
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0462
6530/6530 [==============================] - 0s 27us/step - loss: 0.0458 - val_loss: 0.0434

6528/6530 [============================>.] - ETA: 0s - loss: 0.1562Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0477
6530/6530 [==============================] - 1s 116us/step - loss: 0.1562 - val_loss: 0.1388

2176/6530 [========>.....................] - ETA: 0s - loss: 0.0427
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0427
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0429
6530/6530 [==============================] - 0s 26us/step - loss: 0.0426 - val_loss: 0.0430

# training | RMSE: 0.2643, MAE: 0.2165
worker 1  xfile  [24, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3257062982984602}, 'layer_1_size': 6, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 32, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.2642933250990181, 'rmse': 0.2642933250990181, 'mae': 0.21652722244051137, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  11 | activation: sigmoid | extras: dropout - rate: 29.3% 
layer 2 | size:  77 | activation: relu    | extras: None 
layer 3 | size:  93 | activation: tanh    | extras: None 
layer 4 | size:  40 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7751d79780>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:59 - loss: 1.2174
 416/6530 [>.............................] - ETA: 9s - loss: 0.6052  
# training | RMSE: 0.1709, MAE: 0.1344
worker 2  xfile  [19, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1752856684537518}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.17094881468960468, 'rmse': 0.17094881468960468, 'mae': 0.134357755500935, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:   9 | activation: relu    | extras: dropout - rate: 31.7% 
layer 2 | size:  39 | activation: sigmoid | extras: dropout - rate: 34.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f742f50a908>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 8s - loss: 1.8489
 864/6530 [==>...........................] - ETA: 4s - loss: 0.4187
5376/6530 [=======================>......] - ETA: 0s - loss: 0.3699
1440/6530 [=====>........................] - ETA: 2s - loss: 0.3362
6530/6530 [==============================] - 1s 78us/step - loss: 0.3195 - val_loss: 0.0744
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0807
2048/6530 [========>.....................] - ETA: 1s - loss: 0.2996
6530/6530 [==============================] - 0s 7us/step - loss: 0.0723 - val_loss: 0.0614
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.0668
2688/6530 [===========>..................] - ETA: 1s - loss: 0.2786
6530/6530 [==============================] - 0s 7us/step - loss: 0.0620 - val_loss: 0.0557

3424/6530 [==============>...............] - ETA: 0s - loss: 0.2624
4128/6530 [=================>............] - ETA: 0s - loss: 0.2514
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2436
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2368
# training | RMSE: 0.2044, MAE: 0.1634
worker 0  xfile  [23, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40910277421326435}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2044480147317601, 'rmse': 0.2044480147317601, 'mae': 0.16343653611641712, 'early_stop': False}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  75 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  30 | activation: sigmoid | extras: dropout - rate: 13.5% 
layer 3 | size:  35 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7734026160>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  16/6530 [..............................] - ETA: 3:52 - loss: 0.5412
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2327
 336/6530 [>.............................] - ETA: 11s - loss: 0.2899 
 640/6530 [=>............................] - ETA: 6s - loss: 0.1911 
6530/6530 [==============================] - 1s 183us/step - loss: 0.2305 - val_loss: 0.4348
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2974
 976/6530 [===>..........................] - ETA: 4s - loss: 0.1502
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1972
1280/6530 [====>.........................] - ETA: 3s - loss: 0.1283
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1892
1584/6530 [======>.......................] - ETA: 2s - loss: 0.1131
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1874
1920/6530 [=======>......................] - ETA: 2s - loss: 0.1020
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1884
2256/6530 [=========>....................] - ETA: 1s - loss: 0.0940
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1865
2592/6530 [==========>...................] - ETA: 1s - loss: 0.0882
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1852
2928/6530 [============>.................] - ETA: 1s - loss: 0.0830
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1863
3264/6530 [=============>................] - ETA: 1s - loss: 0.0788
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1856
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0762
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1857
3888/6530 [================>.............] - ETA: 0s - loss: 0.0738
6530/6530 [==============================] - 1s 77us/step - loss: 0.1849 - val_loss: 0.2257
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2065
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0716
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1884
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0700
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1808
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0681
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1804
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0667
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1810
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0658
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1792
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0646
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1771
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0636
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1786
6384/6530 [============================>.] - ETA: 0s - loss: 0.0627
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1780
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1780
6530/6530 [==============================] - 0s 76us/step - loss: 0.1773 - val_loss: 0.4341

6530/6530 [==============================] - 2s 262us/step - loss: 0.0621 - val_loss: 0.0431
Epoch 2/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0576
 368/6530 [>.............................] - ETA: 0s - loss: 0.0440
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0487
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0480
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0457
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0445
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0443
# training | RMSE: 0.2325, MAE: 0.1907
worker 2  xfile  [26, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3170982998056019}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34939336786908626}, 'layer_2_size': 39, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2352907000867381}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4599186566416881}, 'layer_4_size': 32, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2325367619895901, 'rmse': 0.2325367619895901, 'mae': 0.19067307031161007, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  10 | activation: tanh    | extras: batchnorm 
layer 2 | size:  18 | activation: relu    | extras: batchnorm 
layer 3 | size:  95 | activation: tanh    | extras: dropout - rate: 42.7% 
layer 4 | size:  10 | activation: sigmoid | extras: dropout - rate: 30.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f742f677748>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 19s - loss: 0.7328
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0442
3328/6530 [==============>...............] - ETA: 0s - loss: 0.6521 
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0446
6400/6530 [============================>.] - ETA: 0s - loss: 0.5974
3040/6530 [============>.................] - ETA: 0s - loss: 0.0439
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0442
6530/6530 [==============================] - 1s 153us/step - loss: 0.5952 - val_loss: 0.4786
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.4743
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0443
3584/6530 [===============>..............] - ETA: 0s - loss: 0.4153
4032/6530 [=================>............] - ETA: 0s - loss: 0.0445
6530/6530 [==============================] - 0s 17us/step - loss: 0.3717 - val_loss: 0.2739
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2649
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0443
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2222
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0445
6530/6530 [==============================] - 0s 17us/step - loss: 0.1944 - val_loss: 0.1609

5024/6530 [======================>.......] - ETA: 0s - loss: 0.0445
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0449
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0447
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0448
6352/6530 [============================>.] - ETA: 0s - loss: 0.0448
6530/6530 [==============================] - 1s 160us/step - loss: 0.0446 - val_loss: 0.0424
Epoch 3/3

  16/6530 [..............................] - ETA: 1s - loss: 0.0522
 320/6530 [>.............................] - ETA: 1s - loss: 0.0427
# training | RMSE: 0.4798, MAE: 0.4317
worker 1  xfile  [25, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.29332416198982303}, 'layer_1_size': 11, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 40, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.4798176160931224, 'rmse': 0.4798176160931224, 'mae': 0.4316919753602544, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  14 | activation: tanh    | extras: None 
layer 2 | size:  95 | activation: relu    | extras: batchnorm 
layer 3 | size:  92 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7751d795c0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 24s - loss: 1.2411
 624/6530 [=>............................] - ETA: 1s - loss: 0.0480
2944/6530 [============>.................] - ETA: 0s - loss: 0.5292 
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0469
5888/6530 [==========================>...] - ETA: 0s - loss: 0.4099
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0459
6530/6530 [==============================] - 1s 102us/step - loss: 0.3939 - val_loss: 0.6807
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.6584
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0437
2944/6530 [============>.................] - ETA: 0s - loss: 0.2493
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0435
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2306
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0436
6530/6530 [==============================] - 0s 20us/step - loss: 0.2267 - val_loss: 0.3969
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.4391
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0442
2816/6530 [===========>..................] - ETA: 0s - loss: 0.2008
2864/6530 [============>.................] - ETA: 0s - loss: 0.0438
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1894
# training | RMSE: 0.2021, MAE: 0.1561
worker 2  xfile  [28, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4273031903618001}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3015218909611247}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.2021150257850494, 'rmse': 0.2021150257850494, 'mae': 0.15612824076119625, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  22 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f742e2777b8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 256/6530 [>.............................] - ETA: 10s - loss: 0.7933
6530/6530 [==============================] - 0s 20us/step - loss: 0.1882 - val_loss: 0.3849

3184/6530 [=============>................] - ETA: 0s - loss: 0.0432
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0438
6530/6530 [==============================] - 1s 79us/step - loss: 0.6687 - val_loss: 0.5493
Epoch 2/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.5420
3792/6530 [================>.............] - ETA: 0s - loss: 0.0436
6530/6530 [==============================] - 0s 7us/step - loss: 0.4181 - val_loss: 0.3017
Epoch 3/3

 256/6530 [>.............................] - ETA: 0s - loss: 0.2968
6530/6530 [==============================] - 0s 6us/step - loss: 0.2181 - val_loss: 0.1668

4096/6530 [=================>............] - ETA: 0s - loss: 0.0441
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0441
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0441
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0441
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0444
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0443
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0444
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0443
6530/6530 [==============================] - 1s 173us/step - loss: 0.0441 - val_loss: 0.0421

# training | RMSE: 0.4662, MAE: 0.3738
worker 1  xfile  [29, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.46615942071126176, 'rmse': 0.46615942071126176, 'mae': 0.37378494107418186, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 1,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  74 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f775167bb38>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/3

 128/6530 [..............................] - ETA: 17s - loss: 2.2510
4352/6530 [==================>...........] - ETA: 0s - loss: 0.9546 
6530/6530 [==============================] - 0s 76us/step - loss: 0.7268 - val_loss: 0.1681
Epoch 2/3

 128/6530 [..............................] - ETA: 0s - loss: 0.1965
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0944
6530/6530 [==============================] - 0s 12us/step - loss: 0.0845 - val_loss: 0.0537
Epoch 3/3

 128/6530 [..............................] - ETA: 0s - loss: 0.0569
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0530
6530/6530 [==============================] - 0s 11us/step - loss: 0.0519 - val_loss: 0.0506

# training | RMSE: 0.2068, MAE: 0.1646
worker 2  xfile  [30, 3.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 22, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 71, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20681267491120864, 'rmse': 0.20681267491120864, 'mae': 0.1646303967919498, 'early_stop': False}
{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:   6 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f742f57fef0>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  32/6530 [..............................] - ETA: 1:29 - loss: 0.7426
1216/6530 [====>.........................] - ETA: 2s - loss: 0.6594  
# training | RMSE: 0.2272, MAE: 0.1772
worker 1  xfile  [31, 3.0, 0, 100, [], {'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 72, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2877377074137758}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.22717386784638277, 'rmse': 0.22717386784638277, 'mae': 0.17722573741737443, 'early_stop': False}
vggnet done  1

2464/6530 [==========>...................] - ETA: 0s - loss: 0.5507
3744/6530 [================>.............] - ETA: 0s - loss: 0.4497
5152/6530 [======================>.......] - ETA: 0s - loss: 0.3924
6496/6530 [============================>.] - ETA: 0s - loss: 0.3586
6530/6530 [==============================] - 1s 118us/step - loss: 0.3578 - val_loss: 0.2291
Epoch 2/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2190
1184/6530 [====>.........................] - ETA: 0s - loss: 0.2242
2304/6530 [=========>....................] - ETA: 0s - loss: 0.2232
3328/6530 [==============>...............] - ETA: 0s - loss: 0.2235
4320/6530 [==================>...........] - ETA: 0s - loss: 0.2231
5056/6530 [======================>.......] - ETA: 0s - loss: 0.2224
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2231
6530/6530 [==============================] - 0s 55us/step - loss: 0.2238 - val_loss: 0.2174
Epoch 3/3

  32/6530 [..............................] - ETA: 0s - loss: 0.2124
 928/6530 [===>..........................] - ETA: 0s - loss: 0.2158
1824/6530 [=======>......................] - ETA: 0s - loss: 0.2149
2784/6530 [===========>..................] - ETA: 0s - loss: 0.2145
3840/6530 [================>.............] - ETA: 0s - loss: 0.2151
# training | RMSE: 0.2035, MAE: 0.1641
worker 0  xfile  [27, 3.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13515921791326946}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.20348199213985324, 'rmse': 0.20348199213985324, 'mae': 0.16407142671407474, 'early_stop': False}
{'batch_size': 64,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  75 | activation: relu    | extras: batchnorm 
layer 2 | size:  59 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7415e87780>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/3

  64/6530 [..............................] - ETA: 51s - loss: 1.0974
4896/6530 [=====================>........] - ETA: 0s - loss: 0.2139
1344/6530 [=====>........................] - ETA: 2s - loss: 0.6540 
5984/6530 [==========================>...] - ETA: 0s - loss: 0.2134
2688/6530 [===========>..................] - ETA: 0s - loss: 0.5087
6530/6530 [==============================] - 0s 52us/step - loss: 0.2126 - val_loss: 0.2044

4160/6530 [==================>...........] - ETA: 0s - loss: 0.4309
5568/6530 [========================>.....] - ETA: 0s - loss: 0.3872
6530/6530 [==============================] - 1s 129us/step - loss: 0.3655 - val_loss: 0.2171
Epoch 2/3

  64/6530 [..............................] - ETA: 0s - loss: 0.2269
# training | RMSE: 0.2529, MAE: 0.2059
worker 2  xfile  [32, 3.0, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 6, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2022834813979455}, 'layer_4_size': 9, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.2529260099855978, 'rmse': 0.2529260099855978, 'mae': 0.20589012658167063, 'early_stop': False}
vggnet done  2

1408/6530 [=====>........................] - ETA: 0s - loss: 0.2274
3008/6530 [============>.................] - ETA: 0s - loss: 0.2220
4480/6530 [===================>..........] - ETA: 0s - loss: 0.2195
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2154
6530/6530 [==============================] - 0s 39us/step - loss: 0.2146 - val_loss: 0.1908
Epoch 3/3

  64/6530 [..............................] - ETA: 0s - loss: 0.1638
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1942
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1918
3200/6530 [=============>................] - ETA: 0s - loss: 0.1902
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1887
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1883
6530/6530 [==============================] - 0s 47us/step - loss: 0.1871 - val_loss: 0.1699

# training | RMSE: 0.2042, MAE: 0.1583
worker 0  xfile  [33, 3.0, 0, 100, [], {'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 59, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2677434157431451}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.20417058719789852, 'rmse': 0.20417058719789852, 'mae': 0.1583156031701714, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#1 epoch=3.0 loss={'loss': 0.20379584298538703, 'rmse': 0.20379584298538703, 'mae': 0.1602556482816008, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38012202189334443}, 'layer_3_size': 39, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#2 epoch=3.0 loss={'loss': 0.22497816161445844, 'rmse': 0.22497816161445844, 'mae': 0.17691561320360472, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 97, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.36520987335416155}, 'layer_2_size': 41, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 46, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 76, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 82, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#3 epoch=3.0 loss={'loss': 0.31510035236999556, 'rmse': 0.31510035236999556, 'mae': 0.26879114859380004, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 60, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.24309239614529404}, 'layer_2_size': 99, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.475456178893257}, 'layer_3_size': 92, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.39915504322999096}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 39, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#4 epoch=3.0 loss={'loss': 0.30396724932766467, 'rmse': 0.30396724932766467, 'mae': 0.25232013808562925, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.45526819660442785}, 'layer_1_size': 79, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 60, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 76, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'StandardScaler', 'shuffle': False}
#5 epoch=3.0 loss={'loss': 0.24156119538008497, 'rmse': 0.24156119538008497, 'mae': 0.1887513662701486, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 27, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 96, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.20112903734462156}, 'layer_4_size': 90, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 27, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#7 epoch=3.0 loss={'loss': 0.2074859341048814, 'rmse': 0.2074859341048814, 'mae': 0.16734264898711884, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 79, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3578978424837119}, 'layer_2_size': 25, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 17, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 87, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#6 epoch=3.0 loss={'loss': 0.2240687772566506, 'rmse': 0.2240687772566506, 'mae': 0.17167759980235475, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 53, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 93, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 42, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.42072789870156346}, 'layer_4_size': 63, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
#0 epoch=3.0 loss={'loss': 0.1926673153342449, 'rmse': 0.1926673153342449, 'mae': 0.146488326691823, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2683168763307201}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31497950775664185}, 'layer_4_size': 29, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#10 epoch=3.0 loss={'loss': 0.18366187597835126, 'rmse': 0.18366187597835126, 'mae': 0.14721898313333556, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.32905590204389745}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2658416628015635}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#9 epoch=3.0 loss={'loss': 0.17651724691758222, 'rmse': 0.17651724691758222, 'mae': 0.14058402312658005, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17384346890971408}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.494814807006017}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#12 epoch=3.0 loss={'loss': 0.20610084846090226, 'rmse': 0.20610084846090226, 'mae': 0.1688538869804835, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 21, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 97, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2597606869601959}, 'layer_4_size': 82, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
#11 epoch=3.0 loss={'loss': 0.20275045750390155, 'rmse': 0.20275045750390155, 'mae': 0.16694152676556454, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14283571375707005}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 35, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28252178251643656}, 'layer_5_size': 42, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#8 epoch=3.0 loss={'loss': 0.22015346539745975, 'rmse': 0.22015346539745975, 'mae': 0.1768167083729206, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 78, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3994301914875815}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 83, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 46, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#13 epoch=3.0 loss={'loss': 0.33536852317420474, 'rmse': 0.33536852317420474, 'mae': 0.2674671414075762, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 77, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 48, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 74, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 99, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#14 epoch=3.0 loss={'loss': 0.20288699719811606, 'rmse': 0.20288699719811606, 'mae': 0.1601991649080949, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4358447939016047}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#17 epoch=3.0 loss={'loss': 0.24653444441817327, 'rmse': 0.24653444441817327, 'mae': 0.20186190801805987, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.44607532308855846}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 29, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 18, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 9, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.1382648397930245}, 'layer_5_size': 33, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#16 epoch=3.0 loss={'loss': 0.09780316914090126, 'rmse': 0.09780316914090126, 'mae': 0.07636453041303178, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#18 epoch=3.0 loss={'loss': 0.20816220175744168, 'rmse': 0.20816220175744168, 'mae': 0.1677958824886727, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 22, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.19275704376715105}, 'layer_3_size': 16, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 65, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.19448349833074535}, 'layer_5_size': 79, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#15 epoch=3.0 loss={'loss': 0.18243719850255627, 'rmse': 0.18243719850255627, 'mae': 0.14227164753558205, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38572840251416507}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.280536333095057}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4399885133326492}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#20 epoch=3.0 loss={'loss': 0.20502684216008746, 'rmse': 0.20502684216008746, 'mae': 0.1651218371635373, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.15886729165116187}, 'layer_1_size': 26, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 11, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 5, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 24, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 84, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#22 epoch=3.0 loss={'loss': 0.6376169534348238, 'rmse': 0.6376169534348238, 'mae': 0.5329034024942529, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 13, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 88, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23052081187472862}, 'layer_3_size': 22, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 49, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 23, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'StandardScaler', 'shuffle': False}
#21 epoch=3.0 loss={'loss': 0.21876428941521325, 'rmse': 0.21876428941521325, 'mae': 0.18019539923676572, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.39631621748974977}, 'layer_1_size': 31, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3898051523657077}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2324361715500335}, 'layer_3_size': 78, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.41762675118712356}, 'layer_4_size': 73, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 64, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#24 epoch=3.0 loss={'loss': 0.2642933250990181, 'rmse': 0.2642933250990181, 'mae': 0.21652722244051137, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3257062982984602}, 'layer_1_size': 6, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 32, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 76, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 80, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 76, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#19 epoch=3.0 loss={'loss': 0.17094881468960468, 'rmse': 0.17094881468960468, 'mae': 0.134357755500935, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1752856684537518}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
#23 epoch=3.0 loss={'loss': 0.2044480147317601, 'rmse': 0.2044480147317601, 'mae': 0.16343653611641712, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 3, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 15, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 86, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 69, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.40910277421326435}, 'layer_5_size': 13, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#26 epoch=3.0 loss={'loss': 0.2325367619895901, 'rmse': 0.2325367619895901, 'mae': 0.19067307031161007, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.3170982998056019}, 'layer_1_size': 9, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.34939336786908626}, 'layer_2_size': 39, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2352907000867381}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4599186566416881}, 'layer_4_size': 32, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 31, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#25 epoch=3.0 loss={'loss': 0.4798176160931224, 'rmse': 0.4798176160931224, 'mae': 0.4316919753602544, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.29332416198982303}, 'layer_1_size': 11, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 77, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 40, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 64, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#28 epoch=3.0 loss={'loss': 0.2021150257850494, 'rmse': 0.2021150257850494, 'mae': 0.15612824076119625, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4273031903618001}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3015218909611247}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#29 epoch=3.0 loss={'loss': 0.46615942071126176, 'rmse': 0.46615942071126176, 'mae': 0.37378494107418186, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 14, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 95, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 92, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 25, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#30 epoch=3.0 loss={'loss': 0.20681267491120864, 'rmse': 0.20681267491120864, 'mae': 0.1646303967919498, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 22, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 90, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 29, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 71, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#27 epoch=3.0 loss={'loss': 0.20348199213985324, 'rmse': 0.20348199213985324, 'mae': 0.16407142671407474, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13515921791326946}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
#31 epoch=3.0 loss={'loss': 0.22717386784638277, 'rmse': 0.22717386784638277, 'mae': 0.17722573741737443, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 74, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 72, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 57, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2877377074137758}, 'layer_5_size': 32, 'loss': 'mean_squared_error', 'n_layers': 1, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#32 epoch=3.0 loss={'loss': 0.2529260099855978, 'rmse': 0.2529260099855978, 'mae': 0.20589012658167063, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 6, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 54, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2022834813979455}, 'layer_4_size': 9, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 34, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': True}
#33 epoch=3.0 loss={'loss': 0.20417058719789852, 'rmse': 0.20417058719789852, 'mae': 0.1583156031701714, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 59, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 30, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2677434157431451}, 'layer_4_size': 64, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 90, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
get a list [results] of length 155
get a list [loss] of length 34
get a list [val_loss] of length 34
length of indices is (16, 19, 9, 15, 10, 0, 28, 11, 14, 27, 1, 33, 23, 20, 12, 30, 7, 18, 21, 8, 6, 2, 31, 26, 5, 17, 32, 24, 4, 3, 13, 29, 25, 22)
length of indices is 34
length of T is 34
newly formed T structure is:[[0, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [1, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1752856684537518}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [2, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17384346890971408}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.494814807006017}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [3, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38572840251416507}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.280536333095057}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4399885133326492}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}], [4, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.32905590204389745}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2658416628015635}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [5, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2683168763307201}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31497950775664185}, 'layer_4_size': 29, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}], [6, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4273031903618001}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3015218909611247}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}], [7, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14283571375707005}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 35, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28252178251643656}, 'layer_5_size': 42, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}], [8, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4358447939016047}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}], [9, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13515921791326946}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}], [10, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38012202189334443}, 'layer_3_size': 39, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]] 

*** 11.333333333333332 configurations x 9.0 iterations each

34 | Thu Sep 27 23:26:38 2018 | lowest loss so far: 0.0809 (run 0)

{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  64 | activation: relu    | extras: batchnorm 
layer 2 | size:  37 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a777c88>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 1:21 - loss: 0.9352
1152/6530 [====>.........................] - ETA: 4s - loss: 0.1993  
2432/6530 [==========>...................] - ETA: 1s - loss: 0.1395
3776/6530 [================>.............] - ETA: 0s - loss: 0.1141
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0999
6530/6530 [==============================] - 1s 167us/step - loss: 0.0904 - val_loss: 0.0473
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0661
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0435
3136/6530 [=============>................] - ETA: 0s - loss: 0.0415
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0417
6464/6530 [============================>.] - ETA: 0s - loss: 0.0410
6530/6530 [==============================] - 0s 33us/step - loss: 0.0409 - val_loss: 0.0429
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0450{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  48 | activation: relu    | extras: None 
layer 2 | size:  54 | activation: relu    | extras: batchnorm 
layer 3 | size:  23 | activation: relu    | extras: batchnorm 
layer 4 | size:  33 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 1:40 - loss: 0.5895
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0334
1088/6530 [===>..........................] - ETA: 5s - loss: 0.4803  
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0326
2176/6530 [========>.....................] - ETA: 2s - loss: 0.4070
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0331
3264/6530 [=============>................] - ETA: 1s - loss: 0.3262
6530/6530 [==============================] - 0s 31us/step - loss: 0.0326 - val_loss: 0.0302
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0363
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2574
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0285
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2145
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0277
6464/6530 [============================>.] - ETA: 0s - loss: 0.1821
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0283{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  58 | activation: relu    | extras: dropout - rate: 17.5% 
layer 2 | size:  33 | activation: tanh    | extras: batchnorm 
layer 3 | size:  64 | activation: relu    | extras: batchnorm 
layer 4 | size:  75 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 3:42 - loss: 0.8712
6530/6530 [==============================] - 1s 208us/step - loss: 0.1805 - val_loss: 0.0814
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0329
6530/6530 [==============================] - 0s 33us/step - loss: 0.0282 - val_loss: 0.0273
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0318
 480/6530 [=>............................] - ETA: 14s - loss: 0.7004 
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0197
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0255
 928/6530 [===>..........................] - ETA: 7s - loss: 0.5841 
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0180
3264/6530 [=============>................] - ETA: 0s - loss: 0.0248
1376/6530 [=====>........................] - ETA: 4s - loss: 0.5021
3136/6530 [=============>................] - ETA: 0s - loss: 0.0168
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0254
1824/6530 [=======>......................] - ETA: 3s - loss: 0.4550
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0166
6530/6530 [==============================] - 0s 33us/step - loss: 0.0253 - val_loss: 0.0253
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0291
2304/6530 [=========>....................] - ETA: 2s - loss: 0.4158
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0165
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0232
2784/6530 [===========>..................] - ETA: 1s - loss: 0.3882
6336/6530 [============================>.] - ETA: 0s - loss: 0.0160
6530/6530 [==============================] - 0s 52us/step - loss: 0.0161 - val_loss: 0.0414
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0173
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0228
3264/6530 [=============>................] - ETA: 1s - loss: 0.3655
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0142
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0234
3712/6530 [================>.............] - ETA: 1s - loss: 0.3490
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0133
6530/6530 [==============================] - 0s 32us/step - loss: 0.0232 - val_loss: 0.0238
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0271
4160/6530 [==================>...........] - ETA: 0s - loss: 0.3363
3200/6530 [=============>................] - ETA: 0s - loss: 0.0130
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0219
4640/6530 [====================>.........] - ETA: 0s - loss: 0.3244
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0127
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0213
5120/6530 [======================>.......] - ETA: 0s - loss: 0.3133
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0128
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0218
5600/6530 [========================>.....] - ETA: 0s - loss: 0.3042
6464/6530 [============================>.] - ETA: 0s - loss: 0.0126
6528/6530 [============================>.] - ETA: 0s - loss: 0.0217
6530/6530 [==============================] - 0s 33us/step - loss: 0.0217 - val_loss: 0.0226
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0253
6530/6530 [==============================] - 0s 51us/step - loss: 0.0126 - val_loss: 0.0138
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0143
6048/6530 [==========================>...] - ETA: 0s - loss: 0.2973
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0205
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0123
6496/6530 [============================>.] - ETA: 0s - loss: 0.2910
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0202
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0114
6530/6530 [==============================] - 2s 290us/step - loss: 0.2905 - val_loss: 0.1769
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.2194
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0208
3264/6530 [=============>................] - ETA: 0s - loss: 0.0112
 512/6530 [=>............................] - ETA: 0s - loss: 0.1972
6530/6530 [==============================] - 0s 31us/step - loss: 0.0204 - val_loss: 0.0216
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0236
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0110
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1996
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0195
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0111
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1981
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0191
6400/6530 [============================>.] - ETA: 0s - loss: 0.0110
6530/6530 [==============================] - 0s 51us/step - loss: 0.0109 - val_loss: 0.0128
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0103
1888/6530 [=======>......................] - ETA: 0s - loss: 0.2010
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0196
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0106
2368/6530 [=========>....................] - ETA: 0s - loss: 0.2006
6530/6530 [==============================] - 0s 32us/step - loss: 0.0194 - val_loss: 0.0209

2240/6530 [=========>....................] - ETA: 0s - loss: 0.0101
2848/6530 [============>.................] - ETA: 0s - loss: 0.1975
3264/6530 [=============>................] - ETA: 0s - loss: 0.0100
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1953
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0100
3808/6530 [================>.............] - ETA: 0s - loss: 0.1933
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0101
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1921
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0098
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1905
6530/6530 [==============================] - 0s 51us/step - loss: 0.0098 - val_loss: 0.0133
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0121
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1903
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0103
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1896
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0095
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1881
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0094
6530/6530 [==============================] - 1s 112us/step - loss: 0.1883 - val_loss: 0.1732
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1988
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0094
 512/6530 [=>............................] - ETA: 0s - loss: 0.1906
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0092
 992/6530 [===>..........................] - ETA: 0s - loss: 0.1850
6530/6530 [==============================] - 0s 48us/step - loss: 0.0092 - val_loss: 0.0114
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0085
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1803
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0091
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1775
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0087
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1734
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0085
2880/6530 [============>.................] - ETA: 0s - loss: 0.1715
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0086
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1697
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0086
3872/6530 [================>.............] - ETA: 0s - loss: 0.1682
6464/6530 [============================>.] - ETA: 0s - loss: 0.0085
6530/6530 [==============================] - 0s 50us/step - loss: 0.0085 - val_loss: 0.0136
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0127
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1677
# training | RMSE: 0.1276, MAE: 0.1006
worker 2  xfile  [2, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17384346890971408}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.494814807006017}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.12763076842363855, 'rmse': 0.12763076842363855, 'mae': 0.10064097654207667, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  42 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  67 | activation: tanh    | extras: dropout - rate: 38.6% 
layer 3 | size:  15 | activation: relu    | extras: dropout - rate: 28.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a777f60>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 2:25 - loss: 0.6211
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0091
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1670
 384/6530 [>.............................] - ETA: 6s - loss: 0.6025  
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0083
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1646
 736/6530 [==>...........................] - ETA: 3s - loss: 0.4945
3264/6530 [=============>................] - ETA: 0s - loss: 0.0082
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1637
1104/6530 [====>.........................] - ETA: 2s - loss: 0.4172
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0083
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1624
1440/6530 [=====>........................] - ETA: 1s - loss: 0.3726
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0084
6530/6530 [==============================] - 1s 113us/step - loss: 0.1618 - val_loss: 0.1275
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1390
1792/6530 [=======>......................] - ETA: 1s - loss: 0.3430
6400/6530 [============================>.] - ETA: 0s - loss: 0.0082
6530/6530 [==============================] - 0s 50us/step - loss: 0.0081 - val_loss: 0.0126
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0101
 512/6530 [=>............................] - ETA: 0s - loss: 0.1821
2144/6530 [========>.....................] - ETA: 1s - loss: 0.3186
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0083
 992/6530 [===>..........................] - ETA: 0s - loss: 0.1733
2496/6530 [==========>...................] - ETA: 1s - loss: 0.2998
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0079
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1693
2848/6530 [============>.................] - ETA: 0s - loss: 0.2849
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0077
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1664
3200/6530 [=============>................] - ETA: 0s - loss: 0.2738
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0078
3552/6530 [===============>..............] - ETA: 0s - loss: 0.2643
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1624
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0078
3904/6530 [================>.............] - ETA: 0s - loss: 0.2572
2880/6530 [============>.................] - ETA: 0s - loss: 0.1596
6528/6530 [============================>.] - ETA: 0s - loss: 0.0077
6530/6530 [==============================] - 0s 50us/step - loss: 0.0077 - val_loss: 0.0153

3328/6530 [==============>...............] - ETA: 0s - loss: 0.1572
4240/6530 [==================>...........] - ETA: 0s - loss: 0.2521
3776/6530 [================>.............] - ETA: 0s - loss: 0.1560
4576/6530 [====================>.........] - ETA: 0s - loss: 0.2466
4928/6530 [=====================>........] - ETA: 0s - loss: 0.2418
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1558
5280/6530 [=======================>......] - ETA: 0s - loss: 0.2375
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1546
5648/6530 [========================>.....] - ETA: 0s - loss: 0.2337
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1542
6000/6530 [==========================>...] - ETA: 0s - loss: 0.2308
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1527
6368/6530 [============================>.] - ETA: 0s - loss: 0.2278
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1514
6530/6530 [==============================] - 1s 113us/step - loss: 0.1510 - val_loss: 0.1150
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1444
6530/6530 [==============================] - 1s 209us/step - loss: 0.2268 - val_loss: 0.2019
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1536
 512/6530 [=>............................] - ETA: 0s - loss: 0.1409
 368/6530 [>.............................] - ETA: 0s - loss: 0.1710
 992/6530 [===>..........................] - ETA: 0s - loss: 0.1394
 720/6530 [==>...........................] - ETA: 0s - loss: 0.1751
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1364
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1745
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1336
1456/6530 [=====>........................] - ETA: 0s - loss: 0.1734
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1351
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1731
2848/6530 [============>.................] - ETA: 0s - loss: 0.1348
2224/6530 [=========>....................] - ETA: 0s - loss: 0.1726
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1349
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1716
# training | RMSE: 0.1211, MAE: 0.0917
worker 0  xfile  [0, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.12105243943257793, 'rmse': 0.12105243943257793, 'mae': 0.09174401697641522, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  39 | activation: relu    | extras: None 
layer 2 | size:  62 | activation: tanh    | extras: dropout - rate: 32.9% 
layer 3 | size:  90 | activation: sigmoid | extras: None 
layer 4 | size:  63 | activation: sigmoid | extras: dropout - rate: 26.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a777e10>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 14s - loss: 0.0931
3808/6530 [================>.............] - ETA: 0s - loss: 0.1357
2976/6530 [============>.................] - ETA: 0s - loss: 0.1721
2944/6530 [============>.................] - ETA: 0s - loss: 0.0823 
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1357
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1719
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0690
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1357
3696/6530 [===============>..............] - ETA: 0s - loss: 0.1712
6530/6530 [==============================] - 0s 66us/step - loss: 0.0673 - val_loss: 0.1250
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1372
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1357
4048/6530 [=================>............] - ETA: 0s - loss: 0.1728
2944/6530 [============>.................] - ETA: 0s - loss: 0.0550
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1351
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1722
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0504
6530/6530 [==============================] - 0s 19us/step - loss: 0.0497 - val_loss: 0.1207
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1229
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1348
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1727
3072/6530 [=============>................] - ETA: 0s - loss: 0.0462
6530/6530 [==============================] - 1s 113us/step - loss: 0.1347 - val_loss: 0.1117
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1375
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1727
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0437
6530/6530 [==============================] - 0s 18us/step - loss: 0.0437 - val_loss: 0.1085

 512/6530 [=>............................] - ETA: 0s - loss: 0.1380Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1075
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1728
 992/6530 [===>..........................] - ETA: 0s - loss: 0.1313
2944/6530 [============>.................] - ETA: 0s - loss: 0.0423
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1729
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0400
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1314
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1735
6530/6530 [==============================] - 0s 19us/step - loss: 0.0400 - val_loss: 0.1239
Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1271
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1324
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0412
6530/6530 [==============================] - 1s 146us/step - loss: 0.1729 - val_loss: 0.1646
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1961
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1323
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0386
 384/6530 [>.............................] - ETA: 0s - loss: 0.1668
6530/6530 [==============================] - 0s 19us/step - loss: 0.0385 - val_loss: 0.0532
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0538
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1319
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1703
3072/6530 [=============>................] - ETA: 0s - loss: 0.0370
3264/6530 [=============>................] - ETA: 0s - loss: 0.1311
1104/6530 [====>.........................] - ETA: 0s - loss: 0.1679
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0364
3712/6530 [================>.............] - ETA: 0s - loss: 0.1304
6530/6530 [==============================] - 0s 18us/step - loss: 0.0367 - val_loss: 0.0713
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0661
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1667
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1308
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0388
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1675
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1308
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0357
6530/6530 [==============================] - 0s 19us/step - loss: 0.0368 - val_loss: 0.0315
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0289
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1641
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1312
3072/6530 [=============>................] - ETA: 0s - loss: 0.0360
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1645
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1322
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0370
2928/6530 [============>.................] - ETA: 0s - loss: 0.1645
6530/6530 [==============================] - 0s 19us/step - loss: 0.0370 - val_loss: 0.0418
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.0411
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1315
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1643
2944/6530 [============>.................] - ETA: 0s - loss: 0.0335
6530/6530 [==============================] - 1s 115us/step - loss: 0.1311 - val_loss: 0.1129
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1284
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1632
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0341
6530/6530 [==============================] - 0s 19us/step - loss: 0.0352 - val_loss: 0.0330

 512/6530 [=>............................] - ETA: 0s - loss: 0.1361
3936/6530 [=================>............] - ETA: 0s - loss: 0.1629
 992/6530 [===>..........................] - ETA: 0s - loss: 0.1327
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1624
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1286
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1618
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1280
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1609
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1606
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1267
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1604
2848/6530 [============>.................] - ETA: 0s - loss: 0.1262
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1604
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1262
6464/6530 [============================>.] - ETA: 0s - loss: 0.1601
3808/6530 [================>.............] - ETA: 0s - loss: 0.1249
6530/6530 [==============================] - 1s 147us/step - loss: 0.1599 - val_loss: 0.1450
Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1081
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1243
 400/6530 [>.............................] - ETA: 0s - loss: 0.1490
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1236
 752/6530 [==>...........................] - ETA: 0s - loss: 0.1525
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1238
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1513
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1228
1488/6530 [=====>........................] - ETA: 0s - loss: 0.1515
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1230
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1530
6530/6530 [==============================] - 1s 113us/step - loss: 0.1230 - val_loss: 0.1018
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0909
2224/6530 [=========>....................] - ETA: 0s - loss: 0.1536
 512/6530 [=>............................] - ETA: 0s - loss: 0.1412
2576/6530 [==========>...................] - ETA: 0s - loss: 0.1536
 992/6530 [===>..........................] - ETA: 0s - loss: 0.1355
2944/6530 [============>.................] - ETA: 0s - loss: 0.1536
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1307
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1518
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1272
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1505
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1275
4048/6530 [=================>............] - ETA: 0s - loss: 0.1501
2912/6530 [============>.................] - ETA: 0s - loss: 0.1257
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1500
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1247
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1503
3808/6530 [================>.............] - ETA: 0s - loss: 0.1239
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1499
# training | RMSE: 0.1763, MAE: 0.1414
worker 0  xfile  [4, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.32905590204389745}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2658416628015635}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.17632259399066372, 'rmse': 0.17632259399066372, 'mae': 0.14142872924080574, 'early_stop': False}
{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  29 | activation: tanh    | extras: dropout - rate: 26.8% 
layer 2 | size:  11 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  19 | activation: relu    | extras: None 
layer 4 | size:  29 | activation: sigmoid | extras: dropout - rate: 31.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7754100240>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 3:24 - loss: 0.5998
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1220
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1500
 288/6530 [>.............................] - ETA: 12s - loss: 0.4715 
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1215
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1503
 512/6530 [=>............................] - ETA: 7s - loss: 0.3857 
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1211
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1503
 800/6530 [==>...........................] - ETA: 4s - loss: 0.3208
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1207
6512/6530 [============================>.] - ETA: 0s - loss: 0.1495
1104/6530 [====>.........................] - ETA: 3s - loss: 0.2924
6530/6530 [==============================] - 1s 147us/step - loss: 0.1494 - val_loss: 0.1374
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1211
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1203
1376/6530 [=====>........................] - ETA: 2s - loss: 0.2713
 368/6530 [>.............................] - ETA: 0s - loss: 0.1502
6496/6530 [============================>.] - ETA: 0s - loss: 0.1208
6530/6530 [==============================] - 1s 116us/step - loss: 0.1209 - val_loss: 0.0911

1664/6530 [======>.......................] - ETA: 2s - loss: 0.2542Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1171
 720/6530 [==>...........................] - ETA: 0s - loss: 0.1547
1936/6530 [=======>......................] - ETA: 2s - loss: 0.2426
 480/6530 [=>............................] - ETA: 0s - loss: 0.1313
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1554
2224/6530 [=========>....................] - ETA: 1s - loss: 0.2342
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1266
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1544
2512/6530 [==========>...................] - ETA: 1s - loss: 0.2260
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1260
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1537
2800/6530 [===========>..................] - ETA: 1s - loss: 0.2205
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1229
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1522
3088/6530 [=============>................] - ETA: 1s - loss: 0.2163
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1221
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1521
3360/6530 [==============>...............] - ETA: 1s - loss: 0.2124
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1225
2832/6530 [============>.................] - ETA: 0s - loss: 0.1517
3648/6530 [===============>..............] - ETA: 0s - loss: 0.2097
3264/6530 [=============>................] - ETA: 0s - loss: 0.1204
3184/6530 [=============>................] - ETA: 0s - loss: 0.1509
3936/6530 [=================>............] - ETA: 0s - loss: 0.2063
3744/6530 [================>.............] - ETA: 0s - loss: 0.1186
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1510
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2043
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1192
3904/6530 [================>.............] - ETA: 0s - loss: 0.1506
4512/6530 [===================>..........] - ETA: 0s - loss: 0.2020
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1197
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1510
4800/6530 [=====================>........] - ETA: 0s - loss: 0.2006
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1189
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1509
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1992
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1190
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1498
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1977
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1191
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1493
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1959
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1487
6530/6530 [==============================] - 1s 114us/step - loss: 0.1191 - val_loss: 0.0898

5904/6530 [==========================>...] - ETA: 0s - loss: 0.1953
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1481
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1942
6384/6530 [============================>.] - ETA: 0s - loss: 0.1477
6496/6530 [============================>.] - ETA: 0s - loss: 0.1931
6530/6530 [==============================] - 1s 150us/step - loss: 0.1477 - val_loss: 0.1375
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1778
 384/6530 [>.............................] - ETA: 0s - loss: 0.1513
6530/6530 [==============================] - 2s 272us/step - loss: 0.1930 - val_loss: 0.1602
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1880
 752/6530 [==>...........................] - ETA: 0s - loss: 0.1500
 288/6530 [>.............................] - ETA: 1s - loss: 0.1743
1104/6530 [====>.........................] - ETA: 0s - loss: 0.1475
 544/6530 [=>............................] - ETA: 1s - loss: 0.1678
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1465
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1661
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1456
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1666
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1457
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1664
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1456
1744/6530 [=======>......................] - ETA: 0s - loss: 0.1676
2928/6530 [============>.................] - ETA: 0s - loss: 0.1450
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1680
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1440
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1683
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1429
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1678
4048/6530 [=================>............] - ETA: 0s - loss: 0.1432
2928/6530 [============>.................] - ETA: 0s - loss: 0.1676
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1427
3216/6530 [=============>................] - ETA: 0s - loss: 0.1660
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1425
3504/6530 [===============>..............] - ETA: 0s - loss: 0.1662
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1424
3792/6530 [================>.............] - ETA: 0s - loss: 0.1667
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1430
4096/6530 [=================>............] - ETA: 0s - loss: 0.1665
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1428
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1653
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1429
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1648
6448/6530 [============================>.] - ETA: 0s - loss: 0.1428
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1645
6530/6530 [==============================] - 1s 147us/step - loss: 0.1428 - val_loss: 0.1262
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1273
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1643
 368/6530 [>.............................] - ETA: 0s - loss: 0.1287
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1646
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1361
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1644
1104/6530 [====>.........................] - ETA: 0s - loss: 0.1339
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1647
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1342
6384/6530 [============================>.] - ETA: 0s - loss: 0.1645
1808/6530 [=======>......................] - ETA: 0s - loss: 0.1347
6530/6530 [==============================] - 1s 182us/step - loss: 0.1645 - val_loss: 0.1580
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1034
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1339
 304/6530 [>.............................] - ETA: 1s - loss: 0.1567
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1363
 576/6530 [=>............................] - ETA: 1s - loss: 0.1592
2928/6530 [============>.................] - ETA: 0s - loss: 0.1362
 848/6530 [==>...........................] - ETA: 1s - loss: 0.1642
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1365
1120/6530 [====>.........................] - ETA: 1s - loss: 0.1644
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1358
# training | RMSE: 0.1065, MAE: 0.0841
worker 1  xfile  [1, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1752856684537518}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.10645932057167883, 'rmse': 0.10645932057167883, 'mae': 0.08411386919928474, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  10 | activation: tanh    | extras: batchnorm 
layer 2 | size:  18 | activation: relu    | extras: batchnorm 
layer 3 | size:  95 | activation: tanh    | extras: dropout - rate: 42.7% 
layer 4 | size:  10 | activation: sigmoid | extras: dropout - rate: 30.2% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a777f28>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 15s - loss: 0.7328
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1644
3968/6530 [=================>............] - ETA: 0s - loss: 0.1355
3584/6530 [===============>..............] - ETA: 0s - loss: 0.6471 
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1637
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1361
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1640
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1370
6530/6530 [==============================] - 1s 118us/step - loss: 0.5952 - val_loss: 0.4786
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.4743
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1621
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1378
3584/6530 [===============>..............] - ETA: 0s - loss: 0.4153
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1592
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1376
6530/6530 [==============================] - 0s 16us/step - loss: 0.3717 - val_loss: 0.2739
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2649
2896/6530 [============>.................] - ETA: 0s - loss: 0.1581
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1377
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2222
3184/6530 [=============>................] - ETA: 0s - loss: 0.1577
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1372
6530/6530 [==============================] - 0s 17us/step - loss: 0.1944 - val_loss: 0.1609
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1469
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1581
6416/6530 [============================>.] - ETA: 0s - loss: 0.1369
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1478
3728/6530 [================>.............] - ETA: 0s - loss: 0.1584
6530/6530 [==============================] - 1s 148us/step - loss: 0.1366 - val_loss: 0.1230
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1317
6530/6530 [==============================] - 0s 17us/step - loss: 0.1424 - val_loss: 0.1682

4016/6530 [=================>............] - ETA: 0s - loss: 0.1589Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1234
 352/6530 [>.............................] - ETA: 0s - loss: 0.1380
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1575
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1381
 720/6530 [==>...........................] - ETA: 0s - loss: 0.1439
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1572
1072/6530 [===>..........................] - ETA: 0s - loss: 0.1412
6530/6530 [==============================] - 0s 17us/step - loss: 0.1353 - val_loss: 0.1527
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1311
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1563
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1419
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1300
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1403
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1566
6530/6530 [==============================] - 0s 17us/step - loss: 0.1274 - val_loss: 0.1305
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1192
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1395
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1565
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1214
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1403
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1568
6530/6530 [==============================] - 0s 16us/step - loss: 0.1207 - val_loss: 0.1473
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1137
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1566
2848/6530 [============>.................] - ETA: 0s - loss: 0.1396
3840/6530 [================>.............] - ETA: 0s - loss: 0.1142
6352/6530 [============================>.] - ETA: 0s - loss: 0.1559
3200/6530 [=============>................] - ETA: 0s - loss: 0.1387
6530/6530 [==============================] - 0s 16us/step - loss: 0.1133 - val_loss: 0.1421
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1149
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1384
6530/6530 [==============================] - 1s 184us/step - loss: 0.1556 - val_loss: 0.1459

3840/6530 [================>.............] - ETA: 0s - loss: 0.1089Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1091
3936/6530 [=================>............] - ETA: 0s - loss: 0.1378
6530/6530 [==============================] - 0s 16us/step - loss: 0.1070 - val_loss: 0.1278

 304/6530 [>.............................] - ETA: 1s - loss: 0.1571
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1377
 608/6530 [=>............................] - ETA: 1s - loss: 0.1582
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1374
 912/6530 [===>..........................] - ETA: 0s - loss: 0.1587
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1369
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1536
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1366
1520/6530 [=====>........................] - ETA: 0s - loss: 0.1527
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1359
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1528
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1359
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1514
6464/6530 [============================>.] - ETA: 0s - loss: 0.1362
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1520
6530/6530 [==============================] - 1s 148us/step - loss: 0.1361 - val_loss: 0.1239
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1736
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1513
 368/6530 [>.............................] - ETA: 0s - loss: 0.1344
3040/6530 [============>.................] - ETA: 0s - loss: 0.1512
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1366
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1499
1104/6530 [====>.........................] - ETA: 0s - loss: 0.1384
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1504
1456/6530 [=====>........................] - ETA: 0s - loss: 0.1371
# training | RMSE: 0.1586, MAE: 0.1210
worker 1  xfile  [6, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4273031903618001}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3015218909611247}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.15855679500671227, 'rmse': 0.15855679500671227, 'mae': 0.12096191328878625, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  15 | activation: tanh    | extras: None 
layer 2 | size:  15 | activation: sigmoid | extras: None 
layer 3 | size:  98 | activation: tanh    | extras: dropout - rate: 14.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f77785490f0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 1:48 - loss: 0.5138
3952/6530 [=================>............] - ETA: 0s - loss: 0.1513
1808/6530 [=======>......................] - ETA: 0s - loss: 0.1353
 544/6530 [=>............................] - ETA: 3s - loss: 0.1180  
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1516
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0951
2160/6530 [========>.....................] - ETA: 0s - loss: 0.1353
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1508
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0803
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1341
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1506
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0725
2832/6530 [============>.................] - ETA: 0s - loss: 0.1339
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1503
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0673
3184/6530 [=============>................] - ETA: 0s - loss: 0.1349
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1505
3056/6530 [=============>................] - ETA: 0s - loss: 0.0624
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1347
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1505
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0599
3872/6530 [================>.............] - ETA: 0s - loss: 0.1342
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1504
4080/6530 [=================>............] - ETA: 0s - loss: 0.0576
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1338
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1500
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0559
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1336
6528/6530 [============================>.] - ETA: 0s - loss: 0.1496
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0545
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1340
6530/6530 [==============================] - 1s 179us/step - loss: 0.1497 - val_loss: 0.1428
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1554
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0531
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1334
 304/6530 [>.............................] - ETA: 1s - loss: 0.1545
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0523
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1335
 592/6530 [=>............................] - ETA: 1s - loss: 0.1506
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1329
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1510
6530/6530 [==============================] - 1s 149us/step - loss: 0.0513 - val_loss: 0.0399
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0487
6352/6530 [============================>.] - ETA: 0s - loss: 0.1336
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1477
 496/6530 [=>............................] - ETA: 0s - loss: 0.0434
6530/6530 [==============================] - 1s 150us/step - loss: 0.1333 - val_loss: 0.1184

1456/6530 [=====>........................] - ETA: 0s - loss: 0.1504
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0421
1744/6530 [=======>......................] - ETA: 0s - loss: 0.1504
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0398
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1512
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0399
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1512
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0404
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1515
3024/6530 [============>.................] - ETA: 0s - loss: 0.0398
2896/6530 [============>.................] - ETA: 0s - loss: 0.1504
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0401
3184/6530 [=============>................] - ETA: 0s - loss: 0.1502
4064/6530 [=================>............] - ETA: 0s - loss: 0.0402
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1502
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0403
3744/6530 [================>.............] - ETA: 0s - loss: 0.1492
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0404
4048/6530 [=================>............] - ETA: 0s - loss: 0.1493
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0402
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1493
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0403
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1497
6530/6530 [==============================] - 1s 102us/step - loss: 0.0402 - val_loss: 0.0398
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0476
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1492
 528/6530 [=>............................] - ETA: 0s - loss: 0.0426
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1485
# training | RMSE: 0.1516, MAE: 0.1175
worker 2  xfile  [3, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38572840251416507}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.280536333095057}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4399885133326492}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.15156921825314826, 'rmse': 0.15156921825314826, 'mae': 0.11752115361415078, 'early_stop': False}
{'batch_size': 128,
 'init': 'normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  66 | activation: tanh    | extras: None 
layer 2 | size:  52 | activation: sigmoid | extras: dropout - rate: 43.6% 

xnet is <keras.engine.sequential.Sequential object at 0x7f7771e3ce80>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 10s - loss: 0.6499
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0414
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1479
4352/6530 [==================>...........] - ETA: 0s - loss: 0.3313 
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0391
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1479
6530/6530 [==============================] - 0s 46us/step - loss: 0.2846 - val_loss: 0.1715
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1815
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0393
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1483
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1643
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0402
6530/6530 [==============================] - 0s 13us/step - loss: 0.1634 - val_loss: 0.1609
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1625
6464/6530 [============================>.] - ETA: 0s - loss: 0.1483
3088/6530 [=============>................] - ETA: 0s - loss: 0.0392
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1607
6530/6530 [==============================] - 1s 181us/step - loss: 0.1480 - val_loss: 0.1377
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1280
6530/6530 [==============================] - 0s 13us/step - loss: 0.1610 - val_loss: 0.1612
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1625
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0394
 320/6530 [>.............................] - ETA: 1s - loss: 0.1486
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1613
4096/6530 [=================>............] - ETA: 0s - loss: 0.0396
6530/6530 [==============================] - 0s 13us/step - loss: 0.1612 - val_loss: 0.1612

 624/6530 [=>............................] - ETA: 1s - loss: 0.1504Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1626
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0396
4096/6530 [=================>............] - ETA: 0s - loss: 0.1616
 912/6530 [===>..........................] - ETA: 0s - loss: 0.1537
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0397
6530/6530 [==============================] - 0s 13us/step - loss: 0.1613 - val_loss: 0.1613
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1627
1200/6530 [====>.........................] - ETA: 0s - loss: 0.1502
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0394
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1612
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1498
6530/6530 [==============================] - 0s 13us/step - loss: 0.1615 - val_loss: 0.1613
Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1629
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0395
1808/6530 [=======>......................] - ETA: 0s - loss: 0.1484
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1615
6530/6530 [==============================] - 1s 104us/step - loss: 0.0395 - val_loss: 0.0433
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0484
6530/6530 [==============================] - 0s 13us/step - loss: 0.1615 - val_loss: 0.1615

2096/6530 [========>.....................] - ETA: 0s - loss: 0.1484
 464/6530 [=>............................] - ETA: 0s - loss: 0.0427
2384/6530 [=========>....................] - ETA: 0s - loss: 0.1474
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0402
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1472
1488/6530 [=====>........................] - ETA: 0s - loss: 0.0385
2992/6530 [============>.................] - ETA: 0s - loss: 0.1470
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0385
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1464
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0390
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1453
3024/6530 [============>.................] - ETA: 0s - loss: 0.0382
3904/6530 [================>.............] - ETA: 0s - loss: 0.1447
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0384
4208/6530 [==================>...........] - ETA: 0s - loss: 0.1446
4000/6530 [=================>............] - ETA: 0s - loss: 0.0385
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1445
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0384
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1444
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0384
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1445
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0381
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1448
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0383
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1446
6530/6530 [==============================] - 1s 105us/step - loss: 0.0383 - val_loss: 0.0429

6000/6530 [==========================>...] - ETA: 0s - loss: 0.1447Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0476
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1451
 560/6530 [=>............................] - ETA: 0s - loss: 0.0394
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0379
6530/6530 [==============================] - 1s 179us/step - loss: 0.1446 - val_loss: 0.1318
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1444
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0359
 304/6530 [>.............................] - ETA: 1s - loss: 0.1475
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0363
 592/6530 [=>............................] - ETA: 1s - loss: 0.1500
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0368
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1464
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0361
# training | RMSE: 0.2030, MAE: 0.1601
worker 2  xfile  [8, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4358447939016047}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.20301452626903144, 'rmse': 0.20301452626903144, 'mae': 0.16014624635609462, 'early_stop': True}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  75 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  30 | activation: sigmoid | extras: dropout - rate: 13.5% 
layer 3 | size:  35 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7771f45518>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 3:15 - loss: 0.5476
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1445
3792/6530 [================>.............] - ETA: 0s - loss: 0.0359
 368/6530 [>.............................] - ETA: 8s - loss: 0.2729  
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1443
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0358
 704/6530 [==>...........................] - ETA: 4s - loss: 0.1807
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1458
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0360
1056/6530 [===>..........................] - ETA: 3s - loss: 0.1445
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1444
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0359
1408/6530 [=====>........................] - ETA: 2s - loss: 0.1215
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1441
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0358
1744/6530 [=======>......................] - ETA: 2s - loss: 0.1075
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1449
6352/6530 [============================>.] - ETA: 0s - loss: 0.0358
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0985
2896/6530 [============>.................] - ETA: 0s - loss: 0.1457
6530/6530 [==============================] - 1s 101us/step - loss: 0.0357 - val_loss: 0.0380
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0432
2400/6530 [==========>...................] - ETA: 1s - loss: 0.0913
3184/6530 [=============>................] - ETA: 0s - loss: 0.1445
 512/6530 [=>............................] - ETA: 0s - loss: 0.0361
2752/6530 [===========>..................] - ETA: 1s - loss: 0.0857
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1444
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0341
3088/6530 [=============>................] - ETA: 1s - loss: 0.0808
3744/6530 [================>.............] - ETA: 0s - loss: 0.1439
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0322
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0774
4032/6530 [=================>............] - ETA: 0s - loss: 0.1442
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0326
3792/6530 [================>.............] - ETA: 0s - loss: 0.0744
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1444
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0331
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0722
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1439
3008/6530 [============>.................] - ETA: 0s - loss: 0.0325
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0703
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1438
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0325
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0682
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1434
4000/6530 [=================>............] - ETA: 0s - loss: 0.0322
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0667
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1430
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0321
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0655
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1432
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0320
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0644
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1433
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0317
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0634
6320/6530 [============================>.] - ETA: 0s - loss: 0.1432
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0316
6530/6530 [==============================] - 1s 184us/step - loss: 0.1431 - val_loss: 0.1316
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1322
6530/6530 [==============================] - 1s 105us/step - loss: 0.0315 - val_loss: 0.0304
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0366
6530/6530 [==============================] - 2s 232us/step - loss: 0.0622 - val_loss: 0.0431
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0560
 304/6530 [>.............................] - ETA: 1s - loss: 0.1442
 512/6530 [=>............................] - ETA: 0s - loss: 0.0302
 352/6530 [>.............................] - ETA: 0s - loss: 0.0453
 592/6530 [=>............................] - ETA: 1s - loss: 0.1476
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0287
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0489
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1431
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0272
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0485
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0276
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1440
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0464
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0282
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1453
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0443
3024/6530 [============>.................] - ETA: 0s - loss: 0.0278
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1451
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0445
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0278
2064/6530 [========>.....................] - ETA: 0s - loss: 0.1433
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0450
4032/6530 [=================>............] - ETA: 0s - loss: 0.0275
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1429
2832/6530 [============>.................] - ETA: 0s - loss: 0.0448
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0274
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1434
3136/6530 [=============>................] - ETA: 0s - loss: 0.0441
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0273
2880/6530 [============>.................] - ETA: 0s - loss: 0.1432
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0447
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0270
3168/6530 [=============>................] - ETA: 0s - loss: 0.1430
3840/6530 [================>.............] - ETA: 0s - loss: 0.0446
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0271
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1435
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0447
3744/6530 [================>.............] - ETA: 0s - loss: 0.1429
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0449
6530/6530 [==============================] - 1s 105us/step - loss: 0.0270 - val_loss: 0.0247
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0317
4032/6530 [=================>............] - ETA: 0s - loss: 0.1420
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0447
 528/6530 [=>............................] - ETA: 0s - loss: 0.0251
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1417
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0448
1040/6530 [===>..........................] - ETA: 0s - loss: 0.0241
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1423
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0449
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0234
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1420
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0450
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0239
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1416
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0448
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0248
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1415
3088/6530 [=============>................] - ETA: 0s - loss: 0.0244
6530/6530 [==============================] - 1s 153us/step - loss: 0.0446 - val_loss: 0.0423
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0527
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1422
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0245
 368/6530 [>.............................] - ETA: 0s - loss: 0.0438
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1423
4112/6530 [=================>............] - ETA: 0s - loss: 0.0241
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0479
6352/6530 [============================>.] - ETA: 0s - loss: 0.1426
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0242
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0470
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0242
6530/6530 [==============================] - 1s 185us/step - loss: 0.1426 - val_loss: 0.1294
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1503
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0449
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0241
 288/6530 [>.............................] - ETA: 1s - loss: 0.1381
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0433
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0241
 576/6530 [=>............................] - ETA: 1s - loss: 0.1366
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0435
 880/6530 [===>..........................] - ETA: 1s - loss: 0.1405
6530/6530 [==============================] - 1s 104us/step - loss: 0.0240 - val_loss: 0.0227
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0293
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0439
1168/6530 [====>.........................] - ETA: 0s - loss: 0.1406
 512/6530 [=>............................] - ETA: 0s - loss: 0.0229
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0442
1456/6530 [=====>........................] - ETA: 0s - loss: 0.1424
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0221
3152/6530 [=============>................] - ETA: 0s - loss: 0.0433
1744/6530 [=======>......................] - ETA: 0s - loss: 0.1427
1520/6530 [=====>........................] - ETA: 0s - loss: 0.0218
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0440
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1427
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0221
3856/6530 [================>.............] - ETA: 0s - loss: 0.0440
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1422
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0231
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0441
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1429
3056/6530 [=============>................] - ETA: 0s - loss: 0.0227
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0443
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0230
2896/6530 [============>.................] - ETA: 0s - loss: 0.1414
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0442
4080/6530 [=================>............] - ETA: 0s - loss: 0.0227
3184/6530 [=============>................] - ETA: 0s - loss: 0.1412
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0443
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0228
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1402
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0443
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0227
3760/6530 [================>.............] - ETA: 0s - loss: 0.1401
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0445
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0226
4048/6530 [=================>............] - ETA: 0s - loss: 0.1410
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0443
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0226
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1409
6530/6530 [==============================] - 1s 153us/step - loss: 0.0441 - val_loss: 0.0420
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0513
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1405
6530/6530 [==============================] - 1s 104us/step - loss: 0.0226 - val_loss: 0.0218

 368/6530 [>.............................] - ETA: 0s - loss: 0.0440
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1404
 720/6530 [==>...........................] - ETA: 0s - loss: 0.0479
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1405
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0469
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1402
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0449
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1406
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0434
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1402
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0435
6400/6530 [============================>.] - ETA: 0s - loss: 0.1407
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0438
6530/6530 [==============================] - 1s 183us/step - loss: 0.1407 - val_loss: 0.1285

2784/6530 [===========>..................] - ETA: 0s - loss: 0.0439
3104/6530 [=============>................] - ETA: 0s - loss: 0.0432
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0436
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0437
4000/6530 [=================>............] - ETA: 0s - loss: 0.0439
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0439
# training | RMSE: 0.1442, MAE: 0.1124
worker 1  xfile  [7, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14283571375707005}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 35, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28252178251643656}, 'layer_5_size': 42, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.14419427008989955, 'rmse': 0.14419427008989955, 'mae': 0.11237175553251069, 'early_stop': False}
{'batch_size': 64,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 1,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  18 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f775c19aeb8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 23s - loss: 0.7050
# training | RMSE: 0.1627, MAE: 0.1261
worker 0  xfile  [5, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2683168763307201}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31497950775664185}, 'layer_4_size': 29, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.16266354497059898, 'rmse': 0.16266354497059898, 'mae': 0.12606512568930606, 'early_stop': False}
vggnet done  0

4672/6530 [====================>.........] - ETA: 0s - loss: 0.0441
2496/6530 [==========>...................] - ETA: 0s - loss: 0.6874 
4992/6530 [=====================>........] - ETA: 0s - loss: 0.6176
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0441
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0443
6530/6530 [==============================] - 0s 60us/step - loss: 0.5608 - val_loss: 0.3078
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.2931
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0443
2560/6530 [==========>...................] - ETA: 0s - loss: 0.2137
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0442
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1882
6530/6530 [==============================] - 0s 21us/step - loss: 0.1815 - val_loss: 0.1611
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1724
6512/6530 [============================>.] - ETA: 0s - loss: 0.0441
6530/6530 [==============================] - 1s 154us/step - loss: 0.0440 - val_loss: 0.0417
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0569
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1622
 384/6530 [>.............................] - ETA: 0s - loss: 0.0434
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1618
6530/6530 [==============================] - 0s 21us/step - loss: 0.1612 - val_loss: 0.1611
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1725
 752/6530 [==>...........................] - ETA: 0s - loss: 0.0468
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1623
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0465
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1617
1456/6530 [=====>........................] - ETA: 0s - loss: 0.0448
6530/6530 [==============================] - 0s 21us/step - loss: 0.1611 - val_loss: 0.1610
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1727
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0437
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1622
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0433
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1617
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0441
6530/6530 [==============================] - 0s 21us/step - loss: 0.1610 - val_loss: 0.1609
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1725
2928/6530 [============>.................] - ETA: 0s - loss: 0.0437
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1618
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0433
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1614
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0436
6530/6530 [==============================] - 0s 21us/step - loss: 0.1607 - val_loss: 0.1607
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1723
4032/6530 [=================>............] - ETA: 0s - loss: 0.0438
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1613
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1609
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0438
6530/6530 [==============================] - 0s 21us/step - loss: 0.1606 - val_loss: 0.1606
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1716
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0439
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1617
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0440
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1609
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0441
6530/6530 [==============================] - 0s 21us/step - loss: 0.1604 - val_loss: 0.1603
Epoch 9/9

  64/6530 [..............................] - ETA: 0s - loss: 0.1711
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0443
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1607
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0442
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1608
6530/6530 [==============================] - 0s 21us/step - loss: 0.1599 - val_loss: 0.1600

6530/6530 [==============================] - 1s 145us/step - loss: 0.0439 - val_loss: 0.0416
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0473
 368/6530 [>.............................] - ETA: 0s - loss: 0.0434
# training | RMSE: 0.2015, MAE: 0.1588
worker 1  xfile  [10, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38012202189334443}, 'layer_3_size': 39, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.20146456070523674, 'rmse': 0.20146456070523674, 'mae': 0.15878764150306598, 'early_stop': False}
vggnet done  1

 704/6530 [==>...........................] - ETA: 0s - loss: 0.0475
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0468
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0452
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0433
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0435
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0439
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0442
3040/6530 [============>.................] - ETA: 0s - loss: 0.0433
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0436
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0439
4000/6530 [=================>............] - ETA: 0s - loss: 0.0440
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0438
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0441
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0440
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0440
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0443
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0441
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0440
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0442
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0440
6384/6530 [============================>.] - ETA: 0s - loss: 0.0440
6530/6530 [==============================] - 1s 188us/step - loss: 0.0439 - val_loss: 0.0415
Epoch 7/9

  16/6530 [..............................] - ETA: 2s - loss: 0.0492
 192/6530 [..............................] - ETA: 1s - loss: 0.0441
 368/6530 [>.............................] - ETA: 1s - loss: 0.0433
 560/6530 [=>............................] - ETA: 1s - loss: 0.0458
 752/6530 [==>...........................] - ETA: 1s - loss: 0.0466
 944/6530 [===>..........................] - ETA: 1s - loss: 0.0461
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0454
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0441
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0430
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0427
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0433
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0431
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0433
2656/6530 [===========>..................] - ETA: 1s - loss: 0.0439
2832/6530 [============>.................] - ETA: 0s - loss: 0.0434
3024/6530 [============>.................] - ETA: 0s - loss: 0.0429
3200/6530 [=============>................] - ETA: 0s - loss: 0.0426
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0430
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0430
3728/6530 [================>.............] - ETA: 0s - loss: 0.0432
3920/6530 [=================>............] - ETA: 0s - loss: 0.0434
4096/6530 [=================>............] - ETA: 0s - loss: 0.0436
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0434
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0437
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0435
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0435
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0436
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0437
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0438
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0437
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0438
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0438
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0437
6336/6530 [============================>.] - ETA: 0s - loss: 0.0437
6530/6530 [==============================] - 2s 283us/step - loss: 0.0436 - val_loss: 0.0415
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0482
 208/6530 [..............................] - ETA: 1s - loss: 0.0428
 432/6530 [>.............................] - ETA: 1s - loss: 0.0454
 624/6530 [=>............................] - ETA: 1s - loss: 0.0469
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0461
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0466
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0452
1472/6530 [=====>........................] - ETA: 1s - loss: 0.0444
1648/6530 [======>.......................] - ETA: 1s - loss: 0.0434
1808/6530 [=======>......................] - ETA: 1s - loss: 0.0435
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0434
2160/6530 [========>.....................] - ETA: 1s - loss: 0.0432
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0433
2528/6530 [==========>...................] - ETA: 1s - loss: 0.0437
2720/6530 [===========>..................] - ETA: 1s - loss: 0.0437
2896/6530 [============>.................] - ETA: 0s - loss: 0.0434
3088/6530 [=============>................] - ETA: 0s - loss: 0.0430
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0432
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0434
3824/6530 [================>.............] - ETA: 0s - loss: 0.0435
4064/6530 [=================>............] - ETA: 0s - loss: 0.0437
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0437
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0440
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0439
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0438
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0440
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0441
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0441
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0441
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0441
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0440
6352/6530 [============================>.] - ETA: 0s - loss: 0.0440
6530/6530 [==============================] - 2s 261us/step - loss: 0.0438 - val_loss: 0.0415
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0502
 272/6530 [>.............................] - ETA: 1s - loss: 0.0422
 512/6530 [=>............................] - ETA: 1s - loss: 0.0457
 720/6530 [==>...........................] - ETA: 1s - loss: 0.0468
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0462
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0454
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0444
1616/6530 [======>.......................] - ETA: 1s - loss: 0.0435
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0437
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0434
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0434
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0436
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0440
2912/6530 [============>.................] - ETA: 0s - loss: 0.0434
3184/6530 [=============>................] - ETA: 0s - loss: 0.0427
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0431
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0434
3888/6530 [================>.............] - ETA: 0s - loss: 0.0434
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0436
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0436
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0437
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0437
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0437
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0438
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0437
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0437
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0439
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0437
6368/6530 [============================>.] - ETA: 0s - loss: 0.0437
6530/6530 [==============================] - 2s 241us/step - loss: 0.0436 - val_loss: 0.0414

# training | RMSE: 0.2016, MAE: 0.1622
worker 2  xfile  [9, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13515921791326946}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.20162235932858927, 'rmse': 0.20162235932858927, 'mae': 0.16218134076434385, 'early_stop': False}
vggnet done  2
all of workers have been done
calculation finished
#2 epoch=9.0 loss={'loss': 0.12763076842363855, 'rmse': 0.12763076842363855, 'mae': 0.10064097654207667, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17384346890971408}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.494814807006017}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#0 epoch=9.0 loss={'loss': 0.12105243943257793, 'rmse': 0.12105243943257793, 'mae': 0.09174401697641522, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#4 epoch=9.0 loss={'loss': 0.17632259399066372, 'rmse': 0.17632259399066372, 'mae': 0.14142872924080574, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 39, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.32905590204389745}, 'layer_2_size': 62, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 90, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.2658416628015635}, 'layer_4_size': 63, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 24, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#1 epoch=9.0 loss={'loss': 0.10645932057167883, 'rmse': 0.10645932057167883, 'mae': 0.08411386919928474, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1752856684537518}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
#6 epoch=9.0 loss={'loss': 0.15855679500671227, 'rmse': 0.15855679500671227, 'mae': 0.12096191328878625, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 10, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 18, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4273031903618001}, 'layer_3_size': 95, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3015218909611247}, 'layer_4_size': 10, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 42, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': None, 'shuffle': True}
#3 epoch=9.0 loss={'loss': 0.15156921825314826, 'rmse': 0.15156921825314826, 'mae': 0.11752115361415078, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 42, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.38572840251416507}, 'layer_2_size': 67, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.280536333095057}, 'layer_3_size': 15, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4399885133326492}, 'layer_4_size': 26, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': True}
#8 epoch=9.0 loss={'loss': 0.20301452626903144, 'rmse': 0.20301452626903144, 'mae': 0.16014624635609462, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 66, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4358447939016047}, 'layer_2_size': 52, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 85, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 58, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 54, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': False}
#7 epoch=9.0 loss={'loss': 0.14419427008989955, 'rmse': 0.14419427008989955, 'mae': 0.11237175553251069, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 15, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 15, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14283571375707005}, 'layer_3_size': 98, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 35, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.28252178251643656}, 'layer_5_size': 42, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': False}
#5 epoch=9.0 loss={'loss': 0.16266354497059898, 'rmse': 0.16266354497059898, 'mae': 0.12606512568930606, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.2683168763307201}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 11, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31497950775664185}, 'layer_4_size': 29, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 47, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'RobustScaler', 'shuffle': True}
#10 epoch=9.0 loss={'loss': 0.20146456070523674, 'rmse': 0.20146456070523674, 'mae': 0.15878764150306598, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 18, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 49, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.38012202189334443}, 'layer_3_size': 39, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 85, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 61, 'loss': 'mean_absolute_error', 'n_layers': 1, 'optimizer': 'adam', 'scaler': None, 'shuffle': False}
#9 epoch=9.0 loss={'loss': 0.20162235932858927, 'rmse': 0.20162235932858927, 'mae': 0.16218134076434385, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 75, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13515921791326946}, 'layer_2_size': 30, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 35, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 10, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 4, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'MinMaxScaler', 'shuffle': False}
get a list [results] of length 166
get a list [loss] of length 11
get a list [val_loss] of length 11
length of indices is (1, 0, 2, 7, 3, 6, 5, 4, 10, 9, 8)
length of indices is 11
length of T is 11
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1752856684537518}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}], [1, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}], [2, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17384346890971408}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.494814807006017}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]] 

*** 3.7777777777777777 configurations x 27.0 iterations each

10 | Thu Sep 27 23:27:05 2018 | lowest loss so far: 0.0809 (run 0)

{'batch_size': 64,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  64 | activation: relu    | extras: batchnorm 
layer 2 | size:  37 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a763630>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  64/6530 [..............................] - ETA: 1:29 - loss: 0.9352
1472/6530 [=====>........................] - ETA: 3s - loss: 0.1780  
2688/6530 [===========>..................] - ETA: 1s - loss: 0.1332
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1088
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0953
6530/6530 [==============================] - 1s 178us/step - loss: 0.0904 - val_loss: 0.0473
Epoch 2/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0661
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0443
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0418
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0416
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0414
6530/6530 [==============================] - 0s 35us/step - loss: 0.0409 - val_loss: 0.0429
Epoch 3/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0450{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  48 | activation: relu    | extras: None 
layer 2 | size:  54 | activation: relu    | extras: batchnorm 
layer 3 | size:  23 | activation: relu    | extras: batchnorm 
layer 4 | size:  33 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  64/6530 [..............................] - ETA: 1:49 - loss: 0.5895
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0334
1088/6530 [===>..........................] - ETA: 5s - loss: 0.4803  
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0326
2240/6530 [=========>....................] - ETA: 2s - loss: 0.4038
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0331
3200/6530 [=============>................] - ETA: 1s - loss: 0.3322
6530/6530 [==============================] - 0s 32us/step - loss: 0.0326 - val_loss: 0.0302
Epoch 4/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0363
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2657
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0284
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2208
3200/6530 [=============>................] - ETA: 0s - loss: 0.0276{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': True}
layer 1 | size:  58 | activation: relu    | extras: dropout - rate: 17.5% 
layer 2 | size:  33 | activation: tanh    | extras: batchnorm 
layer 3 | size:  64 | activation: relu    | extras: batchnorm 
layer 4 | size:  75 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 3:53 - loss: 0.8712
6336/6530 [============================>.] - ETA: 0s - loss: 0.1867
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0283
 480/6530 [=>............................] - ETA: 15s - loss: 0.7004 
6530/6530 [==============================] - 1s 225us/step - loss: 0.1820 - val_loss: 0.0726
Epoch 2/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0358
6530/6530 [==============================] - 0s 33us/step - loss: 0.0282 - val_loss: 0.0273

 928/6530 [===>..........................] - ETA: 7s - loss: 0.5841 Epoch 5/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0318
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0191
1376/6530 [=====>........................] - ETA: 4s - loss: 0.5021
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0252
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0184
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0248
1856/6530 [=======>......................] - ETA: 3s - loss: 0.4515
3200/6530 [=============>................] - ETA: 0s - loss: 0.0175
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0254
2304/6530 [=========>....................] - ETA: 2s - loss: 0.4158
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0169
6528/6530 [============================>.] - ETA: 0s - loss: 0.0253
2752/6530 [===========>..................] - ETA: 2s - loss: 0.3897
6530/6530 [==============================] - 0s 33us/step - loss: 0.0253 - val_loss: 0.0253
Epoch 6/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0291
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0169
3200/6530 [=============>................] - ETA: 1s - loss: 0.3683
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0232
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0164
3680/6530 [===============>..............] - ETA: 1s - loss: 0.3501
6530/6530 [==============================] - 0s 53us/step - loss: 0.0165 - val_loss: 0.0249

3328/6530 [==============>...............] - ETA: 0s - loss: 0.0228Epoch 3/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0143
4128/6530 [=================>............] - ETA: 0s - loss: 0.3372
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0233
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0131
4576/6530 [====================>.........] - ETA: 0s - loss: 0.3260
6464/6530 [============================>.] - ETA: 0s - loss: 0.0233
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0129
6530/6530 [==============================] - 0s 33us/step - loss: 0.0232 - val_loss: 0.0238
Epoch 7/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0271
4992/6530 [=====================>........] - ETA: 0s - loss: 0.3159
2944/6530 [============>.................] - ETA: 0s - loss: 0.0125
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0220
5376/6530 [=======================>......] - ETA: 0s - loss: 0.3085
3968/6530 [=================>............] - ETA: 0s - loss: 0.0124
2944/6530 [============>.................] - ETA: 0s - loss: 0.0215
5824/6530 [=========================>....] - ETA: 0s - loss: 0.3006
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0126
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0217
6272/6530 [===========================>..] - ETA: 0s - loss: 0.2941
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0124
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0219
6530/6530 [==============================] - 0s 35us/step - loss: 0.0217 - val_loss: 0.0226
Epoch 8/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0253
6530/6530 [==============================] - 0s 54us/step - loss: 0.0124 - val_loss: 0.0236
Epoch 4/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0237
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0205
6530/6530 [==============================] - 2s 303us/step - loss: 0.2905 - val_loss: 0.1769

1152/6530 [====>.........................] - ETA: 0s - loss: 0.0120Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.2194
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0203
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0113
 512/6530 [=>............................] - ETA: 0s - loss: 0.1972
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0208
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0112
 992/6530 [===>..........................] - ETA: 0s - loss: 0.2006
6530/6530 [==============================] - 0s 32us/step - loss: 0.0204 - val_loss: 0.0216
Epoch 9/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0236
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0109
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1980
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0197
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0110
1920/6530 [=======>......................] - ETA: 0s - loss: 0.2013
3264/6530 [=============>................] - ETA: 0s - loss: 0.0191
2368/6530 [=========>....................] - ETA: 0s - loss: 0.2006
6464/6530 [============================>.] - ETA: 0s - loss: 0.0109
6530/6530 [==============================] - 0s 50us/step - loss: 0.0109 - val_loss: 0.0131
Epoch 5/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0085
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0196
2848/6530 [============>.................] - ETA: 0s - loss: 0.1975
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0103
6530/6530 [==============================] - 0s 33us/step - loss: 0.0194 - val_loss: 0.0209

3296/6530 [==============>...............] - ETA: 0s - loss: 0.1955Epoch 10/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0222
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0100
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0187
3776/6530 [================>.............] - ETA: 0s - loss: 0.1936
3200/6530 [=============>................] - ETA: 0s - loss: 0.0100
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1922
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0185
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0099
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0189
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1907
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 0s 32us/step - loss: 0.0186 - val_loss: 0.0203
Epoch 11/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0213
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1903
6400/6530 [============================>.] - ETA: 0s - loss: 0.0099
6530/6530 [==============================] - 0s 51us/step - loss: 0.0098 - val_loss: 0.0171
Epoch 6/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0136
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0180
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1897
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0095
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0177
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1884
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0092
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0180
6528/6530 [============================>.] - ETA: 0s - loss: 0.1883
6530/6530 [==============================] - 1s 115us/step - loss: 0.1883 - val_loss: 0.1732
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1988
3200/6530 [=============>................] - ETA: 0s - loss: 0.0091
6530/6530 [==============================] - 0s 33us/step - loss: 0.0179 - val_loss: 0.0197
Epoch 12/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0205
 512/6530 [=>............................] - ETA: 0s - loss: 0.1906
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0091
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0174
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1848
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0092
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0170
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1806
6336/6530 [============================>.] - ETA: 0s - loss: 0.0090
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0174
6530/6530 [==============================] - 0s 51us/step - loss: 0.0091 - val_loss: 0.0117
Epoch 7/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0078
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1772
6530/6530 [==============================] - 0s 32us/step - loss: 0.0173 - val_loss: 0.0192

1088/6530 [===>..........................] - ETA: 0s - loss: 0.0090Epoch 13/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0198
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1739
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0087
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0171
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1710
3136/6530 [=============>................] - ETA: 0s - loss: 0.0085
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0165
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1702
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0087
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0169
3776/6530 [================>.............] - ETA: 0s - loss: 0.1685
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0088
6530/6530 [==============================] - 0s 32us/step - loss: 0.0167 - val_loss: 0.0188
Epoch 14/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0192
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1681
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0086
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0167
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1673
6530/6530 [==============================] - 0s 51us/step - loss: 0.0086 - val_loss: 0.0190
Epoch 8/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0150
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0161
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1658
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0088
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0164
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1644
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 0s 33us/step - loss: 0.0163 - val_loss: 0.0184
Epoch 15/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0186
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1633
3136/6530 [=============>................] - ETA: 0s - loss: 0.0081
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0160
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 1s 115us/step - loss: 0.1623 - val_loss: 0.1265
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1454
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0159
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0083
 512/6530 [=>............................] - ETA: 0s - loss: 0.1543
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0161
6336/6530 [============================>.] - ETA: 0s - loss: 0.0081
 992/6530 [===>..........................] - ETA: 0s - loss: 0.1536
6530/6530 [==============================] - 0s 51us/step - loss: 0.0082 - val_loss: 0.0096
Epoch 9/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0063
6530/6530 [==============================] - 0s 32us/step - loss: 0.0159 - val_loss: 0.0181
Epoch 16/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0182
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1543
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0081
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0159
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1537
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0078
3264/6530 [=============>................] - ETA: 0s - loss: 0.0153
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1525
3200/6530 [=============>................] - ETA: 0s - loss: 0.0078
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0156
2848/6530 [============>.................] - ETA: 0s - loss: 0.1517
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0079
6528/6530 [============================>.] - ETA: 0s - loss: 0.0155
6530/6530 [==============================] - 0s 33us/step - loss: 0.0155 - val_loss: 0.0178
Epoch 17/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0178
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1501
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0079
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0155
3776/6530 [================>.............] - ETA: 0s - loss: 0.1494
6336/6530 [============================>.] - ETA: 0s - loss: 0.0078
3264/6530 [=============>................] - ETA: 0s - loss: 0.0150
6530/6530 [==============================] - 0s 52us/step - loss: 0.0078 - val_loss: 0.0115
Epoch 10/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0084
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1497
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0153
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0078
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1488
6528/6530 [============================>.] - ETA: 0s - loss: 0.0152
6530/6530 [==============================] - 0s 33us/step - loss: 0.0152 - val_loss: 0.0175
Epoch 18/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0175
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0075
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1491
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0152
3200/6530 [=============>................] - ETA: 0s - loss: 0.0076
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1481
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0147
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0076
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1476
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0150
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0077
6530/6530 [==============================] - 1s 113us/step - loss: 0.1470 - val_loss: 0.1217
Epoch 5/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1549
6528/6530 [============================>.] - ETA: 0s - loss: 0.0149
6400/6530 [============================>.] - ETA: 0s - loss: 0.0076
6530/6530 [==============================] - 0s 33us/step - loss: 0.0149 - val_loss: 0.0173
Epoch 19/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0172
6530/6530 [==============================] - 0s 51us/step - loss: 0.0076 - val_loss: 0.0103
Epoch 11/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0077
 512/6530 [=>............................] - ETA: 0s - loss: 0.1422
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0151
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0076
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1422
3264/6530 [=============>................] - ETA: 0s - loss: 0.0144
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0074
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1373
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0147
3136/6530 [=============>................] - ETA: 0s - loss: 0.0072
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1350
6400/6530 [============================>.] - ETA: 0s - loss: 0.0146
6530/6530 [==============================] - 0s 33us/step - loss: 0.0146 - val_loss: 0.0170
Epoch 20/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0169
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0074
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1356
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0145
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0075
2848/6530 [============>.................] - ETA: 0s - loss: 0.1358
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0141
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0073
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1355
6530/6530 [==============================] - 0s 52us/step - loss: 0.0073 - val_loss: 0.0101
Epoch 12/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0078
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0145
3808/6530 [================>.............] - ETA: 0s - loss: 0.1365
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0075
6530/6530 [==============================] - 0s 33us/step - loss: 0.0143 - val_loss: 0.0168
Epoch 21/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0166
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1371
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0072
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0145
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1373
3200/6530 [=============>................] - ETA: 0s - loss: 0.0071
3264/6530 [=============>................] - ETA: 0s - loss: 0.0139
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1371
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0072
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0142
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1367
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0072
6528/6530 [============================>.] - ETA: 0s - loss: 0.0141
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1366
6530/6530 [==============================] - 0s 33us/step - loss: 0.0141 - val_loss: 0.0166
Epoch 22/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0163
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0070
6530/6530 [==============================] - 0s 52us/step - loss: 0.0071 - val_loss: 0.0104
Epoch 13/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0074
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0143
6530/6530 [==============================] - 1s 114us/step - loss: 0.1365 - val_loss: 0.1161
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1227
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0072
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0137
 512/6530 [=>............................] - ETA: 0s - loss: 0.1379
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0070
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0140
 992/6530 [===>..........................] - ETA: 0s - loss: 0.1322
3136/6530 [=============>................] - ETA: 0s - loss: 0.0068
6464/6530 [============================>.] - ETA: 0s - loss: 0.0139
6530/6530 [==============================] - 0s 33us/step - loss: 0.0139 - val_loss: 0.0164
Epoch 23/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0161
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1304
4096/6530 [=================>............] - ETA: 0s - loss: 0.0069
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0138
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1303
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0070
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0135
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1312
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0068
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0139
6530/6530 [==============================] - 0s 52us/step - loss: 0.0068 - val_loss: 0.0129

2944/6530 [============>.................] - ETA: 0s - loss: 0.1308
6530/6530 [==============================] - 0s 32us/step - loss: 0.0137 - val_loss: 0.0163
Epoch 24/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0159
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1309
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0136
3872/6530 [================>.............] - ETA: 0s - loss: 0.1308
# training | RMSE: 0.1036, MAE: 0.0830
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.10359061304221702, 'rmse': 0.10359061304221702, 'mae': 0.08301587284448238, 'early_stop': True}
vggnet done  1

3456/6530 [==============>...............] - ETA: 0s - loss: 0.0135
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1305
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0136
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1310
6530/6530 [==============================] - 0s 31us/step - loss: 0.0135 - val_loss: 0.0161
Epoch 25/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0157
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1318
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0137
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1313
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0131
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1305
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0134
6530/6530 [==============================] - 1s 112us/step - loss: 0.1305 - val_loss: 0.1114
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1317
6530/6530 [==============================] - 0s 31us/step - loss: 0.0133 - val_loss: 0.0160
Epoch 26/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0154
 544/6530 [=>............................] - ETA: 0s - loss: 0.1393
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0133
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1368
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0131
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1313
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0133
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1305
6530/6530 [==============================] - 0s 30us/step - loss: 0.0131 - val_loss: 0.0159
Epoch 27/27

  64/6530 [..............................] - ETA: 0s - loss: 0.0153
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1298
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0131
3072/6530 [=============>................] - ETA: 0s - loss: 0.1288
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0130
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1281
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0131
4064/6530 [=================>............] - ETA: 0s - loss: 0.1270
6530/6530 [==============================] - 0s 31us/step - loss: 0.0129 - val_loss: 0.0157

4576/6530 [====================>.........] - ETA: 0s - loss: 0.1257
# training | RMSE: 0.1068, MAE: 0.0841
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17384346890971408}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.494814807006017}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.10684398387210901, 'rmse': 0.10684398387210901, 'mae': 0.0840857951063269, 'early_stop': False}
vggnet done  2

5056/6530 [======================>.......] - ETA: 0s - loss: 0.1256
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1249
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1248
6530/6530 [==============================] - 1s 105us/step - loss: 0.1246 - val_loss: 0.0974
Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0892
 544/6530 [=>............................] - ETA: 0s - loss: 0.1390
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1343
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1316
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1285
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1278
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1283
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1286
3040/6530 [============>.................] - ETA: 0s - loss: 0.1282
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1275
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1262
3968/6530 [=================>............] - ETA: 0s - loss: 0.1250
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1245
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1233
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1230
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1225
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1223
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1222
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1219
6336/6530 [============================>.] - ETA: 0s - loss: 0.1217
6530/6530 [==============================] - 1s 164us/step - loss: 0.1217 - val_loss: 0.0880
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1366
 320/6530 [>.............................] - ETA: 1s - loss: 0.1273
 608/6530 [=>............................] - ETA: 1s - loss: 0.1283
 896/6530 [===>..........................] - ETA: 1s - loss: 0.1224
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1230
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1214
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1207
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1187
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1183
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1195
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1183
3008/6530 [============>.................] - ETA: 0s - loss: 0.1182
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1179
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1173
3808/6530 [================>.............] - ETA: 0s - loss: 0.1170
4032/6530 [=================>............] - ETA: 0s - loss: 0.1173
4256/6530 [==================>...........] - ETA: 0s - loss: 0.1178
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1181
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1178
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1178
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1182
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1185
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1188
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1187
6432/6530 [============================>.] - ETA: 0s - loss: 0.1186
6530/6530 [==============================] - 1s 203us/step - loss: 0.1186 - val_loss: 0.1021
Epoch 10/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0981
 384/6530 [>.............................] - ETA: 0s - loss: 0.1148
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1165
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1162
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1161
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1148
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1157
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1157
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1160
3072/6530 [=============>................] - ETA: 0s - loss: 0.1158
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1160
3712/6530 [================>.............] - ETA: 0s - loss: 0.1156
4032/6530 [=================>............] - ETA: 0s - loss: 0.1158
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1158
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1151
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1145
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1147
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1147
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1144
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1142
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1139
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1138
6530/6530 [==============================] - 1s 183us/step - loss: 0.1140 - val_loss: 0.0911
Epoch 11/27

  32/6530 [..............................] - ETA: 1s - loss: 0.1312
 352/6530 [>.............................] - ETA: 1s - loss: 0.1176
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1192
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1187
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1156
1568/6530 [======>.......................] - ETA: 0s - loss: 0.1156
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1146
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1148
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1157
2848/6530 [============>.................] - ETA: 0s - loss: 0.1160
3136/6530 [=============>................] - ETA: 0s - loss: 0.1166
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1159
3712/6530 [================>.............] - ETA: 0s - loss: 0.1153
4032/6530 [=================>............] - ETA: 0s - loss: 0.1154
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1153
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1151
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1146
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1142
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1141
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1140
6368/6530 [============================>.] - ETA: 0s - loss: 0.1138
6530/6530 [==============================] - 1s 171us/step - loss: 0.1138 - val_loss: 0.0861
Epoch 12/27

  32/6530 [..............................] - ETA: 1s - loss: 0.1038
 352/6530 [>.............................] - ETA: 0s - loss: 0.1053
 640/6530 [=>............................] - ETA: 0s - loss: 0.1054
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1040
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1081
1440/6530 [=====>........................] - ETA: 0s - loss: 0.1076
1728/6530 [======>.......................] - ETA: 0s - loss: 0.1099
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1103
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1099
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1090
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1088
3072/6530 [=============>................] - ETA: 0s - loss: 0.1093
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1093
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1088
3840/6530 [================>.............] - ETA: 0s - loss: 0.1094
4096/6530 [=================>............] - ETA: 0s - loss: 0.1086
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1087
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1082
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1087
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1085
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1087
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1086
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1088
6530/6530 [==============================] - 1s 195us/step - loss: 0.1087 - val_loss: 0.0824
Epoch 13/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0945
 288/6530 [>.............................] - ETA: 1s - loss: 0.1178
 544/6530 [=>............................] - ETA: 1s - loss: 0.1110
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1095
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1099
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1084
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1081
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1073
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1066
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1070
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1084
2944/6530 [============>.................] - ETA: 0s - loss: 0.1088
3232/6530 [=============>................] - ETA: 0s - loss: 0.1088
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1080
3808/6530 [================>.............] - ETA: 0s - loss: 0.1070
4064/6530 [=================>............] - ETA: 0s - loss: 0.1072
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1079
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1077
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1079
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1079
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1077
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1078
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1082
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1082
6464/6530 [============================>.] - ETA: 0s - loss: 0.1083
6530/6530 [==============================] - 1s 204us/step - loss: 0.1084 - val_loss: 0.0972
Epoch 14/27

  32/6530 [..............................] - ETA: 1s - loss: 0.1154
 288/6530 [>.............................] - ETA: 1s - loss: 0.1214
 544/6530 [=>............................] - ETA: 1s - loss: 0.1212
 800/6530 [==>...........................] - ETA: 1s - loss: 0.1190
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1120
1376/6530 [=====>........................] - ETA: 1s - loss: 0.1118
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1114
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1107
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1086
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1083
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1081
2976/6530 [============>.................] - ETA: 0s - loss: 0.1079
3200/6530 [=============>................] - ETA: 0s - loss: 0.1071
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1068
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1065
3904/6530 [================>.............] - ETA: 0s - loss: 0.1063
4128/6530 [=================>............] - ETA: 0s - loss: 0.1061
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1061
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1058
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1055
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1051
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1048
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1049
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1050
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1048
6530/6530 [==============================] - 1s 211us/step - loss: 0.1048 - val_loss: 0.0814
Epoch 15/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0762
 288/6530 [>.............................] - ETA: 1s - loss: 0.1033
 512/6530 [=>............................] - ETA: 1s - loss: 0.1021
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0985
 992/6530 [===>..........................] - ETA: 1s - loss: 0.1013
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1015
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1023
1760/6530 [=======>......................] - ETA: 1s - loss: 0.1024
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1029
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1039
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1044
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1035
3008/6530 [============>.................] - ETA: 0s - loss: 0.1034
3264/6530 [=============>................] - ETA: 0s - loss: 0.1027
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1037
3776/6530 [================>.............] - ETA: 0s - loss: 0.1048
4096/6530 [=================>............] - ETA: 0s - loss: 0.1044
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1038
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1042
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1047
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1047
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1042
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1038
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1037
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1037
6496/6530 [============================>.] - ETA: 0s - loss: 0.1035
6530/6530 [==============================] - 1s 210us/step - loss: 0.1034 - val_loss: 0.0939
Epoch 16/27

  32/6530 [..............................] - ETA: 1s - loss: 0.1016
 320/6530 [>.............................] - ETA: 1s - loss: 0.0967
 608/6530 [=>............................] - ETA: 1s - loss: 0.0974
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0973
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0970
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0995
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1021
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1020
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1015
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1012
3008/6530 [============>.................] - ETA: 0s - loss: 0.1011
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1008
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1009
3904/6530 [================>.............] - ETA: 0s - loss: 0.1002
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1000
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1002
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1007
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1005
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1001
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1003
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1003
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1007
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1011
6496/6530 [============================>.] - ETA: 0s - loss: 0.1017
6530/6530 [==============================] - 1s 197us/step - loss: 0.1016 - val_loss: 0.0873
Epoch 17/27

  32/6530 [..............................] - ETA: 1s - loss: 0.1088
 256/6530 [>.............................] - ETA: 1s - loss: 0.1126
 480/6530 [=>............................] - ETA: 1s - loss: 0.1088
 704/6530 [==>...........................] - ETA: 1s - loss: 0.1059
 928/6530 [===>..........................] - ETA: 1s - loss: 0.1071
1152/6530 [====>.........................] - ETA: 1s - loss: 0.1067
1408/6530 [=====>........................] - ETA: 1s - loss: 0.1067
1632/6530 [======>.......................] - ETA: 1s - loss: 0.1057
1856/6530 [=======>......................] - ETA: 1s - loss: 0.1042
2112/6530 [========>.....................] - ETA: 1s - loss: 0.1030
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1013
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1008
2880/6530 [============>.................] - ETA: 0s - loss: 0.1000
3168/6530 [=============>................] - ETA: 0s - loss: 0.1004
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1002
3712/6530 [================>.............] - ETA: 0s - loss: 0.1005
3968/6530 [=================>............] - ETA: 0s - loss: 0.1008
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1008
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1006
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1010
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1013
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1009
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1009
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1010
6432/6530 [============================>.] - ETA: 0s - loss: 0.1015
6530/6530 [==============================] - 1s 207us/step - loss: 0.1014 - val_loss: 0.0870
Epoch 18/27

  32/6530 [..............................] - ETA: 1s - loss: 0.1042
 288/6530 [>.............................] - ETA: 1s - loss: 0.1012
 544/6530 [=>............................] - ETA: 1s - loss: 0.0939
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0934
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0923
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0945
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0966
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0975
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0969
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0969
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0972
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0977
2976/6530 [============>.................] - ETA: 0s - loss: 0.0971
3264/6530 [=============>................] - ETA: 0s - loss: 0.0975
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0978
3776/6530 [================>.............] - ETA: 0s - loss: 0.0983
4032/6530 [=================>............] - ETA: 0s - loss: 0.0986
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0987
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0982
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0981
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0979
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0982
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0983
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0989
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0990
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0986
6530/6530 [==============================] - 1s 220us/step - loss: 0.0989 - val_loss: 0.0784
Epoch 19/27

  32/6530 [..............................] - ETA: 1s - loss: 0.1065
 320/6530 [>.............................] - ETA: 1s - loss: 0.0955
 608/6530 [=>............................] - ETA: 1s - loss: 0.1039
 928/6530 [===>..........................] - ETA: 1s - loss: 0.1021
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1029
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1013
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1013
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1004
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1008
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1005
2912/6530 [============>.................] - ETA: 0s - loss: 0.0999
3136/6530 [=============>................] - ETA: 0s - loss: 0.0995
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0990
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0986
3808/6530 [================>.............] - ETA: 0s - loss: 0.0988
4064/6530 [=================>............] - ETA: 0s - loss: 0.0987
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0992
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0981
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0979
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0976
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0970
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0966
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0968
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0967
6432/6530 [============================>.] - ETA: 0s - loss: 0.0964
6530/6530 [==============================] - 1s 207us/step - loss: 0.0965 - val_loss: 0.0790
Epoch 20/27

  32/6530 [..............................] - ETA: 1s - loss: 0.1003
 288/6530 [>.............................] - ETA: 1s - loss: 0.1073
 512/6530 [=>............................] - ETA: 1s - loss: 0.1058
 736/6530 [==>...........................] - ETA: 1s - loss: 0.1016
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0998
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1002
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1002
1728/6530 [======>.......................] - ETA: 1s - loss: 0.1023
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1011
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1012
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1005
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1005
2944/6530 [============>.................] - ETA: 0s - loss: 0.1010
3200/6530 [=============>................] - ETA: 0s - loss: 0.1008
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1001
3712/6530 [================>.............] - ETA: 0s - loss: 0.1001
4000/6530 [=================>............] - ETA: 0s - loss: 0.0996
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1001
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1003
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1007
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1008
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1007
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1002
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1006
6336/6530 [============================>.] - ETA: 0s - loss: 0.1006
6530/6530 [==============================] - 1s 204us/step - loss: 0.1002 - val_loss: 0.0903
Epoch 21/27

  32/6530 [..............................] - ETA: 1s - loss: 0.1028
 320/6530 [>.............................] - ETA: 1s - loss: 0.1090
 608/6530 [=>............................] - ETA: 1s - loss: 0.1047
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1014
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1005
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0979
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0977
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0993
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0996
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0994
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0990
3104/6530 [=============>................] - ETA: 0s - loss: 0.0993
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0993
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0989
3936/6530 [=================>............] - ETA: 0s - loss: 0.0987
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0985
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0979
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0977
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0969
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0966
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0962
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0967
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0965
6496/6530 [============================>.] - ETA: 0s - loss: 0.0964
6530/6530 [==============================] - 1s 193us/step - loss: 0.0965 - val_loss: 0.0747
Epoch 22/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0984
 288/6530 [>.............................] - ETA: 1s - loss: 0.0911
 576/6530 [=>............................] - ETA: 1s - loss: 0.0938
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0953
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0947
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0932
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0942
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0938
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0937
3136/6530 [=============>................] - ETA: 0s - loss: 0.0945
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0940
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0942
3968/6530 [=================>............] - ETA: 0s - loss: 0.0939
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0948
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0949
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0942
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0945
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0948
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0949
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0948
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0950
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0950
6432/6530 [============================>.] - ETA: 0s - loss: 0.0948
6530/6530 [==============================] - 1s 189us/step - loss: 0.0949 - val_loss: 0.0867
Epoch 23/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0930
 256/6530 [>.............................] - ETA: 1s - loss: 0.0961
 512/6530 [=>............................] - ETA: 1s - loss: 0.0956
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0937
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0930
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0940
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0933
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0935
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0930
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0943
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0950
3008/6530 [============>.................] - ETA: 0s - loss: 0.0945
3232/6530 [=============>................] - ETA: 0s - loss: 0.0944
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0945
3712/6530 [================>.............] - ETA: 0s - loss: 0.0939
3968/6530 [=================>............] - ETA: 0s - loss: 0.0936
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0937
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0935
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0937
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0941
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0941
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0939
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0940
6336/6530 [============================>.] - ETA: 0s - loss: 0.0939
6530/6530 [==============================] - 1s 200us/step - loss: 0.0937 - val_loss: 0.0764
Epoch 24/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0698
 384/6530 [>.............................] - ETA: 0s - loss: 0.0967
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0970
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0971
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0968
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0955
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0950
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0957
2848/6530 [============>.................] - ETA: 0s - loss: 0.0956
3232/6530 [=============>................] - ETA: 0s - loss: 0.0956
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0947
3904/6530 [================>.............] - ETA: 0s - loss: 0.0944
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0947
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0951
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0956
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0952
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0948
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0951
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0955
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0952
6464/6530 [============================>.] - ETA: 0s - loss: 0.0951
6530/6530 [==============================] - 1s 171us/step - loss: 0.0953 - val_loss: 0.0778
Epoch 25/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0823
 256/6530 [>.............................] - ETA: 1s - loss: 0.1001
 512/6530 [=>............................] - ETA: 1s - loss: 0.0976
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0980
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0957
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0953
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0964
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0970
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0978
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0971
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0966
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0963
2976/6530 [============>.................] - ETA: 0s - loss: 0.0961
3264/6530 [=============>................] - ETA: 0s - loss: 0.0960
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0956
3744/6530 [================>.............] - ETA: 0s - loss: 0.0956
4000/6530 [=================>............] - ETA: 0s - loss: 0.0960
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0957
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0956
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0952
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0952
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0956
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0953
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0948
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0947
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0948
6464/6530 [============================>.] - ETA: 0s - loss: 0.0945
6530/6530 [==============================] - 1s 224us/step - loss: 0.0945 - val_loss: 0.0768
Epoch 26/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0958
 288/6530 [>.............................] - ETA: 1s - loss: 0.0962
 544/6530 [=>............................] - ETA: 1s - loss: 0.0933
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0917
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0920
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0901
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0906
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0901
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0916
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0912
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0907
2976/6530 [============>.................] - ETA: 0s - loss: 0.0906
3232/6530 [=============>................] - ETA: 0s - loss: 0.0908
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0901
3808/6530 [================>.............] - ETA: 0s - loss: 0.0906
4096/6530 [=================>............] - ETA: 0s - loss: 0.0912
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0917
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0918
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0915
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0911
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0910
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0910
6400/6530 [============================>.] - ETA: 0s - loss: 0.0912
6530/6530 [==============================] - 1s 188us/step - loss: 0.0913 - val_loss: 0.0760

# training | RMSE: 0.0877, MAE: 0.0684
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1752856684537518}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}]  predicted as label {'loss': 0.08766083836649885, 'rmse': 0.08766083836649885, 'mae': 0.06840654817482518, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#1 epoch=27.0 loss={'loss': 0.10359061304221702, 'rmse': 0.10359061304221702, 'mae': 0.08301587284448238, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
#2 epoch=27.0 loss={'loss': 0.10684398387210901, 'rmse': 0.10684398387210901, 'mae': 0.0840857951063269, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17384346890971408}, 'layer_3_size': 73, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.494814807006017}, 'layer_4_size': 34, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 66, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#0 epoch=27.0 loss={'loss': 0.08766083836649885, 'rmse': 0.08766083836649885, 'mae': 0.06840654817482518, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1752856684537518}, 'layer_1_size': 58, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 33, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 75, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 6, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': True}
get a list [results] of length 169
get a list [loss] of length 3
get a list [val_loss] of length 3
length of indices is (0, 1, 2)
length of indices is 3
length of T is 3
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]] 

*** 1.259259259259259 configurations x 81.0 iterations each

1 | Thu Sep 27 23:27:37 2018 | lowest loss so far: 0.0809 (run 0)

vggnet done  1
vggnet done  2
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  48 | activation: relu    | extras: None 
layer 2 | size:  54 | activation: relu    | extras: batchnorm 
layer 3 | size:  23 | activation: relu    | extras: batchnorm 
layer 4 | size:  33 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  64/6530 [..............................] - ETA: 1:44 - loss: 0.5895
 704/6530 [==>...........................] - ETA: 8s - loss: 0.5196  
1280/6530 [====>.........................] - ETA: 4s - loss: 0.4660
1792/6530 [=======>......................] - ETA: 3s - loss: 0.4367
2304/6530 [=========>....................] - ETA: 2s - loss: 0.4001
2880/6530 [============>.................] - ETA: 1s - loss: 0.3544
3456/6530 [==============>...............] - ETA: 1s - loss: 0.3118
4032/6530 [=================>............] - ETA: 0s - loss: 0.2752
4672/6530 [====================>.........] - ETA: 0s - loss: 0.2425
5312/6530 [=======================>......] - ETA: 0s - loss: 0.2169
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1960
6530/6530 [==============================] - 2s 259us/step - loss: 0.1808 - val_loss: 0.0795
Epoch 2/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0337
 576/6530 [=>............................] - ETA: 0s - loss: 0.0195
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0187
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0189
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0182
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0177
3200/6530 [=============>................] - ETA: 0s - loss: 0.0175
3776/6530 [================>.............] - ETA: 0s - loss: 0.0171
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0166
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0168
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0166
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0163
6530/6530 [==============================] - 1s 95us/step - loss: 0.0163 - val_loss: 0.0204
Epoch 3/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0180
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0147
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0149
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0139
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0137
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0136
3968/6530 [=================>............] - ETA: 0s - loss: 0.0133
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0133
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0133
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0129
6530/6530 [==============================] - 1s 81us/step - loss: 0.0129 - val_loss: 0.0173
Epoch 4/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0199
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0130
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0122
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0118
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0114
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0113
3968/6530 [=================>............] - ETA: 0s - loss: 0.0112
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0112
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0112
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0111
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0109
6530/6530 [==============================] - 1s 93us/step - loss: 0.0110 - val_loss: 0.0128
Epoch 5/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0114
 640/6530 [=>............................] - ETA: 0s - loss: 0.0110
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0107
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0104
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0102
2944/6530 [============>.................] - ETA: 0s - loss: 0.0098
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0100
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0099
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0100
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0098
6400/6530 [============================>.] - ETA: 0s - loss: 0.0099
6530/6530 [==============================] - 1s 86us/step - loss: 0.0099 - val_loss: 0.0155
Epoch 6/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0161
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0105
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0100
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0096
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0095
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0094
4032/6530 [=================>............] - ETA: 0s - loss: 0.0093
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0094
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0094
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0092
6528/6530 [============================>.] - ETA: 0s - loss: 0.0093
6530/6530 [==============================] - 1s 84us/step - loss: 0.0093 - val_loss: 0.0122
Epoch 7/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0105
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0090
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0089
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0088
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0087
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0087
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0087
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0087
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0087
6400/6530 [============================>.] - ETA: 0s - loss: 0.0087
6530/6530 [==============================] - 1s 78us/step - loss: 0.0087 - val_loss: 0.0116
Epoch 8/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0094
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0086
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0086
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0084
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0083
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0082
4096/6530 [=================>............] - ETA: 0s - loss: 0.0083
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0084
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0083
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0082
6530/6530 [==============================] - 1s 80us/step - loss: 0.0083 - val_loss: 0.0158
Epoch 9/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0145
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0089
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0085
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0082
2880/6530 [============>.................] - ETA: 0s - loss: 0.0079
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0079
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0079
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0080
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0080
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0079
6530/6530 [==============================] - 1s 83us/step - loss: 0.0079 - val_loss: 0.0098
Epoch 10/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0083
 640/6530 [=>............................] - ETA: 0s - loss: 0.0084
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0081
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0077
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0076
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0075
3904/6530 [================>.............] - ETA: 0s - loss: 0.0076
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0077
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0078
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0076
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0076
6530/6530 [==============================] - 1s 90us/step - loss: 0.0076 - val_loss: 0.0090
Epoch 11/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0070
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0076
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0074
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0075
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0072
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0074
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0074
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0074
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0073
6530/6530 [==============================] - 0s 71us/step - loss: 0.0073 - val_loss: 0.0116
Epoch 12/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0096
 640/6530 [=>............................] - ETA: 0s - loss: 0.0078
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0075
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0073
2880/6530 [============>.................] - ETA: 0s - loss: 0.0071
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0071
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0072
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0073
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0071
6530/6530 [==============================] - 0s 75us/step - loss: 0.0072 - val_loss: 0.0111
Epoch 13/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0083
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0073
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0071
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0071
2880/6530 [============>.................] - ETA: 0s - loss: 0.0069
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0068
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0070
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0070
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0070
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0069
6530/6530 [==============================] - 1s 80us/step - loss: 0.0069 - val_loss: 0.0139
Epoch 14/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0108
 640/6530 [=>............................] - ETA: 0s - loss: 0.0075
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0074
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0071
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0071
3072/6530 [=============>................] - ETA: 0s - loss: 0.0067
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0067
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0069
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0069
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0069
6400/6530 [============================>.] - ETA: 0s - loss: 0.0068
6530/6530 [==============================] - 1s 86us/step - loss: 0.0068 - val_loss: 0.0110
Epoch 15/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0079
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0069
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0069
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0067
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0068
3264/6530 [=============>................] - ETA: 0s - loss: 0.0065
4032/6530 [=================>............] - ETA: 0s - loss: 0.0066
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0067
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0066
6464/6530 [============================>.] - ETA: 0s - loss: 0.0066
6530/6530 [==============================] - 1s 77us/step - loss: 0.0066 - val_loss: 0.0144

# training | RMSE: 0.1171, MAE: 0.0897
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.11712057041250946, 'rmse': 0.11712057041250946, 'mae': 0.08972844701757891, 'early_stop': True}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=81.0 loss={'loss': 0.11712057041250946, 'rmse': 0.11712057041250946, 'mae': 0.08972844701757891, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 48, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 54, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 23, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 33, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 83, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': False}
get a list [results] of length 170
get a list [loss] of length 1
get a list [val_loss] of length 1
length of indices is (0,)
length of indices is 1
length of T is 1
s=2
T is of size 15
T=[{'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17515930016558992}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4449677908740701}, 'layer_4_size': 91, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 92, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2149531878533118}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25752557491173744}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12246768342475414}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 96, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21691068773488237}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.27617154374503416}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4082346454778355}, 'layer_2_size': 82, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4268159776129282}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 39, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3690655139133455}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44187056290450344}, 'layer_4_size': 98, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47825508775226755}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30764941843595073}, 'layer_3_size': 60, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 24, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.39756340213759234}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 2, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10475440952621362}, 'layer_1_size': 44, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10351097759701844}, 'layer_2_size': 40, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10044009068957292}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4170382118349151}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3165420015541014}, 'layer_4_size': 32, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2629199363211525}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 25, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 67, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}, {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28236583769424184}, 'layer_2_size': 25, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14136913317619815}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20072288229487062}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 100, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.17448042821411805}, 'layer_2_size': 10, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2519769646929365}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 43, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4383623882746951}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]
newly formed T structure is:[[0, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17515930016558992}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4449677908740701}, 'layer_4_size': 91, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [1, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 92, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2149531878533118}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25752557491173744}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12246768342475414}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [2, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 96, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [3, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21691068773488237}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.27617154374503416}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [4, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4082346454778355}, 'layer_2_size': 82, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4268159776129282}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 39, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [5, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3690655139133455}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44187056290450344}, 'layer_4_size': 98, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [6, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47825508775226755}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30764941843595073}, 'layer_3_size': 60, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 24, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.39756340213759234}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}], [7, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 2, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [8, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10475440952621362}, 'layer_1_size': 44, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10351097759701844}, 'layer_2_size': 40, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [9, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10044009068957292}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4170382118349151}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3165420015541014}, 'layer_4_size': 32, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2629199363211525}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}], [10, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 25, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 67, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}], [11, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28236583769424184}, 'layer_2_size': 25, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14136913317619815}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20072288229487062}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}], [12, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 100, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.17448042821411805}, 'layer_2_size': 10, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2519769646929365}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [13, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}], [14, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 43, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4383623882746951}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]] 

*** 15 configurations x 9.0 iterations each

1 | Thu Sep 27 23:27:48 2018 | lowest loss so far: 0.0809 (run 0)

{'batch_size': 256,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  32 | activation: tanh    | extras: None 
layer 2 | size:  65 | activation: tanh    | extras: None 
layer 3 | size:  55 | activation: tanh    | extras: dropout - rate: 17.5% 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 21s - loss: 0.5982
5376/6530 [=======================>......] - ETA: 0s - loss: 0.3304 
6530/6530 [==============================] - 1s 147us/step - loss: 0.2791 - val_loss: 0.0416
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0421{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  92 | activation: tanh    | extras: None 
layer 2 | size:  66 | activation: tanh    | extras: dropout - rate: 21.5% 
layer 3 | size:  38 | activation: relu    | extras: dropout - rate: 25.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 6:09 - loss: 0.4186
6530/6530 [==============================] - 0s 8us/step - loss: 0.0412 - val_loss: 0.0415
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0421
 400/6530 [>.............................] - ETA: 14s - loss: 0.4159 
6530/6530 [==============================] - 0s 8us/step - loss: 0.0412 - val_loss: 0.0415
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0421
 832/6530 [==>...........................] - ETA: 6s - loss: 0.3657 
6530/6530 [==============================] - 0s 8us/step - loss: 0.0413 - val_loss: 0.0415
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0421
1248/6530 [====>.........................] - ETA: 4s - loss: 0.3309
6530/6530 [==============================] - 0s 8us/step - loss: 0.0413 - val_loss: 0.0415
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0421
1648/6530 [======>.......................] - ETA: 3s - loss: 0.3143
6530/6530 [==============================] - 0s 8us/step - loss: 0.0414 - val_loss: 0.0415
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0421
2096/6530 [========>.....................] - ETA: 2s - loss: 0.3007
6530/6530 [==============================] - 0s 9us/step - loss: 0.0414 - val_loss: 0.0415
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0420
2544/6530 [==========>...................] - ETA: 1s - loss: 0.2894
6530/6530 [==============================] - 0s 8us/step - loss: 0.0414 - val_loss: 0.0414
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0420{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  16 | activation: sigmoid | extras: batchnorm 
layer 2 | size:  96 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a763e80>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 6:51 - loss: 1.3178
2976/6530 [============>.................] - ETA: 1s - loss: 0.2782
6530/6530 [==============================] - 0s 8us/step - loss: 0.0414 - val_loss: 0.0414

 368/6530 [>.............................] - ETA: 17s - loss: 0.8625 
3424/6530 [==============>...............] - ETA: 1s - loss: 0.2702
 736/6530 [==>...........................] - ETA: 8s - loss: 0.6767 
3840/6530 [================>.............] - ETA: 0s - loss: 0.2644
1104/6530 [====>.........................] - ETA: 5s - loss: 0.5725
4272/6530 [==================>...........] - ETA: 0s - loss: 0.2575
1456/6530 [=====>........................] - ETA: 4s - loss: 0.5059
4704/6530 [====================>.........] - ETA: 0s - loss: 0.2530
1808/6530 [=======>......................] - ETA: 3s - loss: 0.4664
5152/6530 [======================>.......] - ETA: 0s - loss: 0.2484
2192/6530 [=========>....................] - ETA: 2s - loss: 0.4339
5616/6530 [========================>.....] - ETA: 0s - loss: 0.2428
6080/6530 [==========================>...] - ETA: 0s - loss: 0.2391
2576/6530 [==========>...................] - ETA: 2s - loss: 0.4098
6528/6530 [============================>.] - ETA: 0s - loss: 0.2351
2944/6530 [============>.................] - ETA: 1s - loss: 0.3958
6530/6530 [==============================] - 2s 262us/step - loss: 0.2351 - val_loss: 0.1623
Epoch 2/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1679
3312/6530 [==============>...............] - ETA: 1s - loss: 0.3801
 432/6530 [>.............................] - ETA: 0s - loss: 0.1852
3680/6530 [===============>..............] - ETA: 1s - loss: 0.3678
 848/6530 [==>...........................] - ETA: 0s - loss: 0.1742
4000/6530 [=================>............] - ETA: 0s - loss: 0.3573
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1723
4320/6530 [==================>...........] - ETA: 0s - loss: 0.3485
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1715
4656/6530 [====================>.........] - ETA: 0s - loss: 0.3393
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1710
5008/6530 [======================>.......] - ETA: 0s - loss: 0.3325
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1720
5376/6530 [=======================>......] - ETA: 0s - loss: 0.3254
3056/6530 [=============>................] - ETA: 0s - loss: 0.1687
5744/6530 [=========================>....] - ETA: 0s - loss: 0.3195
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1670
6128/6530 [===========================>..] - ETA: 0s - loss: 0.3131
3968/6530 [=================>............] - ETA: 0s - loss: 0.1650
6480/6530 [============================>.] - ETA: 0s - loss: 0.3074
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1637
6530/6530 [==============================] - 2s 306us/step - loss: 0.3065 - val_loss: 0.1901
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1492
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1628
# training | RMSE: 0.2030, MAE: 0.1651
worker 0  xfile  [0, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17515930016558992}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4449677908740701}, 'layer_4_size': 91, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.2029579956553885, 'rmse': 0.2029579956553885, 'mae': 0.1651154108594286, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  33 | activation: relu    | extras: batchnorm 
layer 2 | size:  21 | activation: relu    | extras: dropout - rate: 21.7% 
layer 3 | size:   4 | activation: relu    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a763198>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 2:43 - loss: 0.5835
 368/6530 [>.............................] - ETA: 0s - loss: 0.2079
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1619
 320/6530 [>.............................] - ETA: 8s - loss: 0.4581  
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1986
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1602
 624/6530 [=>............................] - ETA: 4s - loss: 0.4215
1072/6530 [===>..........................] - ETA: 0s - loss: 0.2003
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1592
 896/6530 [===>..........................] - ETA: 3s - loss: 0.3891
1424/6530 [=====>........................] - ETA: 0s - loss: 0.2007
6530/6530 [==============================] - 1s 118us/step - loss: 0.1583 - val_loss: 0.1239
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1288
1184/6530 [====>.........................] - ETA: 2s - loss: 0.3636
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1997
 448/6530 [=>............................] - ETA: 0s - loss: 0.1362
1456/6530 [=====>........................] - ETA: 2s - loss: 0.3497
2160/6530 [========>.....................] - ETA: 0s - loss: 0.1974
 896/6530 [===>..........................] - ETA: 0s - loss: 0.1383
1744/6530 [=======>......................] - ETA: 1s - loss: 0.3378
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1968
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1359
2032/6530 [========>.....................] - ETA: 1s - loss: 0.3275
2848/6530 [============>.................] - ETA: 0s - loss: 0.1958
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1361
2336/6530 [=========>....................] - ETA: 1s - loss: 0.3189
3216/6530 [=============>................] - ETA: 0s - loss: 0.1951
2256/6530 [=========>....................] - ETA: 0s - loss: 0.1370
2640/6530 [===========>..................] - ETA: 1s - loss: 0.3111
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1941
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1376
2944/6530 [============>.................] - ETA: 1s - loss: 0.3032
3936/6530 [=================>............] - ETA: 0s - loss: 0.1933
3168/6530 [=============>................] - ETA: 0s - loss: 0.1358
3232/6530 [=============>................] - ETA: 0s - loss: 0.2965
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1931
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1346
3520/6530 [===============>..............] - ETA: 0s - loss: 0.2902
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1932
4064/6530 [=================>............] - ETA: 0s - loss: 0.1345
3808/6530 [================>.............] - ETA: 0s - loss: 0.2844
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1919
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1338
4112/6530 [=================>............] - ETA: 0s - loss: 0.2795
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1920
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1336
4416/6530 [===================>..........] - ETA: 0s - loss: 0.2736
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1908
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1337
4720/6530 [====================>.........] - ETA: 0s - loss: 0.2689
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1896
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1331
6496/6530 [============================>.] - ETA: 0s - loss: 0.1882
5024/6530 [======================>.......] - ETA: 0s - loss: 0.2645
6530/6530 [==============================] - 1s 146us/step - loss: 0.1882 - val_loss: 0.1538
Epoch 3/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1551
6352/6530 [============================>.] - ETA: 0s - loss: 0.1326
5328/6530 [=======================>......] - ETA: 0s - loss: 0.2612
6530/6530 [==============================] - 1s 117us/step - loss: 0.1321 - val_loss: 0.1141
Epoch 4/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0947
 384/6530 [>.............................] - ETA: 0s - loss: 0.1666
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2571
 464/6530 [=>............................] - ETA: 0s - loss: 0.1254
 752/6530 [==>...........................] - ETA: 0s - loss: 0.1747
5920/6530 [==========================>...] - ETA: 0s - loss: 0.2543
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1219
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1706
6224/6530 [===========================>..] - ETA: 0s - loss: 0.2509
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1177
1488/6530 [=====>........................] - ETA: 0s - loss: 0.1675
6528/6530 [============================>.] - ETA: 0s - loss: 0.2488
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1187
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1644
6530/6530 [==============================] - 2s 246us/step - loss: 0.2488 - val_loss: 0.1798
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1482
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1194
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1634
 320/6530 [>.............................] - ETA: 1s - loss: 0.1916
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1192
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1652
 624/6530 [=>............................] - ETA: 1s - loss: 0.1848
2928/6530 [============>.................] - ETA: 0s - loss: 0.1649
3184/6530 [=============>................] - ETA: 0s - loss: 0.1182
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1882
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1634
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1186
1232/6530 [====>.........................] - ETA: 0s - loss: 0.1848
4064/6530 [=================>............] - ETA: 0s - loss: 0.1183
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1642
1520/6530 [=====>........................] - ETA: 0s - loss: 0.1828
4016/6530 [=================>............] - ETA: 0s - loss: 0.1641
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1177
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1825
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1177
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1629
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1817
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1184
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1625
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1806
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1180
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1610
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1799
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1176
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1614
3008/6530 [============>.................] - ETA: 0s - loss: 0.1788
6530/6530 [==============================] - 1s 117us/step - loss: 0.1173 - val_loss: 0.0988
Epoch 5/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0621
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1612
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1783
 464/6530 [=>............................] - ETA: 0s - loss: 0.1141
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1607
 912/6530 [===>..........................] - ETA: 0s - loss: 0.1133
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1778
6530/6530 [==============================] - 1s 145us/step - loss: 0.1604 - val_loss: 0.1308
Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0999
3888/6530 [================>.............] - ETA: 0s - loss: 0.1776
1360/6530 [=====>........................] - ETA: 0s - loss: 0.1117
 368/6530 [>.............................] - ETA: 0s - loss: 0.1581
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1769
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1130
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1549
2224/6530 [=========>....................] - ETA: 0s - loss: 0.1122
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1766
1104/6530 [====>.........................] - ETA: 0s - loss: 0.1524
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1124
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1764
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1484
3120/6530 [=============>................] - ETA: 0s - loss: 0.1114
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1757
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1480
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1114
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1748
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1487
4000/6530 [=================>............] - ETA: 0s - loss: 0.1107
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1742
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1495
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1093
5968/6530 [==========================>...] - ETA: 0s - loss: 0.1745
2912/6530 [============>.................] - ETA: 0s - loss: 0.1492
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1089
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1743
3264/6530 [=============>................] - ETA: 0s - loss: 0.1490
5312/6530 [=======================>......] - ETA: 0s - loss: 0.1090
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1482
6530/6530 [==============================] - 1s 179us/step - loss: 0.1739 - val_loss: 0.1503
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1754
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1084
3984/6530 [=================>............] - ETA: 0s - loss: 0.1479
 320/6530 [>.............................] - ETA: 1s - loss: 0.1643
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1083
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1478
 624/6530 [=>............................] - ETA: 1s - loss: 0.1639
6530/6530 [==============================] - 1s 119us/step - loss: 0.1079 - val_loss: 0.0993
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0773
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1477
 912/6530 [===>..........................] - ETA: 0s - loss: 0.1640
 464/6530 [=>............................] - ETA: 0s - loss: 0.1025
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1471
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1624
 912/6530 [===>..........................] - ETA: 0s - loss: 0.1032
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1472
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1598
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1017
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1469
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1586
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1030
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1462
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1578
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1036
6528/6530 [============================>.] - ETA: 0s - loss: 0.1459
2384/6530 [=========>....................] - ETA: 0s - loss: 0.1565
6530/6530 [==============================] - 1s 145us/step - loss: 0.1460 - val_loss: 0.1328
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1326
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1050
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1577
 336/6530 [>.............................] - ETA: 0s - loss: 0.1326
3104/6530 [=============>................] - ETA: 0s - loss: 0.1040
2960/6530 [============>.................] - ETA: 0s - loss: 0.1572
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1350
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1040
3248/6530 [=============>................] - ETA: 0s - loss: 0.1567
3936/6530 [=================>............] - ETA: 0s - loss: 0.1030
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1379
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1571
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1023
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1371
3776/6530 [================>.............] - ETA: 0s - loss: 0.1567
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1022
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1369
4080/6530 [=================>............] - ETA: 0s - loss: 0.1567
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1022
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1370
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1568
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1020
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1377
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1566
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1020
2848/6530 [============>.................] - ETA: 0s - loss: 0.1376
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1567
3216/6530 [=============>................] - ETA: 0s - loss: 0.1383
6530/6530 [==============================] - 1s 120us/step - loss: 0.1021 - val_loss: 0.0944
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0728
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1556
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1379
 464/6530 [=>............................] - ETA: 0s - loss: 0.1014
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1553
3952/6530 [=================>............] - ETA: 0s - loss: 0.1379
 912/6530 [===>..........................] - ETA: 0s - loss: 0.1015
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1551
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1379
1360/6530 [=====>........................] - ETA: 0s - loss: 0.1004
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1545
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1371
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1011
6448/6530 [============================>.] - ETA: 0s - loss: 0.1541
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1369
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1006
6530/6530 [==============================] - 1s 182us/step - loss: 0.1538 - val_loss: 0.1380
Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1132
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1365
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1009
 304/6530 [>.............................] - ETA: 1s - loss: 0.1434
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1366
3104/6530 [=============>................] - ETA: 0s - loss: 0.0993
 608/6530 [=>............................] - ETA: 1s - loss: 0.1455
6128/6530 [===========================>..] - ETA: 0s - loss: 0.1359
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0994
 912/6530 [===>..........................] - ETA: 0s - loss: 0.1489
6496/6530 [============================>.] - ETA: 0s - loss: 0.1359
4000/6530 [=================>............] - ETA: 0s - loss: 0.0994
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1473
6530/6530 [==============================] - 1s 148us/step - loss: 0.1361 - val_loss: 0.1182
Epoch 6/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1435
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0989
1520/6530 [=====>........................] - ETA: 0s - loss: 0.1468
 384/6530 [>.............................] - ETA: 0s - loss: 0.1426
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0990
1808/6530 [=======>......................] - ETA: 0s - loss: 0.1483
 752/6530 [==>...........................] - ETA: 0s - loss: 0.1354
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0991
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1482
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1338
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0989
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1480
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1335
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0993
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1473
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1337
6530/6530 [==============================] - 1s 118us/step - loss: 0.0989 - val_loss: 0.0975
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0935
3008/6530 [============>.................] - ETA: 0s - loss: 0.1461
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1315
 448/6530 [=>............................] - ETA: 0s - loss: 0.1003
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1460
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1301
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0974
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1462
2896/6530 [============>.................] - ETA: 0s - loss: 0.1298
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0962
3888/6530 [================>.............] - ETA: 0s - loss: 0.1468
3264/6530 [=============>................] - ETA: 0s - loss: 0.1302
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0964
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1463
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1300
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0964
3984/6530 [=================>............] - ETA: 0s - loss: 0.1299
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1463
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0974
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1299
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1460
3056/6530 [=============>................] - ETA: 0s - loss: 0.0968
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1454
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1305
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0961
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1444
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1304
3968/6530 [=================>............] - ETA: 0s - loss: 0.0953
5616/6530 [========================>.....] - ETA: 0s - loss: 0.1439
5456/6530 [========================>.....] - ETA: 0s - loss: 0.1303
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0946
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1434
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1306
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0943
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1306
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1432
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0944
6512/6530 [============================>.] - ETA: 0s - loss: 0.1425
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0940
6530/6530 [==============================] - 1s 146us/step - loss: 0.1301 - val_loss: 0.1137
Epoch 7/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1254
6530/6530 [==============================] - 1s 180us/step - loss: 0.1425 - val_loss: 0.1229
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.2078
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0942
 368/6530 [>.............................] - ETA: 0s - loss: 0.1252
 320/6530 [>.............................] - ETA: 1s - loss: 0.1392
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1283
6530/6530 [==============================] - 1s 120us/step - loss: 0.0942 - val_loss: 0.0841
Epoch 9/9

  16/6530 [..............................] - ETA: 0s - loss: 0.0824
 624/6530 [=>............................] - ETA: 1s - loss: 0.1420
1104/6530 [====>.........................] - ETA: 0s - loss: 0.1301
 464/6530 [=>............................] - ETA: 0s - loss: 0.0952
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1423
1456/6530 [=====>........................] - ETA: 0s - loss: 0.1311
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0926
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1430
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1321
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0937
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1436
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1296
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0941
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1415
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1264
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0937
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1415
2928/6530 [============>.................] - ETA: 0s - loss: 0.1274
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0936
2384/6530 [=========>....................] - ETA: 0s - loss: 0.1398
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1277
3152/6530 [=============>................] - ETA: 0s - loss: 0.0921
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1398
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1277
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0921
2976/6530 [============>.................] - ETA: 0s - loss: 0.1391
4032/6530 [=================>............] - ETA: 0s - loss: 0.1271
4080/6530 [=================>............] - ETA: 0s - loss: 0.0915
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1392
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1279
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0912
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1393
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1284
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0917
3872/6530 [================>.............] - ETA: 0s - loss: 0.1382
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0919
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1286
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1376
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0915
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1289
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1373
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1295
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0923
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1368
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1293
6530/6530 [==============================] - 1s 117us/step - loss: 0.0921 - val_loss: 0.0837

5072/6530 [======================>.......] - ETA: 0s - loss: 0.1374
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1370
6530/6530 [==============================] - 1s 145us/step - loss: 0.1288 - val_loss: 0.1024
Epoch 8/9

  16/6530 [..............................] - ETA: 0s - loss: 0.1486
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1370
 352/6530 [>.............................] - ETA: 0s - loss: 0.1294
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1369
 720/6530 [==>...........................] - ETA: 0s - loss: 0.1290
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1369
1104/6530 [====>.........................] - ETA: 0s - loss: 0.1298
6528/6530 [============================>.] - ETA: 0s - loss: 0.1369
1488/6530 [=====>........................] - ETA: 0s - loss: 0.1292
6530/6530 [==============================] - 1s 178us/step - loss: 0.1369 - val_loss: 0.1322
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.2029
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1256
 320/6530 [>.............................] - ETA: 1s - loss: 0.1455
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1238
 624/6530 [=>............................] - ETA: 1s - loss: 0.1343
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1244
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1355
2976/6530 [============>.................] - ETA: 0s - loss: 0.1238
1232/6530 [====>.........................] - ETA: 0s - loss: 0.1354
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1232
# training | RMSE: 0.1023, MAE: 0.0790
worker 1  xfile  [1, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 92, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2149531878533118}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25752557491173744}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12246768342475414}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.10233769525544943, 'rmse': 0.10233769525544943, 'mae': 0.078952522035206, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  50 | activation: relu    | extras: None 
layer 2 | size:  82 | activation: sigmoid | extras: dropout - rate: 40.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a763b00>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 31s - loss: 1.9156
1536/6530 [======>.......................] - ETA: 0s - loss: 0.1335
3712/6530 [================>.............] - ETA: 0s - loss: 0.1233
1024/6530 [===>..........................] - ETA: 1s - loss: 0.8159 
1808/6530 [=======>......................] - ETA: 0s - loss: 0.1338
4048/6530 [=================>............] - ETA: 0s - loss: 0.1237
2016/6530 [========>.....................] - ETA: 0s - loss: 0.5042
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1351
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1245
2976/6530 [============>.................] - ETA: 0s - loss: 0.3970
2384/6530 [=========>....................] - ETA: 0s - loss: 0.1338
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1242
3968/6530 [=================>............] - ETA: 0s - loss: 0.3391
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1331
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1250
4928/6530 [=====================>........] - ETA: 0s - loss: 0.3054
2960/6530 [============>.................] - ETA: 0s - loss: 0.1322
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1247
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2803
3264/6530 [=============>................] - ETA: 0s - loss: 0.1324
5872/6530 [=========================>....] - ETA: 0s - loss: 0.1248
6530/6530 [==============================] - 1s 79us/step - loss: 0.2701 - val_loss: 0.2093
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.2223
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1327
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1244
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1540
3856/6530 [================>.............] - ETA: 0s - loss: 0.1314
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1518
6530/6530 [==============================] - 1s 146us/step - loss: 0.1242 - val_loss: 0.1140

4144/6530 [==================>...........] - ETA: 0s - loss: 0.1318Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1271
3008/6530 [============>.................] - ETA: 0s - loss: 0.1503
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1311
 384/6530 [>.............................] - ETA: 0s - loss: 0.1234
4000/6530 [=================>............] - ETA: 0s - loss: 0.1489
 752/6530 [==>...........................] - ETA: 0s - loss: 0.1258
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1303
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1483
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1271
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1299
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1484
1504/6530 [=====>........................] - ETA: 0s - loss: 0.1245
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1300
6530/6530 [==============================] - 0s 53us/step - loss: 0.1480 - val_loss: 0.2077
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.2346
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1234
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1296
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1462
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1230
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1294
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1411
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1232
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1292
3008/6530 [============>.................] - ETA: 0s - loss: 0.1381
2896/6530 [============>.................] - ETA: 0s - loss: 0.1223
6496/6530 [============================>.] - ETA: 0s - loss: 0.1290
3904/6530 [================>.............] - ETA: 0s - loss: 0.1363
6530/6530 [==============================] - 1s 181us/step - loss: 0.1290 - val_loss: 0.1083
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1353
3264/6530 [=============>................] - ETA: 0s - loss: 0.1225
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1354
 288/6530 [>.............................] - ETA: 1s - loss: 0.1351
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1233
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1342
 576/6530 [=>............................] - ETA: 1s - loss: 0.1350
3968/6530 [=================>............] - ETA: 0s - loss: 0.1218
6530/6530 [==============================] - 0s 54us/step - loss: 0.1342 - val_loss: 0.1327
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1153
 864/6530 [==>...........................] - ETA: 1s - loss: 0.1328
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1212
 992/6530 [===>..........................] - ETA: 0s - loss: 0.1276
1168/6530 [====>.........................] - ETA: 0s - loss: 0.1311
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1214
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1271
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1303
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1219
3040/6530 [============>.................] - ETA: 0s - loss: 0.1264
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1299
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1221
4032/6530 [=================>............] - ETA: 0s - loss: 0.1245
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1301
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1228
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1256
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1298
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1221
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1250
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1296
6464/6530 [============================>.] - ETA: 0s - loss: 0.1217
6530/6530 [==============================] - 0s 53us/step - loss: 0.1233 - val_loss: 0.1916
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1739
6530/6530 [==============================] - 1s 148us/step - loss: 0.1218 - val_loss: 0.1077

2944/6530 [============>.................] - ETA: 0s - loss: 0.1291
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1229
3248/6530 [=============>................] - ETA: 0s - loss: 0.1276
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1188
3008/6530 [============>.................] - ETA: 0s - loss: 0.1173
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1268
4000/6530 [=================>............] - ETA: 0s - loss: 0.1174
3776/6530 [================>.............] - ETA: 0s - loss: 0.1269
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1172
4080/6530 [=================>............] - ETA: 0s - loss: 0.1264
6048/6530 [==========================>...] - ETA: 0s - loss: 0.1150
4400/6530 [===================>..........] - ETA: 0s - loss: 0.1263
6530/6530 [==============================] - 0s 52us/step - loss: 0.1146 - val_loss: 0.1377
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1281
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1262
1024/6530 [===>..........................] - ETA: 0s - loss: 0.1090
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1258
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1100
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1257
3136/6530 [=============>................] - ETA: 0s - loss: 0.1099
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1259
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1087
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1258
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1084
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1258
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1082
6530/6530 [==============================] - 0s 51us/step - loss: 0.1080 - val_loss: 0.2738
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.2767
6530/6530 [==============================] - 1s 176us/step - loss: 0.1258 - val_loss: 0.1097
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1298
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1081
 320/6530 [>.............................] - ETA: 1s - loss: 0.1308
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1056
 624/6530 [=>............................] - ETA: 0s - loss: 0.1322
3008/6530 [============>.................] - ETA: 0s - loss: 0.1045
 944/6530 [===>..........................] - ETA: 0s - loss: 0.1301
4032/6530 [=================>............] - ETA: 0s - loss: 0.1033
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1298
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1042
1552/6530 [======>.......................] - ETA: 0s - loss: 0.1279
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1032
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1284
6530/6530 [==============================] - 0s 52us/step - loss: 0.1031 - val_loss: 0.1061
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0942
# training | RMSE: 0.1276, MAE: 0.0990
worker 2  xfile  [2, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 96, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.1276350782464865, 'rmse': 0.1276350782464865, 'mae': 0.09896830265097417, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  75 | activation: relu    | extras: None 
layer 2 | size:  37 | activation: relu    | extras: None 
layer 3 | size:  66 | activation: relu    | extras: dropout - rate: 36.9% 
layer 4 | size:  98 | activation: tanh    | extras: dropout - rate: 44.2% 
layer 5 | size:  11 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a7630b8>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 1:06 - loss: 0.1797
2160/6530 [========>.....................] - ETA: 0s - loss: 0.1282
 672/6530 [==>...........................] - ETA: 3s - loss: 0.2047  
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1021
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1278
1312/6530 [=====>........................] - ETA: 1s - loss: 0.2055
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1011
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1267
1952/6530 [=======>......................] - ETA: 1s - loss: 0.2015
3168/6530 [=============>................] - ETA: 0s - loss: 0.1001
3072/6530 [=============>................] - ETA: 0s - loss: 0.1261
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1985
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0998
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1267
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0993
3264/6530 [=============>................] - ETA: 0s - loss: 0.1953
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1272
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0991
3936/6530 [=================>............] - ETA: 0s - loss: 0.1901
6530/6530 [==============================] - 0s 51us/step - loss: 0.0987 - val_loss: 0.1776
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1922
3968/6530 [=================>............] - ETA: 0s - loss: 0.1270
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1888
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0992
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1264
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1852
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0999
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1268
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1837
3104/6530 [=============>................] - ETA: 0s - loss: 0.1003
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1276
6528/6530 [============================>.] - ETA: 0s - loss: 0.1813
4128/6530 [=================>............] - ETA: 0s - loss: 0.0991
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1277
6530/6530 [==============================] - 1s 136us/step - loss: 0.1812 - val_loss: 0.3563
Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.4407
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0989
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1277
 640/6530 [=>............................] - ETA: 0s - loss: 0.1776
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0986
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1274
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1664
6530/6530 [==============================] - 0s 51us/step - loss: 0.0986 - val_loss: 0.1018

6080/6530 [==========================>...] - ETA: 0s - loss: 0.1272
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1595
6384/6530 [============================>.] - ETA: 0s - loss: 0.1266
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1556
6530/6530 [==============================] - 1s 175us/step - loss: 0.1262 - val_loss: 0.1082
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1825
3264/6530 [=============>................] - ETA: 0s - loss: 0.1534
 304/6530 [>.............................] - ETA: 1s - loss: 0.1188
3936/6530 [=================>............] - ETA: 0s - loss: 0.1514
 576/6530 [=>............................] - ETA: 1s - loss: 0.1192
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1502
 848/6530 [==>...........................] - ETA: 1s - loss: 0.1204
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1492
1136/6530 [====>.........................] - ETA: 0s - loss: 0.1233
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1480
1424/6530 [=====>........................] - ETA: 0s - loss: 0.1238
6432/6530 [============================>.] - ETA: 0s - loss: 0.1473
6530/6530 [==============================] - 1s 83us/step - loss: 0.1472 - val_loss: 0.2256
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.2243
1712/6530 [======>.......................] - ETA: 0s - loss: 0.1239
 640/6530 [=>............................] - ETA: 0s - loss: 0.1460
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1235
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1416
2320/6530 [=========>....................] - ETA: 0s - loss: 0.1235
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1382
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1237
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1367
# training | RMSE: 0.1263, MAE: 0.0936
worker 1  xfile  [4, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4082346454778355}, 'layer_2_size': 82, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4268159776129282}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 39, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.12626651994845878, 'rmse': 0.12626651994845878, 'mae': 0.09363103322974199, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adamax',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  70 | activation: relu    | extras: dropout - rate: 47.8% 
layer 2 | size:  91 | activation: tanh    | extras: None 
layer 3 | size:  60 | activation: tanh    | extras: dropout - rate: 30.8% 
layer 4 | size:  24 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f77799e7710>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 5s - loss: 0.6992
2896/6530 [============>.................] - ETA: 0s - loss: 0.1226
3200/6530 [=============>................] - ETA: 0s - loss: 0.1357
3184/6530 [=============>................] - ETA: 0s - loss: 0.1222
6530/6530 [==============================] - 0s 50us/step - loss: 0.4386 - val_loss: 0.2120
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2173
3840/6530 [================>.............] - ETA: 0s - loss: 0.1346
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1219
6530/6530 [==============================] - 0s 8us/step - loss: 0.2116 - val_loss: 0.1972

4448/6530 [===================>..........] - ETA: 0s - loss: 0.1337Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1933
3744/6530 [================>.............] - ETA: 0s - loss: 0.1222
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1330
6530/6530 [==============================] - 0s 8us/step - loss: 0.1883 - val_loss: 0.1718
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1810
4000/6530 [=================>............] - ETA: 0s - loss: 0.1224
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1324
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1667
6530/6530 [==============================] - 0s 9us/step - loss: 0.1667 - val_loss: 0.1603
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1733
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1217
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1321
6400/6530 [============================>.] - ETA: 0s - loss: 0.1581
6530/6530 [==============================] - 0s 9us/step - loss: 0.1580 - val_loss: 0.1576
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1591
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1218
6530/6530 [==============================] - 1s 85us/step - loss: 0.1316 - val_loss: 0.1187
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0987
6530/6530 [==============================] - 0s 8us/step - loss: 0.1544 - val_loss: 0.1551
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1648
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1218
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1193
6400/6530 [============================>.] - ETA: 0s - loss: 0.1520
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1215
6530/6530 [==============================] - 0s 9us/step - loss: 0.1520 - val_loss: 0.1533
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1498
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1201
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1208
6530/6530 [==============================] - 0s 8us/step - loss: 0.1506 - val_loss: 0.1515
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.1487
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1213
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1203
6400/6530 [============================>.] - ETA: 0s - loss: 0.1491
6530/6530 [==============================] - 0s 9us/step - loss: 0.1492 - val_loss: 0.1500

2528/6530 [==========>...................] - ETA: 0s - loss: 0.1203
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1204
3168/6530 [=============>................] - ETA: 0s - loss: 0.1212
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1205
3808/6530 [================>.............] - ETA: 0s - loss: 0.1212
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1206
6530/6530 [==============================] - 1s 187us/step - loss: 0.1207 - val_loss: 0.1064

5120/6530 [======================>.......] - ETA: 0s - loss: 0.1200
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1198
6400/6530 [============================>.] - ETA: 0s - loss: 0.1192
6530/6530 [==============================] - 1s 83us/step - loss: 0.1194 - val_loss: 0.1243
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1270
 640/6530 [=>............................] - ETA: 0s - loss: 0.1149
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1152
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1135
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1122
3264/6530 [=============>................] - ETA: 0s - loss: 0.1117
3968/6530 [=================>............] - ETA: 0s - loss: 0.1107
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1103
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1093
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1093
6400/6530 [============================>.] - ETA: 0s - loss: 0.1089
6530/6530 [==============================] - 1s 82us/step - loss: 0.1086 - val_loss: 0.1082
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1076
 736/6530 [==>...........................] - ETA: 0s - loss: 0.1066
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1057
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1063
# training | RMSE: 0.1872, MAE: 0.1427
worker 1  xfile  [6, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47825508775226755}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30764941843595073}, 'layer_3_size': 60, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 24, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.39756340213759234}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.18715969315490485, 'rmse': 0.18715969315490485, 'mae': 0.14265073252949512, 'early_stop': False}
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  64 | activation: relu    | extras: batchnorm 
layer 2 | size:  13 | activation: tanh    | extras: batchnorm 
layer 3 | size:  25 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f776936de48>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 10s - loss: 0.1110
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1069
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0400 
2976/6530 [============>.................] - ETA: 0s - loss: 0.1066
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1068
6530/6530 [==============================] - 1s 82us/step - loss: 0.0370 - val_loss: 0.0291
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0258
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1054
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0232
6530/6530 [==============================] - 0s 12us/step - loss: 0.0231 - val_loss: 0.0228
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0208
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1056
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0188
6530/6530 [==============================] - 0s 11us/step - loss: 0.0186 - val_loss: 0.0193
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0174
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1045
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0159
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1047
6530/6530 [==============================] - 0s 11us/step - loss: 0.0160 - val_loss: 0.0173
Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0153
6530/6530 [==============================] - 1s 87us/step - loss: 0.1043 - val_loss: 0.0935
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1052
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0143
6530/6530 [==============================] - 0s 12us/step - loss: 0.0145 - val_loss: 0.0160
Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0139
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1038
# training | RMSE: 0.1286, MAE: 0.1018
worker 0  xfile  [3, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21691068773488237}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.27617154374503416}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.1285649704712358, 'rmse': 0.1285649704712358, 'mae': 0.10179411007870823, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  44 | activation: relu    | extras: dropout - rate: 10.5% 
layer 2 | size:  40 | activation: relu    | extras: dropout - rate: 10.4% 
layer 3 | size:  10 | activation: relu    | extras: batchnorm 
layer 4 | size:  59 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7778144128>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 3:40 - loss: 0.8391
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0132
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1011
6530/6530 [==============================] - 0s 13us/step - loss: 0.0134 - val_loss: 0.0152
Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0129
 256/6530 [>.............................] - ETA: 14s - loss: 0.5061 
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0983
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0124
 496/6530 [=>............................] - ETA: 7s - loss: 0.3984 
6530/6530 [==============================] - 0s 12us/step - loss: 0.0126 - val_loss: 0.0145

2560/6530 [==========>...................] - ETA: 0s - loss: 0.0981Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0121
 768/6530 [==>...........................] - ETA: 5s - loss: 0.3189
3200/6530 [=============>................] - ETA: 0s - loss: 0.0982
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0118
6530/6530 [==============================] - 0s 12us/step - loss: 0.0120 - val_loss: 0.0141
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.0115
1024/6530 [===>..........................] - ETA: 4s - loss: 0.2644
3808/6530 [================>.............] - ETA: 0s - loss: 0.0983
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0113
1280/6530 [====>.........................] - ETA: 3s - loss: 0.2298
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0985
6530/6530 [==============================] - 0s 12us/step - loss: 0.0115 - val_loss: 0.0136

1536/6530 [======>.......................] - ETA: 2s - loss: 0.2025
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0990
1808/6530 [=======>......................] - ETA: 2s - loss: 0.1814
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0984
2064/6530 [========>.....................] - ETA: 2s - loss: 0.1670
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0986
2320/6530 [=========>....................] - ETA: 1s - loss: 0.1548
6530/6530 [==============================] - 1s 85us/step - loss: 0.0981 - val_loss: 0.1205
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1299
2576/6530 [==========>...................] - ETA: 1s - loss: 0.1453
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0963
2848/6530 [============>.................] - ETA: 1s - loss: 0.1373
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0952
3120/6530 [=============>................] - ETA: 1s - loss: 0.1298
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0953
3392/6530 [==============>...............] - ETA: 1s - loss: 0.1239
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0949
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1187
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0960
3920/6530 [=================>............] - ETA: 0s - loss: 0.1138
3904/6530 [================>.............] - ETA: 0s - loss: 0.0955
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1105
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0950
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1073
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0947
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1042
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0947
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1015
6432/6530 [============================>.] - ETA: 0s - loss: 0.0940
6530/6530 [==============================] - 1s 83us/step - loss: 0.0940 - val_loss: 0.1309
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1489
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0991
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0933
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0970
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0933
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0948
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0909
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0925
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0910
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0904
3232/6530 [=============>................] - ETA: 0s - loss: 0.0914
# training | RMSE: 0.1023, MAE: 0.0781
worker 1  xfile  [7, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 2, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.10233468591449654, 'rmse': 0.10233468591449654, 'mae': 0.0781294667531916, 'early_stop': False}
{'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  30 | activation: tanh    | extras: batchnorm 
layer 2 | size:  63 | activation: tanh    | extras: dropout - rate: 10.0% 
layer 3 | size:  46 | activation: sigmoid | extras: dropout - rate: 41.7% 

xnet is <keras.engine.sequential.Sequential object at 0x7f776936dda0>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 128/6530 [..............................] - ETA: 23s - loss: 0.3650
6496/6530 [============================>.] - ETA: 0s - loss: 0.0885
3840/6530 [================>.............] - ETA: 0s - loss: 0.0907
2560/6530 [==========>...................] - ETA: 0s - loss: 0.2356 
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0906
6530/6530 [==============================] - 2s 294us/step - loss: 0.0882 - val_loss: 0.0445
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0655
4992/6530 [=====================>........] - ETA: 0s - loss: 0.2112
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0908
 288/6530 [>.............................] - ETA: 1s - loss: 0.0467
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0898
6530/6530 [==============================] - 1s 100us/step - loss: 0.2026 - val_loss: 0.1546
Epoch 2/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1718
 560/6530 [=>............................] - ETA: 1s - loss: 0.0431
6368/6530 [============================>.] - ETA: 0s - loss: 0.0899
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1678
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0454
6530/6530 [==============================] - 1s 85us/step - loss: 0.0898 - val_loss: 0.0908

5120/6530 [======================>.......] - ETA: 0s - loss: 0.1650
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0461
6530/6530 [==============================] - 0s 22us/step - loss: 0.1626 - val_loss: 0.1411
Epoch 3/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1553
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0464
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1539
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0456
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1519
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0449
6530/6530 [==============================] - 0s 22us/step - loss: 0.1496 - val_loss: 0.1314
Epoch 4/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1419
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0447
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1413
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0443
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1382
6530/6530 [==============================] - 0s 21us/step - loss: 0.1371 - val_loss: 0.1209

2656/6530 [===========>..................] - ETA: 0s - loss: 0.0440Epoch 5/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1448
2896/6530 [============>.................] - ETA: 0s - loss: 0.0438
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1311
3136/6530 [=============>................] - ETA: 0s - loss: 0.0433
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1292
# training | RMSE: 0.1070, MAE: 0.0830
worker 2  xfile  [5, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3690655139133455}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44187056290450344}, 'layer_4_size': 98, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.10696459894567822, 'rmse': 0.10696459894567822, 'mae': 0.08299957387751915, 'early_stop': False}
{'batch_size': 64,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  89 | activation: sigmoid | extras: None 
layer 2 | size:  25 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7779b2ba58>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  64/6530 [..............................] - ETA: 16s - loss: 0.7004
6530/6530 [==============================] - 0s 22us/step - loss: 0.1283 - val_loss: 0.1161
Epoch 6/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1269
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0427
2368/6530 [=========>....................] - ETA: 0s - loss: 0.2165 
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1248
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0427
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1416
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1229
3920/6530 [=================>............] - ETA: 0s - loss: 0.0425
6530/6530 [==============================] - 0s 22us/step - loss: 0.1221 - val_loss: 0.1095

6530/6530 [==============================] - 0s 51us/step - loss: 0.1192 - val_loss: 0.0629
Epoch 2/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0774Epoch 7/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1274
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0428
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0633
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1210
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0426
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0594
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1177
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0425
6530/6530 [==============================] - 0s 22us/step - loss: 0.1174 - val_loss: 0.1085
Epoch 8/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1199
6530/6530 [==============================] - 0s 24us/step - loss: 0.0576 - val_loss: 0.0532
Epoch 3/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0653
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0421
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1141
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0507
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0418
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1125
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0482
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0416
6530/6530 [==============================] - 0s 22us/step - loss: 0.1129 - val_loss: 0.1042
Epoch 9/9

 128/6530 [..............................] - ETA: 0s - loss: 0.1161
6530/6530 [==============================] - 0s 23us/step - loss: 0.0468 - val_loss: 0.0498
Epoch 4/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0561
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0413
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1128
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0434
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0410
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1098
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0426
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0409
6530/6530 [==============================] - 0s 22us/step - loss: 0.1091 - val_loss: 0.1040

6530/6530 [==============================] - 0s 22us/step - loss: 0.0422 - val_loss: 0.0504
Epoch 5/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0543
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0417
6530/6530 [==============================] - 1s 202us/step - loss: 0.0406 - val_loss: 0.0385
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0316
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0416
 288/6530 [>.............................] - ETA: 1s - loss: 0.0338
6530/6530 [==============================] - 0s 22us/step - loss: 0.0414 - val_loss: 0.0509
Epoch 6/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0545
 560/6530 [=>............................] - ETA: 1s - loss: 0.0340
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0415
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0343
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0416
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0349
6530/6530 [==============================] - 0s 21us/step - loss: 0.0413 - val_loss: 0.0510
Epoch 7/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0547
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0353
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0413
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0340
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0415
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0339
6530/6530 [==============================] - 0s 22us/step - loss: 0.0412 - val_loss: 0.0511
Epoch 8/9

  64/6530 [..............................] - ETA: 0s - loss: 0.0548
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0343
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0413
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0336
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0415
6530/6530 [==============================] - 0s 22us/step - loss: 0.0412 - val_loss: 0.0511

2656/6530 [===========>..................] - ETA: 0s - loss: 0.0337
2896/6530 [============>.................] - ETA: 0s - loss: 0.0334
3168/6530 [=============>................] - ETA: 0s - loss: 0.0334
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0331
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0329
# training | RMSE: 0.1247, MAE: 0.0960
worker 1  xfile  [9, 9.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10044009068957292}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4170382118349151}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3165420015541014}, 'layer_4_size': 32, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2629199363211525}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.12474897352841811, 'rmse': 0.12474897352841811, 'mae': 0.09599872937682807, 'early_stop': False}
{'batch_size': 256,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  45 | activation: relu    | extras: None 
layer 2 | size:  25 | activation: tanh    | extras: dropout - rate: 28.2% 
layer 3 | size:  97 | activation: tanh    | extras: dropout - rate: 14.1% 

xnet is <keras.engine.sequential.Sequential object at 0x7f772007d438>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

 256/6530 [>.............................] - ETA: 8s - loss: 0.9914
3936/6530 [=================>............] - ETA: 0s - loss: 0.0327
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0323
6530/6530 [==============================] - 0s 65us/step - loss: 0.5797 - val_loss: 0.2629
Epoch 2/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.3712
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0323
6530/6530 [==============================] - 0s 8us/step - loss: 0.3199 - val_loss: 0.1979
Epoch 3/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2752
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0323
6530/6530 [==============================] - 0s 8us/step - loss: 0.2541 - val_loss: 0.1806
Epoch 4/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2411
6530/6530 [==============================] - 0s 7us/step - loss: 0.2339 - val_loss: 0.1692

4912/6530 [=====================>........] - ETA: 0s - loss: 0.0321Epoch 5/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2306
6530/6530 [==============================] - 0s 8us/step - loss: 0.2179 - val_loss: 0.1570

5168/6530 [======================>.......] - ETA: 0s - loss: 0.0318Epoch 6/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2056
6530/6530 [==============================] - 0s 8us/step - loss: 0.2099 - val_loss: 0.1541

5424/6530 [=======================>......] - ETA: 0s - loss: 0.0316Epoch 7/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2130
6530/6530 [==============================] - 0s 7us/step - loss: 0.2005 - val_loss: 0.1459
Epoch 8/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2003
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0314
6530/6530 [==============================] - 0s 8us/step - loss: 0.1921 - val_loss: 0.1353
Epoch 9/9

 256/6530 [>.............................] - ETA: 0s - loss: 0.2029
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0311
6530/6530 [==============================] - 0s 8us/step - loss: 0.1856 - val_loss: 0.1414

6224/6530 [===========================>..] - ETA: 0s - loss: 0.0311
6480/6530 [============================>.] - ETA: 0s - loss: 0.0309
6530/6530 [==============================] - 1s 204us/step - loss: 0.0310 - val_loss: 0.0218
Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0259
 288/6530 [>.............................] - ETA: 1s - loss: 0.0337
 576/6530 [=>............................] - ETA: 1s - loss: 0.0321
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0311
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0309
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0302
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0298
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0299
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0289
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0289
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0285
2960/6530 [============>.................] - ETA: 0s - loss: 0.0286
3264/6530 [=============>................] - ETA: 0s - loss: 0.0281
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0281
# training | RMSE: 0.2251, MAE: 0.1870
worker 2  xfile  [10, 9.0, 0, 100, [], {'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 25, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 67, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.22511023019269394, 'rmse': 0.22511023019269394, 'mae': 0.1870310139398925, 'early_stop': True}
{'batch_size': 16,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size: 100 | activation: tanh    | extras: batchnorm 
layer 2 | size:  10 | activation: sigmoid | extras: dropout - rate: 17.4% 
layer 3 | size:  16 | activation: tanh    | extras: batchnorm 
layer 4 | size:   4 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f7779b2b780>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 4:13 - loss: 1.3717
3792/6530 [================>.............] - ETA: 0s - loss: 0.0276
 240/6530 [>.............................] - ETA: 17s - loss: 1.2793 
4064/6530 [=================>............] - ETA: 0s - loss: 0.0276
 448/6530 [=>............................] - ETA: 9s - loss: 1.0746 
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0278
 672/6530 [==>...........................] - ETA: 6s - loss: 0.8853
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0278
 880/6530 [===>..........................] - ETA: 5s - loss: 0.7840
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0275
1104/6530 [====>.........................] - ETA: 4s - loss: 0.6802
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0275
1312/6530 [=====>........................] - ETA: 3s - loss: 0.6063
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0272
1536/6530 [======>.......................] - ETA: 3s - loss: 0.5512
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0272
1760/6530 [=======>......................] - ETA: 2s - loss: 0.5039
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0272
1968/6530 [========>.....................] - ETA: 2s - loss: 0.4661
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0270
2192/6530 [=========>....................] - ETA: 2s - loss: 0.4325
6400/6530 [============================>.] - ETA: 0s - loss: 0.0270
2416/6530 [==========>...................] - ETA: 2s - loss: 0.4044
6530/6530 [==============================] - 1s 199us/step - loss: 0.0271 - val_loss: 0.0210
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0243
2624/6530 [===========>..................] - ETA: 1s - loss: 0.3819
# training | RMSE: 0.1753, MAE: 0.1383
worker 1  xfile  [11, 9.0, 0, 100, [], {'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28236583769424184}, 'layer_2_size': 25, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14136913317619815}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20072288229487062}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.1752743333565342, 'rmse': 0.1752743333565342, 'mae': 0.138287642760544, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  29 | activation: tanh    | extras: batchnorm 
layer 2 | size:  58 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  64 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f74340fddd8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  16/6530 [..............................] - ETA: 4:26 - loss: 0.8299
 272/6530 [>.............................] - ETA: 1s - loss: 0.0263
 272/6530 [>.............................] - ETA: 16s - loss: 0.4143 
2848/6530 [============>.................] - ETA: 1s - loss: 0.3602
 544/6530 [=>............................] - ETA: 1s - loss: 0.0266
 528/6530 [=>............................] - ETA: 8s - loss: 0.3661 
3072/6530 [=============>................] - ETA: 1s - loss: 0.3399
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0252
 784/6530 [==>...........................] - ETA: 5s - loss: 0.3300
3296/6530 [==============>...............] - ETA: 1s - loss: 0.3245
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0245
1040/6530 [===>..........................] - ETA: 4s - loss: 0.3110
3520/6530 [===============>..............] - ETA: 1s - loss: 0.3101
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0246
1296/6530 [====>.........................] - ETA: 3s - loss: 0.2935
3744/6530 [================>.............] - ETA: 1s - loss: 0.2967
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0248
1552/6530 [======>.......................] - ETA: 3s - loss: 0.2842
3968/6530 [=================>............] - ETA: 1s - loss: 0.2851
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0248
1808/6530 [=======>......................] - ETA: 2s - loss: 0.2756
4192/6530 [==================>...........] - ETA: 0s - loss: 0.2744
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0246
2064/6530 [========>.....................] - ETA: 2s - loss: 0.2705
4416/6530 [===================>..........] - ETA: 0s - loss: 0.2649
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0246
2320/6530 [=========>....................] - ETA: 2s - loss: 0.2648
4640/6530 [====================>.........] - ETA: 0s - loss: 0.2560
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0241
2560/6530 [==========>...................] - ETA: 1s - loss: 0.2599
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2474
2880/6530 [============>.................] - ETA: 0s - loss: 0.0245
2816/6530 [===========>..................] - ETA: 1s - loss: 0.2554
5072/6530 [======================>.......] - ETA: 0s - loss: 0.2400
3152/6530 [=============>................] - ETA: 0s - loss: 0.0242
3056/6530 [=============>................] - ETA: 1s - loss: 0.2506
5296/6530 [=======================>......] - ETA: 0s - loss: 0.2327
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0242
3312/6530 [==============>...............] - ETA: 1s - loss: 0.2467
5520/6530 [========================>.....] - ETA: 0s - loss: 0.2265
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0244
3552/6530 [===============>..............] - ETA: 1s - loss: 0.2434
5728/6530 [=========================>....] - ETA: 0s - loss: 0.2211
3936/6530 [=================>............] - ETA: 0s - loss: 0.0242
3792/6530 [================>.............] - ETA: 1s - loss: 0.2404
5952/6530 [==========================>...] - ETA: 0s - loss: 0.2156
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0241
4032/6530 [=================>............] - ETA: 0s - loss: 0.2365
6160/6530 [===========================>..] - ETA: 0s - loss: 0.2106
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0242
4272/6530 [==================>...........] - ETA: 0s - loss: 0.2338
6384/6530 [============================>.] - ETA: 0s - loss: 0.2055
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0242
4528/6530 [===================>..........] - ETA: 0s - loss: 0.2316
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0240
4768/6530 [====================>.........] - ETA: 0s - loss: 0.2288
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0239
5008/6530 [======================>.......] - ETA: 0s - loss: 0.2271
6530/6530 [==============================] - 2s 350us/step - loss: 0.2023 - val_loss: 0.0426
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0722
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0240
5264/6530 [=======================>......] - ETA: 0s - loss: 0.2240
 224/6530 [>.............................] - ETA: 1s - loss: 0.0564
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0242
5520/6530 [========================>.....] - ETA: 0s - loss: 0.2226
 432/6530 [>.............................] - ETA: 1s - loss: 0.0581
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0242
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2217
 640/6530 [=>............................] - ETA: 1s - loss: 0.0642
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0240
6016/6530 [==========================>...] - ETA: 0s - loss: 0.2200
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0637
6480/6530 [============================>.] - ETA: 0s - loss: 0.0242
6256/6530 [===========================>..] - ETA: 0s - loss: 0.2189
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0642
6530/6530 [==============================] - 1s 206us/step - loss: 0.0242 - val_loss: 0.0161
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0143
6496/6530 [============================>.] - ETA: 0s - loss: 0.2169
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0622
 272/6530 [>.............................] - ETA: 1s - loss: 0.0276
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0603
 528/6530 [=>............................] - ETA: 1s - loss: 0.0255
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0600
6530/6530 [==============================] - 2s 323us/step - loss: 0.2166 - val_loss: 0.1601
Epoch 2/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1594
 768/6530 [==>...........................] - ETA: 1s - loss: 0.0242
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0597
 256/6530 [>.............................] - ETA: 1s - loss: 0.1844
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0235
2144/6530 [========>.....................] - ETA: 1s - loss: 0.0597
 512/6530 [=>............................] - ETA: 1s - loss: 0.1802
1280/6530 [====>.........................] - ETA: 1s - loss: 0.0239
2368/6530 [=========>....................] - ETA: 1s - loss: 0.0587
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1812
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0241
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0594
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1778
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0239
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0584
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1761
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0235
3024/6530 [============>.................] - ETA: 0s - loss: 0.0576
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1794
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0234
3232/6530 [=============>................] - ETA: 0s - loss: 0.0569
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1788
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0233
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1755
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0572
2848/6530 [============>.................] - ETA: 0s - loss: 0.0228
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1735
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0568
3104/6530 [=============>................] - ETA: 0s - loss: 0.0228
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1730
3904/6530 [================>.............] - ETA: 0s - loss: 0.0565
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0227
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1726
4128/6530 [=================>............] - ETA: 0s - loss: 0.0564
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0227
3072/6530 [=============>................] - ETA: 0s - loss: 0.1738
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0558
3888/6530 [================>.............] - ETA: 0s - loss: 0.0227
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0562
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1746
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0225
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0559
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1735
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0225
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0554
3872/6530 [================>.............] - ETA: 0s - loss: 0.1732
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0223
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0552
4128/6530 [=================>............] - ETA: 0s - loss: 0.1722
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0222
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0551
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1715
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0224
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0548
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1709
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0224
4896/6530 [=====================>........] - ETA: 0s - loss: 0.1708
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0545
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0222
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1701
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0543
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0223
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1691
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0540
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0222
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1682
6528/6530 [============================>.] - ETA: 0s - loss: 0.0534
6530/6530 [==============================] - 2s 246us/step - loss: 0.0534 - val_loss: 0.0383
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0599
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1681
6530/6530 [==============================] - 1s 203us/step - loss: 0.0223 - val_loss: 0.0170
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0287
 240/6530 [>.............................] - ETA: 1s - loss: 0.0417
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1685
 288/6530 [>.............................] - ETA: 1s - loss: 0.0204
 464/6530 [=>............................] - ETA: 1s - loss: 0.0459
6416/6530 [============================>.] - ETA: 0s - loss: 0.1677
 544/6530 [=>............................] - ETA: 1s - loss: 0.0202
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0458
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0201
6530/6530 [==============================] - 1s 209us/step - loss: 0.1677 - val_loss: 0.1379
Epoch 3/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1496
 912/6530 [===>..........................] - ETA: 1s - loss: 0.0479
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0209
 256/6530 [>.............................] - ETA: 1s - loss: 0.1392
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0486
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0216
 512/6530 [=>............................] - ETA: 1s - loss: 0.1442
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0470
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0215
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1514
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0456
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0216
1008/6530 [===>..........................] - ETA: 1s - loss: 0.1497
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0452
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0215
1264/6530 [====>.........................] - ETA: 1s - loss: 0.1512
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0452
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0213
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1535
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0457
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0217
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1555
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0452
2848/6530 [============>.................] - ETA: 0s - loss: 0.0216
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1572
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0458
3040/6530 [============>.................] - ETA: 0s - loss: 0.0216
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1567
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0458
3264/6530 [=============>................] - ETA: 0s - loss: 0.0214
2384/6530 [=========>....................] - ETA: 0s - loss: 0.1571
2928/6530 [============>.................] - ETA: 0s - loss: 0.0450
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0213
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1572
3104/6530 [=============>................] - ETA: 0s - loss: 0.0442
3728/6530 [================>.............] - ETA: 0s - loss: 0.0213
2832/6530 [============>.................] - ETA: 0s - loss: 0.1570
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0443
3968/6530 [=================>............] - ETA: 0s - loss: 0.0210
3056/6530 [=============>................] - ETA: 0s - loss: 0.1560
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0443
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0210
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1555
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0441
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0209
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1557
3888/6530 [================>.............] - ETA: 0s - loss: 0.0443
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0208
3792/6530 [================>.............] - ETA: 0s - loss: 0.1553
4096/6530 [=================>............] - ETA: 0s - loss: 0.0442
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0209
4032/6530 [=================>............] - ETA: 0s - loss: 0.1543
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0439
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0209
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1535
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0439
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0209
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1530
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0438
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0209
4784/6530 [====================>.........] - ETA: 0s - loss: 0.1532
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0434
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0210
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1533
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0432
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0209
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1530
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0432
6464/6530 [============================>.] - ETA: 0s - loss: 0.0208
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1529
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0431
6530/6530 [==============================] - 1s 215us/step - loss: 0.0209 - val_loss: 0.0135
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0133
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1525
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0431
 272/6530 [>.............................] - ETA: 1s - loss: 0.0192
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1525
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0431
 528/6530 [=>............................] - ETA: 1s - loss: 0.0193
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1519
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0430
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0193
6496/6530 [============================>.] - ETA: 0s - loss: 0.1516
6496/6530 [============================>.] - ETA: 0s - loss: 0.0428
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0184
6530/6530 [==============================] - 1s 224us/step - loss: 0.1518 - val_loss: 0.1419
Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1757
6530/6530 [==============================] - 2s 257us/step - loss: 0.0428 - val_loss: 0.0326

1312/6530 [=====>........................] - ETA: 1s - loss: 0.0187Epoch 4/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0498
 272/6530 [>.............................] - ETA: 1s - loss: 0.1540
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0183
 240/6530 [>.............................] - ETA: 1s - loss: 0.0391
 512/6530 [=>............................] - ETA: 1s - loss: 0.1496
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0188
 448/6530 [=>............................] - ETA: 1s - loss: 0.0416
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1414
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0190
 656/6530 [==>...........................] - ETA: 1s - loss: 0.0432
1008/6530 [===>..........................] - ETA: 1s - loss: 0.1421
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0189
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0414
1264/6530 [====>.........................] - ETA: 1s - loss: 0.1434
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0191
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0424
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1414
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0190
1296/6530 [====>.........................] - ETA: 1s - loss: 0.0408
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1425
3040/6530 [============>.................] - ETA: 0s - loss: 0.0192
1504/6530 [=====>........................] - ETA: 1s - loss: 0.0401
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1426
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0189
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0396
2256/6530 [=========>....................] - ETA: 0s - loss: 0.1414
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0188
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0395
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1410
3744/6530 [================>.............] - ETA: 0s - loss: 0.0187
2096/6530 [========>.....................] - ETA: 1s - loss: 0.0392
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1406
3968/6530 [=================>............] - ETA: 0s - loss: 0.0186
2288/6530 [=========>....................] - ETA: 1s - loss: 0.0399
2960/6530 [============>.................] - ETA: 0s - loss: 0.1400
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0188
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0403
3216/6530 [=============>................] - ETA: 0s - loss: 0.1406
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0187
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0400
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1408
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0186
2928/6530 [============>.................] - ETA: 0s - loss: 0.0395
3728/6530 [================>.............] - ETA: 0s - loss: 0.1410
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0187
3120/6530 [=============>................] - ETA: 0s - loss: 0.0390
3968/6530 [=================>............] - ETA: 0s - loss: 0.1410
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0187
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0392
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1421
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0188
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0392
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1420
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0189
3776/6530 [================>.............] - ETA: 0s - loss: 0.0390
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1416
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0192
4000/6530 [=================>............] - ETA: 0s - loss: 0.0394
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1414
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0191
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0393
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1413
6480/6530 [============================>.] - ETA: 0s - loss: 0.0191
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0395
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1410
6530/6530 [==============================] - 1s 212us/step - loss: 0.0192 - val_loss: 0.0191
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0249
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0397
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1412
 272/6530 [>.............................] - ETA: 1s - loss: 0.0205
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0396
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1407
 528/6530 [=>............................] - ETA: 1s - loss: 0.0215
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0395
6256/6530 [===========================>..] - ETA: 0s - loss: 0.1401
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0214
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0394
6512/6530 [============================>.] - ETA: 0s - loss: 0.1397
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0218
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0391
6530/6530 [==============================] - 1s 213us/step - loss: 0.1398 - val_loss: 0.1138
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1081
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0212
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0392
 272/6530 [>.............................] - ETA: 1s - loss: 0.1223
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0214
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0392
 528/6530 [=>............................] - ETA: 1s - loss: 0.1201
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0210
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0392
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1254
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0209
6432/6530 [============================>.] - ETA: 0s - loss: 0.0389
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1278
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0203
1264/6530 [====>.........................] - ETA: 1s - loss: 0.1295
6530/6530 [==============================] - 2s 250us/step - loss: 0.0388 - val_loss: 0.0294
Epoch 5/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0444
2608/6530 [==========>...................] - ETA: 0s - loss: 0.0201
1504/6530 [=====>........................] - ETA: 1s - loss: 0.1300
 240/6530 [>.............................] - ETA: 1s - loss: 0.0343
2880/6530 [============>.................] - ETA: 0s - loss: 0.0196
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1302
 448/6530 [=>............................] - ETA: 1s - loss: 0.0359
3136/6530 [=============>................] - ETA: 0s - loss: 0.0199
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1300
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0380
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0199
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1299
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0368
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0199
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1308
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0367
3920/6530 [=================>............] - ETA: 0s - loss: 0.0197
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1311
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0357
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0198
3056/6530 [=============>................] - ETA: 0s - loss: 0.1307
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0352
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0198
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1308
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0349
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0196
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1309
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0353
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0194
3776/6530 [================>.............] - ETA: 0s - loss: 0.1303
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0356
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0195
4032/6530 [=================>............] - ETA: 0s - loss: 0.1301
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0360
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0194
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1299
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0365
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0192
4528/6530 [===================>..........] - ETA: 0s - loss: 0.1302
2832/6530 [============>.................] - ETA: 0s - loss: 0.0362
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0191
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1302
3040/6530 [============>.................] - ETA: 0s - loss: 0.0357
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0193
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1295
3248/6530 [=============>................] - ETA: 0s - loss: 0.0355
6480/6530 [============================>.] - ETA: 0s - loss: 0.0194
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1295
6530/6530 [==============================] - 1s 205us/step - loss: 0.0194 - val_loss: 0.0156

3472/6530 [==============>...............] - ETA: 0s - loss: 0.0360
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1299
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0358
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1293
3904/6530 [================>.............] - ETA: 0s - loss: 0.0361
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1294
4128/6530 [=================>............] - ETA: 0s - loss: 0.0360
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1295
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0359
6528/6530 [============================>.] - ETA: 0s - loss: 0.1300
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0361
6530/6530 [==============================] - 1s 212us/step - loss: 0.1300 - val_loss: 0.1136
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1022
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0361
 272/6530 [>.............................] - ETA: 1s - loss: 0.1352
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0360
 528/6530 [=>............................] - ETA: 1s - loss: 0.1289
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0361
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1318
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0361
1024/6530 [===>..........................] - ETA: 1s - loss: 0.1312
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0361
1248/6530 [====>.........................] - ETA: 1s - loss: 0.1287
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0361
1488/6530 [=====>........................] - ETA: 1s - loss: 0.1280
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0361
1744/6530 [=======>......................] - ETA: 0s - loss: 0.1279
6400/6530 [============================>.] - ETA: 0s - loss: 0.0360
2000/6530 [========>.....................] - ETA: 0s - loss: 0.1276
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1287
6530/6530 [==============================] - 2s 243us/step - loss: 0.0360 - val_loss: 0.0265
Epoch 6/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0269
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1285
 256/6530 [>.............................] - ETA: 1s - loss: 0.0320
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1295
 480/6530 [=>............................] - ETA: 1s - loss: 0.0353
3040/6530 [============>.................] - ETA: 0s - loss: 0.1307
 704/6530 [==>...........................] - ETA: 1s - loss: 0.0354
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1309
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0351
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1313
1152/6530 [====>.........................] - ETA: 1s - loss: 0.0355
3840/6530 [================>.............] - ETA: 0s - loss: 0.1312
1376/6530 [=====>........................] - ETA: 1s - loss: 0.0346
4096/6530 [=================>............] - ETA: 0s - loss: 0.1309
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0341
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1303
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0339
4624/6530 [====================>.........] - ETA: 0s - loss: 0.1298
2000/6530 [========>.....................] - ETA: 1s - loss: 0.0342
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1293
2224/6530 [=========>....................] - ETA: 1s - loss: 0.0346
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1288
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0347
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1286
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0348
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1284
2896/6530 [============>.................] - ETA: 0s - loss: 0.0343
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1283
3120/6530 [=============>................] - ETA: 0s - loss: 0.0337
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1286
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0341
6416/6530 [============================>.] - ETA: 0s - loss: 0.1287
# training | RMSE: 0.1163, MAE: 0.0905
worker 0  xfile  [8, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10475440952621362}, 'layer_1_size': 44, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10351097759701844}, 'layer_2_size': 40, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.1162906735842084, 'rmse': 0.1162906735842084, 'mae': 0.09050187040035224, 'early_stop': False}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  54 | activation: relu    | extras: batchnorm 
layer 2 | size:  43 | activation: tanh    | extras: batchnorm 
layer 3 | size:  84 | activation: relu    | extras: None 
layer 4 | size:  28 | activation: tanh    | extras: dropout - rate: 43.8% 
layer 5 | size:  41 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777011c358>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/9

  32/6530 [..............................] - ETA: 2:00 - loss: 2.3370
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0341
 512/6530 [=>............................] - ETA: 7s - loss: 0.7836  
6530/6530 [==============================] - 1s 209us/step - loss: 0.1287 - val_loss: 0.1191

3776/6530 [================>.............] - ETA: 0s - loss: 0.0342Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1455
 960/6530 [===>..........................] - ETA: 4s - loss: 0.6262
3984/6530 [=================>............] - ETA: 0s - loss: 0.0343
 256/6530 [>.............................] - ETA: 1s - loss: 0.1195
1408/6530 [=====>........................] - ETA: 2s - loss: 0.5466
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0344
 496/6530 [=>............................] - ETA: 1s - loss: 0.1242
1888/6530 [=======>......................] - ETA: 1s - loss: 0.4825
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0343
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1263
2368/6530 [=========>....................] - ETA: 1s - loss: 0.4249
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0345
1008/6530 [===>..........................] - ETA: 1s - loss: 0.1220
2816/6530 [===========>..................] - ETA: 1s - loss: 0.3857
1264/6530 [====>.........................] - ETA: 1s - loss: 0.1193
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0343
3328/6530 [==============>...............] - ETA: 0s - loss: 0.3507
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0346
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1210
3808/6530 [================>.............] - ETA: 0s - loss: 0.3226
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0345
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1220
4288/6530 [==================>...........] - ETA: 0s - loss: 0.2997
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0345
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1219
4768/6530 [====================>.........] - ETA: 0s - loss: 0.2809
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0345
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1208
5216/6530 [======================>.......] - ETA: 0s - loss: 0.2666
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1210
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0346
5696/6530 [=========================>....] - ETA: 0s - loss: 0.2517
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0345
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1221
6208/6530 [===========================>..] - ETA: 0s - loss: 0.2382
6352/6530 [============================>.] - ETA: 0s - loss: 0.0345
3024/6530 [============>.................] - ETA: 0s - loss: 0.1222
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1223
6530/6530 [==============================] - 2s 245us/step - loss: 0.0344 - val_loss: 0.0242
Epoch 7/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0235
6530/6530 [==============================] - 1s 213us/step - loss: 0.2304 - val_loss: 0.0666

3536/6530 [===============>..............] - ETA: 0s - loss: 0.1219Epoch 2/9

  32/6530 [..............................] - ETA: 0s - loss: 0.1027
 224/6530 [>.............................] - ETA: 1s - loss: 0.0316
3792/6530 [================>.............] - ETA: 0s - loss: 0.1214
 512/6530 [=>............................] - ETA: 0s - loss: 0.0977
 432/6530 [>.............................] - ETA: 1s - loss: 0.0325
4048/6530 [=================>............] - ETA: 0s - loss: 0.1217
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0913
 640/6530 [=>............................] - ETA: 1s - loss: 0.0339
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1221
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0864
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0324
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1218
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0856
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0329
4752/6530 [====================>.........] - ETA: 0s - loss: 0.1221
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0807
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0325
5008/6530 [======================>.......] - ETA: 0s - loss: 0.1223
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0798
1456/6530 [=====>........................] - ETA: 1s - loss: 0.0323
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1219
3264/6530 [=============>................] - ETA: 0s - loss: 0.0775
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0321
5520/6530 [========================>.....] - ETA: 0s - loss: 0.1226
3744/6530 [================>.............] - ETA: 0s - loss: 0.0768
1888/6530 [=======>......................] - ETA: 1s - loss: 0.0320
5776/6530 [=========================>....] - ETA: 0s - loss: 0.1225
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0756
2112/6530 [========>.....................] - ETA: 1s - loss: 0.0319
6032/6530 [==========================>...] - ETA: 0s - loss: 0.1224
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0743
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0324
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1226
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0741
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0325
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0733
6530/6530 [==============================] - 1s 214us/step - loss: 0.1227 - val_loss: 0.1022

2784/6530 [===========>..................] - ETA: 0s - loss: 0.0323Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1410
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0729
2992/6530 [============>.................] - ETA: 0s - loss: 0.0320
 272/6530 [>.............................] - ETA: 1s - loss: 0.1200
6530/6530 [==============================] - 1s 114us/step - loss: 0.0723 - val_loss: 0.0329
Epoch 3/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0567
 512/6530 [=>............................] - ETA: 1s - loss: 0.1210
3200/6530 [=============>................] - ETA: 0s - loss: 0.0317
 512/6530 [=>............................] - ETA: 0s - loss: 0.0566
 768/6530 [==>...........................] - ETA: 1s - loss: 0.1255
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0318
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0594
1008/6530 [===>..........................] - ETA: 1s - loss: 0.1264
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0319
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0594
3824/6530 [================>.............] - ETA: 0s - loss: 0.0321
1264/6530 [====>.........................] - ETA: 1s - loss: 0.1255
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0589
4032/6530 [=================>............] - ETA: 0s - loss: 0.0323
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1264
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0586
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1261
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0320
2912/6530 [============>.................] - ETA: 0s - loss: 0.0580
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1266
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0323
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0571
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1255
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0324
3872/6530 [================>.............] - ETA: 0s - loss: 0.0571
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1257
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0324
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0558
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1253
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0323
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0546
3056/6530 [=============>................] - ETA: 0s - loss: 0.1238
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0324
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0545
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1232
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0323
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0538
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1230
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0323
6336/6530 [============================>.] - ETA: 0s - loss: 0.0533
3824/6530 [================>.............] - ETA: 0s - loss: 0.1229
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0323
6530/6530 [==============================] - 1s 110us/step - loss: 0.0531 - val_loss: 0.0250
Epoch 4/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0654
4064/6530 [=================>............] - ETA: 0s - loss: 0.1228
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0325
 512/6530 [=>............................] - ETA: 0s - loss: 0.0431
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1229
6448/6530 [============================>.] - ETA: 0s - loss: 0.0323
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0439
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1224
6530/6530 [==============================] - 2s 250us/step - loss: 0.0323 - val_loss: 0.0236
Epoch 8/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0252
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0428
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1227
 224/6530 [>.............................] - ETA: 1s - loss: 0.0322
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0429
5072/6530 [======================>.......] - ETA: 0s - loss: 0.1228
 448/6530 [=>............................] - ETA: 1s - loss: 0.0322
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0428
5328/6530 [=======================>......] - ETA: 0s - loss: 0.1224
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0338
2976/6530 [============>.................] - ETA: 0s - loss: 0.0423
5584/6530 [========================>.....] - ETA: 0s - loss: 0.1226
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0329
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0421
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1221
1104/6530 [====>.........................] - ETA: 1s - loss: 0.0329
3936/6530 [=================>............] - ETA: 0s - loss: 0.0416
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1222
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0324
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0408
6336/6530 [============================>.] - ETA: 0s - loss: 0.1218
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0314
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0405
1760/6530 [=======>......................] - ETA: 1s - loss: 0.0310
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0404
6530/6530 [==============================] - 1s 211us/step - loss: 0.1217 - val_loss: 0.1135
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.1475
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0313
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0402
 272/6530 [>.............................] - ETA: 1s - loss: 0.1153
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0314
6368/6530 [============================>.] - ETA: 0s - loss: 0.0400
 528/6530 [=>............................] - ETA: 1s - loss: 0.1136
6530/6530 [==============================] - 1s 110us/step - loss: 0.0401 - val_loss: 0.0212
Epoch 5/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0258
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0316
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1115
 544/6530 [=>............................] - ETA: 0s - loss: 0.0338
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0320
1040/6530 [===>..........................] - ETA: 1s - loss: 0.1108
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0340
2848/6530 [============>.................] - ETA: 0s - loss: 0.0315
1296/6530 [====>.........................] - ETA: 1s - loss: 0.1146
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0340
3072/6530 [=============>................] - ETA: 0s - loss: 0.0311
1536/6530 [======>.......................] - ETA: 1s - loss: 0.1149
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0348
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0313
1792/6530 [=======>......................] - ETA: 0s - loss: 0.1139
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0340
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0313
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1141
2976/6530 [============>.................] - ETA: 0s - loss: 0.0340
3728/6530 [================>.............] - ETA: 0s - loss: 0.0310
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1148
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0342
3952/6530 [=================>............] - ETA: 0s - loss: 0.0311
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1155
3968/6530 [=================>............] - ETA: 0s - loss: 0.0336
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0311
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1159
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0333
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0311
3040/6530 [============>.................] - ETA: 0s - loss: 0.1161
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0331
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0313
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1165
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0332
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0311
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1170
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0331
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0312
3808/6530 [================>.............] - ETA: 0s - loss: 0.1169
6432/6530 [============================>.] - ETA: 0s - loss: 0.0332
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0311
4064/6530 [=================>............] - ETA: 0s - loss: 0.1169
6530/6530 [==============================] - 1s 109us/step - loss: 0.0330 - val_loss: 0.0269
Epoch 6/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0298
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0311
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1167
 544/6530 [=>............................] - ETA: 0s - loss: 0.0285
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0310
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1167
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0281
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1162
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0309
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0296
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1166
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0309
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0301
5344/6530 [=======================>......] - ETA: 0s - loss: 0.1166
6384/6530 [============================>.] - ETA: 0s - loss: 0.0308
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0300
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1168
3008/6530 [============>.................] - ETA: 0s - loss: 0.0299
6530/6530 [==============================] - 2s 245us/step - loss: 0.0308 - val_loss: 0.0213
Epoch 9/9

  16/6530 [..............................] - ETA: 1s - loss: 0.0405
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1173
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0301
 224/6530 [>.............................] - ETA: 1s - loss: 0.0308
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1174
3936/6530 [=================>............] - ETA: 0s - loss: 0.0295
 448/6530 [=>............................] - ETA: 1s - loss: 0.0301
6368/6530 [============================>.] - ETA: 0s - loss: 0.1175
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0293
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0323
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0292
6530/6530 [==============================] - 1s 209us/step - loss: 0.1173 - val_loss: 0.1009

 896/6530 [===>..........................] - ETA: 1s - loss: 0.0306
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0292
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0308
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0289
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0299
6464/6530 [============================>.] - ETA: 0s - loss: 0.0285
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0297
6530/6530 [==============================] - 1s 109us/step - loss: 0.0285 - val_loss: 0.0197
Epoch 7/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0201
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0295
 544/6530 [=>............................] - ETA: 0s - loss: 0.0269
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0295
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0253
2208/6530 [=========>....................] - ETA: 1s - loss: 0.0301
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0255
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0303
# training | RMSE: 0.1217, MAE: 0.0954
worker 1  xfile  [13, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.12169201224474639, 'rmse': 0.12169201224474639, 'mae': 0.09536560284573513, 'early_stop': False}
vggnet done  1

1984/6530 [========>.....................] - ETA: 0s - loss: 0.0259
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0302
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0250
2864/6530 [============>.................] - ETA: 0s - loss: 0.0297
3008/6530 [============>.................] - ETA: 0s - loss: 0.0248
3104/6530 [=============>................] - ETA: 0s - loss: 0.0295
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0257
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0298
4064/6530 [=================>............] - ETA: 0s - loss: 0.0253
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0300
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0253
3824/6530 [================>.............] - ETA: 0s - loss: 0.0299
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0253
4032/6530 [=================>............] - ETA: 0s - loss: 0.0299
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0253
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0298
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0254
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0299
6530/6530 [==============================] - 1s 106us/step - loss: 0.0254 - val_loss: 0.0234
Epoch 8/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0451
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0298
 544/6530 [=>............................] - ETA: 0s - loss: 0.0272
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0295
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0245
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0294
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0247
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0296
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0246
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0292
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0245
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0292
3040/6530 [============>.................] - ETA: 0s - loss: 0.0239
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0291
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0238
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0291
4000/6530 [=================>............] - ETA: 0s - loss: 0.0234
6464/6530 [============================>.] - ETA: 0s - loss: 0.0289
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0232
6530/6530 [==============================] - 2s 239us/step - loss: 0.0289 - val_loss: 0.0197

4992/6530 [=====================>........] - ETA: 0s - loss: 0.0228
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0225
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0225
6496/6530 [============================>.] - ETA: 0s - loss: 0.0225
6530/6530 [==============================] - 1s 106us/step - loss: 0.0225 - val_loss: 0.0253
Epoch 9/9

  32/6530 [..............................] - ETA: 0s - loss: 0.0466
 544/6530 [=>............................] - ETA: 0s - loss: 0.0216
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0228
# training | RMSE: 0.1352, MAE: 0.1053
worker 2  xfile  [12, 9.0, 0, 100, [], {'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 100, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.17448042821411805}, 'layer_2_size': 10, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2519769646929365}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.1351997246275383, 'rmse': 0.1351997246275383, 'mae': 0.1053125831894223, 'early_stop': False}
vggnet done  2

1536/6530 [======>.......................] - ETA: 0s - loss: 0.0225
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0220
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0216
3072/6530 [=============>................] - ETA: 0s - loss: 0.0214
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0216
3840/6530 [================>.............] - ETA: 0s - loss: 0.0214
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0215
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0215
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0216
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0216
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0216
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0216
6368/6530 [============================>.] - ETA: 0s - loss: 0.0216
6530/6530 [==============================] - 1s 132us/step - loss: 0.0216 - val_loss: 0.0188

# training | RMSE: 0.1281, MAE: 0.1039
worker 0  xfile  [14, 9.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 43, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4383623882746951}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.1281269202929751, 'rmse': 0.1281269202929751, 'mae': 0.10387645796579302, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=9.0 loss={'loss': 0.2029579956553885, 'rmse': 0.2029579956553885, 'mae': 0.1651154108594286, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 65, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.17515930016558992}, 'layer_3_size': 55, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4449677908740701}, 'layer_4_size': 91, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 19, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#1 epoch=9.0 loss={'loss': 0.10233769525544943, 'rmse': 0.10233769525544943, 'mae': 0.078952522035206, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 92, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2149531878533118}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25752557491173744}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12246768342475414}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#2 epoch=9.0 loss={'loss': 0.1276350782464865, 'rmse': 0.1276350782464865, 'mae': 0.09896830265097417, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 16, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 96, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 93, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 30, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 16, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#4 epoch=9.0 loss={'loss': 0.12626651994845878, 'rmse': 0.12626651994845878, 'mae': 0.09363103322974199, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 50, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.4082346454778355}, 'layer_2_size': 82, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4268159776129282}, 'layer_3_size': 93, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 39, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#6 epoch=9.0 loss={'loss': 0.18715969315490485, 'rmse': 0.18715969315490485, 'mae': 0.14265073252949512, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47825508775226755}, 'layer_1_size': 70, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 91, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.30764941843595073}, 'layer_3_size': 60, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 24, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.39756340213759234}, 'layer_5_size': 25, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adamax', 'scaler': None, 'shuffle': True}
#3 epoch=9.0 loss={'loss': 0.1285649704712358, 'rmse': 0.1285649704712358, 'mae': 0.10179411007870823, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.21691068773488237}, 'layer_2_size': 21, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 4, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': None}, 'layer_4_size': 19, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.27617154374503416}, 'layer_5_size': 63, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#7 epoch=9.0 loss={'loss': 0.10233468591449654, 'rmse': 0.10233468591449654, 'mae': 0.0781294667531916, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 2, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#5 epoch=9.0 loss={'loss': 0.10696459894567822, 'rmse': 0.10696459894567822, 'mae': 0.08299957387751915, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3690655139133455}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44187056290450344}, 'layer_4_size': 98, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#9 epoch=9.0 loss={'loss': 0.12474897352841811, 'rmse': 0.12474897352841811, 'mae': 0.09599872937682807, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 30, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10044009068957292}, 'layer_2_size': 63, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.4170382118349151}, 'layer_3_size': 46, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.3165420015541014}, 'layer_4_size': 32, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2629199363211525}, 'layer_5_size': 7, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MinMaxScaler', 'shuffle': False}
#10 epoch=9.0 loss={'loss': 0.22511023019269394, 'rmse': 0.22511023019269394, 'mae': 0.1870310139398925, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 64, 'init': 'normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 89, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': None}, 'layer_2_size': 25, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 14, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 55, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 67, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': False}
#11 epoch=9.0 loss={'loss': 0.1752743333565342, 'rmse': 0.1752743333565342, 'mae': 0.138287642760544, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 256, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 45, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.28236583769424184}, 'layer_2_size': 25, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.14136913317619815}, 'layer_3_size': 97, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 77, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.20072288229487062}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'StandardScaler', 'shuffle': False}
#8 epoch=9.0 loss={'loss': 0.1162906735842084, 'rmse': 0.1162906735842084, 'mae': 0.09050187040035224, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10475440952621362}, 'layer_1_size': 44, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10351097759701844}, 'layer_2_size': 40, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#13 epoch=9.0 loss={'loss': 0.12169201224474639, 'rmse': 0.12169201224474639, 'mae': 0.09536560284573513, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
#12 epoch=9.0 loss={'loss': 0.1351997246275383, 'rmse': 0.1351997246275383, 'mae': 0.1053125831894223, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 100, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.17448042821411805}, 'layer_2_size': 10, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 16, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 4, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.2519769646929365}, 'layer_5_size': 69, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#14 epoch=9.0 loss={'loss': 0.1281269202929751, 'rmse': 0.1281269202929751, 'mae': 0.10387645796579302, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 54, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 43, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 84, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4383623882746951}, 'layer_4_size': 28, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': None, 'shuffle': True}
get a list [results] of length 185
get a list [loss] of length 15
get a list [val_loss] of length 15
length of indices is (7, 1, 5, 8, 13, 9, 4, 2, 14, 3, 12, 11, 6, 0, 10)
length of indices is 15
length of T is 15
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 2, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}], [1, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 92, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2149531878533118}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25752557491173744}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12246768342475414}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}], [2, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3690655139133455}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44187056290450344}, 'layer_4_size': 98, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}], [3, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10475440952621362}, 'layer_1_size': 44, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10351097759701844}, 'layer_2_size': 40, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [4, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]] 

*** 5.0 configurations x 27.0 iterations each

15 | Thu Sep 27 23:28:23 2018 | lowest loss so far: 0.0809 (run 0)

{'batch_size': 16,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': False}
layer 1 | size:  92 | activation: tanh    | extras: None 
layer 2 | size:  66 | activation: tanh    | extras: dropout - rate: 21.5% 
layer 3 | size:  38 | activation: relu    | extras: dropout - rate: 25.8% 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 6:25 - loss: 0.4186
 288/6530 [>.............................] - ETA: 21s - loss: 0.4339 
 560/6530 [=>............................] - ETA: 11s - loss: 0.3879
 880/6530 [===>..........................] - ETA: 7s - loss: 0.3596 {'batch_size': 32,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': True}
layer 1 | size:  75 | activation: relu    | extras: None 
layer 2 | size:  37 | activation: relu    | extras: None 
layer 3 | size:  66 | activation: relu    | extras: dropout - rate: 36.9% 
layer 4 | size:  98 | activation: tanh    | extras: dropout - rate: 44.2% 
layer 5 | size:  11 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 3:02 - loss: 1.0608
1232/6530 [====>.........................] - ETA: 4s - loss: 0.3325
 608/6530 [=>............................] - ETA: 9s - loss: 0.5646  
1664/6530 [======>.......................] - ETA: 3s - loss: 0.3138
1184/6530 [====>.........................] - ETA: 4s - loss: 0.4263
2128/6530 [========>.....................] - ETA: 2s - loss: 0.2996
1760/6530 [=======>......................] - ETA: 2s - loss: 0.3663
2592/6530 [==========>...................] - ETA: 1s - loss: 0.2887{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  64 | activation: relu    | extras: batchnorm 
layer 2 | size:  13 | activation: tanh    | extras: batchnorm 
layer 3 | size:  25 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a77d668>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 256/6530 [>.............................] - ETA: 25s - loss: 0.3501
2336/6530 [=========>....................] - ETA: 1s - loss: 0.3306
3024/6530 [============>.................] - ETA: 1s - loss: 0.2771
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0673 
2944/6530 [============>.................] - ETA: 1s - loss: 0.3063
3472/6530 [==============>...............] - ETA: 1s - loss: 0.2691
3552/6530 [===============>..............] - ETA: 1s - loss: 0.2904
6530/6530 [==============================] - 1s 180us/step - loss: 0.0556 - val_loss: 0.0252
Epoch 2/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0235
3920/6530 [=================>............] - ETA: 0s - loss: 0.2631
4128/6530 [=================>............] - ETA: 0s - loss: 0.2787
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0216
4320/6530 [==================>...........] - ETA: 0s - loss: 0.2573
6530/6530 [==============================] - 0s 13us/step - loss: 0.0209 - val_loss: 0.0196

4768/6530 [====================>.........] - ETA: 0s - loss: 0.2676Epoch 3/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0167
4704/6530 [====================>.........] - ETA: 0s - loss: 0.2530
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2587
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0167
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2487
6530/6530 [==============================] - 0s 12us/step - loss: 0.0165 - val_loss: 0.0169
Epoch 4/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0137
5984/6530 [==========================>...] - ETA: 0s - loss: 0.2503
5552/6530 [========================>.....] - ETA: 0s - loss: 0.2436
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0145
6530/6530 [==============================] - 0s 12us/step - loss: 0.0144 - val_loss: 0.0153
Epoch 5/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0122
6000/6530 [==========================>...] - ETA: 0s - loss: 0.2393
6530/6530 [==============================] - 2s 230us/step - loss: 0.2445 - val_loss: 0.1881
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.2251
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0131
6416/6530 [============================>.] - ETA: 0s - loss: 0.2362
6530/6530 [==============================] - 0s 12us/step - loss: 0.0131 - val_loss: 0.0141
Epoch 6/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0112
 640/6530 [=>............................] - ETA: 0s - loss: 0.1834
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0122
6530/6530 [==============================] - 2s 279us/step - loss: 0.2351 - val_loss: 0.1623
Epoch 2/27

  16/6530 [..............................] - ETA: 0s - loss: 0.1679
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1792
6530/6530 [==============================] - 0s 12us/step - loss: 0.0123 - val_loss: 0.0133
Epoch 7/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0106
 464/6530 [=>............................] - ETA: 0s - loss: 0.1831
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1782
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0117
6530/6530 [==============================] - 0s 12us/step - loss: 0.0116 - val_loss: 0.0127
Epoch 8/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0101
 928/6530 [===>..........................] - ETA: 0s - loss: 0.1727
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1763
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0110
1360/6530 [=====>........................] - ETA: 0s - loss: 0.1707
3104/6530 [=============>................] - ETA: 0s - loss: 0.1753
6530/6530 [==============================] - 0s 12us/step - loss: 0.0111 - val_loss: 0.0122
Epoch 9/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0097
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1712
3712/6530 [================>.............] - ETA: 0s - loss: 0.1737
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0106
6530/6530 [==============================] - 0s 12us/step - loss: 0.0107 - val_loss: 0.0118
Epoch 10/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0094
2224/6530 [=========>....................] - ETA: 0s - loss: 0.1711
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1727
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0103
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1719
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1715
6530/6530 [==============================] - 0s 12us/step - loss: 0.0104 - val_loss: 0.0114
Epoch 11/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0091
3136/6530 [=============>................] - ETA: 0s - loss: 0.1679
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1716
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 0s 11us/step - loss: 0.0100 - val_loss: 0.0111
Epoch 12/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0089
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1665
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1710
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0098
6530/6530 [==============================] - 1s 85us/step - loss: 0.1701 - val_loss: 0.1518
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1754
4048/6530 [=================>............] - ETA: 0s - loss: 0.1648
6530/6530 [==============================] - 0s 11us/step - loss: 0.0098 - val_loss: 0.0109
Epoch 13/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0087
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1637
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1611
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0095
6530/6530 [==============================] - 0s 11us/step - loss: 0.0096 - val_loss: 0.0107
Epoch 14/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0085
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1626
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1598
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0092
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1619
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1584
6530/6530 [==============================] - 0s 12us/step - loss: 0.0093 - val_loss: 0.0105
Epoch 15/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0084
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1602
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1565
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0091
6530/6530 [==============================] - 0s 11us/step - loss: 0.0092 - val_loss: 0.0103
Epoch 16/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0082
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1592
3232/6530 [=============>................] - ETA: 0s - loss: 0.1551
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0090
6530/6530 [==============================] - 1s 118us/step - loss: 0.1583 - val_loss: 0.1239

3904/6530 [================>.............] - ETA: 0s - loss: 0.1552Epoch 3/27

  16/6530 [..............................] - ETA: 0s - loss: 0.1288
6530/6530 [==============================] - 0s 11us/step - loss: 0.0090 - val_loss: 0.0101
Epoch 17/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0081
 448/6530 [=>............................] - ETA: 0s - loss: 0.1362
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1542
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0087
6530/6530 [==============================] - 0s 12us/step - loss: 0.0088 - val_loss: 0.0100
Epoch 18/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0080
 880/6530 [===>..........................] - ETA: 0s - loss: 0.1390
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1531
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0085
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1357
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1524
6530/6530 [==============================] - 0s 12us/step - loss: 0.0087 - val_loss: 0.0099
Epoch 19/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0079
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1359
6464/6530 [============================>.] - ETA: 0s - loss: 0.1515
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0084
6530/6530 [==============================] - 1s 83us/step - loss: 0.1515 - val_loss: 0.1499
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1249
6530/6530 [==============================] - 0s 12us/step - loss: 0.0085 - val_loss: 0.0097
Epoch 20/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0078
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1371
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1480
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0083
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1380
6530/6530 [==============================] - 0s 12us/step - loss: 0.0084 - val_loss: 0.0096
Epoch 21/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0077
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1442
3120/6530 [=============>................] - ETA: 0s - loss: 0.1360
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0083
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1467
6530/6530 [==============================] - 0s 11us/step - loss: 0.0083 - val_loss: 0.0095
Epoch 22/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0076
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1347
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1460
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0082
4032/6530 [=================>............] - ETA: 0s - loss: 0.1348
6530/6530 [==============================] - 0s 12us/step - loss: 0.0082 - val_loss: 0.0095
Epoch 23/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0075
3232/6530 [=============>................] - ETA: 0s - loss: 0.1435
4496/6530 [===================>..........] - ETA: 0s - loss: 0.1338
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0079
3840/6530 [================>.............] - ETA: 0s - loss: 0.1412
6530/6530 [==============================] - 0s 12us/step - loss: 0.0081 - val_loss: 0.0094
Epoch 24/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0074
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1336
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1405
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0078
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1338
6530/6530 [==============================] - 0s 12us/step - loss: 0.0080 - val_loss: 0.0093
Epoch 25/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0073
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1395
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1332
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0077
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1393
6530/6530 [==============================] - 0s 12us/step - loss: 0.0079 - val_loss: 0.0092
Epoch 26/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0073
6320/6530 [============================>.] - ETA: 0s - loss: 0.1325
6400/6530 [============================>.] - ETA: 0s - loss: 0.1389
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0076
6530/6530 [==============================] - 1s 117us/step - loss: 0.1321 - val_loss: 0.1141
Epoch 4/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0947
6530/6530 [==============================] - 1s 83us/step - loss: 0.1387 - val_loss: 0.1953
Epoch 5/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1833
6530/6530 [==============================] - 0s 12us/step - loss: 0.0078 - val_loss: 0.0091
Epoch 27/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0072
 464/6530 [=>............................] - ETA: 0s - loss: 0.1254
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1361
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0077
6530/6530 [==============================] - 0s 11us/step - loss: 0.0077 - val_loss: 0.0091

 928/6530 [===>..........................] - ETA: 0s - loss: 0.1219
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1369
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1177
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1331
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1190
2656/6530 [===========>..................] - ETA: 0s - loss: 0.1324
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1191
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1305
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1193
3936/6530 [=================>............] - ETA: 0s - loss: 0.1306
3120/6530 [=============>................] - ETA: 0s - loss: 0.1179
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1307
3600/6530 [===============>..............] - ETA: 0s - loss: 0.1183
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1302
4048/6530 [=================>............] - ETA: 0s - loss: 0.1177
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1290
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1168
6530/6530 [==============================] - 1s 80us/step - loss: 0.1277 - val_loss: 0.1369
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1693
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1171
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1272
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1176
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1238
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1172
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1235
6320/6530 [============================>.] - ETA: 0s - loss: 0.1173
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1231
6530/6530 [==============================] - 1s 116us/step - loss: 0.1168 - val_loss: 0.0967
Epoch 5/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0798
3264/6530 [=============>................] - ETA: 0s - loss: 0.1236
 496/6530 [=>............................] - ETA: 0s - loss: 0.1126
3936/6530 [=================>............] - ETA: 0s - loss: 0.1230
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1125
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1220
1456/6530 [=====>........................] - ETA: 0s - loss: 0.1117
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1217
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1129
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1214
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1134
6530/6530 [==============================] - 1s 81us/step - loss: 0.1214 - val_loss: 0.1964
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.2119
2880/6530 [============>.................] - ETA: 0s - loss: 0.1134
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1227
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1122
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1164
3856/6530 [================>.............] - ETA: 0s - loss: 0.1110
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1163
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1103
# training | RMSE: 0.0841, MAE: 0.0657
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 2, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.08413190280126606, 'rmse': 0.08413190280126606, 'mae': 0.06569707785999676, 'early_stop': False}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  44 | activation: relu    | extras: dropout - rate: 10.5% 
layer 2 | size:  40 | activation: relu    | extras: dropout - rate: 10.4% 
layer 3 | size:  10 | activation: relu    | extras: batchnorm 
layer 4 | size:  59 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a77d8d0>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 3:16 - loss: 0.7562
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1176
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1094
 304/6530 [>.............................] - ETA: 10s - loss: 0.5641 
3264/6530 [=============>................] - ETA: 0s - loss: 0.1162
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1092
 592/6530 [=>............................] - ETA: 5s - loss: 0.4049 
3936/6530 [=================>............] - ETA: 0s - loss: 0.1152
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1086
 864/6530 [==>...........................] - ETA: 4s - loss: 0.3197
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1084
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1153
1136/6530 [====>.........................] - ETA: 3s - loss: 0.2633
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1155
1408/6530 [=====>........................] - ETA: 2s - loss: 0.2259
6530/6530 [==============================] - 1s 113us/step - loss: 0.1081 - val_loss: 0.0962
Epoch 6/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0803
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1148
1680/6530 [======>.......................] - ETA: 2s - loss: 0.1999
 496/6530 [=>............................] - ETA: 0s - loss: 0.1026
1952/6530 [=======>......................] - ETA: 1s - loss: 0.1812
6530/6530 [==============================] - 1s 81us/step - loss: 0.1144 - val_loss: 0.1093
Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1069
 960/6530 [===>..........................] - ETA: 0s - loss: 0.1028
2224/6530 [=========>....................] - ETA: 1s - loss: 0.1663
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1148
1424/6530 [=====>........................] - ETA: 0s - loss: 0.1023
2496/6530 [==========>...................] - ETA: 1s - loss: 0.1539
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1137
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1022
2768/6530 [===========>..................] - ETA: 1s - loss: 0.1450
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1138
2352/6530 [=========>....................] - ETA: 0s - loss: 0.1027
3024/6530 [============>.................] - ETA: 1s - loss: 0.1383
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1145
2768/6530 [===========>..................] - ETA: 0s - loss: 0.1032
3264/6530 [=============>................] - ETA: 1s - loss: 0.1320
3200/6530 [=============>................] - ETA: 0s - loss: 0.1026
3200/6530 [=============>................] - ETA: 0s - loss: 0.1134
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1263
3664/6530 [===============>..............] - ETA: 0s - loss: 0.1023
3840/6530 [================>.............] - ETA: 0s - loss: 0.1125
3808/6530 [================>.............] - ETA: 0s - loss: 0.1201
4128/6530 [=================>............] - ETA: 0s - loss: 0.1017
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1126
4080/6530 [=================>............] - ETA: 0s - loss: 0.1148
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1012
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1125
4368/6530 [===================>..........] - ETA: 0s - loss: 0.1103
5040/6530 [======================>.......] - ETA: 0s - loss: 0.1014
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1120
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1013
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1061
6432/6530 [============================>.] - ETA: 0s - loss: 0.1119
6530/6530 [==============================] - 1s 83us/step - loss: 0.1119 - val_loss: 0.1115
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1109
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1029
5968/6530 [==========================>...] - ETA: 0s - loss: 0.1011
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1107
6432/6530 [============================>.] - ETA: 0s - loss: 0.1012
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0997
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1107
6530/6530 [==============================] - 1s 115us/step - loss: 0.1010 - val_loss: 0.1009
Epoch 7/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0711
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0968
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1116
 400/6530 [>.............................] - ETA: 0s - loss: 0.0953
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0942
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1106
 848/6530 [==>...........................] - ETA: 0s - loss: 0.1006
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0919
3136/6530 [=============>................] - ETA: 0s - loss: 0.1100
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0995
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0897
3808/6530 [================>.............] - ETA: 0s - loss: 0.1087
1744/6530 [=======>......................] - ETA: 0s - loss: 0.1004
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1088
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1000
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1084
6530/6530 [==============================] - 2s 276us/step - loss: 0.0880 - val_loss: 0.0403
Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0313
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0995
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1082
3104/6530 [=============>................] - ETA: 0s - loss: 0.0983
 256/6530 [>.............................] - ETA: 1s - loss: 0.0387
6304/6530 [===========================>..] - ETA: 0s - loss: 0.1086
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0979
 528/6530 [=>............................] - ETA: 1s - loss: 0.0411
6530/6530 [==============================] - 1s 84us/step - loss: 0.1084 - val_loss: 0.1154
Epoch 10/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1329
4032/6530 [=================>............] - ETA: 0s - loss: 0.0984
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0392
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1095
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0979
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0386
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1031
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0976
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0379
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1018
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0978
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0380
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1038
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0981
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0380
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1042
6352/6530 [============================>.] - ETA: 0s - loss: 0.0987
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0386
3968/6530 [=================>............] - ETA: 0s - loss: 0.1044
6530/6530 [==============================] - 1s 117us/step - loss: 0.0985 - val_loss: 0.0906
Epoch 8/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0770
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0383
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1043
 464/6530 [=>............................] - ETA: 0s - loss: 0.1004
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0380
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1041
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0968
2976/6530 [============>.................] - ETA: 0s - loss: 0.0374
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1037
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0966
3248/6530 [=============>................] - ETA: 0s - loss: 0.0374
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0962
6530/6530 [==============================] - 1s 83us/step - loss: 0.1034 - val_loss: 0.1133

3520/6530 [===============>..............] - ETA: 0s - loss: 0.0372Epoch 11/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0758
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0961
3792/6530 [================>.............] - ETA: 0s - loss: 0.0374
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1020
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0974
4064/6530 [=================>............] - ETA: 0s - loss: 0.0372
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0997
3152/6530 [=============>................] - ETA: 0s - loss: 0.0967
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0369
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0994
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0963
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0368
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1005
4080/6530 [=================>............] - ETA: 0s - loss: 0.0957
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0365
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1003
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0947
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0362
3904/6530 [================>.............] - ETA: 0s - loss: 0.1010
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0946
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1009
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0361
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0946
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1005
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0361
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0943
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1000
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0359
6416/6530 [============================>.] - ETA: 0s - loss: 0.0946
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0355
6496/6530 [============================>.] - ETA: 0s - loss: 0.0995
6530/6530 [==============================] - 1s 82us/step - loss: 0.0995 - val_loss: 0.1003

6530/6530 [==============================] - 1s 116us/step - loss: 0.0945 - val_loss: 0.0859
Epoch 12/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1047Epoch 9/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0700
6480/6530 [============================>.] - ETA: 0s - loss: 0.0355
 640/6530 [=>............................] - ETA: 0s - loss: 0.1018
 480/6530 [=>............................] - ETA: 0s - loss: 0.0939
6530/6530 [==============================] - 1s 197us/step - loss: 0.0355 - val_loss: 0.0267
Epoch 3/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0270
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0998
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0926
 288/6530 [>.............................] - ETA: 1s - loss: 0.0323
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0987
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0926
 544/6530 [=>............................] - ETA: 1s - loss: 0.0312
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0978
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0930
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0303
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0930
3232/6530 [=============>................] - ETA: 0s - loss: 0.0975
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0298
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0931
3872/6530 [================>.............] - ETA: 0s - loss: 0.0977
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0296
3184/6530 [=============>................] - ETA: 0s - loss: 0.0919
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0977
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0302
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0922
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0973
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0302
4096/6530 [=================>............] - ETA: 0s - loss: 0.0916
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0963
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0300
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0913
6464/6530 [============================>.] - ETA: 0s - loss: 0.0964
6530/6530 [==============================] - 1s 82us/step - loss: 0.0964 - val_loss: 0.1450
Epoch 13/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1417
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0303
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0916
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0927
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0299
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0915
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0935
2976/6530 [============>.................] - ETA: 0s - loss: 0.0298
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0915
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0948
3232/6530 [=============>................] - ETA: 0s - loss: 0.0298
6384/6530 [============================>.] - ETA: 0s - loss: 0.0919
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0950
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0293
6530/6530 [==============================] - 1s 116us/step - loss: 0.0917 - val_loss: 0.0806
Epoch 10/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0825
3200/6530 [=============>................] - ETA: 0s - loss: 0.0952
3776/6530 [================>.............] - ETA: 0s - loss: 0.0292
 464/6530 [=>............................] - ETA: 0s - loss: 0.0897
3840/6530 [================>.............] - ETA: 0s - loss: 0.0954
4048/6530 [=================>............] - ETA: 0s - loss: 0.0294
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0901
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0954
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0296
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0899
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0950
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0298
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0897
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0947
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0296
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0900
6368/6530 [============================>.] - ETA: 0s - loss: 0.0944
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0293
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0897
6530/6530 [==============================] - 1s 83us/step - loss: 0.0944 - val_loss: 0.1745
Epoch 14/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1553
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0293
3232/6530 [=============>................] - ETA: 0s - loss: 0.0893
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1011
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0291
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0893
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0977
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0288
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0884
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0950
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0288
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0885
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0930
6496/6530 [============================>.] - ETA: 0s - loss: 0.0290
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0885
3168/6530 [=============>................] - ETA: 0s - loss: 0.0924
6530/6530 [==============================] - 1s 197us/step - loss: 0.0290 - val_loss: 0.0205
Epoch 4/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0192
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0884
3776/6530 [================>.............] - ETA: 0s - loss: 0.0927
 256/6530 [>.............................] - ETA: 1s - loss: 0.0253
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0884
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0928
 512/6530 [=>............................] - ETA: 1s - loss: 0.0247
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0887
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0927
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0251
6530/6530 [==============================] - 1s 118us/step - loss: 0.0887 - val_loss: 0.0788
Epoch 11/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0715
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0923
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0260
 464/6530 [=>............................] - ETA: 0s - loss: 0.0921
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0914
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0265
6530/6530 [==============================] - 1s 85us/step - loss: 0.0912 - val_loss: 0.0851

 912/6530 [===>..........................] - ETA: 0s - loss: 0.0879Epoch 15/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0893
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0267
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0881
 576/6530 [=>............................] - ETA: 0s - loss: 0.0877
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0272
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0882
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0863
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0272
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0882
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0871
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0267
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0890
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0886
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0262
3168/6530 [=============>................] - ETA: 0s - loss: 0.0874
3264/6530 [=============>................] - ETA: 0s - loss: 0.0870
2928/6530 [============>.................] - ETA: 0s - loss: 0.0262
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0879
3904/6530 [================>.............] - ETA: 0s - loss: 0.0874
3184/6530 [=============>................] - ETA: 0s - loss: 0.0263
4112/6530 [=================>............] - ETA: 0s - loss: 0.0872
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0876
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0263
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0868
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0882
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0264
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0864
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0889
3952/6530 [=================>............] - ETA: 0s - loss: 0.0262
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0862
6432/6530 [============================>.] - ETA: 0s - loss: 0.0890
6530/6530 [==============================] - 1s 83us/step - loss: 0.0889 - val_loss: 0.0908
Epoch 16/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0930
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0263
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0864
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0868
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0263
6320/6530 [============================>.] - ETA: 0s - loss: 0.0862
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0884
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0262
6530/6530 [==============================] - 1s 118us/step - loss: 0.0862 - val_loss: 0.0809
Epoch 12/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0762
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0877
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0261
 448/6530 [=>............................] - ETA: 0s - loss: 0.0875
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0880
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0259
 848/6530 [==>...........................] - ETA: 0s - loss: 0.0881
3168/6530 [=============>................] - ETA: 0s - loss: 0.0863
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0259
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0878
3840/6530 [================>.............] - ETA: 0s - loss: 0.0870
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0259
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0871
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0876
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0260
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0869
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0879
6336/6530 [============================>.] - ETA: 0s - loss: 0.0259
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0876
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0871
3184/6530 [=============>................] - ETA: 0s - loss: 0.0865
6530/6530 [==============================] - 1s 203us/step - loss: 0.0260 - val_loss: 0.0199
Epoch 5/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0210
6400/6530 [============================>.] - ETA: 0s - loss: 0.0872
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0869
6530/6530 [==============================] - 1s 84us/step - loss: 0.0872 - val_loss: 0.1004
Epoch 17/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0981
 272/6530 [>.............................] - ETA: 1s - loss: 0.0239
4080/6530 [=================>............] - ETA: 0s - loss: 0.0866
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0842
 544/6530 [=>............................] - ETA: 1s - loss: 0.0239
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0859
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0852
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0248
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0860
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0853
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0245
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0864
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0854
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0249
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0861
3136/6530 [=============>................] - ETA: 0s - loss: 0.0860
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0248
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0862
3744/6530 [================>.............] - ETA: 0s - loss: 0.0855
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0246
6530/6530 [==============================] - 1s 117us/step - loss: 0.0862 - val_loss: 0.0777
Epoch 13/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0783
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0853
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0241
 480/6530 [=>............................] - ETA: 0s - loss: 0.0863
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0859
2384/6530 [=========>....................] - ETA: 0s - loss: 0.0237
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0875
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0852
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0236
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0866
6336/6530 [============================>.] - ETA: 0s - loss: 0.0849
2928/6530 [============>.................] - ETA: 0s - loss: 0.0233
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0864
6530/6530 [==============================] - 1s 84us/step - loss: 0.0851 - val_loss: 0.1109
Epoch 18/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1114
3200/6530 [=============>................] - ETA: 0s - loss: 0.0236
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0856
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0853
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0235
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0857
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0844
3728/6530 [================>.............] - ETA: 0s - loss: 0.0234
3200/6530 [=============>................] - ETA: 0s - loss: 0.0844
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0835
4000/6530 [=================>............] - ETA: 0s - loss: 0.0236
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0845
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0834
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0236
4112/6530 [=================>............] - ETA: 0s - loss: 0.0838
3136/6530 [=============>................] - ETA: 0s - loss: 0.0831
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0234
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0833
3776/6530 [================>.............] - ETA: 0s - loss: 0.0834
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0234
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0835
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0836
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0234
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0838
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0830
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0233
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0841
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0832
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0234
6432/6530 [============================>.] - ETA: 0s - loss: 0.0845
6368/6530 [============================>.] - ETA: 0s - loss: 0.0833
6530/6530 [==============================] - 1s 83us/step - loss: 0.0833 - val_loss: 0.1071

6530/6530 [==============================] - 1s 116us/step - loss: 0.0844 - val_loss: 0.0762
Epoch 19/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1147Epoch 14/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0702
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0234
 464/6530 [=>............................] - ETA: 0s - loss: 0.0854
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0816
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0234
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0821
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0865
6448/6530 [============================>.] - ETA: 0s - loss: 0.0233
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0859
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0836
6530/6530 [==============================] - 1s 198us/step - loss: 0.0233 - val_loss: 0.0237
Epoch 6/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0249
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0858
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0827
 288/6530 [>.............................] - ETA: 1s - loss: 0.0249
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0857
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0826
 544/6530 [=>............................] - ETA: 1s - loss: 0.0237
3904/6530 [================>.............] - ETA: 0s - loss: 0.0822
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0847
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0216
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0816
3200/6530 [=============>................] - ETA: 0s - loss: 0.0837
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0208
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0817
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0836
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0209
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0813
4128/6530 [=================>............] - ETA: 0s - loss: 0.0831
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0214
6496/6530 [============================>.] - ETA: 0s - loss: 0.0808
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0827
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0211
6530/6530 [==============================] - 1s 82us/step - loss: 0.0808 - val_loss: 0.0957

5056/6530 [======================>.......] - ETA: 0s - loss: 0.0834
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0214
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0837
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0213
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0839
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0213
6400/6530 [============================>.] - ETA: 0s - loss: 0.0836
2928/6530 [============>.................] - ETA: 0s - loss: 0.0212
6530/6530 [==============================] - 1s 117us/step - loss: 0.0835 - val_loss: 0.0783
Epoch 15/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0559
3168/6530 [=============>................] - ETA: 0s - loss: 0.0212
 464/6530 [=>............................] - ETA: 0s - loss: 0.0844
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0211
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0850
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0215
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0847
3968/6530 [=================>............] - ETA: 0s - loss: 0.0214
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0835
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0211
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0831
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0212
2896/6530 [============>.................] - ETA: 0s - loss: 0.0824
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0212
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0820
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0213
3872/6530 [================>.............] - ETA: 0s - loss: 0.0818
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0211
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0815
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0210
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0820
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0210
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0819
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0209
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0820
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0209
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0821
6530/6530 [==============================] - 1s 201us/step - loss: 0.0209 - val_loss: 0.0213
Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0122
6530/6530 [==============================] - 1s 109us/step - loss: 0.0818 - val_loss: 0.0717
Epoch 16/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0656
 272/6530 [>.............................] - ETA: 1s - loss: 0.0195
 448/6530 [=>............................] - ETA: 0s - loss: 0.0824
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0843
 560/6530 [=>............................] - ETA: 1s - loss: 0.0200
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0836
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0195
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0832
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0198
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0821
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0195
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0819
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0198
3200/6530 [=============>................] - ETA: 0s - loss: 0.0809
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0202
# training | RMSE: 0.1139, MAE: 0.0907
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3690655139133455}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44187056290450344}, 'layer_4_size': 98, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}]  predicted as label {'loss': 0.1138954499796542, 'rmse': 0.1138954499796542, 'mae': 0.09070319933268269, 'early_stop': True}
{'batch_size': 16,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adam',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  29 | activation: tanh    | extras: batchnorm 
layer 2 | size:  58 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  64 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a77da20>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  16/6530 [..............................] - ETA: 3:34 - loss: 1.4724
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0811
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0202
 272/6530 [>.............................] - ETA: 13s - loss: 0.8406 
4048/6530 [=================>............] - ETA: 0s - loss: 0.0810
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0200
 544/6530 [=>............................] - ETA: 6s - loss: 0.6055 
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0804
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0203
 800/6530 [==>...........................] - ETA: 4s - loss: 0.5032
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0802
2992/6530 [============>.................] - ETA: 0s - loss: 0.0204
1056/6530 [===>..........................] - ETA: 3s - loss: 0.4390
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0806
3264/6530 [=============>................] - ETA: 0s - loss: 0.0203
1328/6530 [=====>........................] - ETA: 3s - loss: 0.3959
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0804
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0202
1584/6530 [======>.......................] - ETA: 2s - loss: 0.3708
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0803
3808/6530 [================>.............] - ETA: 0s - loss: 0.0202
1824/6530 [=======>......................] - ETA: 2s - loss: 0.3504
6530/6530 [==============================] - 1s 118us/step - loss: 0.0804 - val_loss: 0.0770
Epoch 17/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0521
4032/6530 [=================>............] - ETA: 0s - loss: 0.0199
2064/6530 [========>.....................] - ETA: 2s - loss: 0.3361
 448/6530 [=>............................] - ETA: 0s - loss: 0.0811
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0199
2320/6530 [=========>....................] - ETA: 1s - loss: 0.3221
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0798
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0198
2576/6530 [==========>...................] - ETA: 1s - loss: 0.3086
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0813
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0197
2848/6530 [============>.................] - ETA: 1s - loss: 0.2989
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0811
3104/6530 [=============>................] - ETA: 1s - loss: 0.2899
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0197
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0810
3360/6530 [==============>...............] - ETA: 1s - loss: 0.2843
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0197
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0817
3616/6530 [===============>..............] - ETA: 1s - loss: 0.2786
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0197
3184/6530 [=============>................] - ETA: 0s - loss: 0.0813
3872/6530 [================>.............] - ETA: 0s - loss: 0.2722
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0197
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0816
4128/6530 [=================>............] - ETA: 0s - loss: 0.2675
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0197
4112/6530 [=================>............] - ETA: 0s - loss: 0.0812
4400/6530 [===================>..........] - ETA: 0s - loss: 0.2628
6448/6530 [============================>.] - ETA: 0s - loss: 0.0197
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0806
4656/6530 [====================>.........] - ETA: 0s - loss: 0.2580
6530/6530 [==============================] - 1s 200us/step - loss: 0.0196 - val_loss: 0.0217
Epoch 8/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0189
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0803
4912/6530 [=====================>........] - ETA: 0s - loss: 0.2540
 272/6530 [>.............................] - ETA: 1s - loss: 0.0180
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0804
5152/6530 [======================>.......] - ETA: 0s - loss: 0.2513
 544/6530 [=>............................] - ETA: 1s - loss: 0.0183
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0803
5408/6530 [=======================>......] - ETA: 0s - loss: 0.2485
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0182
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0810
5664/6530 [=========================>....] - ETA: 0s - loss: 0.2452
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0187
6530/6530 [==============================] - 1s 117us/step - loss: 0.0808 - val_loss: 0.0733
Epoch 18/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0729
5920/6530 [==========================>...] - ETA: 0s - loss: 0.2420
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0191
 464/6530 [=>............................] - ETA: 0s - loss: 0.0835
6192/6530 [===========================>..] - ETA: 0s - loss: 0.2389
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0190
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0834
6432/6530 [============================>.] - ETA: 0s - loss: 0.2363
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0187
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0821
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0189
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0806
6530/6530 [==============================] - 2s 292us/step - loss: 0.2354 - val_loss: 0.1553
Epoch 2/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1688
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0189
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0806
 240/6530 [>.............................] - ETA: 1s - loss: 0.1705
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0190
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0799
 480/6530 [=>............................] - ETA: 1s - loss: 0.1682
2912/6530 [============>.................] - ETA: 0s - loss: 0.0190
3104/6530 [=============>................] - ETA: 0s - loss: 0.0794
 720/6530 [==>...........................] - ETA: 1s - loss: 0.1691
3168/6530 [=============>................] - ETA: 0s - loss: 0.0188
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0796
 976/6530 [===>..........................] - ETA: 1s - loss: 0.1681
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0192
4000/6530 [=================>............] - ETA: 0s - loss: 0.0793
1232/6530 [====>.........................] - ETA: 1s - loss: 0.1651
3712/6530 [================>.............] - ETA: 0s - loss: 0.0191
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0789
1488/6530 [=====>........................] - ETA: 1s - loss: 0.1672
3984/6530 [=================>............] - ETA: 0s - loss: 0.0191
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0787
1744/6530 [=======>......................] - ETA: 1s - loss: 0.1673
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0190
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0789
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1677
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0190
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0791
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1684
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0190
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0795
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1669
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0190
6530/6530 [==============================] - 1s 119us/step - loss: 0.0796 - val_loss: 0.0763
Epoch 19/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0464
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1670
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0190
 464/6530 [=>............................] - ETA: 0s - loss: 0.0780
3040/6530 [============>.................] - ETA: 0s - loss: 0.1663
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0190
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0786
3264/6530 [=============>................] - ETA: 0s - loss: 0.1657
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0190
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0797
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1663
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0190
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0794
3776/6530 [================>.............] - ETA: 0s - loss: 0.1654
6416/6530 [============================>.] - ETA: 0s - loss: 0.0191
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0783
4032/6530 [=================>............] - ETA: 0s - loss: 0.1647
6530/6530 [==============================] - 1s 202us/step - loss: 0.0190 - val_loss: 0.0154

2656/6530 [===========>..................] - ETA: 0s - loss: 0.0790Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0278
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1644
3104/6530 [=============>................] - ETA: 0s - loss: 0.0785
 272/6530 [>.............................] - ETA: 1s - loss: 0.0179
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1637
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0789
 544/6530 [=>............................] - ETA: 1s - loss: 0.0196
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1623
3984/6530 [=================>............] - ETA: 0s - loss: 0.0782
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0190
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1616
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0775
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0194
5264/6530 [=======================>......] - ETA: 0s - loss: 0.1609
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0774
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0189
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1607
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0775
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0186
5808/6530 [=========================>....] - ETA: 0s - loss: 0.1597
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0773
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0186
6064/6530 [==========================>...] - ETA: 0s - loss: 0.1590
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0774
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0186
6320/6530 [============================>.] - ETA: 0s - loss: 0.1582
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0185
6530/6530 [==============================] - 1s 119us/step - loss: 0.0775 - val_loss: 0.0713
Epoch 20/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0503
6530/6530 [==============================] - 1s 211us/step - loss: 0.1582 - val_loss: 0.1379
Epoch 3/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1462
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0186
 464/6530 [=>............................] - ETA: 0s - loss: 0.0781
 272/6530 [>.............................] - ETA: 1s - loss: 0.1547
2912/6530 [============>.................] - ETA: 0s - loss: 0.0185
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0759
 544/6530 [=>............................] - ETA: 1s - loss: 0.1512
3184/6530 [=============>................] - ETA: 0s - loss: 0.0183
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0779
 800/6530 [==>...........................] - ETA: 1s - loss: 0.1498
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0182
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0776
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1516
3728/6530 [================>.............] - ETA: 0s - loss: 0.0186
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0774
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1512
4000/6530 [=================>............] - ETA: 0s - loss: 0.0185
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0775
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1503
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0185
3184/6530 [=============>................] - ETA: 0s - loss: 0.0767
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1488
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0185
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0768
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1479
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0186
4112/6530 [=================>............] - ETA: 0s - loss: 0.0759
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1472
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0185
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0752
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1465
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0184
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0759
2896/6530 [============>.................] - ETA: 0s - loss: 0.1462
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0184
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0760
3168/6530 [=============>................] - ETA: 0s - loss: 0.1455
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0762
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0183
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1457
6368/6530 [============================>.] - ETA: 0s - loss: 0.0760
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0182
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1450
6400/6530 [============================>.] - ETA: 0s - loss: 0.0183
6530/6530 [==============================] - 1s 118us/step - loss: 0.0759 - val_loss: 0.0700
Epoch 21/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0616
3936/6530 [=================>............] - ETA: 0s - loss: 0.1435
 448/6530 [=>............................] - ETA: 0s - loss: 0.0849
6530/6530 [==============================] - 1s 199us/step - loss: 0.0183 - val_loss: 0.0165
Epoch 10/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0115
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1434
 896/6530 [===>..........................] - ETA: 0s - loss: 0.0809
 272/6530 [>.............................] - ETA: 1s - loss: 0.0198
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1438
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0788
 528/6530 [=>............................] - ETA: 1s - loss: 0.0200
4720/6530 [====================>.........] - ETA: 0s - loss: 0.1442
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0796
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0204
4976/6530 [=====================>........] - ETA: 0s - loss: 0.1438
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0789
1040/6530 [===>..........................] - ETA: 1s - loss: 0.0202
5232/6530 [=======================>......] - ETA: 0s - loss: 0.1440
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0790
1312/6530 [=====>........................] - ETA: 1s - loss: 0.0191
5488/6530 [========================>.....] - ETA: 0s - loss: 0.1435
3168/6530 [=============>................] - ETA: 0s - loss: 0.0779
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0183
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1436
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0780
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0181
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1435
4080/6530 [=================>............] - ETA: 0s - loss: 0.0773
2064/6530 [========>.....................] - ETA: 0s - loss: 0.0180
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1429
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0769
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0181
6496/6530 [============================>.] - ETA: 0s - loss: 0.1427
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0767
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0178
6530/6530 [==============================] - 1s 205us/step - loss: 0.1425 - val_loss: 0.1312
Epoch 4/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1825
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0767
2832/6530 [============>.................] - ETA: 0s - loss: 0.0178
 272/6530 [>.............................] - ETA: 1s - loss: 0.1347
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0766
3104/6530 [=============>................] - ETA: 0s - loss: 0.0182
 528/6530 [=>............................] - ETA: 1s - loss: 0.1259
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0767
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0182
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1283
6530/6530 [==============================] - 1s 119us/step - loss: 0.0766 - val_loss: 0.0680
Epoch 22/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0525
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0180
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1308
 432/6530 [>.............................] - ETA: 0s - loss: 0.0795
3872/6530 [================>.............] - ETA: 0s - loss: 0.0181
1280/6530 [====>.........................] - ETA: 1s - loss: 0.1318
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0802
4128/6530 [=================>............] - ETA: 0s - loss: 0.0183
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1321
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0778
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0182
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1318
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0771
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0179
2032/6530 [========>.....................] - ETA: 0s - loss: 0.1318
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0776
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0180
2288/6530 [=========>....................] - ETA: 0s - loss: 0.1317
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0768
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0181
2544/6530 [==========>...................] - ETA: 0s - loss: 0.1316
3104/6530 [=============>................] - ETA: 0s - loss: 0.0764
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0182
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1318
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0774
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0182
3056/6530 [=============>................] - ETA: 0s - loss: 0.1319
4016/6530 [=================>............] - ETA: 0s - loss: 0.0766
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0181
3312/6530 [==============>...............] - ETA: 0s - loss: 0.1317
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0762
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0181
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1314
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0760
6512/6530 [============================>.] - ETA: 0s - loss: 0.0182
3840/6530 [================>.............] - ETA: 0s - loss: 0.1313
6530/6530 [==============================] - 1s 203us/step - loss: 0.0181 - val_loss: 0.0131
Epoch 11/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0175
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0762
4096/6530 [=================>............] - ETA: 0s - loss: 0.1318
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0762
 288/6530 [>.............................] - ETA: 1s - loss: 0.0183
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1310
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0764
 528/6530 [=>............................] - ETA: 1s - loss: 0.0197
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1309
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0185
6530/6530 [==============================] - 1s 119us/step - loss: 0.0763 - val_loss: 0.0689
Epoch 23/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0569
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1306
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0170
 480/6530 [=>............................] - ETA: 0s - loss: 0.0769
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1310
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0177
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0752
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1307
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0173
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0758
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1307
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0173
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0758
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1305
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0175
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0749
6160/6530 [===========================>..] - ETA: 0s - loss: 0.1304
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0172
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0745
6416/6530 [============================>.] - ETA: 0s - loss: 0.1307
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0173
3200/6530 [=============>................] - ETA: 0s - loss: 0.0736
2960/6530 [============>.................] - ETA: 0s - loss: 0.0171
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0740
6530/6530 [==============================] - 1s 208us/step - loss: 0.1309 - val_loss: 0.1201
Epoch 5/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0990
3200/6530 [=============>................] - ETA: 0s - loss: 0.0170
4032/6530 [=================>............] - ETA: 0s - loss: 0.0737
 256/6530 [>.............................] - ETA: 1s - loss: 0.1415
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0169
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0729
 528/6530 [=>............................] - ETA: 1s - loss: 0.1327
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0168
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0728
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1354
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0730
3952/6530 [=================>............] - ETA: 0s - loss: 0.0168
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1335
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0731
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0169
1328/6530 [=====>........................] - ETA: 1s - loss: 0.1325
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0732
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0168
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1324
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0169
6530/6530 [==============================] - 1s 118us/step - loss: 0.0732 - val_loss: 0.0701
Epoch 24/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0623
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1311
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0168
 448/6530 [=>............................] - ETA: 0s - loss: 0.0790
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1313
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0168
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0752
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1302
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0168
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0761
2608/6530 [==========>...................] - ETA: 0s - loss: 0.1301
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0168
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0755
2864/6530 [============>.................] - ETA: 0s - loss: 0.1285
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0168
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0750
3120/6530 [=============>................] - ETA: 0s - loss: 0.1280
6336/6530 [============================>.] - ETA: 0s - loss: 0.0167
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0748
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1278
3152/6530 [=============>................] - ETA: 0s - loss: 0.0746
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1278
6530/6530 [==============================] - 1s 201us/step - loss: 0.0167 - val_loss: 0.0217
Epoch 12/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0160
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0749
3888/6530 [================>.............] - ETA: 0s - loss: 0.1285
 288/6530 [>.............................] - ETA: 1s - loss: 0.0170
4064/6530 [=================>............] - ETA: 0s - loss: 0.0749
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1279
 560/6530 [=>............................] - ETA: 1s - loss: 0.0178
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0747
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1274
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0169
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0748
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1268
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0173
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0750
4912/6530 [=====================>........] - ETA: 0s - loss: 0.1268
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0171
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0750
5168/6530 [======================>.......] - ETA: 0s - loss: 0.1259
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0172
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0749
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1252
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0167
6530/6530 [==============================] - 1s 117us/step - loss: 0.0749 - val_loss: 0.0666
Epoch 25/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0634
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1250
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0163
 464/6530 [=>............................] - ETA: 0s - loss: 0.0738
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1244
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0163
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0733
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1244
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0164
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0723
6464/6530 [============================>.] - ETA: 0s - loss: 0.1240
2912/6530 [============>.................] - ETA: 0s - loss: 0.0165
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0726
3184/6530 [=============>................] - ETA: 0s - loss: 0.0167
6530/6530 [==============================] - 1s 208us/step - loss: 0.1240 - val_loss: 0.1001
Epoch 6/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0981
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0729
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0168
 288/6530 [>.............................] - ETA: 1s - loss: 0.1253
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0738
3728/6530 [================>.............] - ETA: 0s - loss: 0.0167
 528/6530 [=>............................] - ETA: 1s - loss: 0.1273
3072/6530 [=============>................] - ETA: 0s - loss: 0.0732
3968/6530 [=================>............] - ETA: 0s - loss: 0.0166
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1218
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0741
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0167
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1204
3984/6530 [=================>............] - ETA: 0s - loss: 0.0738
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0169
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1205
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0733
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0168
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1219
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0731
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0167
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1212
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0733
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0166
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1211
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0734
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0165
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1213
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0736
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0164
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1209
6530/6530 [==============================] - 1s 119us/step - loss: 0.0738 - val_loss: 0.0705
Epoch 26/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0544
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0164
2880/6530 [============>.................] - ETA: 0s - loss: 0.1217
 464/6530 [=>............................] - ETA: 0s - loss: 0.0746
6384/6530 [============================>.] - ETA: 0s - loss: 0.0165
3136/6530 [=============>................] - ETA: 0s - loss: 0.1216
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0748
3376/6530 [==============>...............] - ETA: 0s - loss: 0.1222
6530/6530 [==============================] - 1s 201us/step - loss: 0.0164 - val_loss: 0.0141
Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0259
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0748
3632/6530 [===============>..............] - ETA: 0s - loss: 0.1215
 272/6530 [>.............................] - ETA: 1s - loss: 0.0204
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0747
3888/6530 [================>.............] - ETA: 0s - loss: 0.1211
 528/6530 [=>............................] - ETA: 1s - loss: 0.0187
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0740
4144/6530 [==================>...........] - ETA: 0s - loss: 0.1205
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0167
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0733
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1213
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0169
3040/6530 [============>.................] - ETA: 0s - loss: 0.0726
4688/6530 [====================>.........] - ETA: 0s - loss: 0.1212
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0166
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0727
4944/6530 [=====================>........] - ETA: 0s - loss: 0.1214
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0170
3952/6530 [=================>............] - ETA: 0s - loss: 0.0723
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1217
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0167
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0720
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1215
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0165
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0722
5728/6530 [=========================>....] - ETA: 0s - loss: 0.1212
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0164
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0721
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1211
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0165
5776/6530 [=========================>....] - ETA: 0s - loss: 0.0719
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1210
2912/6530 [============>.................] - ETA: 0s - loss: 0.0161
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0724
6512/6530 [============================>.] - ETA: 0s - loss: 0.1206
3168/6530 [=============>................] - ETA: 0s - loss: 0.0164
6530/6530 [==============================] - 1s 119us/step - loss: 0.0723 - val_loss: 0.0678
Epoch 27/27

  16/6530 [..............................] - ETA: 0s - loss: 0.0478
6530/6530 [==============================] - 1s 206us/step - loss: 0.1206 - val_loss: 0.1075
Epoch 7/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1175
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0164
 448/6530 [=>............................] - ETA: 0s - loss: 0.0777
 272/6530 [>.............................] - ETA: 1s - loss: 0.1241
3712/6530 [================>.............] - ETA: 0s - loss: 0.0162
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0751
 528/6530 [=>............................] - ETA: 1s - loss: 0.1224
3984/6530 [=================>............] - ETA: 0s - loss: 0.0161
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0736
 752/6530 [==>...........................] - ETA: 1s - loss: 0.1182
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0162
1808/6530 [=======>......................] - ETA: 0s - loss: 0.0732
1008/6530 [===>..........................] - ETA: 1s - loss: 0.1173
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0165
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0726
1264/6530 [====>.........................] - ETA: 1s - loss: 0.1178
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0166
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0728
1520/6530 [=====>........................] - ETA: 1s - loss: 0.1169
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0166
3072/6530 [=============>................] - ETA: 0s - loss: 0.0722
1776/6530 [=======>......................] - ETA: 0s - loss: 0.1169
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0167
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0729
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1166
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0166
3936/6530 [=================>............] - ETA: 0s - loss: 0.0724
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1173
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0165
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0716
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1161
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0167
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0719
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1152
6368/6530 [============================>.] - ETA: 0s - loss: 0.0167
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0720
3040/6530 [============>.................] - ETA: 0s - loss: 0.1148
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0720
6530/6530 [==============================] - 1s 200us/step - loss: 0.0166 - val_loss: 0.0121
Epoch 14/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0147
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1155
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0722
 288/6530 [>.............................] - ETA: 1s - loss: 0.0174
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1155
 560/6530 [=>............................] - ETA: 1s - loss: 0.0170
6530/6530 [==============================] - 1s 120us/step - loss: 0.0722 - val_loss: 0.0685

3824/6530 [================>.............] - ETA: 0s - loss: 0.1152
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0168
4080/6530 [=================>............] - ETA: 0s - loss: 0.1151
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0161
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1157
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0164
# training | RMSE: 0.0850, MAE: 0.0647
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 92, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2149531878533118}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25752557491173744}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12246768342475414}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}]  predicted as label {'loss': 0.08496839688286277, 'rmse': 0.08496839688286277, 'mae': 0.06471405666552235, 'early_stop': False}
vggnet done  1

4592/6530 [====================>.........] - ETA: 0s - loss: 0.1155
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0163
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1150
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0161
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1148
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0169
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1151
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0171
5712/6530 [=========================>....] - ETA: 0s - loss: 0.1147
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0172
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1144
3024/6530 [============>.................] - ETA: 0s - loss: 0.0171
6288/6530 [===========================>..] - ETA: 0s - loss: 0.1155
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0169
6530/6530 [==============================] - 1s 203us/step - loss: 0.1156 - val_loss: 0.0915

3584/6530 [===============>..............] - ETA: 0s - loss: 0.0170Epoch 8/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1459
 272/6530 [>.............................] - ETA: 1s - loss: 0.1298
3856/6530 [================>.............] - ETA: 0s - loss: 0.0170
 528/6530 [=>............................] - ETA: 1s - loss: 0.1243
4112/6530 [=================>............] - ETA: 0s - loss: 0.0170
 784/6530 [==>...........................] - ETA: 1s - loss: 0.1234
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0168
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1195
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0167
1328/6530 [=====>........................] - ETA: 1s - loss: 0.1202
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0165
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1182
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0163
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1173
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0163
2128/6530 [========>.....................] - ETA: 0s - loss: 0.1166
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0162
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1169
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0163
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1159
6384/6530 [============================>.] - ETA: 0s - loss: 0.0163
2944/6530 [============>.................] - ETA: 0s - loss: 0.1151
6530/6530 [==============================] - 1s 192us/step - loss: 0.0163 - val_loss: 0.0125
Epoch 15/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0099
3216/6530 [=============>................] - ETA: 0s - loss: 0.1149
 304/6530 [>.............................] - ETA: 1s - loss: 0.0154
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1146
 592/6530 [=>............................] - ETA: 1s - loss: 0.0136
3760/6530 [================>.............] - ETA: 0s - loss: 0.1133
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0142
4032/6530 [=================>............] - ETA: 0s - loss: 0.1128
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0144
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1128
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0149
4576/6530 [====================>.........] - ETA: 0s - loss: 0.1125
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0148
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1122
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0144
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1122
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0146
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1119
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0146
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1118
2848/6530 [============>.................] - ETA: 0s - loss: 0.0147
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1123
3136/6530 [=============>................] - ETA: 0s - loss: 0.0147
6240/6530 [===========================>..] - ETA: 0s - loss: 0.1120
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0147
6512/6530 [============================>.] - ETA: 0s - loss: 0.1117
3712/6530 [================>.............] - ETA: 0s - loss: 0.0148
6530/6530 [==============================] - 1s 195us/step - loss: 0.1117 - val_loss: 0.0864
Epoch 9/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0682
4000/6530 [=================>............] - ETA: 0s - loss: 0.0148
 288/6530 [>.............................] - ETA: 1s - loss: 0.1067
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0149
 560/6530 [=>............................] - ETA: 1s - loss: 0.1118
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0148
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1140
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0150
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1122
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0149
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1119
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0148
1648/6530 [======>.......................] - ETA: 0s - loss: 0.1140
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0148
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1156
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0148
2192/6530 [=========>....................] - ETA: 0s - loss: 0.1151
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0148
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1160
6496/6530 [============================>.] - ETA: 0s - loss: 0.0149
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1165
6530/6530 [==============================] - 1s 188us/step - loss: 0.0149 - val_loss: 0.0100
Epoch 16/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0102
3008/6530 [============>.................] - ETA: 0s - loss: 0.1157
 304/6530 [>.............................] - ETA: 1s - loss: 0.0169
3264/6530 [=============>................] - ETA: 0s - loss: 0.1162
 576/6530 [=>............................] - ETA: 1s - loss: 0.0165
3536/6530 [===============>..............] - ETA: 0s - loss: 0.1156
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0164
3792/6530 [================>.............] - ETA: 0s - loss: 0.1157
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0160
4064/6530 [=================>............] - ETA: 0s - loss: 0.1149
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0164
4336/6530 [==================>...........] - ETA: 0s - loss: 0.1141
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0161
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1139
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0160
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1131
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0157
5136/6530 [======================>.......] - ETA: 0s - loss: 0.1133
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0158
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1127
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0161
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1127
3056/6530 [=============>................] - ETA: 0s - loss: 0.0164
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1125
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0165
6224/6530 [===========================>..] - ETA: 0s - loss: 0.1124
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0164
6496/6530 [============================>.] - ETA: 0s - loss: 0.1123
3888/6530 [================>.............] - ETA: 0s - loss: 0.0163
6530/6530 [==============================] - 1s 197us/step - loss: 0.1125 - val_loss: 0.0905
Epoch 10/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1232
4128/6530 [=================>............] - ETA: 0s - loss: 0.0163
 288/6530 [>.............................] - ETA: 1s - loss: 0.1000
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0162
 544/6530 [=>............................] - ETA: 1s - loss: 0.1062
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0162
 800/6530 [==>...........................] - ETA: 1s - loss: 0.1078
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0161
1072/6530 [===>..........................] - ETA: 1s - loss: 0.1080
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0162
1344/6530 [=====>........................] - ETA: 1s - loss: 0.1084
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0163
1616/6530 [======>.......................] - ETA: 0s - loss: 0.1079
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0163
1872/6530 [=======>......................] - ETA: 0s - loss: 0.1081
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0163
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1076
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0162
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1085
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1101
6530/6530 [==============================] - 1s 194us/step - loss: 0.0163 - val_loss: 0.0151
Epoch 17/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0282
2960/6530 [============>.................] - ETA: 0s - loss: 0.1111
 304/6530 [>.............................] - ETA: 1s - loss: 0.0154
3216/6530 [=============>................] - ETA: 0s - loss: 0.1115
 592/6530 [=>............................] - ETA: 1s - loss: 0.0148
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1113
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0141
3760/6530 [================>.............] - ETA: 0s - loss: 0.1114
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0151
4032/6530 [=================>............] - ETA: 0s - loss: 0.1118
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0154
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1114
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0150
4592/6530 [====================>.........] - ETA: 0s - loss: 0.1111
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0153
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1109
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0152
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1102
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0151
5392/6530 [=======================>......] - ETA: 0s - loss: 0.1100
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0150
5664/6530 [=========================>....] - ETA: 0s - loss: 0.1097
3072/6530 [=============>................] - ETA: 0s - loss: 0.0150
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1099
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0150
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1097
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0148
6480/6530 [============================>.] - ETA: 0s - loss: 0.1094
3904/6530 [================>.............] - ETA: 0s - loss: 0.0149
6530/6530 [==============================] - 1s 197us/step - loss: 0.1095 - val_loss: 0.0850
Epoch 11/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0890
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0150
 288/6530 [>.............................] - ETA: 1s - loss: 0.1055
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0150
 560/6530 [=>............................] - ETA: 1s - loss: 0.1096
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0151
 816/6530 [==>...........................] - ETA: 1s - loss: 0.1078
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0151
1088/6530 [===>..........................] - ETA: 1s - loss: 0.1088
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0151
1360/6530 [=====>........................] - ETA: 0s - loss: 0.1073
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0151
1632/6530 [======>.......................] - ETA: 0s - loss: 0.1084
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0151
1904/6530 [=======>......................] - ETA: 0s - loss: 0.1091
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0150
2160/6530 [========>.....................] - ETA: 0s - loss: 0.1095
6432/6530 [============================>.] - ETA: 0s - loss: 0.0150
2416/6530 [==========>...................] - ETA: 0s - loss: 0.1098
6530/6530 [==============================] - 1s 191us/step - loss: 0.0150 - val_loss: 0.0133
Epoch 18/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0273
2672/6530 [===========>..................] - ETA: 0s - loss: 0.1088
 288/6530 [>.............................] - ETA: 1s - loss: 0.0155
2928/6530 [============>.................] - ETA: 0s - loss: 0.1087
 528/6530 [=>............................] - ETA: 1s - loss: 0.0145
3184/6530 [=============>................] - ETA: 0s - loss: 0.1091
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0153
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1097
1056/6530 [===>..........................] - ETA: 1s - loss: 0.0149
3728/6530 [================>.............] - ETA: 0s - loss: 0.1099
1328/6530 [=====>........................] - ETA: 1s - loss: 0.0146
4000/6530 [=================>............] - ETA: 0s - loss: 0.1101
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0147
4272/6530 [==================>...........] - ETA: 0s - loss: 0.1093
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0148
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1088
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0150
4816/6530 [=====================>........] - ETA: 0s - loss: 0.1088
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0151
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1090
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0150
5360/6530 [=======================>......] - ETA: 0s - loss: 0.1094
2976/6530 [============>.................] - ETA: 0s - loss: 0.0151
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1091
3264/6530 [=============>................] - ETA: 0s - loss: 0.0151
5904/6530 [==========================>...] - ETA: 0s - loss: 0.1096
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0150
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1093
3808/6530 [================>.............] - ETA: 0s - loss: 0.0151
6448/6530 [============================>.] - ETA: 0s - loss: 0.1090
4096/6530 [=================>............] - ETA: 0s - loss: 0.0156
6530/6530 [==============================] - 1s 198us/step - loss: 0.1092 - val_loss: 0.0946
Epoch 12/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0989
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0156
 288/6530 [>.............................] - ETA: 1s - loss: 0.1078
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0155
 560/6530 [=>............................] - ETA: 1s - loss: 0.1037
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0154
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1018
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0155
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1035
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0155
1344/6530 [=====>........................] - ETA: 1s - loss: 0.1035
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0155
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0156
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1036
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0155
1840/6530 [=======>......................] - ETA: 0s - loss: 0.1048
6480/6530 [============================>.] - ETA: 0s - loss: 0.0154
2096/6530 [========>.....................] - ETA: 0s - loss: 0.1052
6530/6530 [==============================] - 1s 195us/step - loss: 0.0154 - val_loss: 0.0113
Epoch 19/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0126
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1059
 288/6530 [>.............................] - ETA: 1s - loss: 0.0147
2640/6530 [===========>..................] - ETA: 0s - loss: 0.1069
 576/6530 [=>............................] - ETA: 1s - loss: 0.0145
2928/6530 [============>.................] - ETA: 0s - loss: 0.1074
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0144
3200/6530 [=============>................] - ETA: 0s - loss: 0.1067
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0143
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1066
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0145
3712/6530 [================>.............] - ETA: 0s - loss: 0.1056
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0143
3968/6530 [=================>............] - ETA: 0s - loss: 0.1064
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0141
4240/6530 [==================>...........] - ETA: 0s - loss: 0.1068
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0140
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1067
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0144
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1073
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0146
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1065
2992/6530 [============>.................] - ETA: 0s - loss: 0.0147
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1064
3264/6530 [=============>................] - ETA: 0s - loss: 0.0147
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1061
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0146
5840/6530 [=========================>....] - ETA: 0s - loss: 0.1065
3808/6530 [================>.............] - ETA: 0s - loss: 0.0145
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1063
4096/6530 [=================>............] - ETA: 0s - loss: 0.0146
6384/6530 [============================>.] - ETA: 0s - loss: 0.1063
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0146
6530/6530 [==============================] - 1s 201us/step - loss: 0.1063 - val_loss: 0.0830
Epoch 13/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1133
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0146
 272/6530 [>.............................] - ETA: 1s - loss: 0.1009
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0147
 544/6530 [=>............................] - ETA: 1s - loss: 0.1038
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0147
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1059
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0148
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1058
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0148
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1094
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0147
1648/6530 [======>.......................] - ETA: 0s - loss: 0.1095
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0146
1920/6530 [=======>......................] - ETA: 0s - loss: 0.1100
6512/6530 [============================>.] - ETA: 0s - loss: 0.0146
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1100
6530/6530 [==============================] - 1s 196us/step - loss: 0.0146 - val_loss: 0.0138
Epoch 20/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0131
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1088
 288/6530 [>.............................] - ETA: 1s - loss: 0.0164
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1091
 560/6530 [=>............................] - ETA: 1s - loss: 0.0166
2992/6530 [============>.................] - ETA: 0s - loss: 0.1090
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0159
3248/6530 [=============>................] - ETA: 0s - loss: 0.1101
1088/6530 [===>..........................] - ETA: 1s - loss: 0.0155
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1101
1360/6530 [=====>........................] - ETA: 0s - loss: 0.0159
3792/6530 [================>.............] - ETA: 0s - loss: 0.1105
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0154
4048/6530 [=================>............] - ETA: 0s - loss: 0.1092
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0152
4304/6530 [==================>...........] - ETA: 0s - loss: 0.1088
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0153
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1084
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0155
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1081
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0155
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1073
2960/6530 [============>.................] - ETA: 0s - loss: 0.0154
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1069
3248/6530 [=============>................] - ETA: 0s - loss: 0.0154
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1065
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0152
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1061
3808/6530 [================>.............] - ETA: 0s - loss: 0.0153
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1056
4096/6530 [=================>............] - ETA: 0s - loss: 0.0153
6464/6530 [============================>.] - ETA: 0s - loss: 0.1060
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0153
6530/6530 [==============================] - 1s 197us/step - loss: 0.1059 - val_loss: 0.0910
Epoch 14/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0992
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0153
 288/6530 [>.............................] - ETA: 1s - loss: 0.1112
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0152
 560/6530 [=>............................] - ETA: 1s - loss: 0.1101
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0150
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1104
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0149
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1111
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0150
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1117
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0151
1632/6530 [======>.......................] - ETA: 0s - loss: 0.1104
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0150
1904/6530 [=======>......................] - ETA: 0s - loss: 0.1093
6530/6530 [==============================] - 1s 194us/step - loss: 0.0149 - val_loss: 0.0094
Epoch 21/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0185
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1076
 288/6530 [>.............................] - ETA: 1s - loss: 0.0180
2448/6530 [==========>...................] - ETA: 0s - loss: 0.1062
 560/6530 [=>............................] - ETA: 1s - loss: 0.0164
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1054
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0165
2992/6530 [============>.................] - ETA: 0s - loss: 0.1060
3248/6530 [=============>................] - ETA: 0s - loss: 0.1062
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0155
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1054
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0158
3792/6530 [================>.............] - ETA: 0s - loss: 0.1055
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0155
1968/6530 [========>.....................] - ETA: 0s - loss: 0.0155
4080/6530 [=================>............] - ETA: 0s - loss: 0.1057
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1059
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0152
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1059
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0152
4880/6530 [=====================>........] - ETA: 0s - loss: 0.1057
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0152
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1050
3104/6530 [=============>................] - ETA: 0s - loss: 0.0152
5424/6530 [=======================>......] - ETA: 0s - loss: 0.1051
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0152
5680/6530 [=========================>....] - ETA: 0s - loss: 0.1049
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0152
5936/6530 [==========================>...] - ETA: 0s - loss: 0.1049
3904/6530 [================>.............] - ETA: 0s - loss: 0.0152
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1044
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0150
6480/6530 [============================>.] - ETA: 0s - loss: 0.1042
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0149
6530/6530 [==============================] - 1s 198us/step - loss: 0.1045 - val_loss: 0.0846
Epoch 15/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0838
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0147
 272/6530 [>.............................] - ETA: 1s - loss: 0.0919
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0147
 544/6530 [=>............................] - ETA: 1s - loss: 0.1015
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0146
 832/6530 [==>...........................] - ETA: 1s - loss: 0.1050
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0145
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1028
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0144
1376/6530 [=====>........................] - ETA: 0s - loss: 0.1038
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0144
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1041
6416/6530 [============================>.] - ETA: 0s - loss: 0.0144
1936/6530 [=======>......................] - ETA: 0s - loss: 0.1027
6530/6530 [==============================] - 1s 191us/step - loss: 0.0144 - val_loss: 0.0099
Epoch 22/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0222
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1022
 304/6530 [>.............................] - ETA: 1s - loss: 0.0138
2480/6530 [==========>...................] - ETA: 0s - loss: 0.1018
 576/6530 [=>............................] - ETA: 1s - loss: 0.0132
2736/6530 [===========>..................] - ETA: 0s - loss: 0.1018
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0146
3008/6530 [============>.................] - ETA: 0s - loss: 0.1022
1136/6530 [====>.........................] - ETA: 1s - loss: 0.0144
3280/6530 [==============>...............] - ETA: 0s - loss: 0.1023
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0142
3568/6530 [===============>..............] - ETA: 0s - loss: 0.1018
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0142
3840/6530 [================>.............] - ETA: 0s - loss: 0.1017
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0140
4112/6530 [=================>............] - ETA: 0s - loss: 0.1018
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0139
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1016
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0139
4656/6530 [====================>.........] - ETA: 0s - loss: 0.1014
2832/6530 [============>.................] - ETA: 0s - loss: 0.0138
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1012
3120/6530 [=============>................] - ETA: 0s - loss: 0.0138
5200/6530 [======================>.......] - ETA: 0s - loss: 0.1013
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0137
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1017
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0136
5744/6530 [=========================>....] - ETA: 0s - loss: 0.1023
3968/6530 [=================>............] - ETA: 0s - loss: 0.0139
6000/6530 [==========================>...] - ETA: 0s - loss: 0.1019
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0138
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1019
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0139
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0140
6530/6530 [==============================] - 1s 194us/step - loss: 0.1018 - val_loss: 0.0826
Epoch 16/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1237
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0141
 288/6530 [>.............................] - ETA: 1s - loss: 0.1136
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0143
 560/6530 [=>............................] - ETA: 1s - loss: 0.1077
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0144
 816/6530 [==>...........................] - ETA: 1s - loss: 0.1060
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0143
1056/6530 [===>..........................] - ETA: 1s - loss: 0.1067
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0144
1312/6530 [=====>........................] - ETA: 1s - loss: 0.1059
6400/6530 [============================>.] - ETA: 0s - loss: 0.0144
1584/6530 [======>.......................] - ETA: 0s - loss: 0.1064
6530/6530 [==============================] - 1s 191us/step - loss: 0.0144 - val_loss: 0.0087
Epoch 23/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0094
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1065
 304/6530 [>.............................] - ETA: 1s - loss: 0.0128
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1048
 576/6530 [=>............................] - ETA: 1s - loss: 0.0140
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1059
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0138
2704/6530 [===========>..................] - ETA: 0s - loss: 0.1057
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0149
2960/6530 [============>.................] - ETA: 0s - loss: 0.1059
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0145
3216/6530 [=============>................] - ETA: 0s - loss: 0.1055
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0141
3472/6530 [==============>...............] - ETA: 0s - loss: 0.1051
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0137
3744/6530 [================>.............] - ETA: 0s - loss: 0.1048
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0137
4016/6530 [=================>............] - ETA: 0s - loss: 0.1043
2480/6530 [==========>...................] - ETA: 0s - loss: 0.0141
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1048
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0139
4560/6530 [===================>..........] - ETA: 0s - loss: 0.1045
3040/6530 [============>.................] - ETA: 0s - loss: 0.0140
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1049
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0139
5104/6530 [======================>.......] - ETA: 0s - loss: 0.1051
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0138
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1047
3872/6530 [================>.............] - ETA: 0s - loss: 0.0138
5648/6530 [========================>.....] - ETA: 0s - loss: 0.1043
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0137
5920/6530 [==========================>...] - ETA: 0s - loss: 0.1045
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0136
6192/6530 [===========================>..] - ETA: 0s - loss: 0.1046
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0136
6464/6530 [============================>.] - ETA: 0s - loss: 0.1044
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0136
6530/6530 [==============================] - 1s 198us/step - loss: 0.1044 - val_loss: 0.0833
Epoch 17/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1024
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0137
 304/6530 [>.............................] - ETA: 1s - loss: 0.1024
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0137
 592/6530 [=>............................] - ETA: 1s - loss: 0.0997
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0137
 864/6530 [==>...........................] - ETA: 1s - loss: 0.1036
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0137
1136/6530 [====>.........................] - ETA: 1s - loss: 0.1023
6400/6530 [============================>.] - ETA: 0s - loss: 0.0138
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1026
6530/6530 [==============================] - 1s 190us/step - loss: 0.0138 - val_loss: 0.0120
Epoch 24/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0071
1680/6530 [======>.......................] - ETA: 0s - loss: 0.1001
 288/6530 [>.............................] - ETA: 1s - loss: 0.0115
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0996
 576/6530 [=>............................] - ETA: 1s - loss: 0.0117
2224/6530 [=========>....................] - ETA: 0s - loss: 0.1010
 864/6530 [==>...........................] - ETA: 1s - loss: 0.0124
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1020
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0123
2784/6530 [===========>..................] - ETA: 0s - loss: 0.1023
1424/6530 [=====>........................] - ETA: 0s - loss: 0.0127
3056/6530 [=============>................] - ETA: 0s - loss: 0.1018
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0131
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1011
2000/6530 [========>.....................] - ETA: 0s - loss: 0.0135
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1009
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0138
3888/6530 [================>.............] - ETA: 0s - loss: 0.1004
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0137
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1002
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0137
4432/6530 [===================>..........] - ETA: 0s - loss: 0.1000
3088/6530 [=============>................] - ETA: 0s - loss: 0.0137
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0997
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0138
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0996
3664/6530 [===============>..............] - ETA: 0s - loss: 0.0140
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0997
3952/6530 [=================>............] - ETA: 0s - loss: 0.0140
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0996
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0140
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0996
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0140
6096/6530 [===========================>..] - ETA: 0s - loss: 0.1001
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0140
6368/6530 [============================>.] - ETA: 0s - loss: 0.1000
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0139
6530/6530 [==============================] - 1s 193us/step - loss: 0.1005 - val_loss: 0.0793
Epoch 18/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0720
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0141
 272/6530 [>.............................] - ETA: 1s - loss: 0.1075
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0140
 544/6530 [=>............................] - ETA: 1s - loss: 0.1079
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0140
 816/6530 [==>...........................] - ETA: 1s - loss: 0.1046
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0138
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1047
6448/6530 [============================>.] - ETA: 0s - loss: 0.0139
1392/6530 [=====>........................] - ETA: 0s - loss: 0.1044
6530/6530 [==============================] - 1s 189us/step - loss: 0.0139 - val_loss: 0.0181
Epoch 25/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0474
1680/6530 [======>.......................] - ETA: 0s - loss: 0.1032
 304/6530 [>.............................] - ETA: 1s - loss: 0.0150
1968/6530 [========>.....................] - ETA: 0s - loss: 0.1021
 592/6530 [=>............................] - ETA: 1s - loss: 0.0138
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1032
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0141
2512/6530 [==========>...................] - ETA: 0s - loss: 0.1040
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0137
2800/6530 [===========>..................] - ETA: 0s - loss: 0.1022
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0144
3072/6530 [=============>................] - ETA: 0s - loss: 0.1017
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0143
3344/6530 [==============>...............] - ETA: 0s - loss: 0.1016
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0144
3616/6530 [===============>..............] - ETA: 0s - loss: 0.1015
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0139
3888/6530 [================>.............] - ETA: 0s - loss: 0.1025
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0138
4176/6530 [==================>...........] - ETA: 0s - loss: 0.1032
2880/6530 [============>.................] - ETA: 0s - loss: 0.0141
4464/6530 [===================>..........] - ETA: 0s - loss: 0.1031
3168/6530 [=============>................] - ETA: 0s - loss: 0.0140
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1020
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0138
5024/6530 [======================>.......] - ETA: 0s - loss: 0.1017
3744/6530 [================>.............] - ETA: 0s - loss: 0.0139
5296/6530 [=======================>......] - ETA: 0s - loss: 0.1018
4032/6530 [=================>............] - ETA: 0s - loss: 0.0143
5552/6530 [========================>.....] - ETA: 0s - loss: 0.1021
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0142
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1020
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0142
6112/6530 [===========================>..] - ETA: 0s - loss: 0.1015
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0142
6400/6530 [============================>.] - ETA: 0s - loss: 0.1017
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0141
6530/6530 [==============================] - 1s 192us/step - loss: 0.1020 - val_loss: 0.0803
Epoch 19/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1362
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0140
 304/6530 [>.............................] - ETA: 1s - loss: 0.1062
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0141
 576/6530 [=>............................] - ETA: 1s - loss: 0.1055
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0143
 848/6530 [==>...........................] - ETA: 1s - loss: 0.1042
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0142
1104/6530 [====>.........................] - ETA: 1s - loss: 0.1006
6530/6530 [==============================] - 1s 187us/step - loss: 0.0142 - val_loss: 0.0156
Epoch 26/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0159
1360/6530 [=====>........................] - ETA: 0s - loss: 0.1003
 272/6530 [>.............................] - ETA: 1s - loss: 0.0145
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0986
 544/6530 [=>............................] - ETA: 1s - loss: 0.0139
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0998
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0132
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0996
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0136
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0999
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0131
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0990
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0132
2928/6530 [============>.................] - ETA: 0s - loss: 0.0991
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0129
3200/6530 [=============>................] - ETA: 0s - loss: 0.0983
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0130
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0978
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0134
3760/6530 [================>.............] - ETA: 0s - loss: 0.0983
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0134
4032/6530 [=================>............] - ETA: 0s - loss: 0.0982
2992/6530 [============>.................] - ETA: 0s - loss: 0.0131
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0989
3264/6530 [=============>................] - ETA: 0s - loss: 0.0129
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0986
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0128
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0986
3824/6530 [================>.............] - ETA: 0s - loss: 0.0130
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0988
4096/6530 [=================>............] - ETA: 0s - loss: 0.0129
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0990
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0129
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0990
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0129
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0985
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0131
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0984
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0132
6480/6530 [============================>.] - ETA: 0s - loss: 0.0986
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0134
6530/6530 [==============================] - 1s 196us/step - loss: 0.0987 - val_loss: 0.0778
Epoch 20/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0935
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0134
 272/6530 [>.............................] - ETA: 1s - loss: 0.1022
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0134
 528/6530 [=>............................] - ETA: 1s - loss: 0.1038
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0134
 816/6530 [==>...........................] - ETA: 1s - loss: 0.1013
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0977
6530/6530 [==============================] - 1s 193us/step - loss: 0.0136 - val_loss: 0.0105
Epoch 27/27

  16/6530 [..............................] - ETA: 1s - loss: 0.0101
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0986
 288/6530 [>.............................] - ETA: 1s - loss: 0.0149
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0972
 576/6530 [=>............................] - ETA: 1s - loss: 0.0134
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0970
 848/6530 [==>...........................] - ETA: 1s - loss: 0.0134
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0980
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0136
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0972
1392/6530 [=====>........................] - ETA: 0s - loss: 0.0138
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0969
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0140
2960/6530 [============>.................] - ETA: 0s - loss: 0.0977
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0136
3232/6530 [=============>................] - ETA: 0s - loss: 0.0974
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0133
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0969
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0132
3776/6530 [================>.............] - ETA: 0s - loss: 0.0961
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0130
4064/6530 [=================>............] - ETA: 0s - loss: 0.0966
3072/6530 [=============>................] - ETA: 0s - loss: 0.0133
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0967
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0131
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0966
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0132
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0965
3888/6530 [================>.............] - ETA: 0s - loss: 0.0135
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0966
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0136
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0967
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0138
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0966
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0137
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0974
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0137
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0979
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0137
6496/6530 [============================>.] - ETA: 0s - loss: 0.0978
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0138
6530/6530 [==============================] - 1s 196us/step - loss: 0.0978 - val_loss: 0.0817
Epoch 21/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1297
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0137
 288/6530 [>.............................] - ETA: 1s - loss: 0.0940
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0136
 544/6530 [=>............................] - ETA: 1s - loss: 0.0912
6352/6530 [============================>.] - ETA: 0s - loss: 0.0136
 816/6530 [==>...........................] - ETA: 1s - loss: 0.0956
6530/6530 [==============================] - 1s 191us/step - loss: 0.0136 - val_loss: 0.0099

1088/6530 [===>..........................] - ETA: 1s - loss: 0.0945
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0960
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0959
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0950
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0952
# training | RMSE: 0.0889, MAE: 0.0691
worker 0  xfile  [3, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10475440952621362}, 'layer_1_size': 44, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10351097759701844}, 'layer_2_size': 40, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.08892778735748952, 'rmse': 0.08892778735748952, 'mae': 0.06911410959451197, 'early_stop': True}
vggnet done  0

2400/6530 [==========>...................] - ETA: 0s - loss: 0.0953
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0954
2960/6530 [============>.................] - ETA: 0s - loss: 0.0960
3216/6530 [=============>................] - ETA: 0s - loss: 0.0958
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0970
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0968
3872/6530 [================>.............] - ETA: 0s - loss: 0.0967
4064/6530 [=================>............] - ETA: 0s - loss: 0.0966
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0973
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0971
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0970
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0969
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0971
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0973
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0972
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0969
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0969
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0971
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0970
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0969
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0971
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0972
6432/6530 [============================>.] - ETA: 0s - loss: 0.0968
6530/6530 [==============================] - 2s 267us/step - loss: 0.0966 - val_loss: 0.0845
Epoch 22/27

  16/6530 [..............................] - ETA: 3s - loss: 0.0985
 128/6530 [..............................] - ETA: 3s - loss: 0.0967
 256/6530 [>.............................] - ETA: 2s - loss: 0.0945
 384/6530 [>.............................] - ETA: 2s - loss: 0.0917
 512/6530 [=>............................] - ETA: 2s - loss: 0.0948
 640/6530 [=>............................] - ETA: 2s - loss: 0.0962
 768/6530 [==>...........................] - ETA: 2s - loss: 0.0960
 880/6530 [===>..........................] - ETA: 2s - loss: 0.0966
 992/6530 [===>..........................] - ETA: 2s - loss: 0.0963
1120/6530 [====>.........................] - ETA: 2s - loss: 0.0959
1264/6530 [====>.........................] - ETA: 2s - loss: 0.0956
1424/6530 [=====>........................] - ETA: 2s - loss: 0.0954
1584/6530 [======>.......................] - ETA: 1s - loss: 0.0953
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0949
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0958
2064/6530 [========>.....................] - ETA: 1s - loss: 0.0949
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0945
2384/6530 [=========>....................] - ETA: 1s - loss: 0.0941
2544/6530 [==========>...................] - ETA: 1s - loss: 0.0943
2704/6530 [===========>..................] - ETA: 1s - loss: 0.0948
2848/6530 [============>.................] - ETA: 1s - loss: 0.0946
2992/6530 [============>.................] - ETA: 1s - loss: 0.0937
3136/6530 [=============>................] - ETA: 1s - loss: 0.0940
3280/6530 [==============>...............] - ETA: 1s - loss: 0.0936
3408/6530 [==============>...............] - ETA: 1s - loss: 0.0948
3568/6530 [===============>..............] - ETA: 1s - loss: 0.0955
3712/6530 [================>.............] - ETA: 1s - loss: 0.0955
3872/6530 [================>.............] - ETA: 0s - loss: 0.0956
4016/6530 [=================>............] - ETA: 0s - loss: 0.0957
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0958
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0961
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0959
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0959
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0960
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0961
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0961
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0957
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0959
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0961
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0962
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0962
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0961
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0959
6400/6530 [============================>.] - ETA: 0s - loss: 0.0960
6530/6530 [==============================] - 2s 365us/step - loss: 0.0962 - val_loss: 0.0780
Epoch 23/27

  16/6530 [..............................] - ETA: 2s - loss: 0.1074
 176/6530 [..............................] - ETA: 2s - loss: 0.0872
 368/6530 [>.............................] - ETA: 1s - loss: 0.0918
 576/6530 [=>............................] - ETA: 1s - loss: 0.0935
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0928
 992/6530 [===>..........................] - ETA: 1s - loss: 0.0941
1168/6530 [====>.........................] - ETA: 1s - loss: 0.0950
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0964
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0967
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0970
1872/6530 [=======>......................] - ETA: 1s - loss: 0.0967
2032/6530 [========>.....................] - ETA: 1s - loss: 0.0962
2192/6530 [=========>....................] - ETA: 1s - loss: 0.0952
2336/6530 [=========>....................] - ETA: 1s - loss: 0.0948
2496/6530 [==========>...................] - ETA: 1s - loss: 0.0944
2672/6530 [===========>..................] - ETA: 1s - loss: 0.0939
2848/6530 [============>.................] - ETA: 1s - loss: 0.0934
3024/6530 [============>.................] - ETA: 1s - loss: 0.0931
3216/6530 [=============>................] - ETA: 0s - loss: 0.0930
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0928
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0929
3712/6530 [================>.............] - ETA: 0s - loss: 0.0933
3872/6530 [================>.............] - ETA: 0s - loss: 0.0937
4032/6530 [=================>............] - ETA: 0s - loss: 0.0939
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0941
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0940
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0938
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0939
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0943
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0946
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0945
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0944
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0945
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0946
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0946
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0947
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0946
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0944
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0948
6448/6530 [============================>.] - ETA: 0s - loss: 0.0948
6530/6530 [==============================] - 2s 331us/step - loss: 0.0947 - val_loss: 0.0756
Epoch 24/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0735
 192/6530 [..............................] - ETA: 1s - loss: 0.0909
 384/6530 [>.............................] - ETA: 1s - loss: 0.0886
 544/6530 [=>............................] - ETA: 1s - loss: 0.0890
 688/6530 [==>...........................] - ETA: 1s - loss: 0.0885
 832/6530 [==>...........................] - ETA: 1s - loss: 0.0914
 976/6530 [===>..........................] - ETA: 1s - loss: 0.0907
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0912
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0924
1392/6530 [=====>........................] - ETA: 1s - loss: 0.0933
1536/6530 [======>.......................] - ETA: 1s - loss: 0.0929
1680/6530 [======>.......................] - ETA: 1s - loss: 0.0923
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0932
1968/6530 [========>.....................] - ETA: 1s - loss: 0.0934
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0929
2304/6530 [=========>....................] - ETA: 1s - loss: 0.0931
2464/6530 [==========>...................] - ETA: 1s - loss: 0.0928
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0937
2784/6530 [===========>..................] - ETA: 1s - loss: 0.0943
2928/6530 [============>.................] - ETA: 1s - loss: 0.0939
3088/6530 [=============>................] - ETA: 1s - loss: 0.0939
3248/6530 [=============>................] - ETA: 1s - loss: 0.0939
3392/6530 [==============>...............] - ETA: 1s - loss: 0.0941
3536/6530 [===============>..............] - ETA: 1s - loss: 0.0939
3680/6530 [===============>..............] - ETA: 1s - loss: 0.0938
3824/6530 [================>.............] - ETA: 0s - loss: 0.0936
3952/6530 [=================>............] - ETA: 0s - loss: 0.0934
4096/6530 [=================>............] - ETA: 0s - loss: 0.0931
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0934
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0933
4560/6530 [===================>..........] - ETA: 0s - loss: 0.0929
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0926
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0930
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0926
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0926
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0924
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0925
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0922
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0919
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0921
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0923
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0924
6400/6530 [============================>.] - ETA: 0s - loss: 0.0922
6530/6530 [==============================] - 2s 363us/step - loss: 0.0924 - val_loss: 0.0729
Epoch 25/27

  16/6530 [..............................] - ETA: 2s - loss: 0.1059
 208/6530 [..............................] - ETA: 1s - loss: 0.0878
 416/6530 [>.............................] - ETA: 1s - loss: 0.0942
 592/6530 [=>............................] - ETA: 1s - loss: 0.0956
 784/6530 [==>...........................] - ETA: 1s - loss: 0.0949
 928/6530 [===>..........................] - ETA: 1s - loss: 0.0934
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0920
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0932
1360/6530 [=====>........................] - ETA: 1s - loss: 0.0929
1488/6530 [=====>........................] - ETA: 1s - loss: 0.0922
1632/6530 [======>.......................] - ETA: 1s - loss: 0.0920
1776/6530 [=======>......................] - ETA: 1s - loss: 0.0922
1904/6530 [=======>......................] - ETA: 1s - loss: 0.0928
2048/6530 [========>.....................] - ETA: 1s - loss: 0.0930
2176/6530 [========>.....................] - ETA: 1s - loss: 0.0929
2320/6530 [=========>....................] - ETA: 1s - loss: 0.0929
2480/6530 [==========>...................] - ETA: 1s - loss: 0.0915
2640/6530 [===========>..................] - ETA: 1s - loss: 0.0915
2768/6530 [===========>..................] - ETA: 1s - loss: 0.0915
2912/6530 [============>.................] - ETA: 1s - loss: 0.0915
3072/6530 [=============>................] - ETA: 1s - loss: 0.0915
3232/6530 [=============>................] - ETA: 1s - loss: 0.0911
3376/6530 [==============>...............] - ETA: 1s - loss: 0.0908
3520/6530 [===============>..............] - ETA: 1s - loss: 0.0909
3664/6530 [===============>..............] - ETA: 1s - loss: 0.0910
3808/6530 [================>.............] - ETA: 0s - loss: 0.0908
3952/6530 [=================>............] - ETA: 0s - loss: 0.0909
4096/6530 [=================>............] - ETA: 0s - loss: 0.0904
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0906
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0904
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0905
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0906
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0908
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0910
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0912
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0913
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0914
5648/6530 [========================>.....] - ETA: 0s - loss: 0.0917
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0917
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0918
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0916
6224/6530 [===========================>..] - ETA: 0s - loss: 0.0913
6384/6530 [============================>.] - ETA: 0s - loss: 0.0914
6528/6530 [============================>.] - ETA: 0s - loss: 0.0916
6530/6530 [==============================] - 2s 362us/step - loss: 0.0916 - val_loss: 0.0772
Epoch 26/27

  16/6530 [..............................] - ETA: 2s - loss: 0.0773
 160/6530 [..............................] - ETA: 2s - loss: 0.0945
 304/6530 [>.............................] - ETA: 2s - loss: 0.0978
 448/6530 [=>............................] - ETA: 2s - loss: 0.1006
 592/6530 [=>............................] - ETA: 2s - loss: 0.1025
 752/6530 [==>...........................] - ETA: 2s - loss: 0.1037
 912/6530 [===>..........................] - ETA: 2s - loss: 0.1015
1072/6530 [===>..........................] - ETA: 1s - loss: 0.0996
1248/6530 [====>.........................] - ETA: 1s - loss: 0.0960
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0958
1552/6530 [======>.......................] - ETA: 1s - loss: 0.0955
1696/6530 [======>.......................] - ETA: 1s - loss: 0.0949
1840/6530 [=======>......................] - ETA: 1s - loss: 0.0945
1984/6530 [========>.....................] - ETA: 1s - loss: 0.0948
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0952
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0956
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0950
2576/6530 [==========>...................] - ETA: 1s - loss: 0.0942
2736/6530 [===========>..................] - ETA: 1s - loss: 0.0936
2912/6530 [============>.................] - ETA: 1s - loss: 0.0939
3088/6530 [=============>................] - ETA: 1s - loss: 0.0944
3264/6530 [=============>................] - ETA: 1s - loss: 0.0942
3424/6530 [==============>...............] - ETA: 1s - loss: 0.0945
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0946
3776/6530 [================>.............] - ETA: 0s - loss: 0.0944
3952/6530 [=================>............] - ETA: 0s - loss: 0.0944
4128/6530 [=================>............] - ETA: 0s - loss: 0.0945
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0944
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0942
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0941
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0940
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0940
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0939
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0937
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0932
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0931
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0928
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0926
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0929
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0932
6432/6530 [============================>.] - ETA: 0s - loss: 0.0930
6530/6530 [==============================] - 2s 338us/step - loss: 0.0932 - val_loss: 0.0720
Epoch 27/27

  16/6530 [..............................] - ETA: 1s - loss: 0.1403
 224/6530 [>.............................] - ETA: 1s - loss: 0.0908
 432/6530 [>.............................] - ETA: 1s - loss: 0.0920
 640/6530 [=>............................] - ETA: 1s - loss: 0.0940
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0951
1008/6530 [===>..........................] - ETA: 1s - loss: 0.0948
1200/6530 [====>.........................] - ETA: 1s - loss: 0.0953
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0944
1568/6530 [======>.......................] - ETA: 1s - loss: 0.0953
1744/6530 [=======>......................] - ETA: 1s - loss: 0.0948
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0940
2080/6530 [========>.....................] - ETA: 1s - loss: 0.0938
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0931
2400/6530 [==========>...................] - ETA: 1s - loss: 0.0933
2576/6530 [==========>...................] - ETA: 1s - loss: 0.0931
2752/6530 [===========>..................] - ETA: 1s - loss: 0.0930
2912/6530 [============>.................] - ETA: 1s - loss: 0.0933
3088/6530 [=============>................] - ETA: 0s - loss: 0.0934
3248/6530 [=============>................] - ETA: 0s - loss: 0.0930
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0922
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0921
3792/6530 [================>.............] - ETA: 0s - loss: 0.0923
3984/6530 [=================>............] - ETA: 0s - loss: 0.0923
4176/6530 [==================>...........] - ETA: 0s - loss: 0.0922
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0922
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0919
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0921
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0924
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0926
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0925
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0924
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0922
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0921
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0921
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0916
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0917
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0919
6352/6530 [============================>.] - ETA: 0s - loss: 0.0918
6512/6530 [============================>.] - ETA: 0s - loss: 0.0920
6530/6530 [==============================] - 2s 317us/step - loss: 0.0921 - val_loss: 0.0710

# training | RMSE: 0.0854, MAE: 0.0660
worker 2  xfile  [4, 27.0, 0, 100, [], {'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.08543754031104148, 'rmse': 0.08543754031104148, 'mae': 0.06599986867597708, 'early_stop': False}
vggnet done  2
all of workers have been done
calculation finished
#0 epoch=27.0 loss={'loss': 0.08413190280126606, 'rmse': 0.08413190280126606, 'mae': 0.06569707785999676, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 2, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
#2 epoch=27.0 loss={'loss': 0.1138954499796542, 'rmse': 0.1138954499796542, 'mae': 0.09070319933268269, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 75, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 37, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3690655139133455}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.44187056290450344}, 'layer_4_size': 98, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 11, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': True}
#1 epoch=27.0 loss={'loss': 0.08496839688286277, 'rmse': 0.08496839688286277, 'mae': 0.06471405666552235, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': None}, 'layer_1_size': 92, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2149531878533118}, 'layer_2_size': 66, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.25752557491173744}, 'layer_3_size': 38, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 77, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.12246768342475414}, 'layer_5_size': 28, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': False}
#3 epoch=27.0 loss={'loss': 0.08892778735748952, 'rmse': 0.08892778735748952, 'mae': 0.06911410959451197, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.10475440952621362}, 'layer_1_size': 44, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10351097759701844}, 'layer_2_size': 40, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 10, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 59, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_squared_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#4 epoch=27.0 loss={'loss': 0.08543754031104148, 'rmse': 0.08543754031104148, 'mae': 0.06599986867597708, 'early_stop': False} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 29, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 58, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': None}, 'layer_3_size': 64, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 88, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 68, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adam', 'scaler': None, 'shuffle': True}
get a list [results] of length 190
get a list [loss] of length 5
get a list [val_loss] of length 5
length of indices is (0, 1, 4, 3, 2)
length of indices is 5
length of T is 5
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 2, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]] 

*** 1.6666666666666665 configurations x 81.0 iterations each

5 | Thu Sep 27 23:29:18 2018 | lowest loss so far: 0.0809 (run 0)

vggnet done  1
vggnet done  2
{'batch_size': 256,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': False}
layer 1 | size:  64 | activation: relu    | extras: batchnorm 
layer 2 | size:  13 | activation: tanh    | extras: batchnorm 
layer 3 | size:  25 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a77da90>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

 256/6530 [>.............................] - ETA: 24s - loss: 0.3501
3072/6530 [=============>................] - ETA: 1s - loss: 0.0849 
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0576
6530/6530 [==============================] - 1s 185us/step - loss: 0.0556 - val_loss: 0.0252
Epoch 2/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0235
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0224
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0215
6530/6530 [==============================] - 0s 23us/step - loss: 0.0209 - val_loss: 0.0196
Epoch 3/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0167
3072/6530 [=============>................] - ETA: 0s - loss: 0.0167
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0167
6530/6530 [==============================] - 0s 20us/step - loss: 0.0165 - val_loss: 0.0169
Epoch 4/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0137
3072/6530 [=============>................] - ETA: 0s - loss: 0.0144
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0145
6530/6530 [==============================] - 0s 20us/step - loss: 0.0144 - val_loss: 0.0153
Epoch 5/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0122
3840/6530 [================>.............] - ETA: 0s - loss: 0.0132
6530/6530 [==============================] - 0s 16us/step - loss: 0.0131 - val_loss: 0.0141
Epoch 6/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0112
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0123
6530/6530 [==============================] - 0s 17us/step - loss: 0.0123 - val_loss: 0.0133
Epoch 7/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0106
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0116
6530/6530 [==============================] - 0s 16us/step - loss: 0.0116 - val_loss: 0.0127
Epoch 8/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0101
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0111
6530/6530 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0122
Epoch 9/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0097
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0106
6530/6530 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0118
Epoch 10/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0094
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0103
6530/6530 [==============================] - 0s 17us/step - loss: 0.0104 - val_loss: 0.0114
Epoch 11/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0091
3072/6530 [=============>................] - ETA: 0s - loss: 0.0098
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0101
6530/6530 [==============================] - 0s 20us/step - loss: 0.0100 - val_loss: 0.0111
Epoch 12/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0089
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0096
6400/6530 [============================>.] - ETA: 0s - loss: 0.0099
6530/6530 [==============================] - 0s 19us/step - loss: 0.0098 - val_loss: 0.0109
Epoch 13/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0087
3072/6530 [=============>................] - ETA: 0s - loss: 0.0093
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0096
6530/6530 [==============================] - 0s 20us/step - loss: 0.0096 - val_loss: 0.0107
Epoch 14/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0085
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0092
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0094
6530/6530 [==============================] - 0s 20us/step - loss: 0.0093 - val_loss: 0.0105
Epoch 15/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0084
3072/6530 [=============>................] - ETA: 0s - loss: 0.0089
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0091
6530/6530 [==============================] - 0s 21us/step - loss: 0.0092 - val_loss: 0.0103
Epoch 16/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0082
3072/6530 [=============>................] - ETA: 0s - loss: 0.0087
6530/6530 [==============================] - 0s 17us/step - loss: 0.0090 - val_loss: 0.0101
Epoch 17/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0081
3840/6530 [================>.............] - ETA: 0s - loss: 0.0087
6530/6530 [==============================] - 0s 15us/step - loss: 0.0088 - val_loss: 0.0100
Epoch 18/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0080
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0085
6530/6530 [==============================] - 0s 17us/step - loss: 0.0087 - val_loss: 0.0099
Epoch 19/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0079
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0085
6530/6530 [==============================] - 0s 17us/step - loss: 0.0085 - val_loss: 0.0097
Epoch 20/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0078
3072/6530 [=============>................] - ETA: 0s - loss: 0.0082
6400/6530 [============================>.] - ETA: 0s - loss: 0.0085
6530/6530 [==============================] - 0s 18us/step - loss: 0.0084 - val_loss: 0.0096
Epoch 21/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0077
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0082
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0083
6530/6530 [==============================] - 0s 22us/step - loss: 0.0083 - val_loss: 0.0095
Epoch 22/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0076
3840/6530 [================>.............] - ETA: 0s - loss: 0.0081
6530/6530 [==============================] - 0s 15us/step - loss: 0.0082 - val_loss: 0.0095
Epoch 23/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0075
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0079
6530/6530 [==============================] - 0s 14us/step - loss: 0.0081 - val_loss: 0.0094
Epoch 24/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0074
3840/6530 [================>.............] - ETA: 0s - loss: 0.0079
6530/6530 [==============================] - 0s 15us/step - loss: 0.0080 - val_loss: 0.0093
Epoch 25/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0073
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0077
6530/6530 [==============================] - 0s 14us/step - loss: 0.0079 - val_loss: 0.0092
Epoch 26/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0073
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0076
6530/6530 [==============================] - 0s 13us/step - loss: 0.0078 - val_loss: 0.0091
Epoch 27/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0072
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0076
6530/6530 [==============================] - 0s 14us/step - loss: 0.0077 - val_loss: 0.0091
Epoch 28/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0071
3840/6530 [================>.............] - ETA: 0s - loss: 0.0076
6530/6530 [==============================] - 0s 17us/step - loss: 0.0077 - val_loss: 0.0090
Epoch 29/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0071
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0075
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0075
6530/6530 [==============================] - 0s 21us/step - loss: 0.0076 - val_loss: 0.0089
Epoch 30/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0070
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0074
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0075
6530/6530 [==============================] - 0s 22us/step - loss: 0.0075 - val_loss: 0.0089
Epoch 31/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0070
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0073
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0074
6530/6530 [==============================] - 0s 21us/step - loss: 0.0074 - val_loss: 0.0088
Epoch 32/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0069
3072/6530 [=============>................] - ETA: 0s - loss: 0.0072
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0074
6530/6530 [==============================] - 0s 19us/step - loss: 0.0074 - val_loss: 0.0088
Epoch 33/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0069
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0072
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0073
6530/6530 [==============================] - 0s 20us/step - loss: 0.0073 - val_loss: 0.0087
Epoch 34/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0068
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0071
6530/6530 [==============================] - 0s 16us/step - loss: 0.0073 - val_loss: 0.0087
Epoch 35/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0068
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0071
6530/6530 [==============================] - 0s 16us/step - loss: 0.0072 - val_loss: 0.0086
Epoch 36/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0068
3840/6530 [================>.............] - ETA: 0s - loss: 0.0070
6530/6530 [==============================] - 0s 16us/step - loss: 0.0071 - val_loss: 0.0086
Epoch 37/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0067
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0070
6530/6530 [==============================] - 0s 18us/step - loss: 0.0071 - val_loss: 0.0086
Epoch 38/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0067
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0070
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0070
6530/6530 [==============================] - 0s 21us/step - loss: 0.0070 - val_loss: 0.0085
Epoch 39/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0066
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0069
6530/6530 [==============================] - 0s 17us/step - loss: 0.0070 - val_loss: 0.0085
Epoch 40/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0066
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0068
6530/6530 [==============================] - 0s 14us/step - loss: 0.0070 - val_loss: 0.0084
Epoch 41/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0066
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0068
6530/6530 [==============================] - 0s 15us/step - loss: 0.0069 - val_loss: 0.0084
Epoch 42/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0065
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0068
6530/6530 [==============================] - 0s 16us/step - loss: 0.0069 - val_loss: 0.0084
Epoch 43/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0065
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0068
6530/6530 [==============================] - 0s 17us/step - loss: 0.0068 - val_loss: 0.0083
Epoch 44/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0065
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0066
6530/6530 [==============================] - 0s 14us/step - loss: 0.0068 - val_loss: 0.0083
Epoch 45/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0064
3840/6530 [================>.............] - ETA: 0s - loss: 0.0066
6530/6530 [==============================] - 0s 16us/step - loss: 0.0067 - val_loss: 0.0083
Epoch 46/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0064
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0066
6530/6530 [==============================] - 0s 16us/step - loss: 0.0067 - val_loss: 0.0083
Epoch 47/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0064
3840/6530 [================>.............] - ETA: 0s - loss: 0.0066
6530/6530 [==============================] - 0s 15us/step - loss: 0.0067 - val_loss: 0.0082
Epoch 48/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0064
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0065
6530/6530 [==============================] - 0s 14us/step - loss: 0.0066 - val_loss: 0.0082
Epoch 49/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0063
3840/6530 [================>.............] - ETA: 0s - loss: 0.0065
6400/6530 [============================>.] - ETA: 0s - loss: 0.0066
6530/6530 [==============================] - 0s 18us/step - loss: 0.0066 - val_loss: 0.0082
Epoch 50/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0063
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0064
6400/6530 [============================>.] - ETA: 0s - loss: 0.0066
6530/6530 [==============================] - 0s 19us/step - loss: 0.0066 - val_loss: 0.0082
Epoch 51/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0063
3072/6530 [=============>................] - ETA: 0s - loss: 0.0064
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0065
6530/6530 [==============================] - 0s 21us/step - loss: 0.0065 - val_loss: 0.0081
Epoch 52/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0063
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0065
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0065
6530/6530 [==============================] - 0s 22us/step - loss: 0.0065 - val_loss: 0.0081
Epoch 53/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0063
3072/6530 [=============>................] - ETA: 0s - loss: 0.0063
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0065
6530/6530 [==============================] - 0s 20us/step - loss: 0.0065 - val_loss: 0.0081
Epoch 54/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0062
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0063
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0065
6530/6530 [==============================] - 0s 19us/step - loss: 0.0065 - val_loss: 0.0081
Epoch 55/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0062
3072/6530 [=============>................] - ETA: 0s - loss: 0.0063
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0064
6530/6530 [==============================] - 0s 21us/step - loss: 0.0064 - val_loss: 0.0081
Epoch 56/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0062
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0063
6400/6530 [============================>.] - ETA: 0s - loss: 0.0064
6530/6530 [==============================] - 0s 18us/step - loss: 0.0064 - val_loss: 0.0080
Epoch 57/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0062
3072/6530 [=============>................] - ETA: 0s - loss: 0.0062
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0064
6530/6530 [==============================] - 0s 19us/step - loss: 0.0064 - val_loss: 0.0080
Epoch 58/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0062
3840/6530 [================>.............] - ETA: 0s - loss: 0.0062
6530/6530 [==============================] - 0s 16us/step - loss: 0.0064 - val_loss: 0.0080
Epoch 59/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0061
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0063
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0063
6530/6530 [==============================] - 0s 19us/step - loss: 0.0063 - val_loss: 0.0080
Epoch 60/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0061
3072/6530 [=============>................] - ETA: 0s - loss: 0.0061
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0063
6530/6530 [==============================] - 0s 21us/step - loss: 0.0063 - val_loss: 0.0080
Epoch 61/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0061
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0063
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0062
6530/6530 [==============================] - 0s 22us/step - loss: 0.0063 - val_loss: 0.0079
Epoch 62/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0061
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0062
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0062
6530/6530 [==============================] - 0s 21us/step - loss: 0.0063 - val_loss: 0.0079
Epoch 63/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0061
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0062
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0062
6530/6530 [==============================] - 0s 22us/step - loss: 0.0062 - val_loss: 0.0079
Epoch 64/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0061
3072/6530 [=============>................] - ETA: 0s - loss: 0.0061
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0061
6530/6530 [==============================] - 0s 21us/step - loss: 0.0062 - val_loss: 0.0079
Epoch 65/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0061
3072/6530 [=============>................] - ETA: 0s - loss: 0.0060
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0062
6530/6530 [==============================] - 0s 19us/step - loss: 0.0062 - val_loss: 0.0078
Epoch 66/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0060
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0061
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0061
6530/6530 [==============================] - 0s 22us/step - loss: 0.0062 - val_loss: 0.0078
Epoch 67/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0060
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0061
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0061
6530/6530 [==============================] - 0s 19us/step - loss: 0.0062 - val_loss: 0.0078
Epoch 68/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0060
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0060
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0061
6530/6530 [==============================] - 0s 19us/step - loss: 0.0061 - val_loss: 0.0078
Epoch 69/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0060
3072/6530 [=============>................] - ETA: 0s - loss: 0.0060
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0061
6530/6530 [==============================] - 0s 20us/step - loss: 0.0061 - val_loss: 0.0078
Epoch 70/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0060
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0060
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0060
6530/6530 [==============================] - 0s 21us/step - loss: 0.0061 - val_loss: 0.0078
Epoch 71/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0060
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0060
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0060
6530/6530 [==============================] - 0s 23us/step - loss: 0.0061 - val_loss: 0.0077
Epoch 72/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0060
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0060
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0060
6530/6530 [==============================] - 0s 23us/step - loss: 0.0061 - val_loss: 0.0077
Epoch 73/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0060
3072/6530 [=============>................] - ETA: 0s - loss: 0.0059
6400/6530 [============================>.] - ETA: 0s - loss: 0.0061
6530/6530 [==============================] - 0s 19us/step - loss: 0.0060 - val_loss: 0.0077
Epoch 74/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0060
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0060
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0060
6530/6530 [==============================] - 0s 22us/step - loss: 0.0060 - val_loss: 0.0077
Epoch 75/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0060
3072/6530 [=============>................] - ETA: 0s - loss: 0.0059
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0060
6530/6530 [==============================] - 0s 20us/step - loss: 0.0060 - val_loss: 0.0077
Epoch 76/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0060
3072/6530 [=============>................] - ETA: 0s - loss: 0.0058
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0059
6530/6530 [==============================] - 0s 21us/step - loss: 0.0060 - val_loss: 0.0077
Epoch 77/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0059
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0059
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0059
6530/6530 [==============================] - 0s 19us/step - loss: 0.0060 - val_loss: 0.0077
Epoch 78/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0059
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0059
6400/6530 [============================>.] - ETA: 0s - loss: 0.0060
6530/6530 [==============================] - 0s 18us/step - loss: 0.0059 - val_loss: 0.0076
Epoch 79/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0059
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0060
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0059
6530/6530 [==============================] - 0s 23us/step - loss: 0.0059 - val_loss: 0.0076
Epoch 80/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0059
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0059
5120/6530 [======================>.......] - ETA: 0s - loss: 0.0059
6530/6530 [==============================] - 0s 23us/step - loss: 0.0059 - val_loss: 0.0076
Epoch 81/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0059
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0060
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0057
6530/6530 [==============================] - 0s 24us/step - loss: 0.0059 - val_loss: 0.0076

# training | RMSE: 0.0732, MAE: 0.0576
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 2, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}]  predicted as label {'loss': 0.07319952541679313, 'rmse': 0.07319952541679313, 'mae': 0.057579173821543955, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#0 epoch=81.0 loss={'loss': 0.07319952541679313, 'rmse': 0.07319952541679313, 'mae': 0.057579173821543955, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 2, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}
get a list [results] of length 191
get a list [loss] of length 1
get a list [val_loss] of length 1
length of indices is (0,)
length of indices is 1
length of T is 1
s=1
T is of size 8
T=[{'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1868588514544381}, 'layer_1_size': 11, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4285553013409614}, 'layer_4_size': 26, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43041579991282797}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}, {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.11016353490095027}, 'layer_1_size': 98, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10812814927179115}, 'layer_2_size': 60, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18100602082206294}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.177715460906899}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3293809552116107}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1788316905628177}, 'layer_2_size': 52, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47468977120863254}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3739935226573554}, 'layer_2_size': 84, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44288682818485114}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}, {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.44890790587035734}, 'layer_1_size': 53, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21705550570747711}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13139716854445274}, 'layer_2_size': 85, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]
newly formed T structure is:[[0, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1868588514544381}, 'layer_1_size': 11, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4285553013409614}, 'layer_4_size': 26, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}], [1, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43041579991282797}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}], [2, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.11016353490095027}, 'layer_1_size': 98, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10812814927179115}, 'layer_2_size': 60, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18100602082206294}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.177715460906899}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}], [3, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3293809552116107}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [4, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1788316905628177}, 'layer_2_size': 52, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [5, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47468977120863254}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3739935226573554}, 'layer_2_size': 84, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44288682818485114}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}], [6, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.44890790587035734}, 'layer_1_size': 53, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21705550570747711}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [7, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13139716854445274}, 'layer_2_size': 85, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]] 

*** 8 configurations x 27.0 iterations each

1 | Thu Sep 27 23:29:30 2018 | lowest loss so far: 0.0732 (run 0)

{'batch_size': 128,
 'init': 'glorot_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 4,
 'optimizer': 'adadelta',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  98 | activation: sigmoid | extras: dropout - rate: 11.0% 
layer 2 | size:  60 | activation: sigmoid | extras: dropout - rate: 10.8% 
layer 3 | size:  64 | activation: sigmoid | extras: dropout - rate: 18.1% 
layer 4 | size:  94 | activation: sigmoid | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 128/6530 [..............................] - ETA: 51s - loss: 2.1962
1920/6530 [=======>......................] - ETA: 2s - loss: 0.9123 
3712/6530 [================>.............] - ETA: 0s - loss: 0.5825{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adagrad',
 'scaler': None,
 'shuffle': True}
layer 1 | size:  17 | activation: tanh    | extras: batchnorm 
layer 2 | size:   3 | activation: relu    | extras: batchnorm 
layer 3 | size:  29 | activation: tanh    | extras: dropout - rate: 43.0% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a77dc88>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  64/6530 [..............................] - ETA: 1:43 - loss: 0.7010
5760/6530 [=========================>....] - ETA: 0s - loss: 0.4570
 960/6530 [===>..........................] - ETA: 6s - loss: 0.4350  {'batch_size': 128,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'RobustScaler',
 'shuffle': False}
layer 1 | size:  11 | activation: tanh    | extras: dropout - rate: 18.7% 
layer 2 | size:   8 | activation: sigmoid | extras: batchnorm 
layer 3 | size:  22 | activation: sigmoid | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 128/6530 [..............................] - ETA: 54s - loss: 0.5222
1984/6530 [========>.....................] - ETA: 2s - loss: 0.3526
6530/6530 [==============================] - 1s 193us/step - loss: 0.4302 - val_loss: 0.2145
Epoch 2/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2319
2304/6530 [=========>....................] - ETA: 2s - loss: 0.4322 
3072/6530 [=============>................] - ETA: 1s - loss: 0.3109
2560/6530 [==========>...................] - ETA: 0s - loss: 0.2339
4480/6530 [===================>..........] - ETA: 0s - loss: 0.3666
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2890
5120/6530 [======================>.......] - ETA: 0s - loss: 0.2295
6530/6530 [==============================] - 0s 21us/step - loss: 0.2301 - val_loss: 0.2145
Epoch 3/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2490
5312/6530 [=======================>......] - ETA: 0s - loss: 0.2734
6530/6530 [==============================] - 1s 198us/step - loss: 0.3246 - val_loss: 0.2103
Epoch 2/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2196
2816/6530 [===========>..................] - ETA: 0s - loss: 0.2351
6464/6530 [============================>.] - ETA: 0s - loss: 0.2636
2432/6530 [==========>...................] - ETA: 0s - loss: 0.2117
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2306
6530/6530 [==============================] - 1s 213us/step - loss: 0.2628 - val_loss: 0.2210
Epoch 2/27

  64/6530 [..............................] - ETA: 0s - loss: 0.2112
4864/6530 [=====================>........] - ETA: 0s - loss: 0.2026
6530/6530 [==============================] - 0s 20us/step - loss: 0.2302 - val_loss: 0.2140
Epoch 4/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2391
1216/6530 [====>.........................] - ETA: 0s - loss: 0.2100
6530/6530 [==============================] - 0s 24us/step - loss: 0.1992 - val_loss: 0.1732
Epoch 3/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1965
2688/6530 [===========>..................] - ETA: 0s - loss: 0.2336
2368/6530 [=========>....................] - ETA: 0s - loss: 0.2095
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1842
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2268
6530/6530 [==============================] - 0s 21us/step - loss: 0.2259 - val_loss: 0.2159
Epoch 5/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2380
3584/6530 [===============>..............] - ETA: 0s - loss: 0.2057
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1807
2816/6530 [===========>..................] - ETA: 0s - loss: 0.2320
6530/6530 [==============================] - 0s 23us/step - loss: 0.1791 - val_loss: 0.1673

4736/6530 [====================>.........] - ETA: 0s - loss: 0.2054Epoch 4/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1848
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2263
5888/6530 [==========================>...] - ETA: 0s - loss: 0.2045
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1732
6530/6530 [==============================] - 0s 20us/step - loss: 0.2262 - val_loss: 0.2140
Epoch 6/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2282
6530/6530 [==============================] - 0s 46us/step - loss: 0.2049 - val_loss: 0.1959
Epoch 3/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1815
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1717
2688/6530 [===========>..................] - ETA: 0s - loss: 0.2339
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1978
6530/6530 [==============================] - 0s 24us/step - loss: 0.1706 - val_loss: 0.1691
Epoch 5/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1751
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2283
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1976
6530/6530 [==============================] - 0s 21us/step - loss: 0.2266 - val_loss: 0.2215
Epoch 7/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2350
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1695
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1956
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1682
2688/6530 [===========>..................] - ETA: 0s - loss: 0.2288
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1955
6530/6530 [==============================] - 0s 23us/step - loss: 0.1678 - val_loss: 0.1629
Epoch 6/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1797
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2298
6530/6530 [==============================] - 0s 21us/step - loss: 0.2277 - val_loss: 0.2140
Epoch 8/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2218
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1941
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1655
2688/6530 [===========>..................] - ETA: 0s - loss: 0.2289
6530/6530 [==============================] - 0s 47us/step - loss: 0.1933 - val_loss: 0.1898
Epoch 4/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1721
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1641
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2279
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1865
6530/6530 [==============================] - 0s 23us/step - loss: 0.1645 - val_loss: 0.1543
Epoch 7/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1714
6530/6530 [==============================] - 0s 21us/step - loss: 0.2260 - val_loss: 0.2140
Epoch 9/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2257
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1867
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1606
2688/6530 [===========>..................] - ETA: 0s - loss: 0.2283
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1890
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1606
5248/6530 [=======================>......] - ETA: 0s - loss: 0.2255
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1895
6530/6530 [==============================] - 0s 23us/step - loss: 0.1609 - val_loss: 0.1492
Epoch 8/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1570
6530/6530 [==============================] - 0s 21us/step - loss: 0.2241 - val_loss: 0.2141
Epoch 10/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2274
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1871
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1582
2944/6530 [============>.................] - ETA: 0s - loss: 0.2279
6530/6530 [==============================] - 0s 45us/step - loss: 0.1865 - val_loss: 0.1813
Epoch 5/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1856
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1584
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2253
6530/6530 [==============================] - 0s 20us/step - loss: 0.2243 - val_loss: 0.2139
Epoch 11/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2221
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1845
6530/6530 [==============================] - 0s 22us/step - loss: 0.1585 - val_loss: 0.1471
Epoch 9/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1556
2816/6530 [===========>..................] - ETA: 0s - loss: 0.2271
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1835
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1547
5504/6530 [========================>.....] - ETA: 0s - loss: 0.2248
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1828
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1550
6530/6530 [==============================] - 0s 20us/step - loss: 0.2237 - val_loss: 0.2139
Epoch 12/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2265
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1838
6530/6530 [==============================] - 0s 24us/step - loss: 0.1555 - val_loss: 0.1450
Epoch 10/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1704
2688/6530 [===========>..................] - ETA: 0s - loss: 0.2286
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1826
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1554
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2244
6530/6530 [==============================] - 0s 45us/step - loss: 0.1827 - val_loss: 0.1765
Epoch 6/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1770
6530/6530 [==============================] - 0s 20us/step - loss: 0.2227 - val_loss: 0.2151
Epoch 13/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2250
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1538
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1769
2944/6530 [============>.................] - ETA: 0s - loss: 0.2249
6530/6530 [==============================] - 0s 22us/step - loss: 0.1540 - val_loss: 0.1502
Epoch 11/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1654
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1739
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2247
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1536
6530/6530 [==============================] - 0s 19us/step - loss: 0.2235 - val_loss: 0.2142
Epoch 14/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2366
3712/6530 [================>.............] - ETA: 0s - loss: 0.1765
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1514
2944/6530 [============>.................] - ETA: 0s - loss: 0.2263
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1784
6530/6530 [==============================] - 0s 23us/step - loss: 0.1511 - val_loss: 0.1466
Epoch 12/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1616
5632/6530 [========================>.....] - ETA: 0s - loss: 0.2251
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1787
6530/6530 [==============================] - 0s 20us/step - loss: 0.2237 - val_loss: 0.2140
Epoch 15/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2262
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1480
6530/6530 [==============================] - 0s 45us/step - loss: 0.1782 - val_loss: 0.1738
Epoch 7/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1793
2816/6530 [===========>..................] - ETA: 0s - loss: 0.2272
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1469
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1775
6530/6530 [==============================] - 0s 23us/step - loss: 0.1484 - val_loss: 0.1402
Epoch 13/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1527
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2259
6530/6530 [==============================] - 0s 20us/step - loss: 0.2238 - val_loss: 0.2157

2496/6530 [==========>...................] - ETA: 0s - loss: 0.1755Epoch 16/27

 128/6530 [..............................] - ETA: 0s - loss: 0.2305
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1460
3712/6530 [================>.............] - ETA: 0s - loss: 0.1740
2816/6530 [===========>..................] - ETA: 0s - loss: 0.2243
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1467
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2237
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1752
6530/6530 [==============================] - 0s 23us/step - loss: 0.1474 - val_loss: 0.1376
Epoch 14/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1519
6530/6530 [==============================] - 0s 20us/step - loss: 0.2228 - val_loss: 0.2140

6144/6530 [===========================>..] - ETA: 0s - loss: 0.1756
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1437
6530/6530 [==============================] - 0s 45us/step - loss: 0.1759 - val_loss: 0.1731
Epoch 8/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1619
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1427
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1762
6530/6530 [==============================] - 0s 23us/step - loss: 0.1439 - val_loss: 0.1354
Epoch 15/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1523
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1765
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1440
3712/6530 [================>.............] - ETA: 0s - loss: 0.1730
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1434
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1723
6530/6530 [==============================] - 0s 23us/step - loss: 0.1439 - val_loss: 0.1410
Epoch 16/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1545
6272/6530 [===========================>..] - ETA: 0s - loss: 0.1731
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1404
6530/6530 [==============================] - 0s 43us/step - loss: 0.1730 - val_loss: 0.1700
Epoch 9/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1657
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1412
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1734
6530/6530 [==============================] - 0s 23us/step - loss: 0.1415 - val_loss: 0.1330
Epoch 17/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1487
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1730
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1403
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1725
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1407
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1724
6530/6530 [==============================] - 0s 21us/step - loss: 0.1416 - val_loss: 0.1314
Epoch 18/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1468
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1714
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1399
6530/6530 [==============================] - 0s 45us/step - loss: 0.1710 - val_loss: 0.1685
Epoch 10/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1484
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1392
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1711
6530/6530 [==============================] - 0s 23us/step - loss: 0.1400 - val_loss: 0.1307
Epoch 19/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1433
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1706
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1359
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1680
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1376
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1673
6530/6530 [==============================] - 0s 24us/step - loss: 0.1381 - val_loss: 0.1289
Epoch 20/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1384
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1680
6530/6530 [==============================] - 0s 45us/step - loss: 0.1683 - val_loss: 0.1659

2304/6530 [=========>....................] - ETA: 0s - loss: 0.1362Epoch 11/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1640
# training | RMSE: 0.2648, MAE: 0.2173
worker 2  xfile  [2, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.11016353490095027}, 'layer_1_size': 98, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10812814927179115}, 'layer_2_size': 60, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18100602082206294}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.177715460906899}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.264758540460635, 'rmse': 0.264758540460635, 'mae': 0.2173385953343809, 'early_stop': True}
{'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  99 | activation: tanh    | extras: batchnorm 
layer 2 | size:  74 | activation: tanh    | extras: None 
layer 3 | size:  83 | activation: tanh    | extras: dropout - rate: 32.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a77df60>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 1:17 - loss: 0.8892
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1365
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1645
 640/6530 [=>............................] - ETA: 3s - loss: 0.6262  
6530/6530 [==============================] - 0s 24us/step - loss: 0.1372 - val_loss: 0.1291

2368/6530 [=========>....................] - ETA: 0s - loss: 0.1686Epoch 21/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1312
1248/6530 [====>.........................] - ETA: 2s - loss: 0.5074
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1334
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1673
1856/6530 [=======>......................] - ETA: 1s - loss: 0.4459
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1345
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1673
2432/6530 [==========>...................] - ETA: 0s - loss: 0.4085
6530/6530 [==============================] - 0s 23us/step - loss: 0.1349 - val_loss: 0.1282
Epoch 22/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1398
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1667
3040/6530 [============>.................] - ETA: 0s - loss: 0.3789
6530/6530 [==============================] - 0s 45us/step - loss: 0.1675 - val_loss: 0.1648
Epoch 12/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1559
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1342
3680/6530 [===============>..............] - ETA: 0s - loss: 0.3566
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1678
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1352
4288/6530 [==================>...........] - ETA: 0s - loss: 0.3395
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1663
6530/6530 [==============================] - 0s 23us/step - loss: 0.1355 - val_loss: 0.1350
Epoch 23/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1534
4896/6530 [=====================>........] - ETA: 0s - loss: 0.3256
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1673
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1335
5504/6530 [========================>.....] - ETA: 0s - loss: 0.3118
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1654
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1349
6080/6530 [==========================>...] - ETA: 0s - loss: 0.3009
6530/6530 [==============================] - 0s 23us/step - loss: 0.1350 - val_loss: 0.1273

5760/6530 [=========================>....] - ETA: 0s - loss: 0.1661Epoch 24/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1348
6530/6530 [==============================] - 0s 46us/step - loss: 0.1649 - val_loss: 0.1645
Epoch 13/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1422
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1335
6530/6530 [==============================] - 1s 151us/step - loss: 0.2937 - val_loss: 0.3940
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.5059
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1615
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1352
 640/6530 [=>............................] - ETA: 0s - loss: 0.2014
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1622
6530/6530 [==============================] - 0s 23us/step - loss: 0.1349 - val_loss: 0.1347
Epoch 25/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1487
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1809
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1630
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1299
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1774
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1642
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1309
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1751
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1636
6530/6530 [==============================] - 0s 23us/step - loss: 0.1324 - val_loss: 0.1258
Epoch 26/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1432
2976/6530 [============>.................] - ETA: 0s - loss: 0.1724
6530/6530 [==============================] - 0s 46us/step - loss: 0.1631 - val_loss: 0.1659
Epoch 14/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1836
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1289
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1701
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1670
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1303
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1677
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1659
6530/6530 [==============================] - 0s 23us/step - loss: 0.1309 - val_loss: 0.1253
Epoch 27/27

 128/6530 [..............................] - ETA: 0s - loss: 0.1310
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1666
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1637
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1296
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1648
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1636
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1314
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1630
6530/6530 [==============================] - 0s 22us/step - loss: 0.1320 - val_loss: 0.1247

6016/6530 [==========================>...] - ETA: 0s - loss: 0.1625
6530/6530 [==============================] - 1s 88us/step - loss: 0.1624 - val_loss: 0.3105
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.3754
6530/6530 [==============================] - 0s 45us/step - loss: 0.1628 - val_loss: 0.1618
Epoch 15/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1532
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1621
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1625
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1506
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1628
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1638
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1473
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1635
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1446
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1628
3232/6530 [=============>................] - ETA: 0s - loss: 0.1431
6530/6530 [==============================] - 0s 46us/step - loss: 0.1627 - val_loss: 0.1610
Epoch 16/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1622
3904/6530 [================>.............] - ETA: 0s - loss: 0.1421
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1606
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1418
2304/6530 [=========>....................] - ETA: 0s - loss: 0.1607
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1413
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1606
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1408
4544/6530 [===================>..........] - ETA: 0s - loss: 0.1599
6432/6530 [============================>.] - ETA: 0s - loss: 0.1406
6530/6530 [==============================] - 1s 84us/step - loss: 0.1407 - val_loss: 0.2372
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.2378
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1597
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1436
6530/6530 [==============================] - 0s 46us/step - loss: 0.1605 - val_loss: 0.1608
Epoch 17/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1626
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1366
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1672
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1341
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1627
# training | RMSE: 0.1542, MAE: 0.1188
worker 0  xfile  [0, 27.0, 0, 100, [], {'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1868588514544381}, 'layer_1_size': 11, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4285553013409614}, 'layer_4_size': 26, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}]  predicted as label {'loss': 0.154180059138014, 'rmse': 0.154180059138014, 'mae': 0.11876268097850583, 'early_stop': False}
{'batch_size': 256,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  61 | activation: relu    | extras: batchnorm 
layer 2 | size:  52 | activation: sigmoid | extras: dropout - rate: 17.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a77de10>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 256/6530 [>.............................] - ETA: 7s - loss: 0.6714
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1312
3840/6530 [================>.............] - ETA: 0s - loss: 0.1614
6144/6530 [===========================>..] - ETA: 0s - loss: 0.3314
3104/6530 [=============>................] - ETA: 0s - loss: 0.1290
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1611
6530/6530 [==============================] - 0s 61us/step - loss: 0.3225 - val_loss: 0.1816
Epoch 2/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1831
3744/6530 [================>.............] - ETA: 0s - loss: 0.1283
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1594
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1545
6530/6530 [==============================] - 0s 9us/step - loss: 0.1535 - val_loss: 0.1420
Epoch 3/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1296
6530/6530 [==============================] - 0s 44us/step - loss: 0.1600 - val_loss: 0.1586
Epoch 18/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1727
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1283
6400/6530 [============================>.] - ETA: 0s - loss: 0.1308
6530/6530 [==============================] - 0s 9us/step - loss: 0.1308 - val_loss: 0.1280
Epoch 4/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1233
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1579
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1282
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1191
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1590
6530/6530 [==============================] - 0s 9us/step - loss: 0.1183 - val_loss: 0.1184
Epoch 5/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1236
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1274
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1565
6400/6530 [============================>.] - ETA: 0s - loss: 0.1119
6530/6530 [==============================] - 0s 9us/step - loss: 0.1118 - val_loss: 0.1154
Epoch 6/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1112
6176/6530 [===========================>..] - ETA: 0s - loss: 0.1273
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1572
6530/6530 [==============================] - 1s 86us/step - loss: 0.1272 - val_loss: 0.2684

5888/6530 [==========================>...] - ETA: 0s - loss: 0.1061Epoch 5/27

  32/6530 [..............................] - ETA: 0s - loss: 0.3246
6530/6530 [==============================] - 0s 9us/step - loss: 0.1057 - val_loss: 0.1096
Epoch 7/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0915
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1569
 640/6530 [=>............................] - ETA: 0s - loss: 0.1392
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1010
6530/6530 [==============================] - 0s 9us/step - loss: 0.1008 - val_loss: 0.1048
Epoch 8/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0956
6530/6530 [==============================] - 0s 45us/step - loss: 0.1570 - val_loss: 0.1582
Epoch 19/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1759
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1279
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0966
6530/6530 [==============================] - 0s 9us/step - loss: 0.0964 - val_loss: 0.1028
Epoch 9/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0879
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1597
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1241
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0932
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1585
6530/6530 [==============================] - 0s 9us/step - loss: 0.0929 - val_loss: 0.1058
Epoch 10/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0889
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1229
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1591
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0902
6530/6530 [==============================] - 0s 10us/step - loss: 0.0902 - val_loss: 0.0972
Epoch 11/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0786
2944/6530 [============>.................] - ETA: 0s - loss: 0.1218
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1577
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0890
3552/6530 [===============>..............] - ETA: 0s - loss: 0.1209
6530/6530 [==============================] - 0s 10us/step - loss: 0.0884 - val_loss: 0.0935
Epoch 12/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0918
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1579
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1191
6400/6530 [============================>.] - ETA: 0s - loss: 0.0854
6530/6530 [==============================] - 0s 9us/step - loss: 0.0859 - val_loss: 0.0950
Epoch 13/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0816
6530/6530 [==============================] - 0s 47us/step - loss: 0.1584 - val_loss: 0.1579
Epoch 20/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1692
4768/6530 [====================>.........] - ETA: 0s - loss: 0.1200
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0847
6530/6530 [==============================] - 0s 9us/step - loss: 0.0845 - val_loss: 0.0949
Epoch 14/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0874
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1599
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1193
6400/6530 [============================>.] - ETA: 0s - loss: 0.0824
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1599
6530/6530 [==============================] - 0s 9us/step - loss: 0.0825 - val_loss: 0.0880
Epoch 15/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0820
5984/6530 [==========================>...] - ETA: 0s - loss: 0.1188
3712/6530 [================>.............] - ETA: 0s - loss: 0.1576
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0795
6530/6530 [==============================] - 0s 9us/step - loss: 0.0796 - val_loss: 0.0898
Epoch 16/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0764
6530/6530 [==============================] - 1s 89us/step - loss: 0.1188 - val_loss: 0.1568
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1526
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1566
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0784
6530/6530 [==============================] - 0s 9us/step - loss: 0.0784 - val_loss: 0.0876
Epoch 17/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0851
 576/6530 [=>............................] - ETA: 0s - loss: 0.1259
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1570
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0765
6530/6530 [==============================] - 0s 9us/step - loss: 0.0768 - val_loss: 0.0846
Epoch 18/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0690
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1183
6530/6530 [==============================] - 0s 47us/step - loss: 0.1573 - val_loss: 0.1577
Epoch 21/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1598
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0761
6530/6530 [==============================] - 0s 9us/step - loss: 0.0761 - val_loss: 0.0814
Epoch 19/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0675
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1156
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1541
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0753
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1132
6530/6530 [==============================] - 0s 9us/step - loss: 0.0747 - val_loss: 0.0835

2368/6530 [=========>....................] - ETA: 0s - loss: 0.1558Epoch 20/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0735
2880/6530 [============>.................] - ETA: 0s - loss: 0.1123
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1551
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0744
6530/6530 [==============================] - 0s 9us/step - loss: 0.0742 - val_loss: 0.0793
Epoch 21/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0708
3488/6530 [===============>..............] - ETA: 0s - loss: 0.1124
4736/6530 [====================>.........] - ETA: 0s - loss: 0.1557
6400/6530 [============================>.] - ETA: 0s - loss: 0.0720
6530/6530 [==============================] - 0s 9us/step - loss: 0.0719 - val_loss: 0.0844
Epoch 22/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0775
4096/6530 [=================>............] - ETA: 0s - loss: 0.1114
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1560
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0718
6530/6530 [==============================] - 0s 9us/step - loss: 0.0717 - val_loss: 0.0818
Epoch 23/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0736
6530/6530 [==============================] - 0s 46us/step - loss: 0.1568 - val_loss: 0.1571

4704/6530 [====================>.........] - ETA: 0s - loss: 0.1114Epoch 22/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1446
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0708
6530/6530 [==============================] - 0s 9us/step - loss: 0.0710 - val_loss: 0.0753
Epoch 24/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0664
5280/6530 [=======================>......] - ETA: 0s - loss: 0.1110
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1493
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0715
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1107
6530/6530 [==============================] - 0s 9us/step - loss: 0.0714 - val_loss: 0.0784

2368/6530 [=========>....................] - ETA: 0s - loss: 0.1537Epoch 25/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0695
6496/6530 [============================>.] - ETA: 0s - loss: 0.1110
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1556
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0689
6530/6530 [==============================] - 0s 9us/step - loss: 0.0690 - val_loss: 0.0734
Epoch 26/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0685
6530/6530 [==============================] - 1s 90us/step - loss: 0.1110 - val_loss: 0.2270
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.2782
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1550
6400/6530 [============================>.] - ETA: 0s - loss: 0.0683
6530/6530 [==============================] - 0s 9us/step - loss: 0.0685 - val_loss: 0.0742
Epoch 27/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0691
 640/6530 [=>............................] - ETA: 0s - loss: 0.1223
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1567
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1151
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0678
6530/6530 [==============================] - 0s 9us/step - loss: 0.0679 - val_loss: 0.0722

6530/6530 [==============================] - 0s 46us/step - loss: 0.1561 - val_loss: 0.1578
Epoch 23/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1413
1888/6530 [=======>......................] - ETA: 0s - loss: 0.1124
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1528
2528/6530 [==========>...................] - ETA: 0s - loss: 0.1101
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1518
3104/6530 [=============>................] - ETA: 0s - loss: 0.1082
3840/6530 [================>.............] - ETA: 0s - loss: 0.1528
3680/6530 [===============>..............] - ETA: 0s - loss: 0.1078
5056/6530 [======================>.......] - ETA: 0s - loss: 0.1541
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1077
6336/6530 [============================>.] - ETA: 0s - loss: 0.1552
6530/6530 [==============================] - 0s 43us/step - loss: 0.1554 - val_loss: 0.1560
Epoch 24/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1490
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1078
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1560
5536/6530 [========================>.....] - ETA: 0s - loss: 0.1069
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1568
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1068
3776/6530 [================>.............] - ETA: 0s - loss: 0.1567
6530/6530 [==============================] - 1s 86us/step - loss: 0.1068 - val_loss: 0.1600
Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1251
4992/6530 [=====================>........] - ETA: 0s - loss: 0.1557
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1108
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1550
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1061
6530/6530 [==============================] - 0s 44us/step - loss: 0.1550 - val_loss: 0.1558
Epoch 25/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1564
# training | RMSE: 0.0881, MAE: 0.0657
worker 0  xfile  [4, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1788316905628177}, 'layer_2_size': 52, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.08806374313399128, 'rmse': 0.08806374313399128, 'mae': 0.06566941240009284, 'early_stop': False}
{'batch_size': 32,
 'init': 'glorot_normal',
 'loss': 'mean_squared_error',
 'n_layers': 3,
 'optimizer': 'rmsprop',
 'scaler': 'MinMaxScaler',
 'shuffle': False}
layer 1 | size:  90 | activation: tanh    | extras: dropout - rate: 47.5% 
layer 2 | size:  84 | activation: sigmoid | extras: dropout - rate: 37.4% 
layer 3 | size:   7 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7778685e10>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 48s - loss: 0.5695
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1039
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1509
 992/6530 [===>..........................] - ETA: 1s - loss: 0.3929 
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1031
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1530
1920/6530 [=======>......................] - ETA: 0s - loss: 0.2757
3200/6530 [=============>................] - ETA: 0s - loss: 0.1026
3520/6530 [===============>..............] - ETA: 0s - loss: 0.1534
2816/6530 [===========>..................] - ETA: 0s - loss: 0.2181
3776/6530 [================>.............] - ETA: 0s - loss: 0.1028
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1525
3712/6530 [================>.............] - ETA: 0s - loss: 0.1838
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1028
5824/6530 [=========================>....] - ETA: 0s - loss: 0.1534
4672/6530 [====================>.........] - ETA: 0s - loss: 0.1594
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1027
6530/6530 [==============================] - 0s 46us/step - loss: 0.1541 - val_loss: 0.1551
Epoch 26/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1504
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1435
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1022
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1522
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1021
2496/6530 [==========>...................] - ETA: 0s - loss: 0.1530
6530/6530 [==============================] - 1s 98us/step - loss: 0.1324 - val_loss: 0.0588
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0787
6530/6530 [==============================] - 1s 86us/step - loss: 0.1021 - val_loss: 0.1849
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.2356
3648/6530 [===============>..............] - ETA: 0s - loss: 0.1539
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0655
 640/6530 [=>............................] - ETA: 0s - loss: 0.1145
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1526
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0618
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1050
5952/6530 [==========================>...] - ETA: 0s - loss: 0.1535
2848/6530 [============>.................] - ETA: 0s - loss: 0.0596
1856/6530 [=======>......................] - ETA: 0s - loss: 0.1029
6530/6530 [==============================] - 0s 45us/step - loss: 0.1532 - val_loss: 0.1542
Epoch 27/27

  64/6530 [..............................] - ETA: 0s - loss: 0.1451
3808/6530 [================>.............] - ETA: 0s - loss: 0.0581
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1010
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1542
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0569
3072/6530 [=============>................] - ETA: 0s - loss: 0.0998
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1548
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0556
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0994
3456/6530 [==============>...............] - ETA: 0s - loss: 0.1549
6530/6530 [==============================] - 0s 57us/step - loss: 0.0548 - val_loss: 0.0451
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0603
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0985
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1534
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0523
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0983
5760/6530 [=========================>....] - ETA: 0s - loss: 0.1531
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0499
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0980
6530/6530 [==============================] - 0s 46us/step - loss: 0.1536 - val_loss: 0.1537

2848/6530 [============>.................] - ETA: 0s - loss: 0.0486
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0977
3776/6530 [================>.............] - ETA: 0s - loss: 0.0484
6530/6530 [==============================] - 1s 88us/step - loss: 0.0977 - val_loss: 0.1036
Epoch 10/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1121
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0483
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1039
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0481
1248/6530 [====>.........................] - ETA: 0s - loss: 0.1004
6530/6530 [==============================] - 0s 55us/step - loss: 0.0480 - val_loss: 0.0457
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0654
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0978
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0501
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0969
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0468
3040/6530 [============>.................] - ETA: 0s - loss: 0.0970
2944/6530 [============>.................] - ETA: 0s - loss: 0.0461
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0961
3936/6530 [=================>............] - ETA: 0s - loss: 0.0459
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0961
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0460
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0964
# training | RMSE: 0.1887, MAE: 0.1484
worker 1  xfile  [1, 27.0, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43041579991282797}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}]  predicted as label {'loss': 0.18867522445597557, 'rmse': 0.18867522445597557, 'mae': 0.1484368083079586, 'early_stop': False}
{'batch_size': 32,
 'init': 'uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  53 | activation: sigmoid | extras: dropout - rate: 44.9% 
layer 2 | size:  27 | activation: tanh    | extras: batchnorm 

xnet is <keras.engine.sequential.Sequential object at 0x7f77780e4780>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/27

  32/6530 [..............................] - ETA: 38s - loss: 0.7170
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0458
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0958
1120/6530 [====>.........................] - ETA: 1s - loss: 0.3486 
6530/6530 [==============================] - 0s 55us/step - loss: 0.0455 - val_loss: 0.0446
Epoch 5/27

  32/6530 [..............................] - ETA: 1s - loss: 0.0578
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0955
2176/6530 [========>.....................] - ETA: 0s - loss: 0.2887
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0482
3200/6530 [=============>................] - ETA: 0s - loss: 0.2673
6530/6530 [==============================] - 1s 89us/step - loss: 0.0954 - val_loss: 0.1463
Epoch 11/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1138
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0455
4160/6530 [==================>...........] - ETA: 0s - loss: 0.2558
 576/6530 [=>............................] - ETA: 0s - loss: 0.1017
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0451
5280/6530 [=======================>......] - ETA: 0s - loss: 0.2441
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0953
3776/6530 [================>.............] - ETA: 0s - loss: 0.0447
6368/6530 [============================>.] - ETA: 0s - loss: 0.2392
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0948
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0450
6530/6530 [==============================] - 1s 82us/step - loss: 0.2384 - val_loss: 0.2013
Epoch 2/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1675
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0936
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0449
1120/6530 [====>.........................] - ETA: 0s - loss: 0.2057
2976/6530 [============>.................] - ETA: 0s - loss: 0.0941
6528/6530 [============================>.] - ETA: 0s - loss: 0.0446
6530/6530 [==============================] - 0s 58us/step - loss: 0.0446 - val_loss: 0.0448
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0544
2176/6530 [========>.....................] - ETA: 0s - loss: 0.2057
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0936
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0464
3232/6530 [=============>................] - ETA: 0s - loss: 0.2050
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0935
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0444
4288/6530 [==================>...........] - ETA: 0s - loss: 0.2062
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0942
2880/6530 [============>.................] - ETA: 0s - loss: 0.0451
5408/6530 [=======================>......] - ETA: 0s - loss: 0.2043
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0935
3872/6530 [================>.............] - ETA: 0s - loss: 0.0449
6528/6530 [============================>.] - ETA: 0s - loss: 0.2037
6530/6530 [==============================] - 0s 49us/step - loss: 0.2037 - val_loss: 0.1879
Epoch 3/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1652
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0931
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0452
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1941
6530/6530 [==============================] - 1s 89us/step - loss: 0.0932 - val_loss: 0.1911
Epoch 12/27

  32/6530 [..............................] - ETA: 0s - loss: 0.2218
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0450
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1948
 608/6530 [=>............................] - ETA: 0s - loss: 0.1050
6530/6530 [==============================] - 0s 56us/step - loss: 0.0448 - val_loss: 0.0436
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0519
3264/6530 [=============>................] - ETA: 0s - loss: 0.1932
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0968
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0442
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1906
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0945
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0427
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1913
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0932
2912/6530 [============>.................] - ETA: 0s - loss: 0.0430
6530/6530 [==============================] - 0s 49us/step - loss: 0.1913 - val_loss: 0.1784
Epoch 4/27

  32/6530 [..............................] - ETA: 0s - loss: 0.2159
3040/6530 [============>.................] - ETA: 0s - loss: 0.0922
3872/6530 [================>.............] - ETA: 0s - loss: 0.0429
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1857
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0921
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0431
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1866
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0913
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0432
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1870
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0910
6530/6530 [==============================] - 0s 55us/step - loss: 0.0431 - val_loss: 0.0436
Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0487
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1838
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0898
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0459
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1837
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0897
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0435
6528/6530 [============================>.] - ETA: 0s - loss: 0.1829
6530/6530 [==============================] - 1s 86us/step - loss: 0.0899 - val_loss: 0.1464
Epoch 13/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1100
6530/6530 [==============================] - 0s 49us/step - loss: 0.1829 - val_loss: 0.1667
Epoch 5/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1860
2912/6530 [============>.................] - ETA: 0s - loss: 0.0437
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0992
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1706
3872/6530 [================>.............] - ETA: 0s - loss: 0.0435
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0936
2240/6530 [=========>....................] - ETA: 0s - loss: 0.1759
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0435
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0911
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1776
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0434
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0900
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1772
6530/6530 [==============================] - 0s 55us/step - loss: 0.0434 - val_loss: 0.0431
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0520
3104/6530 [=============>................] - ETA: 0s - loss: 0.0895
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1774
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0448
3744/6530 [================>.............] - ETA: 0s - loss: 0.0885
6530/6530 [==============================] - 0s 48us/step - loss: 0.1779 - val_loss: 0.1621
Epoch 6/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1491
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0426
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0882
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1716
2912/6530 [============>.................] - ETA: 0s - loss: 0.0430
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0881
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1732
3840/6530 [================>.............] - ETA: 0s - loss: 0.0427
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0869
3264/6530 [=============>................] - ETA: 0s - loss: 0.1733
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0430
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0870
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1735
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0430
6530/6530 [==============================] - 1s 87us/step - loss: 0.0871 - val_loss: 0.1484
Epoch 14/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1825
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1734
6530/6530 [==============================] - 0s 55us/step - loss: 0.0429 - val_loss: 0.0432
Epoch 10/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0507
 640/6530 [=>............................] - ETA: 0s - loss: 0.0974
6528/6530 [============================>.] - ETA: 0s - loss: 0.1738
6530/6530 [==============================] - 0s 49us/step - loss: 0.1738 - val_loss: 0.1612
Epoch 7/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1800
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0430
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0926
1120/6530 [====>.........................] - ETA: 0s - loss: 0.1748
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0414
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0890
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1742
2880/6530 [============>.................] - ETA: 0s - loss: 0.0421
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0878
3264/6530 [=============>................] - ETA: 0s - loss: 0.1728
3872/6530 [================>.............] - ETA: 0s - loss: 0.0418
2912/6530 [============>.................] - ETA: 0s - loss: 0.0869
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1735
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0421
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0866
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1731
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0420
4128/6530 [=================>............] - ETA: 0s - loss: 0.0864
6528/6530 [============================>.] - ETA: 0s - loss: 0.1734
6530/6530 [==============================] - 0s 56us/step - loss: 0.0420 - val_loss: 0.0429

6530/6530 [==============================] - 0s 49us/step - loss: 0.1734 - val_loss: 0.1605
Epoch 11/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0555Epoch 8/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1399
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0860
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0426
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1709
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0856
2144/6530 [========>.....................] - ETA: 0s - loss: 0.1731
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0408
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0851
2880/6530 [============>.................] - ETA: 0s - loss: 0.0413
3264/6530 [=============>................] - ETA: 0s - loss: 0.1750
6530/6530 [==============================] - 1s 88us/step - loss: 0.0850 - val_loss: 0.1423

3808/6530 [================>.............] - ETA: 0s - loss: 0.0410
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1737
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0413
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1735
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0412
6530/6530 [==============================] - 0s 49us/step - loss: 0.1738 - val_loss: 0.1608
Epoch 9/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1787
6530/6530 [==============================] - 0s 56us/step - loss: 0.0412 - val_loss: 0.0409
Epoch 12/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0498
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1711
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0418
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1714
2048/6530 [========>.....................] - ETA: 0s - loss: 0.0402
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1701
3072/6530 [=============>................] - ETA: 0s - loss: 0.0404
4480/6530 [===================>..........] - ETA: 0s - loss: 0.1713
4096/6530 [=================>............] - ETA: 0s - loss: 0.0407
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1708
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0409
6530/6530 [==============================] - 0s 47us/step - loss: 0.1705 - val_loss: 0.1610
Epoch 10/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1702
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0411
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1761
6530/6530 [==============================] - 0s 52us/step - loss: 0.0409 - val_loss: 0.0396
Epoch 13/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0471
2176/6530 [========>.....................] - ETA: 0s - loss: 0.1741
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0413
3264/6530 [=============>................] - ETA: 0s - loss: 0.1721
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0394
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1713
3072/6530 [=============>................] - ETA: 0s - loss: 0.0389
5408/6530 [=======================>......] - ETA: 0s - loss: 0.1702
4064/6530 [=================>............] - ETA: 0s - loss: 0.0391
6496/6530 [============================>.] - ETA: 0s - loss: 0.1695
6530/6530 [==============================] - 0s 49us/step - loss: 0.1694 - val_loss: 0.1636
Epoch 11/27

  32/6530 [..............................] - ETA: 0s - loss: 0.2002
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0390
1088/6530 [===>..........................] - ETA: 0s - loss: 0.1651
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0392
2208/6530 [=========>....................] - ETA: 0s - loss: 0.1734
6530/6530 [==============================] - 0s 54us/step - loss: 0.0389 - val_loss: 0.0412
Epoch 14/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0497
3328/6530 [==============>...............] - ETA: 0s - loss: 0.1725
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0390
# training | RMSE: 0.1744, MAE: 0.1371
worker 2  xfile  [3, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3293809552116107}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.17443202194827762, 'rmse': 0.17443202194827762, 'mae': 0.1371217290049391, 'early_stop': True}
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  52 | activation: relu    | extras: None 
layer 2 | size:  85 | activation: tanh    | extras: dropout - rate: 13.1% 
layer 3 | size:  81 | activation: sigmoid | extras: None 
layer 4 | size:  38 | activation: tanh    | extras: None 
layer 5 | size:  41 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f77781e5668>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/27

 256/6530 [>.............................] - ETA: 9s - loss: 0.6291
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1713
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0371
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2264
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1711
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0376
6530/6530 [==============================] - 1s 77us/step - loss: 0.2159 - val_loss: 0.3077
Epoch 2/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.3065
6530/6530 [==============================] - 0s 48us/step - loss: 0.1701 - val_loss: 0.1607
Epoch 12/27

  32/6530 [..............................] - ETA: 0s - loss: 0.1794
3744/6530 [================>.............] - ETA: 0s - loss: 0.0371
5376/6530 [=======================>......] - ETA: 0s - loss: 0.2056
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0376
1152/6530 [====>.........................] - ETA: 0s - loss: 0.1683
6530/6530 [==============================] - 0s 11us/step - loss: 0.2014 - val_loss: 0.1986
Epoch 3/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1961
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0380
2272/6530 [=========>....................] - ETA: 0s - loss: 0.1696
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1887
6530/6530 [==============================] - 0s 10us/step - loss: 0.1841 - val_loss: 0.1765
Epoch 4/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1699
3296/6530 [==============>...............] - ETA: 0s - loss: 0.1692
6530/6530 [==============================] - 0s 56us/step - loss: 0.0381 - val_loss: 0.0363
Epoch 15/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0398
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1752
6530/6530 [==============================] - 0s 11us/step - loss: 0.1715 - val_loss: 0.1841
Epoch 5/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1793
4416/6530 [===================>..........] - ETA: 0s - loss: 0.1689
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0347
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1687
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1686
6530/6530 [==============================] - 0s 11us/step - loss: 0.1659 - val_loss: 0.1629

1952/6530 [=======>......................] - ETA: 0s - loss: 0.0357Epoch 6/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1584
6336/6530 [============================>.] - ETA: 0s - loss: 0.1688
2880/6530 [============>.................] - ETA: 0s - loss: 0.0359
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1610
6530/6530 [==============================] - 0s 50us/step - loss: 0.1686 - val_loss: 0.1607

6530/6530 [==============================] - 0s 11us/step - loss: 0.1582 - val_loss: 0.1553
Epoch 7/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1512
3872/6530 [================>.............] - ETA: 0s - loss: 0.0360
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1483
6530/6530 [==============================] - 0s 11us/step - loss: 0.1483 - val_loss: 0.1343
Epoch 8/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1299
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0361
# training | RMSE: 0.2031, MAE: 0.1604
worker 1  xfile  [6, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.44890790587035734}, 'layer_1_size': 53, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21705550570747711}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.20306987393967085, 'rmse': 0.20306987393967085, 'mae': 0.16037857230516284, 'early_stop': True}
vggnet done  1

5120/6530 [======================>.......] - ETA: 0s - loss: 0.1454
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0363
6530/6530 [==============================] - 0s 11us/step - loss: 0.1441 - val_loss: 0.1587
Epoch 9/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1538
6530/6530 [==============================] - 0s 54us/step - loss: 0.0362 - val_loss: 0.0367
Epoch 16/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0413
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1395
6530/6530 [==============================] - 0s 10us/step - loss: 0.1376 - val_loss: 0.1285
Epoch 10/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1238
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0347
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1317
6530/6530 [==============================] - 0s 10us/step - loss: 0.1304 - val_loss: 0.1325
Epoch 11/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1266
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0327
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1268
6530/6530 [==============================] - 0s 10us/step - loss: 0.1262 - val_loss: 0.1250

3104/6530 [=============>................] - ETA: 0s - loss: 0.0334Epoch 12/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1211
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0335
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1226
6530/6530 [==============================] - 0s 10us/step - loss: 0.1219 - val_loss: 0.1210
Epoch 13/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1165
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0336
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1176
6530/6530 [==============================] - 0s 10us/step - loss: 0.1176 - val_loss: 0.1163
Epoch 14/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1091
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0337
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1170
6530/6530 [==============================] - 0s 52us/step - loss: 0.0334 - val_loss: 0.0349
Epoch 17/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0284
6530/6530 [==============================] - 0s 10us/step - loss: 0.1162 - val_loss: 0.1046
Epoch 15/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0987
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0323
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1126
6530/6530 [==============================] - 0s 10us/step - loss: 0.1128 - val_loss: 0.1027
Epoch 16/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0974
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0327
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1125
6530/6530 [==============================] - 0s 11us/step - loss: 0.1121 - val_loss: 0.1121
Epoch 17/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1061
3072/6530 [=============>................] - ETA: 0s - loss: 0.0330
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1061
4064/6530 [=================>............] - ETA: 0s - loss: 0.0331
6530/6530 [==============================] - 0s 10us/step - loss: 0.1064 - val_loss: 0.1150
Epoch 18/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1093
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0330
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1059
6530/6530 [==============================] - 0s 12us/step - loss: 0.1064 - val_loss: 0.1124
Epoch 19/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1073
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0330
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1022
6530/6530 [==============================] - 0s 55us/step - loss: 0.0326 - val_loss: 0.0331
Epoch 18/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0310
6530/6530 [==============================] - 0s 11us/step - loss: 0.1014 - val_loss: 0.1076
Epoch 20/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1067
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0309
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1017
6530/6530 [==============================] - 0s 11us/step - loss: 0.1003 - val_loss: 0.0939
Epoch 21/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0877
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0308
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0955
6530/6530 [==============================] - 0s 10us/step - loss: 0.0967 - val_loss: 0.1139
Epoch 22/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1097
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0317
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1013
3840/6530 [================>.............] - ETA: 0s - loss: 0.0313
6530/6530 [==============================] - 0s 10us/step - loss: 0.0987 - val_loss: 0.1072
Epoch 23/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.1059
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0315
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1053
6530/6530 [==============================] - 0s 10us/step - loss: 0.1042 - val_loss: 0.0978
Epoch 24/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0943
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0315
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1026
6530/6530 [==============================] - 0s 11us/step - loss: 0.1017 - val_loss: 0.0958
Epoch 25/27

 256/6530 [>.............................] - ETA: 0s - loss: 0.0861
6530/6530 [==============================] - 0s 54us/step - loss: 0.0313 - val_loss: 0.0302
Epoch 19/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0249
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0939
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0287
6530/6530 [==============================] - 0s 11us/step - loss: 0.0938 - val_loss: 0.0950

1856/6530 [=======>......................] - ETA: 0s - loss: 0.0293
# training | RMSE: 0.1146, MAE: 0.0880
worker 2  xfile  [7, 27.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13139716854445274}, 'layer_2_size': 85, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.1145699621146472, 'rmse': 0.1145699621146472, 'mae': 0.08795459232230718, 'early_stop': True}
vggnet done  2

2656/6530 [===========>..................] - ETA: 0s - loss: 0.0306
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0300
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0302
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0303
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0303
6530/6530 [==============================] - 0s 61us/step - loss: 0.0302 - val_loss: 0.0300
Epoch 20/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0313
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0277
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0282
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0291
2976/6530 [============>.................] - ETA: 0s - loss: 0.0293
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0292
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0291
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0297
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0295
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0297
6530/6530 [==============================] - 1s 82us/step - loss: 0.0295 - val_loss: 0.0301
Epoch 21/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0286
 608/6530 [=>............................] - ETA: 0s - loss: 0.0288
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0282
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0285
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0294
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0293
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0291
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0293
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0294
6496/6530 [============================>.] - ETA: 0s - loss: 0.0294
6530/6530 [==============================] - 0s 74us/step - loss: 0.0293 - val_loss: 0.0279
Epoch 22/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0293
 864/6530 [==>...........................] - ETA: 0s - loss: 0.0275
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0274
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0284
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0286
4032/6530 [=================>............] - ETA: 0s - loss: 0.0286
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0290
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0290
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0290
6530/6530 [==============================] - 0s 70us/step - loss: 0.0289 - val_loss: 0.0287
Epoch 23/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0314
 704/6530 [==>...........................] - ETA: 0s - loss: 0.0284
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0271
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0279
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0285
3072/6530 [=============>................] - ETA: 0s - loss: 0.0281
3744/6530 [================>.............] - ETA: 0s - loss: 0.0281
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0278
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0281
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0280
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0282
6530/6530 [==============================] - 1s 91us/step - loss: 0.0281 - val_loss: 0.0289
Epoch 24/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0335
 576/6530 [=>............................] - ETA: 0s - loss: 0.0281
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0264
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0266
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0280
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0286
3104/6530 [=============>................] - ETA: 0s - loss: 0.0280
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0280
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0279
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0280
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0278
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0278
6530/6530 [==============================] - 1s 94us/step - loss: 0.0276 - val_loss: 0.0268
Epoch 25/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0284
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0263
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0266
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0276
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0273
3904/6530 [================>.............] - ETA: 0s - loss: 0.0272
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0270
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0273
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0273
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0273
6400/6530 [============================>.] - ETA: 0s - loss: 0.0273
6530/6530 [==============================] - 1s 85us/step - loss: 0.0273 - val_loss: 0.0257
Epoch 26/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0299
 544/6530 [=>............................] - ETA: 0s - loss: 0.0270
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0263
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0264
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0273
2848/6530 [============>.................] - ETA: 0s - loss: 0.0271
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0272
4032/6530 [=================>............] - ETA: 0s - loss: 0.0271
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0272
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0273
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0273
6496/6530 [============================>.] - ETA: 0s - loss: 0.0271
6530/6530 [==============================] - 1s 90us/step - loss: 0.0271 - val_loss: 0.0238
Epoch 27/27

  32/6530 [..............................] - ETA: 0s - loss: 0.0240
 672/6530 [==>...........................] - ETA: 0s - loss: 0.0262
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0256
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0256
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0258
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0260
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0255
3840/6530 [================>.............] - ETA: 0s - loss: 0.0255
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0253
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0256
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0257
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0258
6496/6530 [============================>.] - ETA: 0s - loss: 0.0256
6530/6530 [==============================] - 1s 99us/step - loss: 0.0256 - val_loss: 0.0220

# training | RMSE: 0.1432, MAE: 0.1112
worker 0  xfile  [5, 27.0, 0, 100, [], {'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47468977120863254}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3739935226573554}, 'layer_2_size': 84, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44288682818485114}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}]  predicted as label {'loss': 0.1431856868806329, 'rmse': 0.1431856868806329, 'mae': 0.11117643503852277, 'early_stop': False}
vggnet done  0
all of workers have been done
calculation finished
#2 epoch=27.0 loss={'loss': 0.264758540460635, 'rmse': 0.264758540460635, 'mae': 0.2173385953343809, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 128, 'init': 'glorot_normal', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.11016353490095027}, 'layer_1_size': 98, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.10812814927179115}, 'layer_2_size': 60, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'dropout', 'rate': 0.18100602082206294}, 'layer_3_size': 64, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 94, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'dropout', 'rate': 0.177715460906899}, 'layer_5_size': 24, 'loss': 'mean_absolute_error', 'n_layers': 4, 'optimizer': 'adadelta', 'scaler': 'MinMaxScaler', 'shuffle': False}
#0 epoch=27.0 loss={'loss': 0.154180059138014, 'rmse': 0.154180059138014, 'mae': 0.11876268097850583, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 128, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.1868588514544381}, 'layer_1_size': 11, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 8, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 22, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'dropout', 'rate': 0.4285553013409614}, 'layer_4_size': 26, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 92, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'RobustScaler', 'shuffle': False}
#4 epoch=27.0 loss={'loss': 0.08806374313399128, 'rmse': 0.08806374313399128, 'mae': 0.06566941240009284, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 61, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.1788316905628177}, 'layer_2_size': 52, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 15, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 22, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': None}, 'layer_5_size': 43, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#1 epoch=27.0 loss={'loss': 0.18867522445597557, 'rmse': 0.18867522445597557, 'mae': 0.1484368083079586, 'early_stop': False} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 17, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 3, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.43041579991282797}, 'layer_3_size': 29, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 40, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adagrad', 'scaler': None, 'shuffle': True}
#3 epoch=27.0 loss={'loss': 0.17443202194827762, 'rmse': 0.17443202194827762, 'mae': 0.1371217290049391, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3293809552116107}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#6 epoch=27.0 loss={'loss': 0.20306987393967085, 'rmse': 0.20306987393967085, 'mae': 0.16037857230516284, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': 'dropout', 'rate': 0.44890790587035734}, 'layer_1_size': 53, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 27, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.21705550570747711}, 'layer_3_size': 48, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 52, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 20, 'loss': 'mean_absolute_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#7 epoch=27.0 loss={'loss': 0.1145699621146472, 'rmse': 0.1145699621146472, 'mae': 0.08795459232230718, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13139716854445274}, 'layer_2_size': 85, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#5 epoch=27.0 loss={'loss': 0.1431856868806329, 'rmse': 0.1431856868806329, 'mae': 0.11117643503852277, 'early_stop': False} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 32, 'init': 'glorot_normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'dropout', 'rate': 0.47468977120863254}, 'layer_1_size': 90, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3739935226573554}, 'layer_2_size': 84, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 7, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 44, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'dropout', 'rate': 0.44288682818485114}, 'layer_5_size': 16, 'loss': 'mean_squared_error', 'n_layers': 3, 'optimizer': 'rmsprop', 'scaler': 'MinMaxScaler', 'shuffle': False}
get a list [results] of length 199
get a list [loss] of length 8
get a list [val_loss] of length 8
length of indices is (4, 7, 5, 0, 3, 1, 6, 2)
length of indices is 8
length of T is 8
newly formed T structure is:[[0, 81.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13139716854445274}, 'layer_2_size': 85, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}], [1, 81.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3293809552116107}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]] 

*** 2.6666666666666665 configurations x 81.0 iterations each

6 | Thu Sep 27 23:29:51 2018 | lowest loss so far: 0.0732 (run 0)

vggnet done  2
{'batch_size': 256,
 'init': 'glorot_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 5,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  52 | activation: relu    | extras: None 
layer 2 | size:  85 | activation: tanh    | extras: dropout - rate: 13.1% 
layer 3 | size:  81 | activation: sigmoid | extras: None 
layer 4 | size:  38 | activation: tanh    | extras: None 
layer 5 | size:  41 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

 256/6530 [>.............................] - ETA: 21s - loss: 1.4536
4864/6530 [=====================>........] - ETA: 0s - loss: 0.3603 {'batch_size': 32,
 'init': 'he_uniform',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adadelta',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  99 | activation: tanh    | extras: batchnorm 
layer 2 | size:  74 | activation: tanh    | extras: None 
layer 3 | size:  83 | activation: tanh    | extras: dropout - rate: 32.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  32/6530 [..............................] - ETA: 2:49 - loss: 0.8250
6530/6530 [==============================] - 1s 154us/step - loss: 0.3188 - val_loss: 0.1752

 704/6530 [==>...........................] - ETA: 7s - loss: 0.5624  Epoch 2/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1782
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1893
1216/6530 [====>.........................] - ETA: 4s - loss: 0.4774
6530/6530 [==============================] - 0s 11us/step - loss: 0.1853 - val_loss: 0.1747
Epoch 3/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1709
1728/6530 [======>.......................] - ETA: 2s - loss: 0.4328
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1830
6530/6530 [==============================] - 0s 12us/step - loss: 0.1814 - val_loss: 0.1815
Epoch 4/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1778
2272/6530 [=========>....................] - ETA: 1s - loss: 0.4009
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1778
2880/6530 [============>.................] - ETA: 1s - loss: 0.3718
6530/6530 [==============================] - 0s 12us/step - loss: 0.1760 - val_loss: 0.1843
Epoch 5/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1810
3520/6530 [===============>..............] - ETA: 0s - loss: 0.3488
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1754
6530/6530 [==============================] - 0s 11us/step - loss: 0.1727 - val_loss: 0.1690
Epoch 6/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1656
4128/6530 [=================>............] - ETA: 0s - loss: 0.3329
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1697
6530/6530 [==============================] - 0s 10us/step - loss: 0.1676 - val_loss: 0.1531
Epoch 7/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1479
4768/6530 [====================>.........] - ETA: 0s - loss: 0.3175
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1595
5440/6530 [=======================>......] - ETA: 0s - loss: 0.3042
6530/6530 [==============================] - 0s 10us/step - loss: 0.1578 - val_loss: 0.1421
Epoch 8/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1372
6080/6530 [==========================>...] - ETA: 0s - loss: 0.2927
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1561
6530/6530 [==============================] - 0s 10us/step - loss: 0.1530 - val_loss: 0.1440
Epoch 9/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1435
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1497
6530/6530 [==============================] - 1s 219us/step - loss: 0.2851 - val_loss: 0.3921
Epoch 2/81

  32/6530 [..............................] - ETA: 0s - loss: 0.4148
6530/6530 [==============================] - 0s 10us/step - loss: 0.1468 - val_loss: 0.1373
Epoch 10/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1374
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1994
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1378
6530/6530 [==============================] - 0s 11us/step - loss: 0.1360 - val_loss: 0.1512
Epoch 11/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1520
1312/6530 [=====>........................] - ETA: 0s - loss: 0.1861
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1331
6530/6530 [==============================] - 0s 10us/step - loss: 0.1311 - val_loss: 0.1183
Epoch 12/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1169
1984/6530 [========>.....................] - ETA: 0s - loss: 0.1809
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1251
2592/6530 [==========>...................] - ETA: 0s - loss: 0.1773
6530/6530 [==============================] - 0s 11us/step - loss: 0.1240 - val_loss: 0.1291
Epoch 13/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1257
3168/6530 [=============>................] - ETA: 0s - loss: 0.1734
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1250
6530/6530 [==============================] - 0s 10us/step - loss: 0.1243 - val_loss: 0.1268
Epoch 14/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1231
3744/6530 [================>.............] - ETA: 0s - loss: 0.1718
4864/6530 [=====================>........] - ETA: 0s - loss: 0.1177
6530/6530 [==============================] - 0s 12us/step - loss: 0.1166 - val_loss: 0.1200
Epoch 15/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1155
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1693
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1154
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1670
6530/6530 [==============================] - 0s 11us/step - loss: 0.1148 - val_loss: 0.1080
Epoch 16/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1069
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1639
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1121
6530/6530 [==============================] - 0s 10us/step - loss: 0.1117 - val_loss: 0.1059
Epoch 17/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1044
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1624
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1097
6530/6530 [==============================] - 1s 86us/step - loss: 0.1613 - val_loss: 0.2763

6530/6530 [==============================] - 0s 11us/step - loss: 0.1100 - val_loss: 0.1061
Epoch 3/81

  32/6530 [..............................] - ETA: 0s - loss: 0.2754Epoch 18/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1018
 608/6530 [=>............................] - ETA: 0s - loss: 0.1619
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1010
6530/6530 [==============================] - 0s 11us/step - loss: 0.1009 - val_loss: 0.1062
Epoch 19/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1042
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1528
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1083
6530/6530 [==============================] - 0s 10us/step - loss: 0.1072 - val_loss: 0.0973
Epoch 20/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0983
1824/6530 [=======>......................] - ETA: 0s - loss: 0.1453
5888/6530 [==========================>...] - ETA: 0s - loss: 0.1040
2464/6530 [==========>...................] - ETA: 0s - loss: 0.1438
6530/6530 [==============================] - 0s 10us/step - loss: 0.1027 - val_loss: 0.0871
Epoch 21/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0835
3104/6530 [=============>................] - ETA: 0s - loss: 0.1415
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0888
6530/6530 [==============================] - 0s 10us/step - loss: 0.0886 - val_loss: 0.1110
Epoch 22/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1038
3776/6530 [================>.............] - ETA: 0s - loss: 0.1395
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0980
6530/6530 [==============================] - 0s 10us/step - loss: 0.0970 - val_loss: 0.1052
Epoch 23/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0991
4384/6530 [===================>..........] - ETA: 0s - loss: 0.1389
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0967
6530/6530 [==============================] - 0s 10us/step - loss: 0.0960 - val_loss: 0.0903
Epoch 24/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.0855
4928/6530 [=====================>........] - ETA: 0s - loss: 0.1374
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0902
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1362
6530/6530 [==============================] - 0s 11us/step - loss: 0.0882 - val_loss: 0.1275
Epoch 25/81

 256/6530 [>.............................] - ETA: 0s - loss: 0.1236
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1359
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1027
6530/6530 [==============================] - 0s 11us/step - loss: 0.1026 - val_loss: 0.0898

6530/6530 [==============================] - 1s 89us/step - loss: 0.1351 - val_loss: 0.2341
Epoch 4/81

  32/6530 [..............................] - ETA: 0s - loss: 0.2415
# training | RMSE: 0.1084, MAE: 0.0852
worker 0  xfile  [0, 81.0, 0, 100, [], {'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13139716854445274}, 'layer_2_size': 85, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.10836966627420098, 'rmse': 0.10836966627420098, 'mae': 0.08523424422133921, 'early_stop': True}
vggnet done  0

 672/6530 [==>...........................] - ETA: 0s - loss: 0.1404
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1299
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1280
2560/6530 [==========>...................] - ETA: 0s - loss: 0.1257
3232/6530 [=============>................] - ETA: 0s - loss: 0.1244
3872/6530 [================>.............] - ETA: 0s - loss: 0.1249
4512/6530 [===================>..........] - ETA: 0s - loss: 0.1246
5120/6530 [======================>.......] - ETA: 0s - loss: 0.1239
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1230
6016/6530 [==========================>...] - ETA: 0s - loss: 0.1233
6464/6530 [============================>.] - ETA: 0s - loss: 0.1229
6530/6530 [==============================] - 1s 93us/step - loss: 0.1228 - val_loss: 0.2005
Epoch 5/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1807
 448/6530 [=>............................] - ETA: 0s - loss: 0.1356
 864/6530 [==>...........................] - ETA: 0s - loss: 0.1262
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1234
1664/6530 [======>.......................] - ETA: 0s - loss: 0.1224
2016/6530 [========>.....................] - ETA: 0s - loss: 0.1201
2368/6530 [=========>....................] - ETA: 0s - loss: 0.1208
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1194
3040/6530 [============>.................] - ETA: 0s - loss: 0.1183
3360/6530 [==============>...............] - ETA: 0s - loss: 0.1183
3712/6530 [================>.............] - ETA: 0s - loss: 0.1180
4096/6530 [=================>............] - ETA: 0s - loss: 0.1178
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1171
4800/6530 [=====================>........] - ETA: 0s - loss: 0.1174
5152/6530 [======================>.......] - ETA: 0s - loss: 0.1163
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1158
5792/6530 [=========================>....] - ETA: 0s - loss: 0.1159
6080/6530 [==========================>...] - ETA: 0s - loss: 0.1157
6368/6530 [============================>.] - ETA: 0s - loss: 0.1154
6530/6530 [==============================] - 1s 153us/step - loss: 0.1154 - val_loss: 0.1343
Epoch 6/81

  32/6530 [..............................] - ETA: 1s - loss: 0.1472
 352/6530 [>.............................] - ETA: 0s - loss: 0.1186
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1150
1056/6530 [===>..........................] - ETA: 0s - loss: 0.1131
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1119
1760/6530 [=======>......................] - ETA: 0s - loss: 0.1112
2112/6530 [========>.....................] - ETA: 0s - loss: 0.1101
2432/6530 [==========>...................] - ETA: 0s - loss: 0.1105
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1096
3104/6530 [=============>................] - ETA: 0s - loss: 0.1090
3392/6530 [==============>...............] - ETA: 0s - loss: 0.1095
3712/6530 [================>.............] - ETA: 0s - loss: 0.1096
4032/6530 [=================>............] - ETA: 0s - loss: 0.1094
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1091
4640/6530 [====================>.........] - ETA: 0s - loss: 0.1093
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1093
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1091
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1088
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1093
6144/6530 [===========================>..] - ETA: 0s - loss: 0.1090
6464/6530 [============================>.] - ETA: 0s - loss: 0.1094
6530/6530 [==============================] - 1s 167us/step - loss: 0.1095 - val_loss: 0.2040
Epoch 7/81

  32/6530 [..............................] - ETA: 0s - loss: 0.2157
 448/6530 [=>............................] - ETA: 0s - loss: 0.1214
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1141
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1103
1600/6530 [======>.......................] - ETA: 0s - loss: 0.1096
1952/6530 [=======>......................] - ETA: 0s - loss: 0.1085
2336/6530 [=========>....................] - ETA: 0s - loss: 0.1075
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1061
3200/6530 [=============>................] - ETA: 0s - loss: 0.1046
3584/6530 [===============>..............] - ETA: 0s - loss: 0.1050
3968/6530 [=================>............] - ETA: 0s - loss: 0.1043
4320/6530 [==================>...........] - ETA: 0s - loss: 0.1043
4704/6530 [====================>.........] - ETA: 0s - loss: 0.1046
5088/6530 [======================>.......] - ETA: 0s - loss: 0.1043
5472/6530 [========================>.....] - ETA: 0s - loss: 0.1036
5856/6530 [=========================>....] - ETA: 0s - loss: 0.1035
6208/6530 [===========================>..] - ETA: 0s - loss: 0.1029
6530/6530 [==============================] - 1s 141us/step - loss: 0.1033 - val_loss: 0.1562
Epoch 8/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1680
 416/6530 [>.............................] - ETA: 0s - loss: 0.1201
 832/6530 [==>...........................] - ETA: 0s - loss: 0.1085
1280/6530 [====>.........................] - ETA: 0s - loss: 0.1059
1696/6530 [======>.......................] - ETA: 0s - loss: 0.1048
2080/6530 [========>.....................] - ETA: 0s - loss: 0.1034
2400/6530 [==========>...................] - ETA: 0s - loss: 0.1032
2720/6530 [===========>..................] - ETA: 0s - loss: 0.1018
3072/6530 [=============>................] - ETA: 0s - loss: 0.1008
3424/6530 [==============>...............] - ETA: 0s - loss: 0.1008
3808/6530 [================>.............] - ETA: 0s - loss: 0.1001
4192/6530 [==================>...........] - ETA: 0s - loss: 0.1005
4608/6530 [====================>.........] - ETA: 0s - loss: 0.1005
4960/6530 [=====================>........] - ETA: 0s - loss: 0.1000
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0997
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0995
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0988
6496/6530 [============================>.] - ETA: 0s - loss: 0.0996
6530/6530 [==============================] - 1s 143us/step - loss: 0.0997 - val_loss: 0.1670
Epoch 9/81

  32/6530 [..............................] - ETA: 1s - loss: 0.1921
 384/6530 [>.............................] - ETA: 0s - loss: 0.1156
 768/6530 [==>...........................] - ETA: 0s - loss: 0.1052
1184/6530 [====>.........................] - ETA: 0s - loss: 0.1024
1632/6530 [======>.......................] - ETA: 0s - loss: 0.1009
2048/6530 [========>.....................] - ETA: 0s - loss: 0.1002
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0992
2880/6530 [============>.................] - ETA: 0s - loss: 0.0984
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0973
3712/6530 [================>.............] - ETA: 0s - loss: 0.0966
4064/6530 [=================>............] - ETA: 0s - loss: 0.0971
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0966
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0971
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0966
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0960
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0961
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0959
6528/6530 [============================>.] - ETA: 0s - loss: 0.0966
6530/6530 [==============================] - 1s 143us/step - loss: 0.0965 - val_loss: 0.1661
Epoch 10/81

  32/6530 [..............................] - ETA: 1s - loss: 0.1890
 384/6530 [>.............................] - ETA: 0s - loss: 0.1054
 704/6530 [==>...........................] - ETA: 0s - loss: 0.1008
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0983
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0972
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0962
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0945
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0948
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0945
2912/6530 [============>.................] - ETA: 0s - loss: 0.0940
3232/6530 [=============>................] - ETA: 0s - loss: 0.0935
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0937
3936/6530 [=================>............] - ETA: 0s - loss: 0.0931
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0935
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0935
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0940
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0933
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0929
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0931
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0931
6530/6530 [==============================] - 1s 167us/step - loss: 0.0934 - val_loss: 0.1078
Epoch 11/81

  32/6530 [..............................] - ETA: 1s - loss: 0.1243
 352/6530 [>.............................] - ETA: 1s - loss: 0.0986
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0932
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0927
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0916
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0903
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0908
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0905
2944/6530 [============>.................] - ETA: 0s - loss: 0.0903
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0893
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0895
3904/6530 [================>.............] - ETA: 0s - loss: 0.0893
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0898
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0898
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0900
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0898
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0892
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0894
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0891
6496/6530 [============================>.] - ETA: 0s - loss: 0.0897
6530/6530 [==============================] - 1s 159us/step - loss: 0.0897 - val_loss: 0.1625
Epoch 12/81

  32/6530 [..............................] - ETA: 1s - loss: 0.1681
 352/6530 [>.............................] - ETA: 0s - loss: 0.1101
 672/6530 [==>...........................] - ETA: 0s - loss: 0.1023
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0977
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0956
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0938
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0923
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0920
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0908
3136/6530 [=============>................] - ETA: 0s - loss: 0.0897
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0898
3872/6530 [================>.............] - ETA: 0s - loss: 0.0892
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0896
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0898
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0897
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0893
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0889
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0893
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0891
6530/6530 [==============================] - 1s 157us/step - loss: 0.0896 - val_loss: 0.1576
Epoch 13/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1813
 416/6530 [>.............................] - ETA: 0s - loss: 0.1011
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0920
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0889
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0884
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0875
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0872
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0864
2912/6530 [============>.................] - ETA: 0s - loss: 0.0858
3264/6530 [=============>................] - ETA: 0s - loss: 0.0847
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0850
3968/6530 [=================>............] - ETA: 0s - loss: 0.0851
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0849
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0857
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0856
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0854
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0856
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0852
6400/6530 [============================>.] - ETA: 0s - loss: 0.0854
6530/6530 [==============================] - 1s 153us/step - loss: 0.0856 - val_loss: 0.1486
Epoch 14/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1662
 384/6530 [>.............................] - ETA: 0s - loss: 0.1047
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0948
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0905
1312/6530 [=====>........................] - ETA: 0s - loss: 0.0904
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0892
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0874
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0874
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0866
2848/6530 [============>.................] - ETA: 0s - loss: 0.0867
3200/6530 [=============>................] - ETA: 0s - loss: 0.0853
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0853
3936/6530 [=================>............] - ETA: 0s - loss: 0.0848
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0850
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0849
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0847
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0845
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0843
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0845
6368/6530 [============================>.] - ETA: 0s - loss: 0.0847
6530/6530 [==============================] - 1s 160us/step - loss: 0.0852 - val_loss: 0.1448
Epoch 15/81

  32/6530 [..............................] - ETA: 0s - loss: 0.1653
 480/6530 [=>............................] - ETA: 0s - loss: 0.0963
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0891
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0871
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0850
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0850
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0838
3200/6530 [=============>................] - ETA: 0s - loss: 0.0830
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0828
3968/6530 [=================>............] - ETA: 0s - loss: 0.0825
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0822
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0826
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0828
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0823
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0823
6464/6530 [============================>.] - ETA: 0s - loss: 0.0827
6530/6530 [==============================] - 1s 125us/step - loss: 0.0827 - val_loss: 0.1447

# training | RMSE: 0.1775, MAE: 0.1410
worker 1  xfile  [1, 81.0, 0, 100, [], {'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3293809552116107}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.17753812906638738, 'rmse': 0.17753812906638738, 'mae': 0.1410452765230236, 'early_stop': True}
vggnet done  1
all of workers have been done
calculation finished
#0 epoch=81.0 loss={'loss': 0.10836966627420098, 'rmse': 0.10836966627420098, 'mae': 0.08523424422133921, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 256, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': None}, 'layer_1_size': 52, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.13139716854445274}, 'layer_2_size': 85, 'layer_3_activation': 'sigmoid', 'layer_3_extras': {'name': None}, 'layer_3_size': 81, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 38, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 41, 'loss': 'mean_absolute_error', 'n_layers': 5, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
#1 epoch=81.0 loss={'loss': 0.17753812906638738, 'rmse': 0.17753812906638738, 'mae': 0.1410452765230236, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 32, 'init': 'he_uniform', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 99, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': None}, 'layer_2_size': 74, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': 'dropout', 'rate': 0.3293809552116107}, 'layer_3_size': 83, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': None}, 'layer_4_size': 32, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, 'layer_5_size': 52, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adadelta', 'scaler': 'StandardScaler', 'shuffle': False}
get a list [results] of length 201
get a list [loss] of length 2
get a list [val_loss] of length 2
length of indices is (0, 1)
length of indices is 2
length of T is 2
s=0
T is of size 5
T=[{'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38353587796938027}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23431882564222387}, 'layer_3_size': 36, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 60, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}, {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13111359748493934}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31119805265189826}, 'layer_4_size': 5, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18939250426965906}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}, {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 22, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3708849838902447}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4028864677505175}, 'layer_5_size': 38, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2291344165235558}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2184897927910865}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27286622920865944}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}, {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3437183352808336}, 'layer_2_size': 64, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10362876348865911}, 'layer_3_size': 12, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 79, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]
newly formed T structure is:[[0, 81, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38353587796938027}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23431882564222387}, 'layer_3_size': 36, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 60, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}], [1, 81, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13111359748493934}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31119805265189826}, 'layer_4_size': 5, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18939250426965906}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}], [2, 81, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 22, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3708849838902447}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4028864677505175}, 'layer_5_size': 38, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [3, 81, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2291344165235558}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2184897927910865}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27286622920865944}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}], [4, 81, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3437183352808336}, 'layer_2_size': 64, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10362876348865911}, 'layer_3_size': 12, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 79, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]] 

*** 5 configurations x 81.0 iterations each

2 | Thu Sep 27 23:30:06 2018 | lowest loss so far: 0.0732 (run 0)

{'batch_size': 16,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'StandardScaler',
 'shuffle': False}
layer 1 | size:  84 | activation: relu    | extras: dropout - rate: 38.4% 
layer 2 | size:  80 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 0
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 5:04 - loss: 0.5048
 416/6530 [>.............................] - ETA: 11s - loss: 0.3188 
 864/6530 [==>...........................] - ETA: 5s - loss: 0.1893 
1360/6530 [=====>........................] - ETA: 3s - loss: 0.1394
1936/6530 [=======>......................] - ETA: 2s - loss: 0.1125
2512/6530 [==========>...................] - ETA: 1s - loss: 0.0991
3024/6530 [============>.................] - ETA: 1s - loss: 0.0902
3376/6530 [==============>...............] - ETA: 1s - loss: 0.0855
3936/6530 [=================>............] - ETA: 0s - loss: 0.0808
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0770
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0735{'batch_size': 64,
 'init': 'he_normal',
 'loss': 'mean_absolute_error',
 'n_layers': 3,
 'optimizer': 'adamax',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  84 | activation: relu    | extras: dropout - rate: 13.1% 
layer 2 | size:  20 | activation: tanh    | extras: batchnorm 
layer 3 | size:  19 | activation: tanh    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  64/6530 [..............................] - ETA: 1:33 - loss: 0.9931
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0710
1344/6530 [=====>........................] - ETA: 3s - loss: 0.7433  
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0690
2496/6530 [==========>...................] - ETA: 1s - loss: 0.6438
6530/6530 [==============================] - 1s 221us/step - loss: 0.0673 - val_loss: 0.0397

3840/6530 [================>.............] - ETA: 0s - loss: 0.5632Epoch 2/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0377
5184/6530 [======================>.......] - ETA: 0s - loss: 0.5037
 576/6530 [=>............................] - ETA: 0s - loss: 0.0422
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0410
6530/6530 [==============================] - 1s 186us/step - loss: 0.4639 - val_loss: 0.2535
Epoch 2/81

  64/6530 [..............................] - ETA: 0s - loss: 0.3087
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0406
1472/6530 [=====>........................] - ETA: 0s - loss: 0.2820
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0405
2944/6530 [============>.................] - ETA: 0s - loss: 0.2751
2848/6530 [============>.................] - ETA: 0s - loss: 0.0412
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2690
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0406
5760/6530 [=========================>....] - ETA: 0s - loss: 0.2659
3952/6530 [=================>............] - ETA: 0s - loss: 0.0408
6530/6530 [==============================] - 0s 37us/step - loss: 0.2621 - val_loss: 0.2103
Epoch 3/81

  64/6530 [..............................] - ETA: 0s - loss: 0.2585
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0409
1536/6530 [======>.......................] - ETA: 0s - loss: 0.2339
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0404
2944/6530 [============>.................] - ETA: 0s - loss: 0.2299
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0403
4352/6530 [==================>...........] - ETA: 0s - loss: 0.2302
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0406
5696/6530 [=========================>....] - ETA: 0s - loss: 0.2258
6530/6530 [==============================] - 1s 95us/step - loss: 0.0405 - val_loss: 0.0343
Epoch 3/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0406
6530/6530 [==============================] - 0s 37us/step - loss: 0.2260 - val_loss: 0.1870
Epoch 4/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1892
 560/6530 [=>............................] - ETA: 0s - loss: 0.0392
1536/6530 [======>.......................] - ETA: 0s - loss: 0.2088
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0384
2944/6530 [============>.................] - ETA: 0s - loss: 0.2085
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0370
4224/6530 [==================>...........] - ETA: 0s - loss: 0.2066
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0373
5568/6530 [========================>.....] - ETA: 0s - loss: 0.2038{'batch_size': 32,
 'init': 'normal',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'adam',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  22 | activation: tanh    | extras: batchnorm 
layer 2 | size:  85 | activation: sigmoid | extras: dropout - rate: 37.1% 
layer 3 | size:  21 | activation: relu    | extras: batchnorm 
layer 4 | size:  71 | activation: tanh    | extras: batchnorm 
layer 5 | size:  38 | activation: sigmoid | extras: dropout - rate: 40.3% 

xnet is <keras.engine.sequential.Sequential object at 0x7f77efa0c278>
vggnet init done 2
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  32/6530 [..............................] - ETA: 4:21 - loss: 0.5917
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0381
6530/6530 [==============================] - 0s 39us/step - loss: 0.2021 - val_loss: 0.1734
Epoch 5/81

  64/6530 [..............................] - ETA: 0s - loss: 0.2192
 384/6530 [>.............................] - ETA: 21s - loss: 0.5536 
3248/6530 [=============>................] - ETA: 0s - loss: 0.0375
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1950
 736/6530 [==>...........................] - ETA: 10s - loss: 0.4218
3792/6530 [================>.............] - ETA: 0s - loss: 0.0372
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1916
1088/6530 [===>..........................] - ETA: 7s - loss: 0.3278 
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0371
4032/6530 [=================>............] - ETA: 0s - loss: 0.1896
1440/6530 [=====>........................] - ETA: 5s - loss: 0.2661
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0372
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1897
1824/6530 [=======>......................] - ETA: 4s - loss: 0.2210
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0370
6530/6530 [==============================] - 0s 40us/step - loss: 0.1887 - val_loss: 0.1612
Epoch 6/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1664
2208/6530 [=========>....................] - ETA: 3s - loss: 0.1912
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0367
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1763
2592/6530 [==========>...................] - ETA: 2s - loss: 0.1702
6448/6530 [============================>.] - ETA: 0s - loss: 0.0364
2880/6530 [============>.................] - ETA: 0s - loss: 0.1791
6530/6530 [==============================] - 1s 98us/step - loss: 0.0363 - val_loss: 0.0296
Epoch 4/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0359
2976/6530 [============>.................] - ETA: 2s - loss: 0.1547
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1788
 560/6530 [=>............................] - ETA: 0s - loss: 0.0329
3360/6530 [==============>...............] - ETA: 1s - loss: 0.1418
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1783
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0314
3712/6530 [================>.............] - ETA: 1s - loss: 0.1329
6530/6530 [==============================] - 0s 39us/step - loss: 0.1774 - val_loss: 0.1522
Epoch 7/81

  64/6530 [..............................] - ETA: 0s - loss: 0.2006
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0305
4064/6530 [=================>............] - ETA: 1s - loss: 0.1253
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1736
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0305
4448/6530 [===================>..........] - ETA: 0s - loss: 0.1185
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1694
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0318
4832/6530 [=====================>........] - ETA: 0s - loss: 0.1127
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1690
3264/6530 [=============>................] - ETA: 0s - loss: 0.0311
5216/6530 [======================>.......] - ETA: 0s - loss: 0.1077
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1692
3808/6530 [================>.............] - ETA: 0s - loss: 0.0311
5600/6530 [========================>.....] - ETA: 0s - loss: 0.1031
6530/6530 [==============================] - 0s 39us/step - loss: 0.1697 - val_loss: 0.1394
Epoch 8/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1731
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0307
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0997
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1647
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0306
6336/6530 [============================>.] - ETA: 0s - loss: 0.0964
2880/6530 [============>.................] - ETA: 0s - loss: 0.1627
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0306
4352/6530 [==================>...........] - ETA: 0s - loss: 0.1623
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0305
5696/6530 [=========================>....] - ETA: 0s - loss: 0.1611
6530/6530 [==============================] - 2s 351us/step - loss: 0.0950 - val_loss: 0.0415

6528/6530 [============================>.] - ETA: 0s - loss: 0.0302Epoch 2/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0492
6530/6530 [==============================] - 1s 98us/step - loss: 0.0302 - val_loss: 0.0239

6530/6530 [==============================] - 0s 38us/step - loss: 0.1601 - val_loss: 0.1344
Epoch 5/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0279Epoch 9/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1718
 416/6530 [>.............................] - ETA: 0s - loss: 0.0433
 528/6530 [=>............................] - ETA: 0s - loss: 0.0274
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1590
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0443
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0257
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1588
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0451
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0247
4096/6530 [=================>............] - ETA: 0s - loss: 0.1583
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0443
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0249
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1571
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0446
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0256
6530/6530 [==============================] - 0s 40us/step - loss: 0.1563 - val_loss: 0.1330
Epoch 10/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1426
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0440
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0253
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1492
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0439
3840/6530 [================>.............] - ETA: 0s - loss: 0.0251
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1496
3072/6530 [=============>................] - ETA: 0s - loss: 0.0435
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0251
4096/6530 [=================>............] - ETA: 0s - loss: 0.1514
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0430
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0250
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1509
3808/6530 [================>.............] - ETA: 0s - loss: 0.0431
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0250
6530/6530 [==============================] - 0s 39us/step - loss: 0.1497 - val_loss: 0.1245
Epoch 11/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1420
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0434
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0247
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1444
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0436
2880/6530 [============>.................] - ETA: 0s - loss: 0.1460
6530/6530 [==============================] - 1s 97us/step - loss: 0.0247 - val_loss: 0.0189
Epoch 6/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0179
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0433
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1457
 576/6530 [=>............................] - ETA: 0s - loss: 0.0217
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0432
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1469
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0211
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0436
6530/6530 [==============================] - 0s 39us/step - loss: 0.1466 - val_loss: 0.1206
Epoch 12/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1637
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0210
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0436
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1449
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0211
6496/6530 [============================>.] - ETA: 0s - loss: 0.0432
2880/6530 [============>.................] - ETA: 0s - loss: 0.1410
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0217
6530/6530 [==============================] - 1s 143us/step - loss: 0.0432 - val_loss: 0.0374
Epoch 3/81

  32/6530 [..............................] - ETA: 1s - loss: 0.0405
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1413
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0217
 384/6530 [>.............................] - ETA: 0s - loss: 0.0359
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1415
3888/6530 [================>.............] - ETA: 0s - loss: 0.0216
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0383
6530/6530 [==============================] - 0s 40us/step - loss: 0.1416 - val_loss: 0.1160
Epoch 13/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1423
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0211
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0385
1216/6530 [====>.........................] - ETA: 0s - loss: 0.1372
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0212
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0398
2624/6530 [===========>..................] - ETA: 0s - loss: 0.1370
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0210
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0399
3968/6530 [=================>............] - ETA: 0s - loss: 0.1359
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0210
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0390
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1379
6530/6530 [==============================] - 1s 96us/step - loss: 0.0209 - val_loss: 0.0166
Epoch 7/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0241
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0390
6530/6530 [==============================] - 0s 40us/step - loss: 0.1376 - val_loss: 0.1130
Epoch 14/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1382
 560/6530 [=>............................] - ETA: 0s - loss: 0.0187
3008/6530 [============>.................] - ETA: 0s - loss: 0.0387
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1350
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0182
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0381
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1341
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0180
3744/6530 [================>.............] - ETA: 0s - loss: 0.0386
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1335
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0183
4128/6530 [=================>............] - ETA: 0s - loss: 0.0385
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1340
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0185
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0381
6530/6530 [==============================] - 0s 39us/step - loss: 0.1347 - val_loss: 0.1089
Epoch 15/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1213
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0185
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0379
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1347
3824/6530 [================>.............] - ETA: 0s - loss: 0.0183
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0378
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1336
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0184
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0373
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1340
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0184
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0370
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1330
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0185
6432/6530 [============================>.] - ETA: 0s - loss: 0.0372
6530/6530 [==============================] - 0s 41us/step - loss: 0.1324 - val_loss: 0.1086
Epoch 16/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1319
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0185
6530/6530 [==============================] - 1s 145us/step - loss: 0.0372 - val_loss: 0.0346
Epoch 4/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0252
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1279
6530/6530 [==============================] - 1s 96us/step - loss: 0.0185 - val_loss: 0.0149
Epoch 8/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0229
 416/6530 [>.............................] - ETA: 0s - loss: 0.0350
2880/6530 [============>.................] - ETA: 0s - loss: 0.1287
 576/6530 [=>............................] - ETA: 0s - loss: 0.0161
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0371
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1298
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0167
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0357
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1293
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0166
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0356
6530/6530 [==============================] - 0s 39us/step - loss: 0.1288 - val_loss: 0.1053
Epoch 17/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1299
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0167
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0367
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1268
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0169
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0358
2880/6530 [============>.................] - ETA: 0s - loss: 0.1262
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0164
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0355
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1267
3936/6530 [=================>............] - ETA: 0s - loss: 0.0164
3072/6530 [=============>................] - ETA: 0s - loss: 0.0357
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1261
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0163
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0349
6530/6530 [==============================] - 0s 40us/step - loss: 0.1270 - val_loss: 0.1065
Epoch 18/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1444
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0162
3840/6530 [================>.............] - ETA: 0s - loss: 0.0344
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1279
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0163
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1262
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0344
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0163
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1253
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0343
6530/6530 [==============================] - 1s 95us/step - loss: 0.0162 - val_loss: 0.0135
Epoch 9/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0093
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1258
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0344
 576/6530 [=>............................] - ETA: 0s - loss: 0.0139
6530/6530 [==============================] - 0s 38us/step - loss: 0.1257 - val_loss: 0.1012
Epoch 19/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1179
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0345
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0153
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1185
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0344
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0151
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1193
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0344
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0153
4096/6530 [=================>............] - ETA: 0s - loss: 0.1202
6432/6530 [============================>.] - ETA: 0s - loss: 0.0343
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0154
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1200
6530/6530 [==============================] - 1s 144us/step - loss: 0.0342 - val_loss: 0.0303
Epoch 5/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0434
3216/6530 [=============>................] - ETA: 0s - loss: 0.0151
6530/6530 [==============================] - 0s 40us/step - loss: 0.1209 - val_loss: 0.1033
Epoch 20/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1179
 416/6530 [>.............................] - ETA: 0s - loss: 0.0330
3760/6530 [================>.............] - ETA: 0s - loss: 0.0151
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1204
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0334
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0151
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1222
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0326
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0151
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1208
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0326
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0152
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1203
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0318
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0152
6530/6530 [==============================] - 0s 40us/step - loss: 0.1209 - val_loss: 0.1010
Epoch 21/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1323
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0317
6448/6530 [============================>.] - ETA: 0s - loss: 0.0153
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1213
6530/6530 [==============================] - 1s 98us/step - loss: 0.0153 - val_loss: 0.0137
Epoch 10/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0217
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0319
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1207
 560/6530 [=>............................] - ETA: 0s - loss: 0.0138
2976/6530 [============>.................] - ETA: 0s - loss: 0.0327
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1201
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0154
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0330
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1203
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0155
3744/6530 [================>.............] - ETA: 0s - loss: 0.0329
6530/6530 [==============================] - 0s 39us/step - loss: 0.1202 - val_loss: 0.0994
Epoch 22/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1110
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0153
4128/6530 [=================>............] - ETA: 0s - loss: 0.0330
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1196
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0153
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0331
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1196
3264/6530 [=============>................] - ETA: 0s - loss: 0.0150
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0329
4032/6530 [=================>............] - ETA: 0s - loss: 0.1183
3776/6530 [================>.............] - ETA: 0s - loss: 0.0150
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0328
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1178
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0149
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0326
6530/6530 [==============================] - 0s 40us/step - loss: 0.1180 - val_loss: 0.0987

4896/6530 [=====================>........] - ETA: 0s - loss: 0.0149Epoch 23/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1006
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0326
5456/6530 [========================>.....] - ETA: 0s - loss: 0.0150
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1148
6400/6530 [============================>.] - ETA: 0s - loss: 0.0324
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0150
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1121
6530/6530 [==============================] - 1s 144us/step - loss: 0.0323 - val_loss: 0.0281
Epoch 6/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0312
6528/6530 [============================>.] - ETA: 0s - loss: 0.0150
4032/6530 [=================>............] - ETA: 0s - loss: 0.1142
 384/6530 [>.............................] - ETA: 0s - loss: 0.0309
6530/6530 [==============================] - 1s 97us/step - loss: 0.0150 - val_loss: 0.0125
Epoch 11/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0121
5184/6530 [======================>.......] - ETA: 0s - loss: 0.1146
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0307
 576/6530 [=>............................] - ETA: 0s - loss: 0.0139
6528/6530 [============================>.] - ETA: 0s - loss: 0.1149
6530/6530 [==============================] - 0s 41us/step - loss: 0.1149 - val_loss: 0.0966
Epoch 24/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1198
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0318
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0139
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1167
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0317
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0140
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1155
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0317
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0140
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1153
2832/6530 [============>.................] - ETA: 0s - loss: 0.0140
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0325
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1150
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0140
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0324
6530/6530 [==============================] - 0s 39us/step - loss: 0.1141 - val_loss: 0.0966
Epoch 25/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1200
3952/6530 [=================>............] - ETA: 0s - loss: 0.0140
3040/6530 [============>.................] - ETA: 0s - loss: 0.0326
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1106
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0138
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0323
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1100
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0138
3776/6530 [================>.............] - ETA: 0s - loss: 0.0319
4032/6530 [=================>............] - ETA: 0s - loss: 0.1106
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0137
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0319
5440/6530 [=======================>......] - ETA: 0s - loss: 0.1104
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0139
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0318
6530/6530 [==============================] - 0s 40us/step - loss: 0.1104 - val_loss: 0.0951
Epoch 26/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1253
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0321
6530/6530 [==============================] - 1s 96us/step - loss: 0.0139 - val_loss: 0.0111
Epoch 12/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0107
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1143
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0321
 544/6530 [=>............................] - ETA: 0s - loss: 0.0136
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1133
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0323
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0134
4032/6530 [=================>............] - ETA: 0s - loss: 0.1125
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0323
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0137
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1120
6336/6530 [============================>.] - ETA: 0s - loss: 0.0321
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0135
6530/6530 [==============================] - 0s 40us/step - loss: 0.1122 - val_loss: 0.0939
Epoch 27/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1089
6530/6530 [==============================] - 1s 144us/step - loss: 0.0321 - val_loss: 0.0360

2736/6530 [===========>..................] - ETA: 0s - loss: 0.0133Epoch 7/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0234
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1057
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0131
 416/6530 [>.............................] - ETA: 0s - loss: 0.0290
2880/6530 [============>.................] - ETA: 0s - loss: 0.1094
3840/6530 [================>.............] - ETA: 0s - loss: 0.0131
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0316
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1089
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0131
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0309
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1102
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0132
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0306
6530/6530 [==============================] - 0s 38us/step - loss: 0.1097 - val_loss: 0.0927
Epoch 28/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0984
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0132
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0299
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1100
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0133
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0293
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1109
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0304
6530/6530 [==============================] - 1s 97us/step - loss: 0.0134 - val_loss: 0.0110
Epoch 13/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0098
4032/6530 [=================>............] - ETA: 0s - loss: 0.1106
3040/6530 [============>.................] - ETA: 0s - loss: 0.0302
 560/6530 [=>............................] - ETA: 0s - loss: 0.0129
5248/6530 [=======================>......] - ETA: 0s - loss: 0.1103
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0300
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0141
6528/6530 [============================>.] - ETA: 0s - loss: 0.1101
6530/6530 [==============================] - 0s 41us/step - loss: 0.1101 - val_loss: 0.0924
Epoch 29/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1012
3776/6530 [================>.............] - ETA: 0s - loss: 0.0298
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0137
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1109
4128/6530 [=================>............] - ETA: 0s - loss: 0.0298
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0138
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1078
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0299
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0138
4096/6530 [=================>............] - ETA: 0s - loss: 0.1098
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0302
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0139
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1090
3808/6530 [================>.............] - ETA: 0s - loss: 0.0136
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0304
6530/6530 [==============================] - 0s 39us/step - loss: 0.1087 - val_loss: 0.0922
Epoch 30/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1020
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0133
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0304
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1052
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0133
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0304
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1039
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0133
6400/6530 [============================>.] - ETA: 0s - loss: 0.0304
4032/6530 [=================>............] - ETA: 0s - loss: 0.1059
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0132
6530/6530 [==============================] - 1s 144us/step - loss: 0.0303 - val_loss: 0.0267
Epoch 8/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0290
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1068
6448/6530 [============================>.] - ETA: 0s - loss: 0.0132
 416/6530 [>.............................] - ETA: 0s - loss: 0.0303
6530/6530 [==============================] - 1s 98us/step - loss: 0.0132 - val_loss: 0.0111
Epoch 14/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0138
6530/6530 [==============================] - 0s 40us/step - loss: 0.1074 - val_loss: 0.0936
Epoch 31/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1023
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0325
 560/6530 [=>............................] - ETA: 0s - loss: 0.0139
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1086
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0302
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0137
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1085
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0310
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0136
4096/6530 [=================>............] - ETA: 0s - loss: 0.1085
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0305
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0133
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1071
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0308
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0131
6530/6530 [==============================] - 0s 39us/step - loss: 0.1073 - val_loss: 0.0912
Epoch 32/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1168
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0311
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0128
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1071
3008/6530 [============>.................] - ETA: 0s - loss: 0.0308
3824/6530 [================>.............] - ETA: 0s - loss: 0.0128
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1049
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0305
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0126
4096/6530 [=================>............] - ETA: 0s - loss: 0.1058
3776/6530 [================>.............] - ETA: 0s - loss: 0.0306
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0126
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1057
4128/6530 [=================>............] - ETA: 0s - loss: 0.0305
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0126
6530/6530 [==============================] - 0s 40us/step - loss: 0.1058 - val_loss: 0.0925
Epoch 33/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1290
4480/6530 [===================>..........] - ETA: 0s - loss: 0.0302
6064/6530 [==========================>...] - ETA: 0s - loss: 0.0126
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1091
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0299
6530/6530 [==============================] - 1s 96us/step - loss: 0.0127 - val_loss: 0.0109

2880/6530 [============>.................] - ETA: 0s - loss: 0.1077Epoch 15/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0097
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0299
4224/6530 [==================>...........] - ETA: 0s - loss: 0.1070
 576/6530 [=>............................] - ETA: 0s - loss: 0.0122
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0296
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1068
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0125
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0296
6530/6530 [==============================] - 0s 39us/step - loss: 0.1064 - val_loss: 0.0894

1568/6530 [======>.......................] - ETA: 0s - loss: 0.0125Epoch 34/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0871
6336/6530 [============================>.] - ETA: 0s - loss: 0.0295
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0126
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1051
6530/6530 [==============================] - 1s 144us/step - loss: 0.0293 - val_loss: 0.0263
Epoch 9/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0265
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0129
2880/6530 [============>.................] - ETA: 0s - loss: 0.1051
 416/6530 [>.............................] - ETA: 0s - loss: 0.0281
3232/6530 [=============>................] - ETA: 0s - loss: 0.0127
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1058
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0280
3792/6530 [================>.............] - ETA: 0s - loss: 0.0126
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1062
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0290
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0127
6530/6530 [==============================] - 0s 39us/step - loss: 0.1060 - val_loss: 0.0899
Epoch 35/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0966
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0295
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0126
1408/6530 [=====>........................] - ETA: 0s - loss: 0.1108
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0283
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0125
2752/6530 [===========>..................] - ETA: 0s - loss: 0.1051
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0125
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0285
4096/6530 [=================>............] - ETA: 0s - loss: 0.1051
6512/6530 [============================>.] - ETA: 0s - loss: 0.0125
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0283
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1046
6530/6530 [==============================] - 1s 97us/step - loss: 0.0125 - val_loss: 0.0100
Epoch 16/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0090
6530/6530 [==============================] - 0s 39us/step - loss: 0.1043 - val_loss: 0.0882

3040/6530 [============>.................] - ETA: 0s - loss: 0.0285Epoch 36/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0833
 576/6530 [=>............................] - ETA: 0s - loss: 0.0134
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0997
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0288
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0127
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1004
3808/6530 [================>.............] - ETA: 0s - loss: 0.0285
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0126
4160/6530 [==================>...........] - ETA: 0s - loss: 0.1022
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0285
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0124
5568/6530 [========================>.....] - ETA: 0s - loss: 0.1026
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0285
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0124
6530/6530 [==============================] - 0s 39us/step - loss: 0.1027 - val_loss: 0.0870
Epoch 37/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1019
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0281
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0123
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1011
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0279
3840/6530 [================>.............] - ETA: 0s - loss: 0.0123
2880/6530 [============>.................] - ETA: 0s - loss: 0.1015
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0278
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0121
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1013
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0276
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0121
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1015
6336/6530 [============================>.] - ETA: 0s - loss: 0.0278
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0121
6530/6530 [==============================] - 0s 39us/step - loss: 0.1017 - val_loss: 0.0856
Epoch 38/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1025
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0121
6530/6530 [==============================] - 1s 147us/step - loss: 0.0277 - val_loss: 0.0224
Epoch 10/81

  32/6530 [..............................] - ETA: 1s - loss: 0.0284
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1015
6448/6530 [============================>.] - ETA: 0s - loss: 0.0121
 384/6530 [>.............................] - ETA: 0s - loss: 0.0268
2688/6530 [===========>..................] - ETA: 0s - loss: 0.1025
6530/6530 [==============================] - 1s 98us/step - loss: 0.0121 - val_loss: 0.0099
Epoch 17/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0072
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0289
4096/6530 [=================>............] - ETA: 0s - loss: 0.1021
 576/6530 [=>............................] - ETA: 0s - loss: 0.0129
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0286
5504/6530 [========================>.....] - ETA: 0s - loss: 0.1026
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0122
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0277
6530/6530 [==============================] - 0s 39us/step - loss: 0.1025 - val_loss: 0.0873
Epoch 39/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0984
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0123
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0265
1472/6530 [=====>........................] - ETA: 0s - loss: 0.1023
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0123
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0262
2880/6530 [============>.................] - ETA: 0s - loss: 0.1033
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0122
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0261
4288/6530 [==================>...........] - ETA: 0s - loss: 0.1025
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0120
3008/6530 [============>.................] - ETA: 0s - loss: 0.0267
5632/6530 [========================>.....] - ETA: 0s - loss: 0.1024
3904/6530 [================>.............] - ETA: 0s - loss: 0.0120
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0265
6530/6530 [==============================] - 0s 38us/step - loss: 0.1025 - val_loss: 0.0858
Epoch 40/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0762
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0119
3776/6530 [================>.............] - ETA: 0s - loss: 0.0263
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0994
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0119
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0261
2816/6530 [===========>..................] - ETA: 0s - loss: 0.1003
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0119
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0259
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0999
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0118
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0258
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0999
6530/6530 [==============================] - 1s 95us/step - loss: 0.0120 - val_loss: 0.0097
Epoch 18/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0077
6530/6530 [==============================] - 0s 39us/step - loss: 0.1001 - val_loss: 0.0864
Epoch 41/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0954
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0261
 464/6530 [=>............................] - ETA: 0s - loss: 0.0123
1344/6530 [=====>........................] - ETA: 0s - loss: 0.1008
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0262
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0118
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0994
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0260
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0120
4032/6530 [=================>............] - ETA: 0s - loss: 0.0998
6368/6530 [============================>.] - ETA: 0s - loss: 0.0261
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0120
5376/6530 [=======================>......] - ETA: 0s - loss: 0.1009
6530/6530 [==============================] - 1s 145us/step - loss: 0.0261 - val_loss: 0.0221
Epoch 11/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0204
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0119
6530/6530 [==============================] - 0s 40us/step - loss: 0.1003 - val_loss: 0.0849
Epoch 42/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1074
 416/6530 [>.............................] - ETA: 0s - loss: 0.0286
3232/6530 [=============>................] - ETA: 0s - loss: 0.0115
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0986
3776/6530 [================>.............] - ETA: 0s - loss: 0.0115
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0296
2880/6530 [============>.................] - ETA: 0s - loss: 0.0985
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0114
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0278
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0987
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0115
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0278
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0991
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0116
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0277
6530/6530 [==============================] - 0s 39us/step - loss: 0.0986 - val_loss: 0.0853
Epoch 43/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0995
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0116
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0270
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0990
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0272
6530/6530 [==============================] - 1s 97us/step - loss: 0.0116 - val_loss: 0.0099
Epoch 19/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0099
2880/6530 [============>.................] - ETA: 0s - loss: 0.0997
3040/6530 [============>.................] - ETA: 0s - loss: 0.0270
 592/6530 [=>............................] - ETA: 0s - loss: 0.0111
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0989
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0269
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0118
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0992
3808/6530 [================>.............] - ETA: 0s - loss: 0.0270
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0115
6530/6530 [==============================] - 0s 39us/step - loss: 0.0993 - val_loss: 0.0831
Epoch 44/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0880
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0275
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0116
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0961
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0273
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0115
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0979
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0270
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0113
3840/6530 [================>.............] - ETA: 0s - loss: 0.0984
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0266
3840/6530 [================>.............] - ETA: 0s - loss: 0.0111
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0980
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0264
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0111
6528/6530 [============================>.] - ETA: 0s - loss: 0.0981
6530/6530 [==============================] - 0s 42us/step - loss: 0.0981 - val_loss: 0.0849
Epoch 45/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0846
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0263
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0112
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0953
6336/6530 [============================>.] - ETA: 0s - loss: 0.0264
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0112
2880/6530 [============>.................] - ETA: 0s - loss: 0.0984
6530/6530 [==============================] - 1s 145us/step - loss: 0.0265 - val_loss: 0.0219

5984/6530 [==========================>...] - ETA: 0s - loss: 0.0112Epoch 12/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0257
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0981
 416/6530 [>.............................] - ETA: 0s - loss: 0.0277
6530/6530 [==============================] - 1s 97us/step - loss: 0.0112 - val_loss: 0.0101
Epoch 20/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0108
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0982
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0262
 560/6530 [=>............................] - ETA: 0s - loss: 0.0109
6530/6530 [==============================] - 0s 38us/step - loss: 0.0987 - val_loss: 0.0839
Epoch 46/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1119
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0254
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0119
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0960
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0258
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0118
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0977
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0265
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0117
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0980
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0264
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0116
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0972
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0263
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0115
6530/6530 [==============================] - 0s 39us/step - loss: 0.0972 - val_loss: 0.0826
Epoch 47/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1232
3072/6530 [=============>................] - ETA: 0s - loss: 0.0258
3888/6530 [================>.............] - ETA: 0s - loss: 0.0115
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0973
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0258
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0114
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0979
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0114
3840/6530 [================>.............] - ETA: 0s - loss: 0.0257
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0960
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0114
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0257
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0962
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0114
6530/6530 [==============================] - 0s 39us/step - loss: 0.0968 - val_loss: 0.0837
Epoch 48/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0850
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0257
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0945
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0258
6530/6530 [==============================] - 1s 96us/step - loss: 0.0115 - val_loss: 0.0094
Epoch 21/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0126
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0966
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0258
 560/6530 [=>............................] - ETA: 0s - loss: 0.0113
4032/6530 [=================>............] - ETA: 0s - loss: 0.0975
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0258
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0117
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0965
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0116
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0256
6530/6530 [==============================] - 0s 39us/step - loss: 0.0971 - val_loss: 0.0817
Epoch 49/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1053
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0116
6528/6530 [============================>.] - ETA: 0s - loss: 0.0257
6530/6530 [==============================] - 1s 144us/step - loss: 0.0258 - val_loss: 0.0228
Epoch 13/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0349
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0950
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0114
 384/6530 [>.............................] - ETA: 0s - loss: 0.0297
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0955
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0113
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0273
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0962
3856/6530 [================>.............] - ETA: 0s - loss: 0.0113
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0263
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0963
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0112
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0263
6530/6530 [==============================] - 0s 39us/step - loss: 0.0968 - val_loss: 0.0818
Epoch 50/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1145
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0112
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0255
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0958
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0113
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0941
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0256
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0113
4096/6530 [=================>............] - ETA: 0s - loss: 0.0953
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0259
6530/6530 [==============================] - 1s 97us/step - loss: 0.0113 - val_loss: 0.0090
Epoch 22/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0101
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0952
3072/6530 [=============>................] - ETA: 0s - loss: 0.0265
 576/6530 [=>............................] - ETA: 0s - loss: 0.0104
6530/6530 [==============================] - 0s 40us/step - loss: 0.0955 - val_loss: 0.0809
Epoch 51/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1018
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0264
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0116
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0964
3840/6530 [================>.............] - ETA: 0s - loss: 0.0264
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0113
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0959
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0262
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0112
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0956
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0263
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0112
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0959
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0264
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0110
6530/6530 [==============================] - 0s 39us/step - loss: 0.0958 - val_loss: 0.0798
Epoch 52/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0913
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0258
3904/6530 [================>.............] - ETA: 0s - loss: 0.0111
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0935
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0259
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0110
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0932
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0258
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0110
4032/6530 [=================>............] - ETA: 0s - loss: 0.0945
6464/6530 [============================>.] - ETA: 0s - loss: 0.0258
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0110
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0947
6530/6530 [==============================] - 1s 145us/step - loss: 0.0258 - val_loss: 0.0189
Epoch 14/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0321
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0110
6530/6530 [==============================] - 0s 41us/step - loss: 0.0947 - val_loss: 0.0819
Epoch 53/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1062
 416/6530 [>.............................] - ETA: 0s - loss: 0.0279
6464/6530 [============================>.] - ETA: 0s - loss: 0.0110
6530/6530 [==============================] - 1s 98us/step - loss: 0.0110 - val_loss: 0.0089
Epoch 23/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0097
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0932
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0264
 560/6530 [=>............................] - ETA: 0s - loss: 0.0105
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0928
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0259
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0108
3968/6530 [=================>............] - ETA: 0s - loss: 0.0945
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0271
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0109
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0943
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0269
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0108
6530/6530 [==============================] - 0s 40us/step - loss: 0.0946 - val_loss: 0.0801
Epoch 54/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1086
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0264
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0109
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0934
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0260
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0108
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0930
3072/6530 [=============>................] - ETA: 0s - loss: 0.0261
3824/6530 [================>.............] - ETA: 0s - loss: 0.0108
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0928
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0256
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0107
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0930
3840/6530 [================>.............] - ETA: 0s - loss: 0.0254
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0108
6530/6530 [==============================] - 0s 39us/step - loss: 0.0930 - val_loss: 0.0801
Epoch 55/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0987
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0253
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0108
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0902
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0253
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0108
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0919
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0252
4032/6530 [=================>............] - ETA: 0s - loss: 0.0926
6530/6530 [==============================] - 1s 97us/step - loss: 0.0109 - val_loss: 0.0104
Epoch 24/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0117
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0250
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0933
 528/6530 [=>............................] - ETA: 0s - loss: 0.0108
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0248
6530/6530 [==============================] - 0s 40us/step - loss: 0.0934 - val_loss: 0.0790
Epoch 56/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0946
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0113
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0247
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0941
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0112
6464/6530 [============================>.] - ETA: 0s - loss: 0.0246
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0927
2080/6530 [========>.....................] - ETA: 0s - loss: 0.0111
6530/6530 [==============================] - 1s 143us/step - loss: 0.0247 - val_loss: 0.0192
Epoch 15/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0243
4032/6530 [=================>............] - ETA: 0s - loss: 0.0913
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0110
 416/6530 [>.............................] - ETA: 0s - loss: 0.0224
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0917
3168/6530 [=============>................] - ETA: 0s - loss: 0.0107
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0224
6530/6530 [==============================] - 0s 40us/step - loss: 0.0928 - val_loss: 0.0785
Epoch 57/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0813
3712/6530 [================>.............] - ETA: 0s - loss: 0.0107
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0230
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0941
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0107
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0233
2880/6530 [============>.................] - ETA: 0s - loss: 0.0911
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0106
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0233
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0927
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0108
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0237
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0107
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0929
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0231
6530/6530 [==============================] - 0s 39us/step - loss: 0.0929 - val_loss: 0.0779
Epoch 58/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0707
6352/6530 [============================>.] - ETA: 0s - loss: 0.0108
3008/6530 [============>.................] - ETA: 0s - loss: 0.0231
6530/6530 [==============================] - 1s 100us/step - loss: 0.0108 - val_loss: 0.0092
Epoch 25/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0077
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0942
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0227
 544/6530 [=>............................] - ETA: 0s - loss: 0.0100
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0945
3712/6530 [================>.............] - ETA: 0s - loss: 0.0230
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0101
4032/6530 [=================>............] - ETA: 0s - loss: 0.0927
4032/6530 [=================>............] - ETA: 0s - loss: 0.0230
1616/6530 [======>.......................] - ETA: 0s - loss: 0.0100
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0923
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0229
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0099
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0929
6530/6530 [==============================] - 0s 43us/step - loss: 0.0928 - val_loss: 0.0788
Epoch 59/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0861
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0232
2640/6530 [===========>..................] - ETA: 0s - loss: 0.0103
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0934
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0233
3200/6530 [=============>................] - ETA: 0s - loss: 0.0101
2880/6530 [============>.................] - ETA: 0s - loss: 0.0940
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0234
3744/6530 [================>.............] - ETA: 0s - loss: 0.0102
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0932
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0233
4272/6530 [==================>...........] - ETA: 0s - loss: 0.0102
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0926
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0233
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0102
6530/6530 [==============================] - 0s 39us/step - loss: 0.0924 - val_loss: 0.0796
Epoch 60/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0964
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0102
6530/6530 [==============================] - 1s 148us/step - loss: 0.0231 - val_loss: 0.0169
Epoch 16/81

  32/6530 [..............................] - ETA: 1s - loss: 0.0158
1344/6530 [=====>........................] - ETA: 0s - loss: 0.0925
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0102
 416/6530 [>.............................] - ETA: 0s - loss: 0.0206
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0924
6448/6530 [============================>.] - ETA: 0s - loss: 0.0104
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0207
4032/6530 [=================>............] - ETA: 0s - loss: 0.0920
6530/6530 [==============================] - 1s 99us/step - loss: 0.0104 - val_loss: 0.0087
Epoch 26/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0089
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0210
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0920
 576/6530 [=>............................] - ETA: 0s - loss: 0.0104
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0215
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0104
6530/6530 [==============================] - 0s 40us/step - loss: 0.0924 - val_loss: 0.0800
Epoch 61/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0897
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0217
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0104
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0926
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0216
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0103
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0915
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0216
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0103
4096/6530 [=================>............] - ETA: 0s - loss: 0.0907
3008/6530 [============>.................] - ETA: 0s - loss: 0.0217
3280/6530 [==============>...............] - ETA: 0s - loss: 0.0102
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0904
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0217
3824/6530 [================>.............] - ETA: 0s - loss: 0.0103
6530/6530 [==============================] - 0s 39us/step - loss: 0.0905 - val_loss: 0.0776
Epoch 62/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0956
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0103
3776/6530 [================>.............] - ETA: 0s - loss: 0.0218
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0931
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0103
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0220
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0926
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0104
3840/6530 [================>.............] - ETA: 0s - loss: 0.0928
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0221
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0104
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0914
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0223
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0221
6530/6530 [==============================] - 0s 40us/step - loss: 0.0912 - val_loss: 0.0770
Epoch 63/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0895
6530/6530 [==============================] - 1s 97us/step - loss: 0.0104 - val_loss: 0.0092
Epoch 27/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0112
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0221
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0925
 512/6530 [=>............................] - ETA: 0s - loss: 0.0110
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0222
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0917
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0108
6432/6530 [============================>.] - ETA: 0s - loss: 0.0222
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0925
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0107
6530/6530 [==============================] - 1s 145us/step - loss: 0.0222 - val_loss: 0.0164
Epoch 17/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0193
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0918
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0104
 416/6530 [>.............................] - ETA: 0s - loss: 0.0223
6530/6530 [==============================] - 0s 39us/step - loss: 0.0916 - val_loss: 0.0768
Epoch 64/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0887
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0105
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0217
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0967
3216/6530 [=============>................] - ETA: 0s - loss: 0.0101
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0223
3760/6530 [================>.............] - ETA: 0s - loss: 0.0102
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0927
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0228
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0101
3968/6530 [=================>............] - ETA: 0s - loss: 0.0915
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0225
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0100
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0905
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0100
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0222
6530/6530 [==============================] - 0s 41us/step - loss: 0.0909 - val_loss: 0.0755
Epoch 65/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1035
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0101
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0222
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0902
6352/6530 [============================>.] - ETA: 0s - loss: 0.0102
2944/6530 [============>.................] - ETA: 0s - loss: 0.0221
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0878
6530/6530 [==============================] - 1s 100us/step - loss: 0.0102 - val_loss: 0.0084
Epoch 28/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0134
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0225
3968/6530 [=================>............] - ETA: 0s - loss: 0.0890
 560/6530 [=>............................] - ETA: 0s - loss: 0.0107
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0223
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0893
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0102
4064/6530 [=================>............] - ETA: 0s - loss: 0.0221
6530/6530 [==============================] - 0s 41us/step - loss: 0.0893 - val_loss: 0.0762
Epoch 66/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1083
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0103
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0221
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0902
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0103
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0223
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0901
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0104
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0224
4096/6530 [=================>............] - ETA: 0s - loss: 0.0899
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0103
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0223
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0896
3840/6530 [================>.............] - ETA: 0s - loss: 0.0102
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0224
6530/6530 [==============================] - 0s 39us/step - loss: 0.0893 - val_loss: 0.0758
Epoch 67/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0795
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0101
6336/6530 [============================>.] - ETA: 0s - loss: 0.0223
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0866
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 1s 145us/step - loss: 0.0224 - val_loss: 0.0171
Epoch 18/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0145
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0874
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0102
 384/6530 [>.............................] - ETA: 0s - loss: 0.0198
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0891
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0102
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0194
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0897
6530/6530 [==============================] - 1s 97us/step - loss: 0.0103 - val_loss: 0.0087
Epoch 29/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0057
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0202
6530/6530 [==============================] - 0s 40us/step - loss: 0.0904 - val_loss: 0.0756
Epoch 68/81

  64/6530 [..............................] - ETA: 0s - loss: 0.1082
 576/6530 [=>............................] - ETA: 0s - loss: 0.0103
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0215
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0864
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0103
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0220
2880/6530 [============>.................] - ETA: 0s - loss: 0.0866
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0105
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0217
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0872
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0102
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0220
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0874
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0100
2880/6530 [============>.................] - ETA: 0s - loss: 0.0216
6530/6530 [==============================] - 0s 39us/step - loss: 0.0873 - val_loss: 0.0756
Epoch 69/81

  64/6530 [..............................] - ETA: 0s - loss: 0.0867
3200/6530 [=============>................] - ETA: 0s - loss: 0.0098
3200/6530 [=============>................] - ETA: 0s - loss: 0.0215
1280/6530 [====>.........................] - ETA: 0s - loss: 0.0903
3744/6530 [================>.............] - ETA: 0s - loss: 0.0099
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0217
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0890
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0098
3872/6530 [================>.............] - ETA: 0s - loss: 0.0218
3904/6530 [================>.............] - ETA: 0s - loss: 0.0890
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0098
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0217
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0885
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0099
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0217
6530/6530 [==============================] - 0s 41us/step - loss: 0.0886 - val_loss: 0.0773

5952/6530 [==========================>...] - ETA: 0s - loss: 0.0099
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0214
6512/6530 [============================>.] - ETA: 0s - loss: 0.0100
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0215
6530/6530 [==============================] - 1s 97us/step - loss: 0.0100 - val_loss: 0.0086
Epoch 30/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0183
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0216
 544/6530 [=>............................] - ETA: 0s - loss: 0.0104
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0215
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0105
6528/6530 [============================>.] - ETA: 0s - loss: 0.0215
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0104
6530/6530 [==============================] - 1s 147us/step - loss: 0.0215 - val_loss: 0.0165
Epoch 19/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0280
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0104
 416/6530 [>.............................] - ETA: 0s - loss: 0.0220
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0103
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0251
3344/6530 [==============>...............] - ETA: 0s - loss: 0.0101
# training | RMSE: 0.0895, MAE: 0.0683
worker 1  xfile  [1, 81, 0, 100, [], {'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13111359748493934}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31119805265189826}, 'layer_4_size': 5, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18939250426965906}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.08947785953090155, 'rmse': 0.08947785953090155, 'mae': 0.06826165350469851, 'early_stop': True}
{'batch_size': 16,
 'init': 'uniform',
 'loss': 'mean_squared_error',
 'n_layers': 2,
 'optimizer': 'adamax',
 'scaler': 'MaxAbsScaler',
 'shuffle': True}
layer 1 | size:  83 | activation: sigmoid | extras: None 
layer 2 | size:  23 | activation: tanh    | extras: dropout - rate: 22.9% 

xnet is <keras.engine.sequential.Sequential object at 0x7f777a7947b8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 1:01 - loss: 0.7499
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0237
3856/6530 [================>.............] - ETA: 0s - loss: 0.0102
 576/6530 [=>............................] - ETA: 2s - loss: 0.1751  
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0234
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0100
1168/6530 [====>.........................] - ETA: 1s - loss: 0.1192
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0232
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0101
1712/6530 [======>.......................] - ETA: 0s - loss: 0.1021
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0233
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0101
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0920
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0231
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0100
2880/6530 [============>.................] - ETA: 0s - loss: 0.0882
3040/6530 [============>.................] - ETA: 0s - loss: 0.0227
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0858
6530/6530 [==============================] - 1s 99us/step - loss: 0.0100 - val_loss: 0.0082
Epoch 31/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0091
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0225
3872/6530 [================>.............] - ETA: 0s - loss: 0.0814
 576/6530 [=>............................] - ETA: 0s - loss: 0.0093
3776/6530 [================>.............] - ETA: 0s - loss: 0.0222
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0785
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0099
4128/6530 [=================>............] - ETA: 0s - loss: 0.0223
5008/6530 [======================>.......] - ETA: 0s - loss: 0.0760
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0101
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0222
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0737
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0103
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0221
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0720
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0101
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0220
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0098
6530/6530 [==============================] - 1s 119us/step - loss: 0.0710 - val_loss: 0.0512
Epoch 2/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0506
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0216
3856/6530 [================>.............] - ETA: 0s - loss: 0.0099
 608/6530 [=>............................] - ETA: 0s - loss: 0.0520
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0215
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0097
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0510
6400/6530 [============================>.] - ETA: 0s - loss: 0.0216
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0096
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0519
6530/6530 [==============================] - 1s 145us/step - loss: 0.0215 - val_loss: 0.0195
Epoch 20/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0290
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0097
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0506
 384/6530 [>.............................] - ETA: 0s - loss: 0.0299
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0098
2976/6530 [============>.................] - ETA: 0s - loss: 0.0496
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0278
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0485
6530/6530 [==============================] - 1s 97us/step - loss: 0.0098 - val_loss: 0.0081
Epoch 32/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0078
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0262
4112/6530 [=================>............] - ETA: 0s - loss: 0.0476
 560/6530 [=>............................] - ETA: 0s - loss: 0.0096
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0247
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0475
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0108
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0246
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0467
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0105
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0240
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0457
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0104
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0231
6464/6530 [============================>.] - ETA: 0s - loss: 0.0453
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0104
3008/6530 [============>.................] - ETA: 0s - loss: 0.0233
6530/6530 [==============================] - 1s 90us/step - loss: 0.0453 - val_loss: 0.0416
Epoch 3/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0646
3184/6530 [=============>................] - ETA: 0s - loss: 0.0101
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0229
 576/6530 [=>............................] - ETA: 0s - loss: 0.0419
3728/6530 [================>.............] - ETA: 0s - loss: 0.0102
3776/6530 [================>.............] - ETA: 0s - loss: 0.0225
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0420
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0101
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0222
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0415
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0099
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0221
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0415
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0099
2928/6530 [============>.................] - ETA: 0s - loss: 0.0411
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0224
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0098
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0416
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0223
6512/6530 [============================>.] - ETA: 0s - loss: 0.0098
4080/6530 [=================>............] - ETA: 0s - loss: 0.0414
6530/6530 [==============================] - 1s 97us/step - loss: 0.0098 - val_loss: 0.0081
Epoch 33/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0173
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0220
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0410
 576/6530 [=>............................] - ETA: 0s - loss: 0.0099
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0218
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0416
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0100
6464/6530 [============================>.] - ETA: 0s - loss: 0.0218
6530/6530 [==============================] - 1s 143us/step - loss: 0.0217 - val_loss: 0.0148

5824/6530 [=========================>....] - ETA: 0s - loss: 0.0413Epoch 21/81

  32/6530 [..............................] - ETA: 1s - loss: 0.0259
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0100
6400/6530 [============================>.] - ETA: 0s - loss: 0.0417
 416/6530 [>.............................] - ETA: 0s - loss: 0.0211
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0099
6530/6530 [==============================] - 1s 91us/step - loss: 0.0417 - val_loss: 0.0416
Epoch 4/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0248
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0097
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0199
 624/6530 [=>............................] - ETA: 0s - loss: 0.0399
3216/6530 [=============>................] - ETA: 0s - loss: 0.0095
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0197
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0399
3760/6530 [================>.............] - ETA: 0s - loss: 0.0096
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0199
1776/6530 [=======>......................] - ETA: 0s - loss: 0.0407
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0096
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0197
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0408
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0096
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0206
2880/6530 [============>.................] - ETA: 0s - loss: 0.0409
5328/6530 [=======================>......] - ETA: 0s - loss: 0.0095
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0204
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0408
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0095
3040/6530 [============>.................] - ETA: 0s - loss: 0.0206
4064/6530 [=================>............] - ETA: 0s - loss: 0.0412
6432/6530 [============================>.] - ETA: 0s - loss: 0.0096
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0204
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0412
6530/6530 [==============================] - 1s 99us/step - loss: 0.0096 - val_loss: 0.0083
Epoch 34/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0112
3808/6530 [================>.............] - ETA: 0s - loss: 0.0203
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0413
 560/6530 [=>............................] - ETA: 0s - loss: 0.0097
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0204
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0414
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0100
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0201
6368/6530 [============================>.] - ETA: 0s - loss: 0.0414
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 1s 92us/step - loss: 0.0415 - val_loss: 0.0417
Epoch 5/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0435
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0201
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0101
 576/6530 [=>............................] - ETA: 0s - loss: 0.0379
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0199
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0099
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0397
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0200
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0097
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0406
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0201
3856/6530 [================>.............] - ETA: 0s - loss: 0.0097
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0418
6336/6530 [============================>.] - ETA: 0s - loss: 0.0200
4400/6530 [===================>..........] - ETA: 0s - loss: 0.0095
2944/6530 [============>.................] - ETA: 0s - loss: 0.0426
6530/6530 [==============================] - 1s 146us/step - loss: 0.0200 - val_loss: 0.0146
Epoch 22/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0140
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0095
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0425
 416/6530 [>.............................] - ETA: 0s - loss: 0.0208
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0095
4112/6530 [=================>............] - ETA: 0s - loss: 0.0420
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0228
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0096
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0416
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0235
6530/6530 [==============================] - 1s 97us/step - loss: 0.0097 - val_loss: 0.0080
Epoch 35/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0083
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0412
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0231
 560/6530 [=>............................] - ETA: 0s - loss: 0.0100
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0413
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0227
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0102
6448/6530 [============================>.] - ETA: 0s - loss: 0.0412
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0226
6530/6530 [==============================] - 1s 90us/step - loss: 0.0411 - val_loss: 0.0416
Epoch 6/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0193
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0100
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0220
 592/6530 [=>............................] - ETA: 0s - loss: 0.0435
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0100
3008/6530 [============>.................] - ETA: 0s - loss: 0.0218
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0426
2672/6530 [===========>..................] - ETA: 0s - loss: 0.0100
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0215
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0420
3216/6530 [=============>................] - ETA: 0s - loss: 0.0097
3744/6530 [================>.............] - ETA: 0s - loss: 0.0215
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0420
3744/6530 [================>.............] - ETA: 0s - loss: 0.0097
2864/6530 [============>.................] - ETA: 0s - loss: 0.0424
4128/6530 [=================>............] - ETA: 0s - loss: 0.0216
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0098
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0422
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0214
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0097
4064/6530 [=================>............] - ETA: 0s - loss: 0.0418
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0213
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0096
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0414
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0096
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0215
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0410
6512/6530 [============================>.] - ETA: 0s - loss: 0.0097
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0214
6530/6530 [==============================] - 1s 97us/step - loss: 0.0097 - val_loss: 0.0079
Epoch 36/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0147
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0410
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0213
 560/6530 [=>............................] - ETA: 0s - loss: 0.0091
6416/6530 [============================>.] - ETA: 0s - loss: 0.0413
6432/6530 [============================>.] - ETA: 0s - loss: 0.0210
6530/6530 [==============================] - 1s 91us/step - loss: 0.0413 - val_loss: 0.0479
Epoch 7/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0443
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0096
6530/6530 [==============================] - 1s 145us/step - loss: 0.0211 - val_loss: 0.0158
Epoch 23/81

  32/6530 [..............................] - ETA: 1s - loss: 0.0214
 608/6530 [=>............................] - ETA: 0s - loss: 0.0428
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0096
 416/6530 [>.............................] - ETA: 0s - loss: 0.0248
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0421
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0094
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0222
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0411
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0095
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0225
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0410
3264/6530 [=============>................] - ETA: 0s - loss: 0.0093
1472/6530 [=====>........................] - ETA: 0s - loss: 0.0234
2880/6530 [============>.................] - ETA: 0s - loss: 0.0413
3824/6530 [================>.............] - ETA: 0s - loss: 0.0094
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0228
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0415
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0093
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0224
4048/6530 [=================>............] - ETA: 0s - loss: 0.0412
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0093
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0222
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0411
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0093
3008/6530 [============>.................] - ETA: 0s - loss: 0.0218
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0419
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0093
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0215
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0419
6530/6530 [==============================] - 1s 96us/step - loss: 0.0093 - val_loss: 0.0085
Epoch 37/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0075
6416/6530 [============================>.] - ETA: 0s - loss: 0.0416
3776/6530 [================>.............] - ETA: 0s - loss: 0.0212
 560/6530 [=>............................] - ETA: 0s - loss: 0.0100
6530/6530 [==============================] - 1s 90us/step - loss: 0.0416 - val_loss: 0.0428
Epoch 8/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0148
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0209
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0100
 624/6530 [=>............................] - ETA: 0s - loss: 0.0394
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0208
1680/6530 [======>.......................] - ETA: 0s - loss: 0.0100
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0397
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0205
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0101
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0394
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0205
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0099
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0404
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0205
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0096
2992/6530 [============>.................] - ETA: 0s - loss: 0.0413
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0205
3920/6530 [=================>............] - ETA: 0s - loss: 0.0096
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0418
6464/6530 [============================>.] - ETA: 0s - loss: 0.0203
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0095
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0419
6530/6530 [==============================] - 1s 143us/step - loss: 0.0202 - val_loss: 0.0144
Epoch 24/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0091
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0095
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0423
 416/6530 [>.............................] - ETA: 0s - loss: 0.0156
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0094
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0418
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0199
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0095
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0416
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0200
6530/6530 [==============================] - 1s 96us/step - loss: 0.0095 - val_loss: 0.0078
Epoch 38/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0076
6530/6530 [==============================] - 1s 89us/step - loss: 0.0414 - val_loss: 0.0413
Epoch 9/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0335
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0194
 544/6530 [=>............................] - ETA: 0s - loss: 0.0095
 576/6530 [=>............................] - ETA: 0s - loss: 0.0428
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0195
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0097
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0428
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0200
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0098
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0422
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0200
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0097
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0412
3072/6530 [=============>................] - ETA: 0s - loss: 0.0201
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0094
2928/6530 [============>.................] - ETA: 0s - loss: 0.0414
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0197
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0093
3536/6530 [===============>..............] - ETA: 0s - loss: 0.0418
3808/6530 [================>.............] - ETA: 0s - loss: 0.0195
3840/6530 [================>.............] - ETA: 0s - loss: 0.0093
4112/6530 [=================>............] - ETA: 0s - loss: 0.0419
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0197
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0092
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0420
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0196
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0093
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0417
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0195
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0094
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0415
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0196
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0093
6416/6530 [============================>.] - ETA: 0s - loss: 0.0415
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0197
6530/6530 [==============================] - 1s 91us/step - loss: 0.0415 - val_loss: 0.0413

6464/6530 [============================>.] - ETA: 0s - loss: 0.0094Epoch 10/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0590
6530/6530 [==============================] - 1s 98us/step - loss: 0.0094 - val_loss: 0.0079
Epoch 39/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0145
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0197
 560/6530 [=>............................] - ETA: 0s - loss: 0.0423
 560/6530 [=>............................] - ETA: 0s - loss: 0.0103
6496/6530 [============================>.] - ETA: 0s - loss: 0.0196
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0409
6530/6530 [==============================] - 1s 143us/step - loss: 0.0196 - val_loss: 0.0181

1040/6530 [===>..........................] - ETA: 0s - loss: 0.0099Epoch 25/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0143
1712/6530 [======>.......................] - ETA: 0s - loss: 0.0431
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0101
 384/6530 [>.............................] - ETA: 0s - loss: 0.0213
2288/6530 [=========>....................] - ETA: 0s - loss: 0.0432
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0099
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0213
2864/6530 [============>.................] - ETA: 0s - loss: 0.0423
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0098
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0214
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0416
3232/6530 [=============>................] - ETA: 0s - loss: 0.0096
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0214
4032/6530 [=================>............] - ETA: 0s - loss: 0.0416
3776/6530 [================>.............] - ETA: 0s - loss: 0.0096
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0215
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0409
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0095
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0211
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0412
4880/6530 [=====================>........] - ETA: 0s - loss: 0.0094
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0209
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0412
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0095
3008/6530 [============>.................] - ETA: 0s - loss: 0.0206
6384/6530 [============================>.] - ETA: 0s - loss: 0.0417
5936/6530 [==========================>...] - ETA: 0s - loss: 0.0095
6530/6530 [==============================] - 1s 91us/step - loss: 0.0417 - val_loss: 0.0415
Epoch 11/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0304
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0205
6480/6530 [============================>.] - ETA: 0s - loss: 0.0095
 608/6530 [=>............................] - ETA: 0s - loss: 0.0404
3776/6530 [================>.............] - ETA: 0s - loss: 0.0203
6530/6530 [==============================] - 1s 98us/step - loss: 0.0095 - val_loss: 0.0077
Epoch 40/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0050
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0413
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0203
 464/6530 [=>............................] - ETA: 0s - loss: 0.0102
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0411
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0202
1024/6530 [===>..........................] - ETA: 0s - loss: 0.0097
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0414
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0199
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0097
2912/6530 [============>.................] - ETA: 0s - loss: 0.0422
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0200
2032/6530 [========>.....................] - ETA: 0s - loss: 0.0095
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0420
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0200
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0095
4016/6530 [=================>............] - ETA: 0s - loss: 0.0418
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0198
3088/6530 [=============>................] - ETA: 0s - loss: 0.0092
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0421
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0198
3600/6530 [===============>..............] - ETA: 0s - loss: 0.0093
5200/6530 [======================>.......] - ETA: 0s - loss: 0.0416
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0092
6530/6530 [==============================] - 1s 146us/step - loss: 0.0199 - val_loss: 0.0139
Epoch 26/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0371
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0415
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0090
 384/6530 [>.............................] - ETA: 0s - loss: 0.0284
6336/6530 [============================>.] - ETA: 0s - loss: 0.0417
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0090
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0254
6530/6530 [==============================] - 1s 92us/step - loss: 0.0416 - val_loss: 0.0420
Epoch 12/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0409
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0090
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0249
 592/6530 [=>............................] - ETA: 0s - loss: 0.0411
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0091
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0242
1200/6530 [====>.........................] - ETA: 0s - loss: 0.0409
6530/6530 [==============================] - 1s 101us/step - loss: 0.0092 - val_loss: 0.0079
Epoch 41/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0068
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0406
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0239
 560/6530 [=>............................] - ETA: 0s - loss: 0.0092
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0403
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0238
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0092
2976/6530 [============>.................] - ETA: 0s - loss: 0.0402
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0234
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0096
3568/6530 [===============>..............] - ETA: 0s - loss: 0.0405
3072/6530 [=============>................] - ETA: 0s - loss: 0.0231
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0096
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0414
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0229
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0095
4720/6530 [====================>.........] - ETA: 0s - loss: 0.0419
3808/6530 [================>.............] - ETA: 0s - loss: 0.0227
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0092
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0422
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0225
3872/6530 [================>.............] - ETA: 0s - loss: 0.0093
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0419
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0223
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0092
6448/6530 [============================>.] - ETA: 0s - loss: 0.0416
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0221
6530/6530 [==============================] - 1s 90us/step - loss: 0.0415 - val_loss: 0.0443

4960/6530 [=====================>........] - ETA: 0s - loss: 0.0092Epoch 13/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0502
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0219
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0093
 592/6530 [=>............................] - ETA: 0s - loss: 0.0466
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0217
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0094
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0440
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0216
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0423
6530/6530 [==============================] - 1s 97us/step - loss: 0.0094 - val_loss: 0.0076
Epoch 42/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0068
6464/6530 [============================>.] - ETA: 0s - loss: 0.0214
2352/6530 [=========>....................] - ETA: 0s - loss: 0.0430
 560/6530 [=>............................] - ETA: 0s - loss: 0.0093
6530/6530 [==============================] - 1s 144us/step - loss: 0.0213 - val_loss: 0.0150
Epoch 27/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0163
2928/6530 [============>.................] - ETA: 0s - loss: 0.0423
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0092
 416/6530 [>.............................] - ETA: 0s - loss: 0.0182
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0423
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0092
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0198
4096/6530 [=================>............] - ETA: 0s - loss: 0.0426
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0091
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0192
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0424
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0091
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0191
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0420
3248/6530 [=============>................] - ETA: 0s - loss: 0.0090
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0193
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0420
3792/6530 [================>.............] - ETA: 0s - loss: 0.0091
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0192
6432/6530 [============================>.] - ETA: 0s - loss: 0.0418
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0092
6530/6530 [==============================] - 1s 91us/step - loss: 0.0418 - val_loss: 0.0500
Epoch 14/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0541
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0188
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0091
 560/6530 [=>............................] - ETA: 0s - loss: 0.0464
3072/6530 [=============>................] - ETA: 0s - loss: 0.0191
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0092
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0444
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0188
5952/6530 [==========================>...] - ETA: 0s - loss: 0.0092
1744/6530 [=======>......................] - ETA: 0s - loss: 0.0448
3840/6530 [================>.............] - ETA: 0s - loss: 0.0189
6512/6530 [============================>.] - ETA: 0s - loss: 0.0092
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0424
6530/6530 [==============================] - 1s 97us/step - loss: 0.0093 - val_loss: 0.0077
Epoch 43/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0077
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0188
2912/6530 [============>.................] - ETA: 0s - loss: 0.0422
 544/6530 [=>............................] - ETA: 0s - loss: 0.0088
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0188
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0417
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0095
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0188
4064/6530 [=================>............] - ETA: 0s - loss: 0.0418
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0095
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0190
4656/6530 [====================>.........] - ETA: 0s - loss: 0.0416
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0094
5760/6530 [=========================>....] - ETA: 0s - loss: 0.0188
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0416
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0094
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0188
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0420
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0092
6496/6530 [============================>.] - ETA: 0s - loss: 0.0188
6416/6530 [============================>.] - ETA: 0s - loss: 0.0421
3856/6530 [================>.............] - ETA: 0s - loss: 0.0093
6530/6530 [==============================] - 1s 143us/step - loss: 0.0188 - val_loss: 0.0137
Epoch 28/81

  32/6530 [..............................] - ETA: 1s - loss: 0.0254
6530/6530 [==============================] - 1s 90us/step - loss: 0.0421 - val_loss: 0.0418

4432/6530 [===================>..........] - ETA: 0s - loss: 0.0092
 416/6530 [>.............................] - ETA: 0s - loss: 0.0205
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0092
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0208
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0092
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0205
6032/6530 [==========================>...] - ETA: 0s - loss: 0.0092
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0203
6530/6530 [==============================] - 1s 96us/step - loss: 0.0093 - val_loss: 0.0080

1920/6530 [=======>......................] - ETA: 0s - loss: 0.0204Epoch 44/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0063
 576/6530 [=>............................] - ETA: 0s - loss: 0.0097
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0206
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0099
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0204
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0099
3008/6530 [============>.................] - ETA: 0s - loss: 0.0203
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0097
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0202
2832/6530 [============>.................] - ETA: 0s - loss: 0.0095
3808/6530 [================>.............] - ETA: 0s - loss: 0.0199
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0093
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0200
3872/6530 [================>.............] - ETA: 0s - loss: 0.0093
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0200
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0092
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0201
4976/6530 [=====================>........] - ETA: 0s - loss: 0.0091
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0201
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0092
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0200
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0092
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0200
6530/6530 [==============================] - 1s 96us/step - loss: 0.0092 - val_loss: 0.0075
Epoch 45/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0117
6464/6530 [============================>.] - ETA: 0s - loss: 0.0201
6530/6530 [==============================] - 1s 143us/step - loss: 0.0200 - val_loss: 0.0160
Epoch 29/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0073
 592/6530 [=>............................] - ETA: 0s - loss: 0.0095
 416/6530 [>.............................] - ETA: 0s - loss: 0.0229
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0093
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0194
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0092
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0197
2320/6530 [=========>....................] - ETA: 0s - loss: 0.0093
# training | RMSE: 0.2029, MAE: 0.1610
worker 1  xfile  [3, 81, 0, 100, [], {'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2291344165235558}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2184897927910865}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27286622920865944}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.2029150402128102, 'rmse': 0.2029150402128102, 'mae': 0.16096343566942442, 'early_stop': True}
{'batch_size': 16,
 'init': 'glorot_uniform',
 'loss': 'mean_squared_error',
 'n_layers': 5,
 'optimizer': 'rmsprop',
 'scaler': 'RobustScaler',
 'shuffle': True}
layer 1 | size:  32 | activation: relu    | extras: batchnorm 
layer 2 | size:  64 | activation: tanh    | extras: dropout - rate: 34.4% 
layer 3 | size:  12 | activation: relu    | extras: dropout - rate: 10.4% 
layer 4 | size:  79 | activation: sigmoid | extras: None 
layer 5 | size:  27 | activation: relu    | extras: None 

xnet is <keras.engine.sequential.Sequential object at 0x7f7771f21eb8>
vggnet init done 1
Train on 6530 samples, validate on 824 samples
Epoch 1/81

  16/6530 [..............................] - ETA: 2:50 - loss: 1.2755
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0195
2880/6530 [============>.................] - ETA: 0s - loss: 0.0092
 320/6530 [>.............................] - ETA: 9s - loss: 0.1896  
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0195
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0091
 624/6530 [=>............................] - ETA: 4s - loss: 0.1300
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0195
4000/6530 [=================>............] - ETA: 0s - loss: 0.0091
 928/6530 [===>..........................] - ETA: 3s - loss: 0.1065
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0198
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0089
1232/6530 [====>.........................] - ETA: 2s - loss: 0.0928
3072/6530 [=============>................] - ETA: 0s - loss: 0.0199
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0090
1536/6530 [======>.......................] - ETA: 2s - loss: 0.0840
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0202
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0090
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0776
6256/6530 [===========================>..] - ETA: 0s - loss: 0.0089
3808/6530 [================>.............] - ETA: 0s - loss: 0.0201
2128/6530 [========>.....................] - ETA: 1s - loss: 0.0729
6530/6530 [==============================] - 1s 94us/step - loss: 0.0089 - val_loss: 0.0080
Epoch 46/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0125
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0200
2448/6530 [==========>...................] - ETA: 1s - loss: 0.0694
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0198
 576/6530 [=>............................] - ETA: 0s - loss: 0.0095
2752/6530 [===========>..................] - ETA: 1s - loss: 0.0669
1136/6530 [====>.........................] - ETA: 0s - loss: 0.0097
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0198
3072/6530 [=============>................] - ETA: 1s - loss: 0.0653
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0098
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0196
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0632
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0097
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0196
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0619
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0095
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0194
4000/6530 [=================>............] - ETA: 0s - loss: 0.0609
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0094
6464/6530 [============================>.] - ETA: 0s - loss: 0.0195
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0595
6530/6530 [==============================] - 1s 142us/step - loss: 0.0194 - val_loss: 0.0155

3920/6530 [=================>............] - ETA: 0s - loss: 0.0093Epoch 30/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0325
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0579
 416/6530 [>.............................] - ETA: 0s - loss: 0.0222
4496/6530 [===================>..........] - ETA: 0s - loss: 0.0092
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0569
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0091
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0206
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0559
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0091
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0199
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0546
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0092
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0192
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0540
6530/6530 [==============================] - 1s 95us/step - loss: 0.0093 - val_loss: 0.0077
Epoch 47/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0060
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0194
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0537
 576/6530 [=>............................] - ETA: 0s - loss: 0.0104
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0192
6384/6530 [============================>.] - ETA: 0s - loss: 0.0531
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0095
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0193
1696/6530 [======>.......................] - ETA: 0s - loss: 0.0095
3104/6530 [=============>................] - ETA: 0s - loss: 0.0194
6530/6530 [==============================] - 2s 245us/step - loss: 0.0527 - val_loss: 0.1407
Epoch 2/81

  16/6530 [..............................] - ETA: 1s - loss: 0.1270
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0094
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0195
 320/6530 [>.............................] - ETA: 1s - loss: 0.0392
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0094
3872/6530 [================>.............] - ETA: 0s - loss: 0.0194
 640/6530 [=>............................] - ETA: 1s - loss: 0.0406
3328/6530 [==============>...............] - ETA: 0s - loss: 0.0093
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0194
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0404
3888/6530 [================>.............] - ETA: 0s - loss: 0.0091
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0193
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0383
4448/6530 [===================>..........] - ETA: 0s - loss: 0.0090
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0195
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0382
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0090
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0194
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0367
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0090
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0193
2160/6530 [========>.....................] - ETA: 0s - loss: 0.0359
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0091
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0357
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0194
6530/6530 [==============================] - 1s 95us/step - loss: 0.0091 - val_loss: 0.0077
Epoch 48/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0064
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0365
6530/6530 [==============================] - 1s 142us/step - loss: 0.0194 - val_loss: 0.0148
Epoch 31/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0176
 544/6530 [=>............................] - ETA: 0s - loss: 0.0087
3056/6530 [=============>................] - ETA: 0s - loss: 0.0366
 416/6530 [>.............................] - ETA: 0s - loss: 0.0185
1072/6530 [===>..........................] - ETA: 0s - loss: 0.0091
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0360
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0092
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0186
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0357
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0091
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0190
3920/6530 [=================>............] - ETA: 0s - loss: 0.0354
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0092
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0196
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0352
3264/6530 [=============>................] - ETA: 0s - loss: 0.0090
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0195
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0351
3808/6530 [================>.............] - ETA: 0s - loss: 0.0091
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0194
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0349
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0090
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0190
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0346
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0090
3072/6530 [=============>................] - ETA: 0s - loss: 0.0192
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0345
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0090
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0192
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0343
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0090
3776/6530 [================>.............] - ETA: 0s - loss: 0.0190
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0340
6480/6530 [============================>.] - ETA: 0s - loss: 0.0091
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0189
6288/6530 [===========================>..] - ETA: 0s - loss: 0.0338
6530/6530 [==============================] - 1s 99us/step - loss: 0.0091 - val_loss: 0.0081
Epoch 49/81

  16/6530 [..............................] - ETA: 0s - loss: 0.0069
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0190
 560/6530 [=>............................] - ETA: 0s - loss: 0.0092
6530/6530 [==============================] - 1s 176us/step - loss: 0.0338 - val_loss: 0.0738
Epoch 3/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0820
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0191
1104/6530 [====>.........................] - ETA: 0s - loss: 0.0095
 320/6530 [>.............................] - ETA: 1s - loss: 0.0322
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0192
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0092
 608/6530 [=>............................] - ETA: 1s - loss: 0.0340
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0190
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0091
 912/6530 [===>..........................] - ETA: 0s - loss: 0.0316
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0190
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0091
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0317
6432/6530 [============================>.] - ETA: 0s - loss: 0.0188
3264/6530 [=============>................] - ETA: 0s - loss: 0.0089
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0308
6530/6530 [==============================] - 1s 144us/step - loss: 0.0189 - val_loss: 0.0140
Epoch 32/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0385
3808/6530 [================>.............] - ETA: 0s - loss: 0.0091
1792/6530 [=======>......................] - ETA: 0s - loss: 0.0308
 416/6530 [>.............................] - ETA: 0s - loss: 0.0217
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0090
2096/6530 [========>.....................] - ETA: 0s - loss: 0.0307
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0197
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0089
2416/6530 [==========>...................] - ETA: 0s - loss: 0.0305
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0194
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0090
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0297
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0193
6000/6530 [==========================>...] - ETA: 0s - loss: 0.0090
3024/6530 [============>.................] - ETA: 0s - loss: 0.0294
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0195
6496/6530 [============================>.] - ETA: 0s - loss: 0.0091
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0295
6530/6530 [==============================] - 1s 98us/step - loss: 0.0091 - val_loss: 0.0077

2336/6530 [=========>....................] - ETA: 0s - loss: 0.0195
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0292
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0195
3920/6530 [=================>............] - ETA: 0s - loss: 0.0290
3104/6530 [=============>................] - ETA: 0s - loss: 0.0192
4208/6530 [==================>...........] - ETA: 0s - loss: 0.0289
# training | RMSE: 0.0780, MAE: 0.0603
worker 0  xfile  [0, 81, 0, 100, [], {'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38353587796938027}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23431882564222387}, 'layer_3_size': 36, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 60, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}]  predicted as label {'loss': 0.07803373420876421, 'rmse': 0.07803373420876421, 'mae': 0.06028837508959817, 'early_stop': True}
vggnet done  0

3488/6530 [===============>..............] - ETA: 0s - loss: 0.0189
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0285
3872/6530 [================>.............] - ETA: 0s - loss: 0.0187
4848/6530 [=====================>........] - ETA: 0s - loss: 0.0285
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0184
5168/6530 [======================>.......] - ETA: 0s - loss: 0.0283
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0186
5488/6530 [========================>.....] - ETA: 0s - loss: 0.0282
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0185
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0282
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0184
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0282
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0184
6432/6530 [============================>.] - ETA: 0s - loss: 0.0281
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0184
6530/6530 [==============================] - 1s 173us/step - loss: 0.0282 - val_loss: 0.0169
Epoch 4/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0325
 320/6530 [>.............................] - ETA: 1s - loss: 0.0270
6530/6530 [==============================] - 1s 140us/step - loss: 0.0184 - val_loss: 0.0134
Epoch 33/81

  32/6530 [..............................] - ETA: 1s - loss: 0.0192
 624/6530 [=>............................] - ETA: 0s - loss: 0.0280
 416/6530 [>.............................] - ETA: 0s - loss: 0.0196
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0264
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0201
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0274
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0195
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0273
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0190
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0269
2016/6530 [========>.....................] - ETA: 0s - loss: 0.0187
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0272
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0184
2544/6530 [==========>...................] - ETA: 0s - loss: 0.0263
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0184
2832/6530 [============>.................] - ETA: 0s - loss: 0.0260
3200/6530 [=============>................] - ETA: 0s - loss: 0.0190
3136/6530 [=============>................] - ETA: 0s - loss: 0.0261
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0188
3472/6530 [==============>...............] - ETA: 0s - loss: 0.0262
3968/6530 [=================>............] - ETA: 0s - loss: 0.0186
3792/6530 [================>.............] - ETA: 0s - loss: 0.0261
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0184
4112/6530 [=================>............] - ETA: 0s - loss: 0.0262
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0185
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0262
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0186
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0259
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0186
5072/6530 [======================>.......] - ETA: 0s - loss: 0.0258
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0185
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0256
6400/6530 [============================>.] - ETA: 0s - loss: 0.0185
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0256
6530/6530 [==============================] - 1s 136us/step - loss: 0.0185 - val_loss: 0.0139
Epoch 34/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0172
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0256
 416/6530 [>.............................] - ETA: 0s - loss: 0.0185
6336/6530 [============================>.] - ETA: 0s - loss: 0.0256
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0201
6530/6530 [==============================] - 1s 168us/step - loss: 0.0255 - val_loss: 0.0352
Epoch 5/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0460
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0196
 336/6530 [>.............................] - ETA: 1s - loss: 0.0252
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0197
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0256
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0193
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0236
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0187
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0225
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0189
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0232
3104/6530 [=============>................] - ETA: 0s - loss: 0.0188
1904/6530 [=======>......................] - ETA: 0s - loss: 0.0234
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0187
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0236
3904/6530 [================>.............] - ETA: 0s - loss: 0.0185
2576/6530 [==========>...................] - ETA: 0s - loss: 0.0231
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0185
2896/6530 [============>.................] - ETA: 0s - loss: 0.0232
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0184
3216/6530 [=============>................] - ETA: 0s - loss: 0.0236
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0185
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0234
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0184
3824/6530 [================>.............] - ETA: 0s - loss: 0.0231
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0185
4144/6530 [==================>...........] - ETA: 0s - loss: 0.0232
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0185
4464/6530 [===================>..........] - ETA: 0s - loss: 0.0235
6530/6530 [==============================] - 1s 138us/step - loss: 0.0187 - val_loss: 0.0129
Epoch 35/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0272
4784/6530 [====================>.........] - ETA: 0s - loss: 0.0233
 448/6530 [=>............................] - ETA: 0s - loss: 0.0193
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0233
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0179
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0236
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0178
5744/6530 [=========================>....] - ETA: 0s - loss: 0.0236
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0180
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0234
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0179
6352/6530 [============================>.] - ETA: 0s - loss: 0.0232
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0176
6530/6530 [==============================] - 1s 167us/step - loss: 0.0233 - val_loss: 0.0178
Epoch 6/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0233
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0174
 336/6530 [>.............................] - ETA: 0s - loss: 0.0239
3072/6530 [=============>................] - ETA: 0s - loss: 0.0173
 640/6530 [=>............................] - ETA: 0s - loss: 0.0223
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0175
 960/6530 [===>..........................] - ETA: 0s - loss: 0.0218
3840/6530 [================>.............] - ETA: 0s - loss: 0.0179
1264/6530 [====>.........................] - ETA: 0s - loss: 0.0221
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0182
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0220
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0180
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0222
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0182
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0219
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0182
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0219
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0182
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0219
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0182
3120/6530 [=============>................] - ETA: 0s - loss: 0.0220
6496/6530 [============================>.] - ETA: 0s - loss: 0.0181
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0219
6530/6530 [==============================] - 1s 142us/step - loss: 0.0182 - val_loss: 0.0133
Epoch 36/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0178
3744/6530 [================>.............] - ETA: 0s - loss: 0.0221
 416/6530 [>.............................] - ETA: 0s - loss: 0.0178
4064/6530 [=================>............] - ETA: 0s - loss: 0.0221
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0174
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0220
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0185
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0222
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0187
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0222
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0180
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0222
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0182
5520/6530 [========================>.....] - ETA: 0s - loss: 0.0220
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0183
5808/6530 [=========================>....] - ETA: 0s - loss: 0.0221
2912/6530 [============>.................] - ETA: 0s - loss: 0.0184
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0221
3232/6530 [=============>................] - ETA: 0s - loss: 0.0183
6384/6530 [============================>.] - ETA: 0s - loss: 0.0220
3616/6530 [===============>..............] - ETA: 0s - loss: 0.0182
6530/6530 [==============================] - 1s 175us/step - loss: 0.0220 - val_loss: 0.0220
Epoch 7/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0078
4000/6530 [=================>............] - ETA: 0s - loss: 0.0181
 336/6530 [>.............................] - ETA: 1s - loss: 0.0210
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0179
 640/6530 [=>............................] - ETA: 0s - loss: 0.0205
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0183
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0198
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0182
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0198
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0182
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0210
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0182
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0211
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0182
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0216
6530/6530 [==============================] - 1s 145us/step - loss: 0.0182 - val_loss: 0.0137
Epoch 37/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0219
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0215
 416/6530 [>.............................] - ETA: 0s - loss: 0.0168
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0215
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0173
3072/6530 [=============>................] - ETA: 0s - loss: 0.0215
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0171
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0214
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0171
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0216
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0172
3968/6530 [=================>............] - ETA: 0s - loss: 0.0218
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0168
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0218
2592/6530 [==========>...................] - ETA: 0s - loss: 0.0169
4528/6530 [===================>..........] - ETA: 0s - loss: 0.0217
2944/6530 [============>.................] - ETA: 0s - loss: 0.0171
4816/6530 [=====================>........] - ETA: 0s - loss: 0.0218
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0174
5104/6530 [======================>.......] - ETA: 0s - loss: 0.0218
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0174
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0219
4032/6530 [=================>............] - ETA: 0s - loss: 0.0176
5680/6530 [=========================>....] - ETA: 0s - loss: 0.0217
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0176
5968/6530 [==========================>...] - ETA: 0s - loss: 0.0214
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0179
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0212
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0180
6530/6530 [==============================] - 1s 179us/step - loss: 0.0213 - val_loss: 0.0154
Epoch 8/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0551
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0180
 304/6530 [>.............................] - ETA: 1s - loss: 0.0234
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0178
 592/6530 [=>............................] - ETA: 1s - loss: 0.0217
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0178
 880/6530 [===>..........................] - ETA: 1s - loss: 0.0207
6496/6530 [============================>.] - ETA: 0s - loss: 0.0178
6530/6530 [==============================] - 1s 149us/step - loss: 0.0179 - val_loss: 0.0140
Epoch 38/81

  32/6530 [..............................] - ETA: 1s - loss: 0.0156
1168/6530 [====>.........................] - ETA: 0s - loss: 0.0198
 384/6530 [>.............................] - ETA: 0s - loss: 0.0195
1440/6530 [=====>........................] - ETA: 0s - loss: 0.0205
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0191
1728/6530 [======>.......................] - ETA: 0s - loss: 0.0203
1088/6530 [===>..........................] - ETA: 0s - loss: 0.0187
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0200
1408/6530 [=====>........................] - ETA: 0s - loss: 0.0187
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0197
1760/6530 [=======>......................] - ETA: 0s - loss: 0.0185
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0200
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0186
2864/6530 [============>.................] - ETA: 0s - loss: 0.0201
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0188
3152/6530 [=============>................] - ETA: 0s - loss: 0.0202
2912/6530 [============>.................] - ETA: 0s - loss: 0.0186
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0203
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0190
3760/6530 [================>.............] - ETA: 0s - loss: 0.0202
3648/6530 [===============>..............] - ETA: 0s - loss: 0.0188
4048/6530 [=================>............] - ETA: 0s - loss: 0.0203
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0201
4000/6530 [=================>............] - ETA: 0s - loss: 0.0188
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0202
4352/6530 [==================>...........] - ETA: 0s - loss: 0.0187
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0200
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0185
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0200
5152/6530 [======================>.......] - ETA: 0s - loss: 0.0184
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0201
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0185
5872/6530 [=========================>....] - ETA: 0s - loss: 0.0202
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0183
6160/6530 [===========================>..] - ETA: 0s - loss: 0.0202
6304/6530 [===========================>..] - ETA: 0s - loss: 0.0183
6464/6530 [============================>.] - ETA: 0s - loss: 0.0203
6530/6530 [==============================] - 1s 146us/step - loss: 0.0182 - val_loss: 0.0124
Epoch 39/81

  32/6530 [..............................] - ETA: 1s - loss: 0.0161
6530/6530 [==============================] - 1s 181us/step - loss: 0.0203 - val_loss: 0.0163
Epoch 9/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0214
 384/6530 [>.............................] - ETA: 0s - loss: 0.0185
 304/6530 [>.............................] - ETA: 1s - loss: 0.0184
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0190
 624/6530 [=>............................] - ETA: 0s - loss: 0.0196
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0190
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0196
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0188
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0194
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0189
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0195
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0186
1840/6530 [=======>......................] - ETA: 0s - loss: 0.0197
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0188
2128/6530 [========>.....................] - ETA: 0s - loss: 0.0197
3008/6530 [============>.................] - ETA: 0s - loss: 0.0190
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0195
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0184
2736/6530 [===========>..................] - ETA: 0s - loss: 0.0197
3776/6530 [================>.............] - ETA: 0s - loss: 0.0183
3056/6530 [=============>................] - ETA: 0s - loss: 0.0194
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0185
3376/6530 [==============>...............] - ETA: 0s - loss: 0.0191
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0184
3696/6530 [===============>..............] - ETA: 0s - loss: 0.0193
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0185
4000/6530 [=================>............] - ETA: 0s - loss: 0.0193
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0183
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0192
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0182
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0193
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0183
4944/6530 [=====================>........] - ETA: 0s - loss: 0.0192
6464/6530 [============================>.] - ETA: 0s - loss: 0.0181
5264/6530 [=======================>......] - ETA: 0s - loss: 0.0191
6530/6530 [==============================] - 1s 141us/step - loss: 0.0181 - val_loss: 0.0128
Epoch 40/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0100
5584/6530 [========================>.....] - ETA: 0s - loss: 0.0191
 448/6530 [=>............................] - ETA: 0s - loss: 0.0185
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0192
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0179
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0193
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0181
6528/6530 [============================>.] - ETA: 0s - loss: 0.0192
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0172
6530/6530 [==============================] - 1s 170us/step - loss: 0.0192 - val_loss: 0.0136
Epoch 10/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0133
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0173
 336/6530 [>.............................] - ETA: 0s - loss: 0.0209
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0174
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0192
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0173
 976/6530 [===>..........................] - ETA: 0s - loss: 0.0188
3136/6530 [=============>................] - ETA: 0s - loss: 0.0172
1296/6530 [====>.........................] - ETA: 0s - loss: 0.0197
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0169
1584/6530 [======>.......................] - ETA: 0s - loss: 0.0188
3872/6530 [================>.............] - ETA: 0s - loss: 0.0169
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0190
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0168
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0188
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0168
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0188
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0168
2848/6530 [============>.................] - ETA: 0s - loss: 0.0187
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0168
3168/6530 [=============>................] - ETA: 0s - loss: 0.0186
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0170
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0187
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0172
3792/6530 [================>.............] - ETA: 0s - loss: 0.0186
6528/6530 [============================>.] - ETA: 0s - loss: 0.0172
4112/6530 [=================>............] - ETA: 0s - loss: 0.0186
6530/6530 [==============================] - 1s 141us/step - loss: 0.0172 - val_loss: 0.0150
Epoch 41/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0140
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0186
 416/6530 [>.............................] - ETA: 0s - loss: 0.0173
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0186
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0173
5040/6530 [======================>.......] - ETA: 0s - loss: 0.0186
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0170
5360/6530 [=======================>......] - ETA: 0s - loss: 0.0186
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0171
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0185
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0171
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0184
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0173
6272/6530 [===========================>..] - ETA: 0s - loss: 0.0186
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0173
6530/6530 [==============================] - 1s 170us/step - loss: 0.0186 - val_loss: 0.0143
Epoch 11/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0330
3136/6530 [=============>................] - ETA: 0s - loss: 0.0170
 320/6530 [>.............................] - ETA: 1s - loss: 0.0201
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0174
 624/6530 [=>............................] - ETA: 0s - loss: 0.0201
3904/6530 [================>.............] - ETA: 0s - loss: 0.0175
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0176
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0174
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0172
4704/6530 [====================>.........] - ETA: 0s - loss: 0.0174
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0174
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0174
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0173
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0174
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0176
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0172
2464/6530 [==========>...................] - ETA: 0s - loss: 0.0178
6240/6530 [===========================>..] - ETA: 0s - loss: 0.0172
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0179
6530/6530 [==============================] - 1s 140us/step - loss: 0.0172 - val_loss: 0.0121
Epoch 42/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0161
3072/6530 [=============>................] - ETA: 0s - loss: 0.0178
 416/6530 [>.............................] - ETA: 0s - loss: 0.0176
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0179
 832/6530 [==>...........................] - ETA: 0s - loss: 0.0177
3680/6530 [===============>..............] - ETA: 0s - loss: 0.0180
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0176
3984/6530 [=================>............] - ETA: 0s - loss: 0.0181
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0176
4304/6530 [==================>...........] - ETA: 0s - loss: 0.0180
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0173
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0179
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0172
4928/6530 [=====================>........] - ETA: 0s - loss: 0.0179
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0173
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0178
3136/6530 [=============>................] - ETA: 0s - loss: 0.0176
5552/6530 [========================>.....] - ETA: 0s - loss: 0.0177
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0177
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0177
3968/6530 [=================>............] - ETA: 0s - loss: 0.0179
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0178
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0178
6528/6530 [============================>.] - ETA: 0s - loss: 0.0178
6530/6530 [==============================] - 1s 170us/step - loss: 0.0178 - val_loss: 0.0137
Epoch 12/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0125
4800/6530 [=====================>........] - ETA: 0s - loss: 0.0178
 352/6530 [>.............................] - ETA: 0s - loss: 0.0190
5216/6530 [======================>.......] - ETA: 0s - loss: 0.0177
 688/6530 [==>...........................] - ETA: 0s - loss: 0.0184
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0178
1008/6530 [===>..........................] - ETA: 0s - loss: 0.0196
5984/6530 [==========================>...] - ETA: 0s - loss: 0.0178
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0188
6400/6530 [============================>.] - ETA: 0s - loss: 0.0177
1648/6530 [======>.......................] - ETA: 0s - loss: 0.0188
6530/6530 [==============================] - 1s 135us/step - loss: 0.0176 - val_loss: 0.0121
Epoch 43/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0167
1936/6530 [=======>......................] - ETA: 0s - loss: 0.0186
 384/6530 [>.............................] - ETA: 0s - loss: 0.0180
2224/6530 [=========>....................] - ETA: 0s - loss: 0.0185
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0168
2528/6530 [==========>...................] - ETA: 0s - loss: 0.0185
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0167
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0185
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0170
3136/6530 [=============>................] - ETA: 0s - loss: 0.0189
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0172
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0191
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0173
3728/6530 [================>.............] - ETA: 0s - loss: 0.0189
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0171
4048/6530 [=================>............] - ETA: 0s - loss: 0.0186
3104/6530 [=============>................] - ETA: 0s - loss: 0.0172
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0188
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0171
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0187
3872/6530 [================>.............] - ETA: 0s - loss: 0.0172
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0186
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0169
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0184
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0168
5616/6530 [========================>.....] - ETA: 0s - loss: 0.0183
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0170
5904/6530 [==========================>...] - ETA: 0s - loss: 0.0184
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0170
6192/6530 [===========================>..] - ETA: 0s - loss: 0.0183
5792/6530 [=========================>....] - ETA: 0s - loss: 0.0169
6496/6530 [============================>.] - ETA: 0s - loss: 0.0183
6176/6530 [===========================>..] - ETA: 0s - loss: 0.0169
6530/6530 [==============================] - 1s 170us/step - loss: 0.0183 - val_loss: 0.0162
Epoch 13/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0231
 336/6530 [>.............................] - ETA: 0s - loss: 0.0181
6530/6530 [==============================] - 1s 139us/step - loss: 0.0168 - val_loss: 0.0130
Epoch 44/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0260
 624/6530 [=>............................] - ETA: 1s - loss: 0.0172
 384/6530 [>.............................] - ETA: 0s - loss: 0.0227
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0177
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0214
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0180
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0211
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0175
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0201
1856/6530 [=======>......................] - ETA: 0s - loss: 0.0174
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0198
2144/6530 [========>.....................] - ETA: 0s - loss: 0.0176
2240/6530 [=========>....................] - ETA: 0s - loss: 0.0194
2448/6530 [==========>...................] - ETA: 0s - loss: 0.0176
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0189
2768/6530 [===========>..................] - ETA: 0s - loss: 0.0177
3008/6530 [============>.................] - ETA: 0s - loss: 0.0187
3104/6530 [=============>................] - ETA: 0s - loss: 0.0175
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0184
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0176
3776/6530 [================>.............] - ETA: 0s - loss: 0.0188
3744/6530 [================>.............] - ETA: 0s - loss: 0.0176
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0186
4064/6530 [=================>............] - ETA: 0s - loss: 0.0175
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0184
4368/6530 [===================>..........] - ETA: 0s - loss: 0.0174
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0182
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0174
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0183
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0176
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0181
5248/6530 [=======================>......] - ETA: 0s - loss: 0.0176
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0183
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0177
6464/6530 [============================>.] - ETA: 0s - loss: 0.0181
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0178
6530/6530 [==============================] - 1s 142us/step - loss: 0.0181 - val_loss: 0.0123
Epoch 45/81

  32/6530 [..............................] - ETA: 1s - loss: 0.0100
6096/6530 [===========================>..] - ETA: 0s - loss: 0.0177
 416/6530 [>.............................] - ETA: 0s - loss: 0.0202
6368/6530 [============================>.] - ETA: 0s - loss: 0.0176
 768/6530 [==>...........................] - ETA: 0s - loss: 0.0203
6530/6530 [==============================] - 1s 175us/step - loss: 0.0175 - val_loss: 0.0117
Epoch 14/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0059
1120/6530 [====>.........................] - ETA: 0s - loss: 0.0192
 336/6530 [>.............................] - ETA: 1s - loss: 0.0155
1504/6530 [=====>........................] - ETA: 0s - loss: 0.0197
 656/6530 [==>...........................] - ETA: 0s - loss: 0.0161
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0188
 992/6530 [===>..........................] - ETA: 0s - loss: 0.0159
2272/6530 [=========>....................] - ETA: 0s - loss: 0.0185
1328/6530 [=====>........................] - ETA: 0s - loss: 0.0167
2656/6530 [===========>..................] - ETA: 0s - loss: 0.0182
1632/6530 [======>.......................] - ETA: 0s - loss: 0.0169
3040/6530 [============>.................] - ETA: 0s - loss: 0.0178
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0171
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0174
2256/6530 [=========>....................] - ETA: 0s - loss: 0.0176
3808/6530 [================>.............] - ETA: 0s - loss: 0.0173
2560/6530 [==========>...................] - ETA: 0s - loss: 0.0172
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0172
2880/6530 [============>.................] - ETA: 0s - loss: 0.0169
4576/6530 [====================>.........] - ETA: 0s - loss: 0.0172
3184/6530 [=============>................] - ETA: 0s - loss: 0.0169
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0170
3504/6530 [===============>..............] - ETA: 0s - loss: 0.0168
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0170
3808/6530 [================>.............] - ETA: 0s - loss: 0.0168
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0169
4128/6530 [=================>............] - ETA: 0s - loss: 0.0169
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0169
4432/6530 [===================>..........] - ETA: 0s - loss: 0.0167
6496/6530 [============================>.] - ETA: 0s - loss: 0.0169
6530/6530 [==============================] - 1s 141us/step - loss: 0.0169 - val_loss: 0.0118
Epoch 46/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0126
4752/6530 [====================>.........] - ETA: 0s - loss: 0.0167
 416/6530 [>.............................] - ETA: 0s - loss: 0.0182
5088/6530 [======================>.......] - ETA: 0s - loss: 0.0167
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0171
5392/6530 [=======================>......] - ETA: 0s - loss: 0.0166
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0169
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0168
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0170
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0168
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0168
6336/6530 [============================>.] - ETA: 0s - loss: 0.0169
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0170
6530/6530 [==============================] - 1s 168us/step - loss: 0.0169 - val_loss: 0.0207
Epoch 15/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0202
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0176
 320/6530 [>.............................] - ETA: 1s - loss: 0.0183
3104/6530 [=============>................] - ETA: 0s - loss: 0.0174
 608/6530 [=>............................] - ETA: 1s - loss: 0.0167
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0175
 928/6530 [===>..........................] - ETA: 0s - loss: 0.0179
3904/6530 [================>.............] - ETA: 0s - loss: 0.0171
1232/6530 [====>.........................] - ETA: 0s - loss: 0.0174
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0166
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0175
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0171
1824/6530 [=======>......................] - ETA: 0s - loss: 0.0173
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0169
2112/6530 [========>.....................] - ETA: 0s - loss: 0.0177
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0169
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0174
5664/6530 [=========================>....] - ETA: 0s - loss: 0.0169
2704/6530 [===========>..................] - ETA: 0s - loss: 0.0177
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0171
3008/6530 [============>.................] - ETA: 0s - loss: 0.0175
6400/6530 [============================>.] - ETA: 0s - loss: 0.0171
3312/6530 [==============>...............] - ETA: 0s - loss: 0.0174
6530/6530 [==============================] - 1s 142us/step - loss: 0.0172 - val_loss: 0.0121
Epoch 47/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0177
3632/6530 [===============>..............] - ETA: 0s - loss: 0.0171
 416/6530 [>.............................] - ETA: 0s - loss: 0.0202
3920/6530 [=================>............] - ETA: 0s - loss: 0.0172
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0195
4240/6530 [==================>...........] - ETA: 0s - loss: 0.0172
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0184
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0173
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0180
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0173
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0179
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0171
2336/6530 [=========>....................] - ETA: 0s - loss: 0.0178
5504/6530 [========================>.....] - ETA: 0s - loss: 0.0169
2720/6530 [===========>..................] - ETA: 0s - loss: 0.0173
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0168
3104/6530 [=============>................] - ETA: 0s - loss: 0.0174
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0167
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0173
6448/6530 [============================>.] - ETA: 0s - loss: 0.0168
3904/6530 [================>.............] - ETA: 0s - loss: 0.0176
6530/6530 [==============================] - 1s 171us/step - loss: 0.0168 - val_loss: 0.0134
Epoch 16/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0381
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0177
 320/6530 [>.............................] - ETA: 1s - loss: 0.0198
4672/6530 [====================>.........] - ETA: 0s - loss: 0.0175
 624/6530 [=>............................] - ETA: 0s - loss: 0.0188
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0175
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0176
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0173
5440/6530 [=======================>......] - ETA: 0s - loss: 0.0175
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0177
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0174
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0173
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0176
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0172
6530/6530 [==============================] - 1s 139us/step - loss: 0.0175 - val_loss: 0.0122
Epoch 48/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0100
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0172
 416/6530 [>.............................] - ETA: 0s - loss: 0.0202
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0173
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0190
3120/6530 [=============>................] - ETA: 0s - loss: 0.0171
1152/6530 [====>.........................] - ETA: 0s - loss: 0.0188
3424/6530 [==============>...............] - ETA: 0s - loss: 0.0168
1536/6530 [======>.......................] - ETA: 0s - loss: 0.0186
3744/6530 [================>.............] - ETA: 0s - loss: 0.0170
1920/6530 [=======>......................] - ETA: 0s - loss: 0.0188
4064/6530 [=================>............] - ETA: 0s - loss: 0.0170
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0187
4384/6530 [===================>..........] - ETA: 0s - loss: 0.0169
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0185
4688/6530 [====================>.........] - ETA: 0s - loss: 0.0168
3072/6530 [=============>................] - ETA: 0s - loss: 0.0190
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0168
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0188
5296/6530 [=======================>......] - ETA: 0s - loss: 0.0167
3808/6530 [================>.............] - ETA: 0s - loss: 0.0184
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0167
4160/6530 [==================>...........] - ETA: 0s - loss: 0.0181
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0168
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0180
6128/6530 [===========================>..] - ETA: 0s - loss: 0.0168
4896/6530 [=====================>........] - ETA: 0s - loss: 0.0180
6432/6530 [============================>.] - ETA: 0s - loss: 0.0167
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0180
6530/6530 [==============================] - 1s 173us/step - loss: 0.0168 - val_loss: 0.0165
Epoch 17/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0189
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0179
 320/6530 [>.............................] - ETA: 1s - loss: 0.0159
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0177
 624/6530 [=>............................] - ETA: 1s - loss: 0.0165
6400/6530 [============================>.] - ETA: 0s - loss: 0.0175
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0171
6530/6530 [==============================] - 1s 142us/step - loss: 0.0176 - val_loss: 0.0115
Epoch 49/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0088
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0167
 416/6530 [>.............................] - ETA: 0s - loss: 0.0164
1552/6530 [======>.......................] - ETA: 0s - loss: 0.0175
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0174
1872/6530 [=======>......................] - ETA: 0s - loss: 0.0170
1216/6530 [====>.........................] - ETA: 0s - loss: 0.0163
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0168
1600/6530 [======>.......................] - ETA: 0s - loss: 0.0163
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0170
1984/6530 [========>.....................] - ETA: 0s - loss: 0.0163
2800/6530 [===========>..................] - ETA: 0s - loss: 0.0168
2368/6530 [=========>....................] - ETA: 0s - loss: 0.0166
3104/6530 [=============>................] - ETA: 0s - loss: 0.0168
2752/6530 [===========>..................] - ETA: 0s - loss: 0.0164
3408/6530 [==============>...............] - ETA: 0s - loss: 0.0167
3136/6530 [=============>................] - ETA: 0s - loss: 0.0165
3712/6530 [================>.............] - ETA: 0s - loss: 0.0168
3520/6530 [===============>..............] - ETA: 0s - loss: 0.0165
4032/6530 [=================>............] - ETA: 0s - loss: 0.0168
3904/6530 [================>.............] - ETA: 0s - loss: 0.0163
4320/6530 [==================>...........] - ETA: 0s - loss: 0.0169
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0162
4592/6530 [====================>.........] - ETA: 0s - loss: 0.0168
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0164
4864/6530 [=====================>........] - ETA: 0s - loss: 0.0166
4960/6530 [=====================>........] - ETA: 0s - loss: 0.0166
5136/6530 [======================>.......] - ETA: 0s - loss: 0.0167
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0166
5424/6530 [=======================>......] - ETA: 0s - loss: 0.0167
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0165
5712/6530 [=========================>....] - ETA: 0s - loss: 0.0167
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0164
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0167
6400/6530 [============================>.] - ETA: 0s - loss: 0.0164
6320/6530 [============================>.] - ETA: 0s - loss: 0.0168
6530/6530 [==============================] - 1s 143us/step - loss: 0.0164 - val_loss: 0.0117
Epoch 50/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0175
6530/6530 [==============================] - 1s 176us/step - loss: 0.0167 - val_loss: 0.0184
Epoch 18/81

  16/6530 [..............................] - ETA: 1s - loss: 0.0221
 416/6530 [>.............................] - ETA: 0s - loss: 0.0184
 336/6530 [>.............................] - ETA: 1s - loss: 0.0163
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0165
 624/6530 [=>............................] - ETA: 1s - loss: 0.0166
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0171
 944/6530 [===>..........................] - ETA: 0s - loss: 0.0167
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0171
1248/6530 [====>.........................] - ETA: 0s - loss: 0.0168
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0171
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0167
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0169
1888/6530 [=======>......................] - ETA: 0s - loss: 0.0167
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0168
2192/6530 [=========>....................] - ETA: 0s - loss: 0.0165
3072/6530 [=============>................] - ETA: 0s - loss: 0.0167
2512/6530 [==========>...................] - ETA: 0s - loss: 0.0168
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0170
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0169
3840/6530 [================>.............] - ETA: 0s - loss: 0.0172
3136/6530 [=============>................] - ETA: 0s - loss: 0.0167
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0173
3440/6530 [==============>...............] - ETA: 0s - loss: 0.0166
4608/6530 [====================>.........] - ETA: 0s - loss: 0.0174
3744/6530 [================>.............] - ETA: 0s - loss: 0.0165
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0174
4048/6530 [=================>............] - ETA: 0s - loss: 0.0165
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0174
4336/6530 [==================>...........] - ETA: 0s - loss: 0.0163
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0174
4624/6530 [====================>.........] - ETA: 0s - loss: 0.0162
6048/6530 [==========================>...] - ETA: 0s - loss: 0.0173
4912/6530 [=====================>........] - ETA: 0s - loss: 0.0161
6432/6530 [============================>.] - ETA: 0s - loss: 0.0170
5232/6530 [=======================>......] - ETA: 0s - loss: 0.0160
6530/6530 [==============================] - 1s 143us/step - loss: 0.0170 - val_loss: 0.0112
Epoch 51/81

  32/6530 [..............................] - ETA: 1s - loss: 0.0126
5536/6530 [========================>.....] - ETA: 0s - loss: 0.0161
 416/6530 [>.............................] - ETA: 0s - loss: 0.0194
5840/6530 [=========================>....] - ETA: 0s - loss: 0.0162
 800/6530 [==>...........................] - ETA: 0s - loss: 0.0173
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0161
1184/6530 [====>.........................] - ETA: 0s - loss: 0.0168
6432/6530 [============================>.] - ETA: 0s - loss: 0.0159
1568/6530 [======>.......................] - ETA: 0s - loss: 0.0171
6530/6530 [==============================] - 1s 173us/step - loss: 0.0159 - val_loss: 0.0151

1952/6530 [=======>......................] - ETA: 0s - loss: 0.0169
2304/6530 [=========>....................] - ETA: 0s - loss: 0.0175
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0178
3072/6530 [=============>................] - ETA: 0s - loss: 0.0174
3456/6530 [==============>...............] - ETA: 0s - loss: 0.0170
# training | RMSE: 0.1145, MAE: 0.0905
worker 1  xfile  [4, 81, 0, 100, [], {'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3437183352808336}, 'layer_2_size': 64, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10362876348865911}, 'layer_3_size': 12, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 79, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}]  predicted as label {'loss': 0.11450257799230738, 'rmse': 0.11450257799230738, 'mae': 0.09054407819936257, 'early_stop': True}
vggnet done  1

3808/6530 [================>.............] - ETA: 0s - loss: 0.0168
4224/6530 [==================>...........] - ETA: 0s - loss: 0.0165
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0165
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0165
5408/6530 [=======================>......] - ETA: 0s - loss: 0.0164
5728/6530 [=========================>....] - ETA: 0s - loss: 0.0164
6080/6530 [==========================>...] - ETA: 0s - loss: 0.0165
6464/6530 [============================>.] - ETA: 0s - loss: 0.0167
6530/6530 [==============================] - 1s 142us/step - loss: 0.0167 - val_loss: 0.0116
Epoch 52/81

  32/6530 [..............................] - ETA: 0s - loss: 0.0092
 384/6530 [>.............................] - ETA: 0s - loss: 0.0165
 736/6530 [==>...........................] - ETA: 0s - loss: 0.0181
1056/6530 [===>..........................] - ETA: 0s - loss: 0.0182
1376/6530 [=====>........................] - ETA: 0s - loss: 0.0182
1664/6530 [======>.......................] - ETA: 0s - loss: 0.0182
1952/6530 [=======>......................] - ETA: 0s - loss: 0.0187
2208/6530 [=========>....................] - ETA: 0s - loss: 0.0187
2400/6530 [==========>...................] - ETA: 0s - loss: 0.0188
2624/6530 [===========>..................] - ETA: 0s - loss: 0.0182
2912/6530 [============>.................] - ETA: 0s - loss: 0.0184
3200/6530 [=============>................] - ETA: 0s - loss: 0.0180
3488/6530 [===============>..............] - ETA: 0s - loss: 0.0180
3744/6530 [================>.............] - ETA: 0s - loss: 0.0179
3968/6530 [=================>............] - ETA: 0s - loss: 0.0176
4192/6530 [==================>...........] - ETA: 0s - loss: 0.0175
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0172
4640/6530 [====================>.........] - ETA: 0s - loss: 0.0171
4832/6530 [=====================>........] - ETA: 0s - loss: 0.0172
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0171
5280/6530 [=======================>......] - ETA: 0s - loss: 0.0171
5472/6530 [========================>.....] - ETA: 0s - loss: 0.0172
5696/6530 [=========================>....] - ETA: 0s - loss: 0.0172
5920/6530 [==========================>...] - ETA: 0s - loss: 0.0172
6112/6530 [===========================>..] - ETA: 0s - loss: 0.0171
6336/6530 [============================>.] - ETA: 0s - loss: 0.0172
6528/6530 [============================>.] - ETA: 0s - loss: 0.0172
6530/6530 [==============================] - 1s 223us/step - loss: 0.0172 - val_loss: 0.0113
Epoch 53/81

  32/6530 [..............................] - ETA: 2s - loss: 0.0129
 224/6530 [>.............................] - ETA: 1s - loss: 0.0140
 416/6530 [>.............................] - ETA: 1s - loss: 0.0178
 608/6530 [=>............................] - ETA: 1s - loss: 0.0168
 800/6530 [==>...........................] - ETA: 1s - loss: 0.0159
1024/6530 [===>..........................] - ETA: 1s - loss: 0.0163
1216/6530 [====>.........................] - ETA: 1s - loss: 0.0164
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0164
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0169
1792/6530 [=======>......................] - ETA: 1s - loss: 0.0170
2016/6530 [========>.....................] - ETA: 1s - loss: 0.0168
2240/6530 [=========>....................] - ETA: 1s - loss: 0.0170
2432/6530 [==========>...................] - ETA: 1s - loss: 0.0173
2624/6530 [===========>..................] - ETA: 1s - loss: 0.0172
2816/6530 [===========>..................] - ETA: 0s - loss: 0.0170
3008/6530 [============>.................] - ETA: 0s - loss: 0.0169
3200/6530 [=============>................] - ETA: 0s - loss: 0.0167
3392/6530 [==============>...............] - ETA: 0s - loss: 0.0166
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0167
3776/6530 [================>.............] - ETA: 0s - loss: 0.0166
4000/6530 [=================>............] - ETA: 0s - loss: 0.0169
4288/6530 [==================>...........] - ETA: 0s - loss: 0.0168
4544/6530 [===================>..........] - ETA: 0s - loss: 0.0169
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0170
4992/6530 [=====================>........] - ETA: 0s - loss: 0.0169
5184/6530 [======================>.......] - ETA: 0s - loss: 0.0168
5376/6530 [=======================>......] - ETA: 0s - loss: 0.0168
5600/6530 [========================>.....] - ETA: 0s - loss: 0.0167
5824/6530 [=========================>....] - ETA: 0s - loss: 0.0169
6016/6530 [==========================>...] - ETA: 0s - loss: 0.0168
6208/6530 [===========================>..] - ETA: 0s - loss: 0.0167
6432/6530 [============================>.] - ETA: 0s - loss: 0.0168
6530/6530 [==============================] - 2s 266us/step - loss: 0.0168 - val_loss: 0.0115
Epoch 54/81

  32/6530 [..............................] - ETA: 1s - loss: 0.0108
 256/6530 [>.............................] - ETA: 1s - loss: 0.0131
 448/6530 [=>............................] - ETA: 1s - loss: 0.0138
 672/6530 [==>...........................] - ETA: 1s - loss: 0.0160
 896/6530 [===>..........................] - ETA: 1s - loss: 0.0167
1120/6530 [====>.........................] - ETA: 1s - loss: 0.0167
1344/6530 [=====>........................] - ETA: 1s - loss: 0.0166
1600/6530 [======>.......................] - ETA: 1s - loss: 0.0168
1824/6530 [=======>......................] - ETA: 1s - loss: 0.0167
2016/6530 [========>.....................] - ETA: 1s - loss: 0.0164
2272/6530 [=========>....................] - ETA: 1s - loss: 0.0167
2496/6530 [==========>...................] - ETA: 0s - loss: 0.0167
2784/6530 [===========>..................] - ETA: 0s - loss: 0.0167
3040/6530 [============>.................] - ETA: 0s - loss: 0.0166
3296/6530 [==============>...............] - ETA: 0s - loss: 0.0166
3552/6530 [===============>..............] - ETA: 0s - loss: 0.0165
3840/6530 [================>.............] - ETA: 0s - loss: 0.0164
4128/6530 [=================>............] - ETA: 0s - loss: 0.0164
4416/6530 [===================>..........] - ETA: 0s - loss: 0.0163
4736/6530 [====================>.........] - ETA: 0s - loss: 0.0163
5024/6530 [======================>.......] - ETA: 0s - loss: 0.0165
5344/6530 [=======================>......] - ETA: 0s - loss: 0.0165
5632/6530 [========================>.....] - ETA: 0s - loss: 0.0165
5888/6530 [==========================>...] - ETA: 0s - loss: 0.0166
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0164
6400/6530 [============================>.] - ETA: 0s - loss: 0.0164
6530/6530 [==============================] - 1s 216us/step - loss: 0.0164 - val_loss: 0.0116
Epoch 55/81

  32/6530 [..............................] - ETA: 1s - loss: 0.0184
 288/6530 [>.............................] - ETA: 1s - loss: 0.0175
 512/6530 [=>............................] - ETA: 1s - loss: 0.0171
 736/6530 [==>...........................] - ETA: 1s - loss: 0.0162
 960/6530 [===>..........................] - ETA: 1s - loss: 0.0170
1184/6530 [====>.........................] - ETA: 1s - loss: 0.0166
1408/6530 [=====>........................] - ETA: 1s - loss: 0.0159
1664/6530 [======>.......................] - ETA: 1s - loss: 0.0157
1920/6530 [=======>......................] - ETA: 1s - loss: 0.0158
2176/6530 [========>.....................] - ETA: 0s - loss: 0.0160
2432/6530 [==========>...................] - ETA: 0s - loss: 0.0160
2688/6530 [===========>..................] - ETA: 0s - loss: 0.0163
2912/6530 [============>.................] - ETA: 0s - loss: 0.0162
3136/6530 [=============>................] - ETA: 0s - loss: 0.0162
3360/6530 [==============>...............] - ETA: 0s - loss: 0.0161
3584/6530 [===============>..............] - ETA: 0s - loss: 0.0160
3808/6530 [================>.............] - ETA: 0s - loss: 0.0160
4032/6530 [=================>............] - ETA: 0s - loss: 0.0160
4256/6530 [==================>...........] - ETA: 0s - loss: 0.0158
4512/6530 [===================>..........] - ETA: 0s - loss: 0.0157
4768/6530 [====================>.........] - ETA: 0s - loss: 0.0158
5056/6530 [======================>.......] - ETA: 0s - loss: 0.0160
5312/6530 [=======================>......] - ETA: 0s - loss: 0.0159
5568/6530 [========================>.....] - ETA: 0s - loss: 0.0161
5856/6530 [=========================>....] - ETA: 0s - loss: 0.0162
6144/6530 [===========================>..] - ETA: 0s - loss: 0.0163
6368/6530 [============================>.] - ETA: 0s - loss: 0.0163
6530/6530 [==============================] - 1s 225us/step - loss: 0.0161 - val_loss: 0.0121

# training | RMSE: 0.1010, MAE: 0.0787
worker 2  xfile  [2, 81, 0, 100, [], {'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 22, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3708849838902447}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4028864677505175}, 'layer_5_size': 38, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}]  predicted as label {'loss': 0.10101290038949735, 'rmse': 0.10101290038949735, 'mae': 0.07873859950022044, 'early_stop': True}
vggnet done  2
all of workers have been done
calculation finished
#1 epoch=81 loss={'loss': 0.08947785953090155, 'rmse': 0.08947785953090155, 'mae': 0.06826165350469851, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 64, 'init': 'he_normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.13111359748493934}, 'layer_1_size': 84, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, 'layer_2_size': 20, 'layer_3_activation': 'tanh', 'layer_3_extras': {'name': None}, 'layer_3_size': 19, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': 'dropout', 'rate': 0.31119805265189826}, 'layer_4_size': 5, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.18939250426965906}, 'layer_5_size': 37, 'loss': 'mean_absolute_error', 'n_layers': 3, 'optimizer': 'adamax', 'scaler': 'RobustScaler', 'shuffle': True}
#3 epoch=81 loss={'loss': 0.2029150402128102, 'rmse': 0.2029150402128102, 'mae': 0.16096343566942442, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'uniform', 'layer_1_activation': 'sigmoid', 'layer_1_extras': {'name': None}, 'layer_1_size': 83, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.2291344165235558}, 'layer_2_size': 23, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.2184897927910865}, 'layer_3_size': 69, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'dropout', 'rate': 0.27286622920865944}, 'layer_4_size': 46, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 94, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'MaxAbsScaler', 'shuffle': True}
#0 epoch=81 loss={'loss': 0.07803373420876421, 'rmse': 0.07803373420876421, 'mae': 0.06028837508959817, 'early_stop': True} #th_GPU=0 time=<built-in function time> configuration={'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'dropout', 'rate': 0.38353587796938027}, 'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': None}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.23431882564222387}, 'layer_3_size': 36, 'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 60, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 2, 'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}
#4 epoch=81 loss={'loss': 0.11450257799230738, 'rmse': 0.11450257799230738, 'mae': 0.09054407819936257, 'early_stop': True} #th_GPU=1 time=<built-in function time> configuration={'batch_size': 16, 'init': 'glorot_uniform', 'layer_1_activation': 'relu', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 32, 'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3437183352808336}, 'layer_2_size': 64, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'dropout', 'rate': 0.10362876348865911}, 'layer_3_size': 12, 'layer_4_activation': 'sigmoid', 'layer_4_extras': {'name': None}, 'layer_4_size': 79, 'layer_5_activation': 'relu', 'layer_5_extras': {'name': None}, 'layer_5_size': 27, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'rmsprop', 'scaler': 'RobustScaler', 'shuffle': True}
#2 epoch=81 loss={'loss': 0.10101290038949735, 'rmse': 0.10101290038949735, 'mae': 0.07873859950022044, 'early_stop': True} #th_GPU=2 time=<built-in function time> configuration={'batch_size': 32, 'init': 'normal', 'layer_1_activation': 'tanh', 'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 22, 'layer_2_activation': 'sigmoid', 'layer_2_extras': {'name': 'dropout', 'rate': 0.3708849838902447}, 'layer_2_size': 85, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': 'batchnorm'}, 'layer_3_size': 21, 'layer_4_activation': 'tanh', 'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 71, 'layer_5_activation': 'sigmoid', 'layer_5_extras': {'name': 'dropout', 'rate': 0.4028864677505175}, 'layer_5_size': 38, 'loss': 'mean_squared_error', 'n_layers': 5, 'optimizer': 'adam', 'scaler': 'MaxAbsScaler', 'shuffle': True}
get a list [results] of length 206
get a list [loss] of length 5
get a list [val_loss] of length 5
length of indices is (0, 1, 2, 4, 3)
length of indices is 5
length of T is 5
206 total, best:

loss: 7.32%  | #81.0th iterations | run 0 
("{'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', "
 "'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, "
 "'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, "
 "'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': "
 "{'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'sigmoid', "
 "'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 2, "
 "'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, "
 "'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 3, "
 "'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}")

loss: 7.80%  | #81th iterations | run 0 
("{'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', "
 "'layer_1_extras': {'name': 'dropout', 'rate': 0.38353587796938027}, "
 "'layer_1_size': 84, 'layer_2_activation': 'relu', 'layer_2_extras': {'name': "
 "None}, 'layer_2_size': 80, 'layer_3_activation': 'relu', 'layer_3_extras': "
 "{'name': 'dropout', 'rate': 0.23431882564222387}, 'layer_3_size': 36, "
 "'layer_4_activation': 'relu', 'layer_4_extras': {'name': 'batchnorm'}, "
 "'layer_4_size': 60, 'layer_5_activation': 'tanh', 'layer_5_extras': {'name': "
 "None}, 'layer_5_size': 65, 'loss': 'mean_squared_error', 'n_layers': 2, "
 "'optimizer': 'adamax', 'scaler': 'StandardScaler', 'shuffle': False}")

loss: 8.09%  | #27.0th iterations | run 0 
("{'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', "
 "'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, "
 "'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, "
 "'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': "
 "None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': "
 "{'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', "
 "'layer_5_extras': {'name': 'dropout', 'rate': 0.44587267964897137}, "
 "'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 2, "
 "'optimizer': 'adam', 'scaler': None, 'shuffle': False}")

loss: 8.41%  | #27.0th iterations | run 0 
("{'batch_size': 256, 'init': 'glorot_normal', 'layer_1_activation': 'relu', "
 "'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 64, "
 "'layer_2_activation': 'tanh', 'layer_2_extras': {'name': 'batchnorm'}, "
 "'layer_2_size': 13, 'layer_3_activation': 'sigmoid', 'layer_3_extras': "
 "{'name': None}, 'layer_3_size': 25, 'layer_4_activation': 'sigmoid', "
 "'layer_4_extras': {'name': 'batchnorm'}, 'layer_4_size': 2, "
 "'layer_5_activation': 'tanh', 'layer_5_extras': {'name': 'batchnorm'}, "
 "'layer_5_size': 68, 'loss': 'mean_squared_error', 'n_layers': 3, "
 "'optimizer': 'adagrad', 'scaler': None, 'shuffle': False}")

loss: 8.49%  | #9.0th iterations | run 0 
("{'batch_size': 16, 'init': 'normal', 'layer_1_activation': 'relu', "
 "'layer_1_extras': {'name': 'batchnorm'}, 'layer_1_size': 33, "
 "'layer_2_activation': 'relu', 'layer_2_extras': {'name': 'batchnorm'}, "
 "'layer_2_size': 34, 'layer_3_activation': 'relu', 'layer_3_extras': {'name': "
 "None}, 'layer_3_size': 66, 'layer_4_activation': 'tanh', 'layer_4_extras': "
 "{'name': None}, 'layer_4_size': 17, 'layer_5_activation': 'tanh', "
 "'layer_5_extras': {'name': 'dropout', 'rate': 0.44587267964897137}, "
 "'layer_5_size': 97, 'loss': 'mean_squared_error', 'n_layers': 2, "
 "'optimizer': 'adam', 'scaler': None, 'shuffle': False}")

saving...

481 seconds.
